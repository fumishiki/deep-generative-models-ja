---
title: "ç¬¬7å›: æœ€å°¤æ¨å®šã¨çµ±è¨ˆçš„æ¨è«–: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸ—ºï¸"
type: "tech"
topics: ["machinelearning", "deeplearning", "statistics", "python"]
published: true
---

# ç¬¬7å›: æœ€å°¤æ¨å®šã¨çµ±è¨ˆçš„æ¨è«– â€” æ¨å®šé‡ã®æ•°å­¦ãŒæ‹“ãç¢ºç‡ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®ä¸–ç•Œ

> **æ¨å®šé‡ã®è¨­è¨ˆã¯æ•°å­¦ã®è¨­è¨ˆã ã€‚MLE ã®100å¹´ãŒã€ç¢ºç‡ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®å…¨ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’ç”Ÿã‚“ã ã€‚6è¬›ç¾©ã®æ•°å­¦æ­¦è£…ãŒã€ã“ã“ã‹ã‚‰ç‰™ã‚’å‰¥ãã€‚**

ç¬¬6å›ã§æƒ…å ±ç†è«–ã¨æœ€é©åŒ–ã®æ­¦å™¨ã‚’æ‰‹ã«ã—ãŸã€‚Cross-Entropy æœ€å°åŒ–ãŒ KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®æœ€å°åŒ–ã¨ç­‰ä¾¡ã§ã‚ã‚‹ã“ã¨ã€‚Adam ãŒ SGD ã‚’é©å¿œçš„ã«æ”¹è‰¯ã—ãŸã“ã¨ã€‚ã“ã‚Œã‚‰ã¯å…¨ã¦ã€ã‚ã‚‹ç›®çš„ã®ãŸã‚ã®é“å…·ã ã£ãŸ â€” **ãƒ‡ãƒ¼ã‚¿ã®ç¢ºç‡åˆ†å¸ƒ $p(x)$ ã‚’ãƒ¢ãƒ‡ãƒ« $q_\theta(x)$ ã§è¿‘ä¼¼ã™ã‚‹**ã¨ã„ã†ç›®çš„ã®ãŸã‚ã®ã€‚

æœ¬è¬›ç¾©ã§ã¯ã€ã„ã‚ˆã„ã‚ˆãã®ç›®çš„ã«æ­£é¢ã‹ã‚‰å‘ãåˆã†ã€‚æœ€å°¤æ¨å®šï¼ˆMLEï¼‰ã®æ•°å­¦çš„æ§‹é€ ã‚’å®Œå…¨ã«è§£å‰–ã—ã€MLE ãŒ Cross-Entropy æœ€å°åŒ–ãƒ»KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹æœ€å°åŒ–ã¨ç­‰ä¾¡ã§ã‚ã‚‹ã“ã¨ã‚’è¨¼æ˜ã—ã€ã“ã®æ¨å®šåŸç†ã®å¤‰å½¢ã¨ã—ã¦ VAEãƒ»GANãƒ»Flowãƒ»Diffusion ãŒã©ã†ä½ç½®ã¥ã‘ã‚‰ã‚Œã‚‹ã‹ã®åœ°å›³ã‚’æãã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2025-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚
:::

```mermaid
graph LR
    A["ğŸ—ºï¸ æ¡ä»¶ä»˜ã vs å‘¨è¾ºå°¤åº¦<br/>MLEã®2å¯¾è±¡"] --> B["ğŸ“ æœ€å°¤æ¨å®š MLE<br/>CE = KL ç­‰ä¾¡æ€§"]
    B --> C["ğŸ”€ æ¨å®šé‡ã®3å¤‰å½¢<br/>å¤‰æ•°å¤‰æ›ãƒ»æš—é»™çš„ãƒ»ã‚¹ã‚³ã‚¢"]
    C --> D["ğŸ“Š çµ±è¨ˆçš„è·é›¢<br/>FIDãƒ»KIDãƒ»CMMD"]
    D --> E["ğŸ¯ MLEâ†’EMâ†’å¤‰åˆ†æ¨è«–<br/>ç¬¬8å›ã¸ã®æ¥ç¶š"]
    style A fill:#e1f5fe
    style E fill:#c8e6c9
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 6 | æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” 30è¡Œã§MLEã®é™ç•Œã‚’ä½“æ„Ÿã™ã‚‹

```python
import numpy as np
np.random.seed(42)

# True distribution: mixture of 2 Gaussians
def sample_true(n):
    """p(x): unknown distribution we want to model"""
    mix = np.random.rand(n) < 0.4
    return np.where(mix, np.random.normal(-2, 0.5, n),
                         np.random.normal(3, 1.0, n))

# Model: single Gaussian q_Î¸(x) = N(x; Î¼, ÏƒÂ²)
def log_likelihood(data, mu, sigma):
    """log q_Î¸(x) = -Â½((x-Î¼)/Ïƒ)Â² - log(Ïƒâˆš(2Ï€))"""
    return -0.5 * ((data - mu) / sigma) ** 2 - np.log(sigma * np.sqrt(2 * np.pi))

# Maximum Likelihood Estimation (MLE)
data = sample_true(1000)
mu_hat = np.mean(data)               # MLE for Î¼
sigma_hat = np.std(data, ddof=0)     # MLE for Ïƒ

print(f"MLE result: Î¼Ì‚ = {mu_hat:.3f}, ÏƒÌ‚ = {sigma_hat:.3f}")
print(f"Average log-likelihood: {np.mean(log_likelihood(data, mu_hat, sigma_hat)):.4f}")
print(f"True data: bimodal (-2, 0.5) and (3, 1.0)")
print(f"â†’ Single Gaussian CANNOT capture bimodality. This is MLE's limit.")
```

**å‡ºåŠ›ä¾‹:**
```
MLE result: Î¼Ì‚ = 1.035, ÏƒÌ‚ = 2.481
Average log-likelihood: -2.2847
True data: bimodal (-2, 0.5) and (3, 1.0)
â†’ Single Gaussian CANNOT capture bimodality. This is MLE's limit.
```

ãŸã£ãŸ30è¡Œã§ã€å¯†åº¦æ¨å®šã®æœ¬è³ªçš„èª²é¡ŒãŒè¦‹ãˆã‚‹ã€‚ãƒ‡ãƒ¼ã‚¿ã®çœŸã®åˆ†å¸ƒ $p(x)$ ã¯è¤‡é›‘ï¼ˆåŒå³°æ€§ï¼‰ãªã®ã«ã€ãƒ¢ãƒ‡ãƒ« $q_\theta(x)$ ãŒå˜ç´”ã™ãã‚‹ã¨ MLE ã¯ã€Œæœ€å–„ã®å¦¥å”ç‚¹ã€ã«è½ã¡ç€ãã€‚ã“ã®å¦¥å”ç‚¹ã¯æ•°å­¦çš„ã«ã¯æœ€é©ã ãŒã€ç›´æ„Ÿçš„ã«ã¯å…¨ãä¸ååˆ†ã ã€‚

> **æ ¸å¿ƒ**: MLE ã¯ã€Œãƒ¢ãƒ‡ãƒ«æ—ã®ä¸­ã§ã®æœ€è‰¯ã€ã‚’è¦‹ã¤ã‘ã‚‹ã€‚ãƒ¢ãƒ‡ãƒ«æ—ãŒè²§å¼±ãªã‚‰ã€çµæœã‚‚è²§å¼±ã€‚ã ã‹ã‚‰ã“ãã€è¡¨ç¾åŠ›ã®é«˜ã„æ¨å®šé‡ï¼ˆãƒ¢ãƒ‡ãƒ« + æ¨å®šæ‰‹æ³•ã®çµ„ï¼‰ãŒå¿…è¦ã«ãªã‚‹ â€” VAE ã® ELBO æœ€å¤§åŒ–ã€GAN ã®æ•µå¯¾çš„è¨“ç·´ã€Flow ã®å¤‰æ•°å¤‰æ›å°¤åº¦ã€Diffusion ã®ã‚¹ã‚³ã‚¢æ¨å®šã¯ã€å…¨ã¦ã“ã®å•é¡Œã¸ã®å›ç­”ã ã€‚

:::message
**é€²æ—: 3% å®Œäº†** â€” MLE ã®é™ç•Œã‚’30ç§’ã§ä½“æ„Ÿã—ãŸã€‚ã“ã“ã‹ã‚‰æ¨å®šé‡è¨­è¨ˆã®å…¨ä½“åƒã«è¸ã¿è¾¼ã‚€ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” æ¡ä»¶ä»˜ãå°¤åº¦ vs å‘¨è¾ºå°¤åº¦ã€MLEã®2å¯¾è±¡

### 1.1 æ¡ä»¶ä»˜ãå°¤åº¦ vs å‘¨è¾ºå°¤åº¦ â€” 2ã¤ã®MLE

ã¾ãšæ ¹æœ¬çš„ãªé•ã„ã‚’æ˜ç¢ºã«ã—ã‚ˆã†ã€‚

```python
import numpy as np

# === Discriminative model: learns p(y|x) ===
# Given features x, predict label y
# Example: logistic regression
def discriminative_predict(x, w, b):
    """p(y=1|x) = sigmoid(wÂ·x + b)"""
    logit = np.dot(w, x) + b
    return 1.0 / (1.0 + np.exp(-logit))

# === Generative model: learns p(x) ===
# Model the data distribution itself
# Example: Gaussian mixture model
def generative_sample(mu1, sigma1, mu2, sigma2, pi, n):
    """Sample from p(x) = Ï€Â·N(Î¼â‚,Ïƒâ‚Â²) + (1-Ï€)Â·N(Î¼â‚‚,Ïƒâ‚‚Â²)"""
    mix = np.random.rand(n) < pi
    return np.where(mix, np.random.normal(mu1, sigma1, n),
                         np.random.normal(mu2, sigma2, n))

# Discriminative: "Is this a cat or dog?" â†’ boundary
# Generative: "What does a cat look like?" â†’ distribution
print("Discriminative: p(y|x) â€” decision boundary")
print("Generative:     p(x)   â€” data distribution")
print("Generative+:    p(x,y) = p(x|y)p(y) â€” joint â†’ can do BOTH")
```

| ç‰¹æ€§ | æ¡ä»¶ä»˜ãå°¤åº¦ $p(y \mid x;\theta)$ | å‘¨è¾ºå°¤åº¦ $p(x;\theta)$ |
|:-----|:---------------------|:-------------------|
| **MLEå¯¾è±¡** | æ¡ä»¶ä»˜ãåˆ†å¸ƒï¼ˆåˆ¤åˆ¥ãƒ¢ãƒ‡ãƒ«ï¼‰ | ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒãã®ã‚‚ã®ï¼ˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ï¼‰ |
| **æ¨å®šã®ç›®çš„** | åˆ†é¡ãƒ»å›å¸° | ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆãƒ»å¯†åº¦æ¨å®šãƒ»ç•°å¸¸æ¤œçŸ¥ |
| **å¿…è¦ãªä»®å®š** | æ±ºå®šå¢ƒç•Œã®å½¢çŠ¶ã®ã¿ | ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆéç¨‹å…¨ä½“ |
| **å…¸å‹çš„æ¨å®šé‡** | ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°, SVM, NN | GMM, VAE, GAN, Diffusion |
| **LLM ã¨ã®é–¢ä¿‚** | BERTï¼ˆåŒæ–¹å‘åˆ†é¡å™¨ï¼‰ | GPTï¼ˆè‡ªå·±å›å¸°ç”Ÿæˆï¼‰ |
| **æ¨å®šã®é›£æ˜“åº¦** | ä½ï¼ˆå¢ƒç•Œã ã‘å­¦ã¹ã°ã„ã„ï¼‰ | é«˜ï¼ˆåˆ†å¸ƒå…¨ä½“ã‚’å­¦ã¶å¿…è¦ï¼‰ |
| **æ¬¡å…ƒã®å½±éŸ¿** | æ¯”è¼ƒçš„è»½ã„ | **æ¬¡å…ƒã®å‘ªã„**ãŒç›´æ’ƒ |

### 1.2 MLEå¿œç”¨ã®ç³»è­œ â€” æ¨å®šé‡ã®è¨­è¨ˆã¨ã—ã¦é³¥ç°

```mermaid
graph TD
    G[MLE ã®å¤‰å½¢<br>å°¤åº¦é–¢æ•°ã®æ‰±ã„æ–¹] --> L[æ˜ç¤ºçš„å°¤åº¦<br>Prescribed]
    G --> I[æš—é»™çš„å°¤åº¦<br>Implicit]
    G --> S[ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹<br>å¯†åº¦ä¸è¦]

    L --> VAE[VAE<br>Kingma 2013]
    L --> Flow[Normalizing Flow<br>Rezende 2015]
    L --> AR[è‡ªå·±å›å¸°<br>GPTç³»]

    I --> GAN[GAN<br>Goodfellow 2014]

    S --> SM[Score Matching<br>Song 2019]
    S --> Diff[Diffusion<br>Ho 2020]

    VAE -.->|ELBOæœ€å¤§åŒ–| LB[å¤‰åˆ†ä¸‹ç•Œæ¨å®š]
    Flow -.->|å¤‰æ•°å¤‰æ›| LB2[æ­£ç¢ºãªå°¤åº¦è¨ˆç®—]
    GAN -.->|æ•µå¯¾çš„è¨“ç·´| LB3[æš—é»™çš„æ¨å®šé‡]
    Diff -.->|denoising| LB4[ã‚¹ã‚³ã‚¢æ¨å®šé‡]

    style VAE fill:#e8f5e9
    style GAN fill:#fff3e0
    style Flow fill:#e3f2fd
    style Diff fill:#fce4ec
```

```python
# 4 paradigms in 4 lines of pseudocode
paradigms = {
    "VAE":       "maximize E[log p(x|z)] - KL[q(z|x) || p(z)]",
    "GAN":       "min_G max_D E[log D(x)] + E[log(1-D(G(z)))]",
    "Flow":      "maximize log p(z) + log |det(df/dz)|",
    "Diffusion": "minimize E[||Îµ - Îµ_Î¸(x_t, t)||Â²]",
}

for name, obj in paradigms.items():
    print(f"{name:10s}: {obj}")
```

**å‡ºåŠ›:**
```
VAE       : maximize E[log p(x|z)] - KL[q(z|x) || p(z)]
GAN       : min_G max_D E[log D(x)] + E[log(1-D(G(z)))]
Flow      : maximize log p(z) + log |det(df/dz)|
Diffusion : minimize E[||Îµ - Îµ_Î¸(x_t, t)||Â²]
```

4è¡Œã®ç›®çš„é–¢æ•°ã¯ã€å…¨ã¦ã€Œæ¨å®šæ‰‹æ³•ã®è¨­è¨ˆã€ã®å¤‰å½¢ã ã€‚VAE/GAN/Flow/Diffusion ã¯ãƒ¢ãƒ‡ãƒ«ï¼ˆç¢ºç‡åˆ†å¸ƒã®æ—ï¼‰ã§ã‚ã‚Šã€ELBO æœ€å¤§åŒ–/æ•µå¯¾çš„è¨“ç·´/å¤‰æ•°å¤‰æ›å°¤åº¦/ã‚¹ã‚³ã‚¢æ¨å®šãŒãã‚Œãã‚Œã®æ¨å®šæ‰‹æ³•ã€‚å°¤åº¦é–¢æ•°ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ–¹æ³•ãŒç•°ãªã‚‹ã ã‘ã§ã€æ ¹åº•ã«ã‚ã‚‹åŸç†ã¯ MLE ã«ã‚ã‚‹ã€‚ã“ã‚Œã‚’ã€Œãªãœã“ã®å½¢ã«ãªã‚‹ã®ã‹ã€ã¾ã§ç†è§£ã™ã‚‹ã®ãŒã€ç¬¬8å›ä»¥é™ã®æ—…ã ã€‚

### 1.3 MLEå¿œç”¨ã®ç³»è­œ â€” æ¨å®šé‡è¨­è¨ˆã®ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³

```mermaid
graph LR
    A0[Fisher MLE<br>1922] --> A[RBM<br>2006]
    A --> B[VAE<br>ELBOæ¨å®š 2013]
    B --> C[GAN<br>æš—é»™çš„æ¨å®š 2014]
    C --> D[Flow<br>å¤‰æ•°å¤‰æ›å°¤åº¦ 2014-15]
    D --> E[Diffusion<br>ãƒã‚¤ã‚ºæ¨å®š 2015]
    E --> F[Score Matching<br>ã‚¹ã‚³ã‚¢æ¨å®š 2019]
    F --> G[DDPM<br>2020]
    G --> H[è‡ªå·±å›å¸°MLE<br>GPT-4 2023]

    style A0 fill:#fff9c4
    style B fill:#e8f5e9
    style C fill:#fff3e0
    style D fill:#e3f2fd
    style G fill:#fce4ec
```

### 1.4 PyTorch/JAX ã¨ã®å¯¾å¿œ â€” `loss.backward()` = $\nabla_\theta L$

:::details PyTorch/JAX ã§å„æ¨å®šé‡ã®æå¤±é–¢æ•°ã‚’æ›¸ãã¨...

```python
import torch
import torch.nn.functional as F

# === 1. VAE Loss ===
def vae_loss(x, x_recon, mu, logvar):
    """ELBO = Reconstruction + KL"""
    recon = F.binary_cross_entropy(x_recon, x, reduction='sum')
    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return recon + kl

# === 2. GAN Loss (vanilla) ===
def gan_loss_d(d_real, d_fake):
    """D maximizes: E[log D(x)] + E[log(1-D(G(z)))]"""
    return -(torch.log(d_real).mean() + torch.log(1 - d_fake).mean())

def gan_loss_g(d_fake):
    """G minimizes: -E[log D(G(z))]"""
    return -torch.log(d_fake).mean()

# === 3. Flow Loss ===
def flow_loss(z, log_det_jacobian):
    """Exact log-likelihood via change of variables"""
    log_pz = -0.5 * (z ** 2).sum(dim=1)  # Standard normal prior
    return -(log_pz + log_det_jacobian).mean()

# === 4. Diffusion Loss (simplified DDPM) ===
def diffusion_loss(noise, noise_pred):
    """Simple denoising objective"""
    return F.mse_loss(noise_pred, noise)

print("All 4 losses: pure PyTorch, < 5 lines each")
print("Key pattern: loss.backward(); optimizer.step() = Î¸ â† Î¸ - Î·âˆ‡_Î¸L")
```

```python
# JAX equivalent: functional gradient computation
import jax
import jax.numpy as jnp

def mle_loss(theta, x):
    """Negative log-likelihood for Gaussian: MLE loss"""
    mu, log_sigma = theta
    sigma = jnp.exp(log_sigma)
    return -jnp.mean(-0.5 * ((x - mu) / sigma)**2 - log_sigma)

# jax.grad computes âˆ‡_Î¸ L analytically
grad_fn = jax.grad(mle_loss)
theta = (jnp.array(0.0), jnp.array(0.0))  # (Î¼, log Ïƒ)
x = jnp.array([1.0, 2.0, 3.0])
grads = grad_fn(theta, x)
print(f"JAX: âˆ‡_Î¸ L = {grads}")
print(f"â†’ jax.grad(loss)(theta) = âˆ‡_Î¸ L â€” same math, functional style")
```
:::

:::message
**é€²æ—: 10% å®Œäº†** â€” MLE ã®æ¨å®šé‡ã¨ã—ã¦ã®4å¤‰å½¢ã‚’æ¦‚è¦³ã—ãŸã€‚ã“ã‚Œã‹ã‚‰ã€Œãªãœå¯†åº¦æ¨å®šãŒé›£ã—ã„ã®ã‹ã€ã®ç›´æ„Ÿã‚’æ´ã‚€ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœå¯†åº¦æ¨å®šã¯é›£ã—ã„ã®ã‹

### 2.1 æœ¬ã‚·ãƒªãƒ¼ã‚ºã«ãŠã‘ã‚‹ä½ç½®ã¥ã‘

| å› | ãƒ†ãƒ¼ãƒ | ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ | æœ¬è¬›ç¾©ã¨ã®é–¢ä¿‚ |
|:---|:-------|:-----------|:--------------|
| ç¬¬1å› | Python ç’°å¢ƒæ§‹ç¯‰ | NumPy, Matplotlib | å®Ÿè£…åŸºç›¤ |
| ç¬¬2å› | ç·šå½¢ä»£æ•° | è¡Œåˆ—, å›ºæœ‰å€¤ | æ½œåœ¨ç©ºé–“ã®å¹¾ä½•å­¦ |
| ç¬¬3å› | å¾®åˆ†ç©åˆ† | å‹¾é…, ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ | Flow ã®å¤‰æ•°å¤‰æ› |
| ç¬¬4å› | ç¢ºç‡çµ±è¨ˆ | ãƒ™ã‚¤ã‚º, æ¡ä»¶ä»˜ã | ç¢ºç‡ãƒ¢ãƒ‡ãƒ«ã®è¨€èª |
| ç¬¬5å› | æ¸¬åº¦è«– | Lebesgue, Radon-Nikodym | å¯†åº¦æ¯”æ¨å®šã®åŸºç›¤ |
| ç¬¬6å› | æƒ…å ±ç†è«–ãƒ»æœ€é©åŒ– | KL, Cross-Entropy, Adam | **æå¤±é–¢æ•°ã®è¨­è¨ˆåŸç†** |
| **ç¬¬7å›** | **æœ€å°¤æ¨å®šã¨çµ±è¨ˆçš„æ¨è«–** | **MLE, æ¨å®šé‡, çµ±è¨ˆçš„è·é›¢** | **â†’ æœ¬è¬›ç¾©** |
| ç¬¬8å› | æ½œåœ¨å¤‰æ•° & EM | ELBO, E-step, M-step | VAE ã¸ã®æ©‹æ¸¡ã— |

```mermaid
graph TD
    subgraph "Course I: æ•°å­¦åŸºç›¤ (ç¬¬1-8å›)"
        L1[ç¬¬1å›: Python] --> L2[ç¬¬2å›: ç·šå½¢ä»£æ•°]
        L2 --> L3[ç¬¬3å›: å¾®åˆ†ç©åˆ†]
        L3 --> L4[ç¬¬4å›: ç¢ºç‡çµ±è¨ˆ]
        L4 --> L5[ç¬¬5å›: æ¸¬åº¦è«–]
        L5 --> L6[ç¬¬6å›: æƒ…å ±ç†è«–ãƒ»æœ€é©åŒ–]
        L6 --> L7[ç¬¬7å›: æœ€å°¤æ¨å®šã¨çµ±è¨ˆçš„æ¨è«–]
        L7 --> L8[ç¬¬8å›: æ½œåœ¨å¤‰æ•°ãƒ»EM]
    end

    subgraph "Course II: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«åŸºç¤ (ç¬¬9-16å›)"
        L8 --> L9[ç¬¬9å›: VAE]
        L9 --> L12[ç¬¬12å›: GAN]
        L12 --> L15[ç¬¬15å›: Flow]
        L15 --> L16[ç¬¬16å›: Transformer]
    end

    L7 -.->|æ¨å®šé‡ã®å¤‰å½¢| L9
    L7 -.->|æš—é»™çš„æ¨å®š| L12
    L7 -.->|çµ±è¨ˆçš„è·é›¢| L15

    style L7 fill:#ff9800,color:#fff
```

### 2.2 æ¾å°¾ãƒ»å²©æ¾¤ç ”ã¨ã®æ¯”è¼ƒ

| è¦³ç‚¹ | æ¾å°¾ãƒ»å²©æ¾¤ç ” | æœ¬ã‚·ãƒªãƒ¼ã‚º |
|:-----|:-------------|:-----------|
| æ•°å­¦åŸºç›¤ | ã€Œå‰æçŸ¥è­˜ã€ã¨ã—ã¦çœç•¥ | 6è¬›ç¾©ã‹ã‘ã¦å¾¹åº•æ§‹ç¯‰ |
| MLE ã®å°å…¥ | ã„ããªã‚Š VAE | MLE ã®æ•°å­¦ â†’ æ¨å®šé‡ã®åˆ†é¡ â†’ æ½œåœ¨å¤‰æ•° â†’ VAE |
| MLE ã®æ‰±ã„ | æ•°è¡Œã®èª¬æ˜ | å®Œå…¨å°å‡º + CE/KLç­‰ä¾¡æ€§è¨¼æ˜ + æ¼¸è¿‘è«– |
| çµ±è¨ˆçš„è·é›¢ | FID ã®ç´¹ä»‹ | FID/KID/CMMD + æ•°å­¦çš„å®šç¾©ã¨é™ç•Œåˆ†æ |
| æ¨å®šé‡ã®åˆ†é¡ä½“ç³» | VAEâ†’GANâ†’Flowâ†’æ‹¡æ•£ ã®é †åºç´¹ä»‹ | æ˜ç¤ºçš„ vs æš—é»™çš„æ¨å®šé‡ + æ•°å­¦çš„åˆ†é¡ |
| Python ã®é€Ÿã•å•é¡Œ | è¨€åŠãªã— | MLE åå¾©è¨ˆç®—ã§ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚° |

### 2.3 3ã¤ã®ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼ â€” æ¨å®šé‡è¨­è¨ˆã®é›£ã—ã•

**ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼ 1: åœ°å›³ã¨é ˜åœŸ**

æ¡ä»¶ä»˜ãæ¨å®šï¼ˆ$p(y|x)$ï¼‰ã¯ã€Œé“è·¯ã®åˆ†å²ç‚¹ã€ã‚’å­¦ã¶ã€‚ã€Œå³ã«è¡Œã‘ã°æ±äº¬ã€å·¦ã«è¡Œã‘ã°å¤§é˜ªã€â€” åˆ†é¡ã¯åˆ†å²ç‚¹ã•ãˆåˆ†ã‹ã‚Œã°ã„ã„ã€‚ä¸€æ–¹ã€å¯†åº¦æ¨å®šï¼ˆ$p(x)$ï¼‰ã¯ã€Œæ—¥æœ¬å…¨åœŸã®è©³ç´°ãªåœ°å›³ã€ã‚’ä½œã‚‹ã€‚å±±ãŒã©ã“ã«ã‚ã‚Šã€å·ãŒã©ã†æµã‚Œã€è¡—ãŒã©ã†é…ç½®ã•ã‚Œã¦ã„ã‚‹ã‹ â€” å…¨ã¦ã‚’çŸ¥ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚ã©ã¡ã‚‰ãŒé›£ã—ã„ã‹ã¯æ˜ç™½ã ã€‚

**ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼ 2: è©¦é¨“ã®æ¡ç‚¹è€… vs è©¦é¨“å•é¡Œã®ä½œæˆè€…**

æ¡ä»¶ä»˜ãæ¨å®šã¯ã€Œç­”æ¡ˆã‚’è¦‹ã¦æ­£èª¤ã‚’åˆ¤å®šã™ã‚‹æ¡ç‚¹è€…ã€ã€‚ç­”ãˆã®å¢ƒç•Œã‚’çŸ¥ã£ã¦ã„ã‚Œã°ã„ã„ã€‚å¯†åº¦æ¨å®šã¯ã€Œè‰¯å•ã‚’ä½œæˆã™ã‚‹å‡ºé¡Œè€…ã€ã€‚ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ ã‚’æ·±ãç†è§£ã—ã€ãã®æ§‹é€ ã‹ã‚‰è‡ªç„¶ãªå•é¡Œã‚’ç”Ÿã¿å‡ºã™å¿…è¦ãŒã‚ã‚‹ã€‚æ¡ç‚¹ã‚ˆã‚Šå‡ºé¡ŒãŒé¥ã‹ã«é›£ã—ã„ã®ã¯ã€æ•™è‚²ã«æºã‚ã‚‹äººé–“ãªã‚‰èª°ã§ã‚‚çŸ¥ã£ã¦ã„ã‚‹ã€‚

**ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼ 3: çµ±è¨ˆåŠ›å­¦ã®ã‚¢ãƒŠãƒ­ã‚¸ãƒ¼**

åˆ†å¸ƒ $p(x)$ ã‚’å­¦ã¶ã“ã¨ã¯ã€ç‰©ç†å­¦ã§è¨€ãˆã°ã€Œç³»ã®åˆ†é…é–¢æ•° $Z$ ã‚’è¨ˆç®—ã™ã‚‹ã€ã“ã¨ã«å¯¾å¿œã™ã‚‹ã€‚åˆ†é…é–¢æ•°ã¯ç³»ã®å…¨ã‚¨ãƒãƒ«ã‚®ãƒ¼æº–ä½ã®å’Œ $Z = \sum_i e^{-E_i / k_B T}$ ã§ã‚ã‚Šã€é«˜æ¬¡å…ƒã§ã¯è¨ˆç®—ä¸èƒ½ã«ãªã‚‹ã€‚ã“ã‚ŒãŒå¯†åº¦æ¨å®šã®æ ¹æœ¬çš„é›£ã—ã•ã®ç‰©ç†å­¦çš„ãªå¯¾å¿œç‰©ã ã€‚Sohl-Dickstein+ (2015) [^13] ãŒ Diffusion Model ã‚’éå¹³è¡¡ç†±åŠ›å­¦ã‹ã‚‰ç€æƒ³ã—ãŸã®ã¯å¶ç„¶ã§ã¯ãªã„ã€‚

### 2.4 æ¬¡å…ƒã®å‘ªã„ â€” ãªãœé«˜æ¬¡å…ƒã¯ç›´æ„Ÿã‚’è£åˆ‡ã‚‹ã‹

å¯†åº¦æ¨å®šãŒé›£ã—ã„æ ¹æœ¬åŸå› ã¯**æ¬¡å…ƒã®å‘ªã„**ï¼ˆcurse of dimensionalityï¼‰ã ã€‚

```python
import numpy as np

# Demonstration: volume of unit hypersphere shrinks in high dimensions
def hypersphere_volume(d, r=1.0):
    """Volume of d-dimensional unit sphere"""
    if d == 0:
        return 1.0
    return (np.pi ** (d / 2) / np.math.gamma(d / 2 + 1)) * r ** d

def hypercube_volume(d, side=2.0):
    """Volume of d-dimensional hypercube [-1,1]^d"""
    return side ** d

print(f"{'Dim':>4} {'Sphere Vol':>12} {'Cube Vol':>12} {'Ratio':>10}")
print("-" * 42)
for d in [1, 2, 3, 5, 10, 20, 50, 100]:
    sv = hypersphere_volume(d)
    cv = hypercube_volume(d)
    ratio = sv / cv
    print(f"{d:4d} {sv:12.4e} {cv:12.4e} {ratio:10.4e}")
```

**å‡ºåŠ›:**
```
 Dim   Sphere Vol     Cube Vol      Ratio
------------------------------------------
   1   2.0000e+00   2.0000e+00 1.0000e+00
   2   3.1416e+00   4.0000e+00 7.8540e-01
   3   4.1888e+00   8.0000e+00 5.2360e-01
   5   5.2638e+00   3.2000e+01 1.6449e-01
  10   2.5502e+00   1.0240e+03 2.4902e-03
  20   2.5807e-01   1.0486e+06 2.4613e-07
  50   2.3684e-07   1.1259e+15 2.1036e-22
 100   2.3685e-24   1.2677e+30 1.8685e-54
```

100æ¬¡å…ƒç©ºé–“ã§ã¯ã€è¶…çƒã®ä½“ç©ã¯è¶…ç«‹æ–¹ä½“ã® $10^{-54}$ å€ã—ã‹ãªã„ã€‚ãƒ‡ãƒ¼ã‚¿ã¯é«˜æ¬¡å…ƒç©ºé–“ã®ã€Œæ®»ã€ï¼ˆshellï¼‰ã«é›†ä¸­ã—ã€å†…éƒ¨ã¯ã»ã¼ç©ºè™šã ã€‚å¯†åº¦æ¨å®šãŒç ´æ»…çš„ã«é›£ã—ããªã‚‹ç†ç”±ãŒã“ã“ã«ã‚ã‚‹ã€‚

### 2.5 å¤šæ§˜ä½“ä»®èª¬ â€” æ•‘ã„ã®å…‰

å¹¸ã„ã€è‡ªç„¶ãƒ‡ãƒ¼ã‚¿ã¯é«˜æ¬¡å…ƒç©ºé–“ã®å…¨ä½“ã«å‡ä¸€ã«ã¯åˆ†å¸ƒã—ãªã„ã€‚

> **å¤šæ§˜ä½“ä»®èª¬**: é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ $x \in \mathbb{R}^D$ ã¯ã€ä½æ¬¡å…ƒå¤šæ§˜ä½“ $\mathcal{M} \subset \mathbb{R}^D$ï¼ˆ$\dim \mathcal{M} = d \ll D$ï¼‰ä¸Šã¾ãŸã¯ãã®è¿‘å‚ã«é›†ä¸­ã—ã¦ã„ã‚‹ã€‚

ä¾‹ãˆã° $64 \times 64$ ã®é¡”ç”»åƒã¯ $D = 64 \times 64 \times 3 = 12{,}288$ æ¬¡å…ƒç©ºé–“ã«ä½ã‚“ã§ã„ã‚‹ãŒã€ã€Œé¡”ã‚‰ã—ã„ã€ç”»åƒã¯ã”ãä½æ¬¡å…ƒã®å¤šæ§˜ä½“ã®ä¸Šã«ã‚ã‚‹ã€‚ã“ã®å¤šæ§˜ä½“ä¸Šã®å¯†åº¦ã‚’æ¨å®šã™ã‚‹ã“ã¨ãŒã€é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®æœ¬è³ªã ã€‚

```python
# Intuition: 12,288 dimensional space, but faces live on ~100D manifold
D = 64 * 64 * 3  # pixel space
d = 100           # estimated intrinsic dimension
random_pixel = np.random.rand(D)  # random point in pixel space

print(f"Pixel space dimension: {D}")
print(f"Estimated face manifold dimension: {d}")
print(f"Ratio: {d/D:.4f} ({d/D*100:.2f}%)")
print(f"Random pixel image: {'face' if False else 'noise'}")
print(f"â†’ Almost ALL points in pixel space are NOT faces")
```

```
Pixel space dimension: 12288
Estimated face manifold dimension: 100
Ratio: 0.0081 (0.81%)
Random pixel image: noise
â†’ Almost ALL points in pixel space are NOT faces
```

### 2.6 ç¢ºç‡å¯†åº¦æ¨å®š â€” ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ vs ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯

æ¨å®šé‡è¨­è¨ˆã®å•é¡Œã‚’æŠ½è±¡åŒ–ã™ã‚‹ã¨ã€**å¯†åº¦æ¨å®š**ï¼ˆdensity estimationï¼‰ã«å¸°ç€ã™ã‚‹ã€‚ãƒ‡ãƒ¼ã‚¿ $\{x_1, \ldots, x_N\}$ ã‹ã‚‰ $p(x)$ ã‚’æ¨å®šã™ã‚‹å•é¡Œã ã€‚

**ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¨å®š**: ãƒ¢ãƒ‡ãƒ«æ— $\{q_\theta\}$ ã‚’ä»®å®šã—ã€MLE ã§ $\theta$ ã‚’æ±ºã‚ã‚‹ã€‚

```python
import numpy as np
from scipy import stats

# Parametric: assume Gaussian, estimate Î¼ and Ïƒ
data = np.concatenate([np.random.normal(-2, 0.5, 300),
                        np.random.normal(3, 1.0, 700)])

mu_param = np.mean(data)
sigma_param = np.std(data)
print(f"Parametric (Gaussian): Î¼={mu_param:.2f}, Ïƒ={sigma_param:.2f}")
print(f"â†’ Single mode, cannot capture bimodality")
```

**ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¨å®š**: ãƒ¢ãƒ‡ãƒ«æ—ã‚’ä»®å®šã›ãšã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç›´æ¥å¯†åº¦ã‚’æ¨å®šã€‚

```python
# Nonparametric: Kernel Density Estimation (KDE)
def kde(x_eval, data, bandwidth):
    """
    pÌ‚(x) = (1/Nh) Î£ K((x - xáµ¢)/h)
    K = Gaussian kernel
    """
    N = len(data)
    densities = np.zeros_like(x_eval)
    for xi in data:
        densities += np.exp(-0.5 * ((x_eval - xi) / bandwidth)**2)
    densities /= (N * bandwidth * np.sqrt(2 * np.pi))
    return densities

x_eval = np.linspace(-5, 6, 500)

# Different bandwidths
for h in [0.1, 0.3, 1.0, 3.0]:
    density = kde(x_eval, data, h)
    peak_x = x_eval[np.argmax(density)]
    print(f"  h={h:.1f}: peak at x={peak_x:.2f}, max density={max(density):.3f}")

print("\nh too small â†’ noisy (overfitting)")
print("h too large â†’ smooth (underfitting)")
print("h just right â†’ captures bimodality")
```

KDE ã¯ä½æ¬¡å…ƒï¼ˆ$D \leq 5$ ç¨‹åº¦ï¼‰ã§ã¯æœ‰åŠ¹ã ãŒã€é«˜æ¬¡å…ƒã§ã¯ç ´ç¶»ã™ã‚‹ã€‚å¿…è¦ãªãƒ‡ãƒ¼ã‚¿é‡ãŒ $O(N^{D})$ ã§ã‚¹ã‚±ãƒ¼ãƒ«ã™ã‚‹ãŸã‚ã ã€‚ç”»åƒï¼ˆ$D = 12{,}288$ï¼‰ã®å¯†åº¦æ¨å®šã« KDE ã¯ä½¿ãˆãªã„ â€” ã ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§æ¨å®šé‡ã‚’æ§‹æˆã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚

| æ‰‹æ³• | ä»®å®š | é•·æ‰€ | çŸ­æ‰€ | é«˜æ¬¡å…ƒ |
|:-----|:-----|:-----|:-----|:-------|
| **ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯** (MLE) | ãƒ¢ãƒ‡ãƒ«æ—ã‚’ä»®å®š | å°‘ãƒ‡ãƒ¼ã‚¿ã§æ¨å®šå¯èƒ½ | ãƒ¢ãƒ‡ãƒ«ä¸é©åˆ | ä½¿ãˆã‚‹ |
| **ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯** (KDE) | ãªã— | æŸ”è»Ÿ | $O(N^D)$ å¿…è¦ | ä½¿ãˆãªã„ |
| **ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æ¨å®šé‡** (VAE/GAN/Flow/Diffusion) | NN ã®è¡¨ç¾åŠ› | é«˜æ¬¡å…ƒOK | å¤§é‡ãƒ‡ãƒ¼ã‚¿ + GPU | **ä¸»åŠ›** |

### 2.7 Pushforwardæ¸¬åº¦ â€” å¤‰æ•°å¤‰æ›ã®æ¸¬åº¦è«–çš„è¡¨ç¾

ç¬¬5å›ã®æ¸¬åº¦è«–ã§å­¦ã‚“ã è¨€èªã‚’ä½¿ã†ã¨ã€å¯†åº¦æ¨å®šã¯æ¬¡ã®ã‚ˆã†ã«å®šå¼åŒ–ã§ãã‚‹ã€‚

æ½œåœ¨ç©ºé–“ $(\mathcal{Z}, \mu)$ ã‹ã‚‰è¦³æ¸¬ç©ºé–“ $(\mathcal{X}, \nu)$ ã¸ã®å†™åƒ $G_\theta: \mathcal{Z} \to \mathcal{X}$ ãŒã‚ã‚‹ã¨ãã€ç”Ÿæˆåˆ†å¸ƒã¯ **pushforward æ¸¬åº¦**:

$$q_\theta = G_{\theta \#} \mu, \quad \text{i.e.,} \quad q_\theta(A) = \mu(G_\theta^{-1}(A)) \quad \forall A \in \mathcal{B}(\mathcal{X})$$

GAN ã®ç”Ÿæˆå™¨ã¯ã¾ã•ã«ã“ã® pushforward ã ã€‚$z \sim \mathcal{N}(0, I)$ ã‚’ $G_\theta(z)$ ã§æŠ¼ã—å‡ºã—ã¦ç”Ÿæˆåˆ†å¸ƒã‚’ä½œã‚‹ã€‚Radon-Nikodym å¾®åˆ†ãŒå­˜åœ¨ã™ã‚‹ã¨ãï¼ˆç¬¬5å›ï¼‰ã€å¯†åº¦æ¯”ãŒè¨ˆç®—ã§ãã€KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ãŒæ„å‘³ã‚’æŒã¤ã€‚

```python
# Pushforward in action
import numpy as np

# Latent space: z ~ N(0, 1)
z = np.random.normal(0, 1, 10000)

# Generator: G(z) = 2z + 3 (simple affine)
x_affine = 2 * z + 3  # pushforward â†’ N(3, 4)

# Generator: G(z) = zÂ³ (nonlinear)
x_cubic = z ** 3  # pushforward â†’ non-Gaussian!

print(f"z ~ N(0,1):    mean={np.mean(z):.3f}, std={np.std(z):.3f}")
print(f"G(z) = 2z+3:   mean={np.mean(x_affine):.3f}, std={np.std(x_affine):.3f}")
print(f"G(z) = zÂ³:     mean={np.mean(x_cubic):.3f}, std={np.std(x_cubic):.3f}")
print(f"\nAffine push: N(0,1) â†’ N(3,4) â€” distribution stays Gaussian")
print(f"Cubic push: N(0,1) â†’ heavy-tailed non-Gaussian")
print(f"â†’ Neural net G_Î¸(z) creates ARBITRARY distributions from simple z")
```

:::details å­¦ç¿’æˆ¦ç•¥ã®ãƒ’ãƒ³ãƒˆ
æœ¬è¬›ç¾©ã¯ã€Œæ¨å®šé‡ã®æ•°å­¦ã€ã‚’æ­¦å™¨ã«ã™ã‚‹å›ã ã€‚å„æ¨å®šé‡ã®å¿œç”¨è©³ç´°ã¯ç¬¬8-16å›ã§å¾¹åº•çš„ã«æ˜ã‚Šä¸‹ã’ã‚‹ã€‚ã“ã“ã§ã¯3ã¤ã®ã“ã¨ã«é›†ä¸­ã—ã¦ã»ã—ã„: (1) MLE ã®æ•°å­¦çš„æ§‹é€ ï¼ˆCE/KLç­‰ä¾¡æ€§ã€æ¼¸è¿‘è«–ï¼‰ã‚’å®Œå…¨ã«ç†è§£ã™ã‚‹ã€(2) å°¤åº¦é–¢æ•°ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹å½¢æ…‹ã§æ¨å®šé‡ãŒã©ã†åˆ†å²ã™ã‚‹ã‹ã‚’æ´ã‚€ã€(3) çµ±è¨ˆçš„è·é›¢ãŒä½•ã‚’æ¸¬ã£ã¦ã„ã‚‹ã‹ã‚’çŸ¥ã‚‹ã€‚è©³ç´°ãªå°å‡ºã‚„å®Ÿè£…ã¯å¾Œã®å›ã«è­²ã‚‹ â€” ç„¦ã‚‰ãªãã¦ã„ã„ã€‚
:::

:::details ãƒˆãƒ­ã‚¤ã®æœ¨é¦¬: Python ã®é™ç•ŒãŒè¦‹ãˆå§‹ã‚ã‚‹
Zone 4 ã§ MLE ã®åå¾©è¨ˆç®—ã‚’ Python ã§å®Ÿè£…ã™ã‚‹ã€‚1000æ¬¡å…ƒã®ã‚¬ã‚¦ã‚¹åˆ†å¸ƒãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã« for ãƒ«ãƒ¼ãƒ—ã‚’ä½¿ã†ã¨ã€å®Ÿè¡Œæ™‚é–“ãŒã©ã†ãªã‚‹ã‹ â€” ç¬¬6å›ã® Adam å®Ÿè£…ã§æ„Ÿã˜ãŸã€Œé…ã•ã€ãŒã€ã“ã“ã§ã•ã‚‰ã«å¢—å¹…ã•ã‚Œã‚‹ã€‚ç¬¬9-10å›ã§ã€Œã‚‚ã† Python ã§ã¯ç„¡ç†ã€ã¨æ„Ÿã˜ãŸç¬é–“ãŒã€Julia ãƒ‡ãƒ“ãƒ¥ãƒ¼ã®ãƒˆãƒªã‚¬ãƒ¼ã«ãªã‚‹ã€‚è¦šãˆã¦ãŠã„ã¦ã»ã—ã„ã€‚
:::

:::message
**é€²æ—: 20% å®Œäº†** â€” ãªãœå¯†åº¦æ¨å®šãŒé›£ã—ã„ã‹ã€Pushforwardæ¸¬åº¦ã®æ„å‘³ã‚’æ´ã‚“ã ã€‚ã“ã“ã‹ã‚‰æ•°å¼ä¿®è¡Œã«å…¥ã‚‹ã€‚
:::

### 2.7 çµ±è¨ˆçš„æ¨å®šã®ç ”ç©¶ç³»è­œ

```mermaid
graph TD
    subgraph "å¤å…¸: æ¨å®šé‡ã®åŸºç¤ (1922-2000)"
        Fisher[Fisher MLE<br>1922] --> EM[EMç®—æ³•<br>Dempster 1977]
        EM --> MCMC[MCMCæ¨å®š<br>Gibbs/MH]
        Fisher --> CramerRao[CramÃ©r-Raoä¸‹ç•Œ<br>1945-46]
    end

    subgraph "ç¬¬1ä¸–ä»£: NNæ¨å®šé‡ (2006-2012)"
        RBM[RBM<br>ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ™ãƒ¼ã‚¹æ¨å®š] --> DBN[DBN<br>æ·±å±¤ä¿¡å¿µãƒãƒƒãƒˆ]
    end

    subgraph "ç¬¬2ä¸–ä»£: æ˜ç¤ºçš„+æš—é»™çš„æ¨å®šé‡ (2013-2016)"
        VAE[VAE<br>å¤‰åˆ†MLE 2013] --> CVAE[Conditional VAE]
        GAN[GAN<br>æš—é»™çš„æ¨å®š 2014] --> DCGAN[DCGAN 2015]
        NICE[Flow<br>å¤‰æ•°å¤‰æ›MLE 2014] --> RealNVP[Real NVP<br>2016]
    end

    subgraph "ç¬¬3ä¸–ä»£: ã‚¹ã‚³ã‚¢æ¨å®šé‡ (2015-2021)"
        DiffOrig[Diffusion<br>Sohl-Dickstein 2015] --> NCSN[NCSN<br>Song 2019]
        NCSN --> DDPM[DDPM<br>Ho 2020]
        DDPM --> SDE[Score SDE<br>Song 2020]
    end

    subgraph "çµ±åˆ: MLE beyond i.i.d. (2021-)"
        FM[Flow Matching 2022]
        CM[Consistency Models 2023]
        AR[è‡ªå·±å›å¸°MLE<br>GPT-4 2023]
    end

    Fisher -.->|æ¨å®šåŸç†| VAE
    EM -.->|æ½œåœ¨å¤‰æ•°| VAE
    Fisher -.->|å°¤åº¦ä¸è¦åŒ–| GAN
    NICE -.->|å¯é€†å†™åƒ| FM
    SDE -.->|é€£ç¶šåŒ–| FM

    style Fisher fill:#fff9c4
    style VAE fill:#e8f5e9
    style GAN fill:#fff3e0
    style NICE fill:#e3f2fd
    style DDPM fill:#fce4ec
```

### 2.8 ãƒ¢ãƒ‡ãƒ«é–“ã®æ•°å­¦çš„é–¢ä¿‚

æ¨å®šé‡ã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã¯ä¸€è¦‹ãƒãƒ©ãƒãƒ©ã«è¦‹ãˆã‚‹ãŒã€æ·±ã„æ•°å­¦çš„ã¤ãªãŒã‚ŠãŒã‚ã‚‹ã€‚

| æ¥ç¶š | é–¢ä¿‚ | è©³ç´° |
|:-----|:-----|:-----|
| MLE â†’ VAE | ELBO = MLE ã®å¤‰åˆ†è¿‘ä¼¼ | $\log p(x) \geq \text{ELBO}$ â†’ ELBO æœ€å¤§åŒ– $\approx$ MLE |
| KL â†’ GAN | GAN = JSD æœ€å°åŒ– | JSD ã¯ KL ã®å¯¾ç§°åŒ–ç‰ˆ |
| VAE â†’ Diffusion | éšå±¤çš„ VAE ã®æ¥µé™ | $T \to \infty$ ã§ Diffusion ã«ä¸€è‡´ |
| Flow â†’ Diffusion | ç¢ºç‡ãƒ•ãƒ­ãƒ¼ ODE | Song+ (2020) ãŒçµ±ä¸€ |
| Score â†’ Diffusion | denoising score matching | DDPM loss $\equiv$ score matching |
| MLE â†’ LLM | æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ | GPT = autoregressive MLE |
| f-Divergence â†’ GAN | å¤‰åˆ†è¡¨ç¾ | f-GAN = ä»»æ„ã® f-divergence ã§ GAN |

```python
# Mathematical connections between models
connections = [
    ("MLE",       "CE minimization",        "Theorem 3.2"),
    ("CE",        "KL minimization",         "Theorem 3.3 (constant H(pÌ‚))"),
    ("KL forward","VAE (ELBO)",              "ELBO = E[log p(x|z)] - KL[q(z|x)||p(z)]"),
    ("KL reverse","GAN (approximately)",     "Mode-seeking â†’ sharp samples"),
    ("JSD",       "Vanilla GAN",             "min_G JSD(p_data, p_g) - log4"),
    ("Score fn",  "Diffusion (DDPM)",        "Îµ-prediction â‰¡ score matching"),
    ("Change var","Normalizing Flow",        "log q(x) = log p(z) + log|det J|"),
    ("MLE auto",  "LLM (GPT)",              "CE loss = autoregressive MLE"),
]

print(f"{'From':>15} {'â†’':>3} {'To':>25}  {'Via':>45}")
print("-" * 95)
for src, dst, via in connections:
    print(f"{src:>15} {'â†’':>3} {dst:>25}  {via:>45}")
```



---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” MLE ã®æ•°å­¦æ§‹é€ ã¨æ¨å®šé‡ã®åˆ†é¡

æœ¬è¬›ç¾©ã®æ•°å­¦ã‚¾ãƒ¼ãƒ³ã¯3ã¤ã®å±±ã‚’æ”»ç•¥ã™ã‚‹:

1. **æœ€å°¤æ¨å®šï¼ˆMLEï¼‰** â€” æ¨å®šé‡ã®æ•°å­¦çš„åŸºç›¤ã¨æ¼¸è¿‘è«–
2. **å°¤åº¦é–¢æ•°ã®ã‚¢ã‚¯ã‚»ã‚¹å½¢æ…‹** â€” æ˜ç¤ºçš„ vs æš—é»™çš„æ¨å®šé‡
3. **çµ±è¨ˆçš„è·é›¢ã®å¿œç”¨** â€” FID, KID, CMMD ã®å®šç¾©ã¨é™ç•Œ

```mermaid
graph TD
    A[MLE<br>å®šç¾© 3.1] --> B[MLE = CEæœ€å°åŒ–<br>å®šç† 3.2]
    B --> C[MLE = KLæœ€å°åŒ–<br>å®šç† 3.3]
    C --> D[MLE ã®æ¼¸è¿‘è«–<br>Fisher 1922]
    D --> E[MLE ã®é™ç•Œ<br>æ½œåœ¨å¤‰æ•°ã¸ã®å‹•æ©Ÿ]

    F[æ˜ç¤ºçš„æ¨å®šé‡<br>Prescribed å®šç¾© 3.5] --> H[å°¤åº¦è¨ˆç®—å¯èƒ½]
    G[æš—é»™çš„æ¨å®šé‡<br>Implicit å®šç¾© 3.6] --> I[å°¤åº¦è¨ˆç®—ä¸èƒ½]

    H --> J[VAE / Flow]
    I --> K[GAN]

    E --> L[æ½œåœ¨å¤‰æ•°ã®å°å…¥<br>ç¬¬8å›ã¸]

    M[FID<br>Wâ‚‚è·é›¢] --> N[KID<br>MMD]
    N --> O[CMMD<br>CLIP-MMD]

    style A fill:#e8f5e9
    style B fill:#e8f5e9
    style C fill:#e8f5e9
    style M fill:#e3f2fd
```

### 3.1 æœ€å°¤æ¨å®šï¼ˆMLEï¼‰ã®å®šç¾©

:::message
ã“ã“ã‹ã‚‰æœ¬è¬›ç¾©ã®æ ¸å¿ƒã«å…¥ã‚‹ã€‚ç¬¬6å›ã® Cross-Entropy ã¨ KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ãŒã€ã“ã“ã§ã€Œåˆæµã€ã™ã‚‹ã€‚ãƒšãƒ³ã¨ç´™ã‚’ç”¨æ„ã—ã¦ã€ä¸€è¡Œãšã¤è¿½ã£ã¦ã»ã—ã„ã€‚
:::

**å®šç¾© 3.1ï¼ˆæœ€å°¤æ¨å®šé‡ï¼‰**

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ $\mathcal{D} = \{x_1, x_2, \ldots, x_N\}$ ãŒçœŸã®åˆ†å¸ƒ $p_\text{data}(x)$ ã‹ã‚‰ i.i.d. ã«ç”Ÿæˆã•ã‚ŒãŸã¨ã™ã‚‹ã€‚ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ãƒ¢ãƒ‡ãƒ« $q_\theta(x)$ ã«å¯¾ã—ã¦ã€**æœ€å°¤æ¨å®šé‡**ï¼ˆMaximum Likelihood Estimator, MLEï¼‰ã¯:

$$\hat{\theta}_\text{MLE} = \arg\max_\theta \prod_{i=1}^{N} q_\theta(x_i)$$

å¯¾æ•°ã‚’å–ã‚‹ã¨ï¼ˆ$\log$ ã¯å˜èª¿å¢—åŠ ãªã®ã§ $\arg\max$ ã¯å¤‰ã‚ã‚‰ãªã„ï¼‰:

$$\hat{\theta}_\text{MLE} = \arg\max_\theta \sum_{i=1}^{N} \log q_\theta(x_i) = \arg\max_\theta \frac{1}{N} \sum_{i=1}^{N} \log q_\theta(x_i)$$

Fisher (1922) [^1] ãŒã€ŒOn the mathematical foundations of theoretical statisticsã€ã§ä½“ç³»åŒ–ã—ãŸæ‰‹æ³•ã§ã‚ã‚Šã€çµ±è¨ˆå­¦ã§100å¹´ä»¥ä¸Šã®æ­´å²ã‚’æŒã¤ã€‚

```python
import numpy as np

# MLE for Gaussian: analytical solution
data = np.array([1.2, 2.3, 1.8, 2.1, 1.5, 2.7, 1.9, 2.4])

# MLE estimates
mu_mle = np.mean(data)          # Î¼Ì‚ = (1/N) Î£ xáµ¢
sigma_mle = np.std(data, ddof=0)  # ÏƒÌ‚ = âˆš((1/N) Î£(xáµ¢ - Î¼Ì‚)Â²)

# Average log-likelihood
log_lik = -0.5 * np.log(2 * np.pi * sigma_mle**2) - 0.5 * ((data - mu_mle) / sigma_mle)**2
avg_log_lik = np.mean(log_lik)

print(f"Data: {data}")
print(f"MLE: Î¼Ì‚ = {mu_mle:.4f}, ÏƒÌ‚ = {sigma_mle:.4f}")
print(f"Average log-likelihood: {avg_log_lik:.4f}")

# Verify: this is the maximum
for mu_test in [1.5, 1.99, mu_mle, 2.1, 2.5]:
    ll = np.mean(-0.5 * np.log(2 * np.pi * sigma_mle**2)
                  - 0.5 * ((data - mu_test) / sigma_mle)**2)
    marker = " â† MLE (maximum)" if abs(mu_test - mu_mle) < 1e-10 else ""
    print(f"  Î¼ = {mu_test:.4f}: avg log-lik = {ll:.4f}{marker}")
```

### 3.2 MLE ã¨ Cross-Entropy ã®ç­‰ä¾¡æ€§

**å®šç† 3.2ï¼ˆMLE = Cross-Entropy æœ€å°åŒ–ï¼‰**

ä»»æ„ã®æœ‰é™ $N$ ã«å¯¾ã—ã¦:

$$\hat{\theta}_\text{MLE} = \arg\min_\theta H(\hat{p}, q_\theta)$$

ã“ã“ã§ $\hat{p}(x) = \frac{1}{N}\sum_{i=1}^N \delta(x - x_i)$ ã¯çµŒé¨“åˆ†å¸ƒã€$H(\hat{p}, q_\theta)$ ã¯ Cross-Entropyã€‚ã“ã®ç­‰å¼ã¯ $N \to \infty$ ã‚’å¿…è¦ã¨ã—ãªã„ â€” çµŒé¨“åˆ†å¸ƒ $\hat{p}$ ã«å¯¾ã™ã‚‹ç­‰ä¾¡æ€§ã¯æœ‰é™ $N$ ã§å³å¯†ã«æˆç«‹ã™ã‚‹ã€‚$N \to \infty$ ãŒå¿…è¦ãªã®ã¯ $\hat{p} \to p_\text{data}$ ã®æ„å‘³ã§ã®ä¸€è‡´æ€§ï¼ˆæ€§è³ª 3.4aï¼‰ã€‚

**å°å‡º:**

Step 1: çµŒé¨“åˆ†å¸ƒ $\hat{p}(x) = \frac{1}{N}\sum_{i=1}^{N} \delta(x - x_i)$ ã‚’å°å…¥ã™ã‚‹ã€‚

Step 2: MLE ã®ç›®çš„é–¢æ•°ã‚’å¤‰å½¢ã™ã‚‹:

$$\frac{1}{N} \sum_{i=1}^{N} \log q_\theta(x_i) = \mathbb{E}_{\hat{p}}[\log q_\theta(x)]$$

Step 3: ã“ã‚Œã¯ Cross-Entropy ã®ç¬¦å·åè»¢ã«ç­‰ã—ã„:

$$\mathbb{E}_{\hat{p}}[\log q_\theta(x)] = -H(\hat{p}, q_\theta)$$

Step 4: ã‚ˆã£ã¦:

$$\arg\max_\theta \mathbb{E}_{\hat{p}}[\log q_\theta(x)] = \arg\min_\theta H(\hat{p}, q_\theta) \quad \blacksquare$$

ã“ã®ç­‰ä¾¡æ€§ã¯å¼·åŠ›ã ã€‚ç¬¬6å›ã§å­¦ã‚“ã  Cross-Entropy ã®ã‚ã‚‰ã‚†ã‚‹æ€§è³ªãŒã€MLE ã«ãã®ã¾ã¾é©ç”¨ã§ãã‚‹ã€‚

### 3.3 MLE ã¨ KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®ç­‰ä¾¡æ€§

**å®šç† 3.3ï¼ˆMLE = KL æœ€å°åŒ–ï¼‰**

$$\hat{\theta}_\text{MLE} = \arg\min_\theta D_\text{KL}(\hat{p} \| q_\theta)$$

**å°å‡º:**

Step 1: Cross-Entropy ã®åˆ†è§£ï¼ˆç¬¬6å› å®šç† 3.4ï¼‰ã‚’æ€ã„å‡ºã™:

$$H(\hat{p}, q_\theta) = H(\hat{p}) + D_\text{KL}(\hat{p} \| q_\theta)$$

Step 2: $H(\hat{p})$ ã¯ $\theta$ ã«ä¾å­˜ã—ãªã„ï¼ˆãƒ‡ãƒ¼ã‚¿ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã¯å®šæ•°ï¼‰ã€‚

Step 3: ã‚ˆã£ã¦:

$$\arg\min_\theta H(\hat{p}, q_\theta) = \arg\min_\theta D_\text{KL}(\hat{p} \| q_\theta) \quad \blacksquare$$

:::message
ã“ã“ã§å…¨ã¦ãŒç¹‹ãŒã£ãŸã€‚**MLE = CE æœ€å°åŒ– = KL æœ€å°åŒ–**ã€‚ç¬¬6å›ã§å­¦ã‚“ã  KL ã®æ€§è³ªãŒå…¨ã¦ MLE ã«é©ç”¨ã§ãã‚‹:
- $D_\text{KL} \geq 0$ï¼ˆGibbs ã®ä¸ç­‰å¼ï¼‰â†’ MLE ã¯æœ€é©ã§éè² ã®èª¤å·®
- $D_\text{KL} = 0 \Leftrightarrow \hat{p} = q_\theta$ â†’ MLE ã¯çœŸã®åˆ†å¸ƒã§æå¤±ã‚¼ãƒ­
- KL ã¯éå¯¾ç§° â†’ MLE ã¯ **mode-covering**ï¼ˆå…¨ã¦ã®ãƒ¢ãƒ¼ãƒ‰ã‚’ã‚«ãƒãƒ¼ã—ã‚ˆã†ã¨ã™ã‚‹ï¼‰
:::

```python
import numpy as np

# Numerical verification: MLE = CE minimization = KL minimization
np.random.seed(42)
data = np.random.normal(2.0, 1.0, 10000)  # true: N(2, 1)

# Scan over Î¼ values, fix Ïƒ=1
mus = np.linspace(0, 4, 100)
avg_log_liks = []
cross_entropies = []
kl_divs = []

# Empirical entropy H(pÌ‚) (constant)
H_p = 0.5 * np.log(2 * np.pi * np.e * np.var(data))

for mu in mus:
    sigma = 1.0
    # Average log-likelihood
    ll = np.mean(-0.5 * np.log(2 * np.pi * sigma**2) - 0.5 * ((data - mu) / sigma)**2)
    avg_log_liks.append(ll)
    # Cross-entropy H(pÌ‚, q_Î¸) = -E[log q_Î¸(x)]
    ce = -ll
    cross_entropies.append(ce)
    # KL = CE - H(pÌ‚)
    kl = ce - H_p
    kl_divs.append(kl)

# Find optima
best_mle = mus[np.argmax(avg_log_liks)]
best_ce = mus[np.argmin(cross_entropies)]
best_kl = mus[np.argmin(kl_divs)]

print(f"argmax log-likelihood: Î¼ = {best_mle:.4f}")
print(f"argmin Cross-Entropy:  Î¼ = {best_ce:.4f}")
print(f"argmin KL divergence:  Î¼ = {best_kl:.4f}")
print(f"All three agree: {np.allclose(best_mle, best_ce) and np.allclose(best_ce, best_kl)}")
print(f"(True Î¼ = 2.0, sample mean = {np.mean(data):.4f})")
```

### 3.4 MLE ã®æ¼¸è¿‘è«– â€” Fisher ã®éºç”£

Fisher (1922) [^1] ã¯ MLE ã®3ã¤ã®æ¼¸è¿‘çš„æ€§è³ªã‚’ï¼ˆãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã«ï¼‰ç¤ºã—ãŸ:

**æ€§è³ª 3.4aï¼ˆä¸€è‡´æ€§, Consistencyï¼‰**

$$\hat{\theta}_\text{MLE} \xrightarrow{p} \theta^* \quad (N \to \infty)$$

MLE ã¯ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°çœŸã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ç¢ºç‡åæŸã™ã‚‹ã€‚

**æ€§è³ª 3.4bï¼ˆæ¼¸è¿‘æ­£è¦æ€§, Asymptotic Normalityï¼‰**

$$\sqrt{N}(\hat{\theta}_\text{MLE} - \theta^*) \xrightarrow{d} \mathcal{N}(0, \mathcal{I}(\theta^*)^{-1})$$

ã“ã“ã§ $\mathcal{I}(\theta)$ ã¯ **Fisher æƒ…å ±è¡Œåˆ—**ï¼ˆç¬¬6å› Zone 6 ã§å°å…¥ï¼‰:

$$\mathcal{I}(\theta)_{ij} = -\mathbb{E}_{p_\theta}\left[\frac{\partial^2}{\partial \theta_i \partial \theta_j} \log p_\theta(x)\right]$$

**æ€§è³ª 3.4cï¼ˆæ¼¸è¿‘æœ‰åŠ¹æ€§, Asymptotic Efficiencyï¼‰**

**Cramer-Rao ä¸ç­‰å¼** (CramÃ©r 1946 [^14] / Rao 1945 [^15]): ä»»æ„ã®ä¸åæ¨å®šé‡ $\hat{\theta}$ ã«å¯¾ã—ã¦:

$$\text{Var}(\hat{\theta}) \geq [\mathcal{I}(\theta)]^{-1}$$

ã“ã®ä¸‹ç•Œã‚’**Cramer-Rao ä¸‹ç•Œ**ã¨å‘¼ã¶ã€‚MLE ã¯ã“ã®ä¸‹ç•Œã‚’æ¼¸è¿‘çš„ã«é”æˆã™ã‚‹ã€‚ã¤ã¾ã‚Šã€æ¼¸è¿‘çš„ã«æœ€å°åˆ†æ•£ã®ä¸åæ¨å®šé‡ã«ç­‰ã—ã„ã€‚

```python
import numpy as np

# Demonstration: MLE convergence and asymptotic normality
np.random.seed(42)
true_mu, true_sigma = 3.0, 2.0
sample_sizes = [10, 50, 100, 500, 1000, 5000]
n_trials = 1000

print(f"True parameters: Î¼ = {true_mu}, Ïƒ = {true_sigma}")
print(f"Fisher info for Î¼: I(Î¼) = 1/ÏƒÂ² = {1/true_sigma**2:.4f}")
print(f"Asymptotic variance of Î¼Ì‚: 1/(NÂ·I(Î¼)) = ÏƒÂ²/N")
print()
print(f"{'N':>6} {'Mean(Î¼Ì‚)':>10} {'Std(Î¼Ì‚)':>10} {'Theory':>10} {'Ratio':>8}")
print("-" * 50)

for N in sample_sizes:
    mu_hats = []
    for _ in range(n_trials):
        data = np.random.normal(true_mu, true_sigma, N)
        mu_hats.append(np.mean(data))

    empirical_std = np.std(mu_hats)
    theoretical_std = true_sigma / np.sqrt(N)

    print(f"{N:6d} {np.mean(mu_hats):10.4f} {empirical_std:10.4f} "
          f"{theoretical_std:10.4f} {empirical_std/theoretical_std:8.4f}")
```

**å‡ºåŠ›ä¾‹:**
```
True parameters: Î¼ = 3.0, Ïƒ = 2.0
Fisher info for Î¼: I(Î¼) = 1/ÏƒÂ² = 0.2500
Asymptotic variance of Î¼Ì‚: 1/(NÂ·I(Î¼)) = ÏƒÂ²/N

     N    Mean(Î¼Ì‚)    Std(Î¼Ì‚)     Theory    Ratio
--------------------------------------------------
    10     3.0012     0.6367     0.6325    1.0067
    50     2.9992     0.2826     0.2828    0.9994
   100     3.0037     0.1988     0.2000    0.9940
   500     3.0003     0.0897     0.0894    1.0030
  1000     2.9999     0.0628     0.0632    0.9934
  5000     3.0001     0.0283     0.0283    1.0005
```

Ratio ãŒã»ã¼ 1.0 â€” MLE ã®åˆ†æ•£ãŒ Fisher æƒ…å ±è¡Œåˆ—ã‹ã‚‰äºˆæ¸¬ã•ã‚Œã‚‹ç†è«–å€¤ã«ä¸€è‡´ã—ã¦ã„ã‚‹ã€‚

### 3.5 MLE ã®é™ç•Œã¨æ½œåœ¨å¤‰æ•°ã¸ã®å‹•æ©Ÿ

MLE ã«ã¯æ ¹æœ¬çš„ãªé™ç•ŒãŒã‚ã‚‹ã€‚

**é™ç•Œ 1: ãƒ¢ãƒ‡ãƒ«æ—ã®è¡¨ç¾åŠ›ã«ä¾å­˜**

Zone 0 ã§è¦‹ãŸé€šã‚Šã€å˜å³°ã‚¬ã‚¦ã‚¹ã§åŒå³°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã™ã‚‹ã¨ã€ã€Œæœ€è‰¯ã®å¦¥å”ã€ã«ã—ã‹ãªã‚‰ãªã„ã€‚

**é™ç•Œ 2: é«˜æ¬¡å…ƒã§ã®è¨ˆç®—å›°é›£æ€§**

$p_\theta(x)$ ã®æ­£è¦åŒ–å®šæ•°ã®è¨ˆç®—:

$$Z(\theta) = \int p_\theta(x) \, dx$$

ãŒé«˜æ¬¡å…ƒã§ã¯ tractable ã§ãªããªã‚‹ã€‚ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å‡ºåŠ›ã« $\text{softmax}$ ã‚’ä½¿ãˆã°é›¢æ•£çš„ãªæ­£è¦åŒ–ã¯ã§ãã‚‹ãŒã€é€£ç¶šç©ºé–“ã§ã®æ­£è¦åŒ–ã¯ä¸€èˆ¬ã«ä¸å¯èƒ½ã€‚

**é™ç•Œ 3: å‘¨è¾ºåŒ–ã®å›°é›£æ€§**

æ½œåœ¨å¤‰æ•° $z$ ã‚’å°å…¥ã™ã‚‹ã¨:

$$p_\theta(x) = \int p_\theta(x, z) \, dz = \int p_\theta(x | z) \, p(z) \, dz$$

ã“ã®ç©åˆ†ã¯ã€$p_\theta(x|z)$ ãŒãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å ´åˆã€è§£æçš„ã«è¨ˆç®—ã§ããªã„ã€‚

```python
import numpy as np
from scipy import stats

# Limitation 1: model misspecification
np.random.seed(42)

# True distribution: mixture of 3 Gaussians
def true_pdf(x):
    return (0.3 * stats.norm.pdf(x, -3, 0.5) +
            0.4 * stats.norm.pdf(x, 0, 1.0) +
            0.3 * stats.norm.pdf(x, 4, 0.7))

# Sample from true distribution
def sample_true(n):
    components = np.random.choice(3, size=n, p=[0.3, 0.4, 0.3])
    mus = [-3, 0, 4]
    sigmas = [0.5, 1.0, 0.7]
    return np.array([np.random.normal(mus[c], sigmas[c]) for c in components])

data = sample_true(5000)

# MLE with single Gaussian â†’ bad fit
mu_single = np.mean(data)
sigma_single = np.std(data)

# KL divergence (approximate via Monte Carlo)
x_grid = np.linspace(-6, 7, 10000)
p_true = true_pdf(x_grid)
q_model = stats.norm.pdf(x_grid, mu_single, sigma_single)

# Avoid log(0)
mask = (p_true > 1e-10) & (q_model > 1e-10)
kl_approx = np.trapz(p_true[mask] * np.log(p_true[mask] / q_model[mask]), x_grid[mask])

print(f"True distribution: 3-component Gaussian mixture")
print(f"MLE (single Gaussian): Î¼ = {mu_single:.3f}, Ïƒ = {sigma_single:.3f}")
print(f"KL(p_true || q_model) â‰ˆ {kl_approx:.4f} nats")
print(f"â†’ Large KL because single Gaussian cannot capture 3 modes")
print(f"\nSolution: introduce LATENT VARIABLES (Lecture 8)")
print(f"  p(x) = Î£_k Ï€_k Â· N(x; Î¼_k, Ïƒ_kÂ²)  â† mixture model")
print(f"  p(x) = âˆ« p(x|z) p(z) dz             â† continuous latent (VAE)")
```

:::message
ã“ã“ãŒç¬¬8å›ï¼ˆæ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ« & EMç®—æ³•ï¼‰ã¸ã®æ¥ç¶šç‚¹ã ã€‚MLE ã®é™ç•Œã‚’æ‰“ç ´ã™ã‚‹ãŸã‚ã«ã€æ½œåœ¨å¤‰æ•° $z$ ã‚’å°å…¥ã—ã¦ $p(x) = \int p(x|z)p(z)dz$ ã¨åˆ†è§£ã™ã‚‹ã€‚ã ãŒã€ã“ã®ç©åˆ†ã¯è§£æçš„ã«è¨ˆç®—ã§ããªã„ã€‚EMç®—æ³•ãŒãã‚Œã‚’è¿‘ä¼¼çš„ã«è§£ãã€ã•ã‚‰ã« VAE ãŒ neural network ã§å¼·åŠ›ã«ã™ã‚‹ã€‚ã“ã®æµã‚Œã‚’é ­ã«å…¥ã‚Œã¦ãŠã„ã¦ã»ã—ã„ã€‚
:::

### 3.6 å°¤åº¦é–¢æ•°ã®ã‚¢ã‚¯ã‚»ã‚¹å½¢æ…‹ â€” æ˜ç¤ºçš„ vs æš—é»™çš„æ¨å®šé‡

Mohamed & Lakshminarayanan (2016) [^6] ã¯ã€ç¢ºç‡ãƒ¢ãƒ‡ãƒ«ã®æ¨å®šæ‰‹æ³•ã‚’å°¤åº¦é–¢æ•°ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹å½¢æ…‹ã§2ã¤ã«å¤§åˆ¥ã—ãŸã€‚

**å®šç¾© 3.5ï¼ˆPrescribed Model / è¦å®šãƒ¢ãƒ‡ãƒ«ï¼‰**

ç¢ºç‡å¯†åº¦é–¢æ•° $q_\theta(x)$ ãŒé™½ã«å®šç¾©ã§ãã€$x$ ã‚’ä»£å…¥ã—ã¦ $q_\theta(x)$ ã®å€¤ãŒè¨ˆç®—å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã€‚

$$\text{Prescribed}: \quad q_\theta(x) \text{ is explicitly defined and evaluable}$$

ä¾‹: ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã€GMMã€VAEï¼ˆELBO çµŒç”±ï¼‰ã€Normalizing Flow

**å®šç¾© 3.6ï¼ˆImplicit Model / æš—é»™çš„ãƒ¢ãƒ‡ãƒ«ï¼‰**

ç¢ºç‡å¯†åº¦é–¢æ•°ã‚’é™½ã«å®šç¾©ã›ãšã€ç”Ÿæˆéç¨‹ï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ‰‹ç¶šãï¼‰ã®ã¿ã‚’å®šç¾©ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã€‚

$$\text{Implicit}: \quad x = G_\theta(z), \quad z \sim p(z)$$

å¯†åº¦ $q_\theta(x)$ ã¯å®šç¾©ã¯ã•ã‚Œã‚‹ãŒã€è¨ˆç®—ä¸èƒ½ï¼ˆintractableï¼‰ã€‚

ä¾‹: GAN

```python
# Prescribed model: can compute q_Î¸(x)
def prescribed_density(x, mu, sigma):
    """Gaussian: density is COMPUTABLE"""
    return np.exp(-0.5 * ((x - mu) / sigma)**2) / (sigma * np.sqrt(2 * np.pi))

# Implicit model: can only SAMPLE
def implicit_sample(z, generator_weights):
    """GAN generator: density is NOT computable, but sampling is easy"""
    # x = G_Î¸(z) â€” a neural network transform
    # We can get x, but CANNOT compute p(x)
    return z  # placeholder for neural net

x_test = 1.5

# Prescribed: "the probability of x = 1.5 is 0.242"
print(f"Prescribed: q(x={x_test}) = {prescribed_density(x_test, 2.0, 1.0):.4f}")

# Implicit: "I can generate samples, but can't tell you p(x = 1.5)"
print(f"Implicit: q(x={x_test}) = ??? (not computable)")
print(f"Implicit: samples = {np.random.normal(2.0, 1.0, 5).round(3)}")
```

ã“ã®åˆ†é¡ãŒæ·±ã„æ„å‘³ã‚’æŒã¤ã®ã¯ã€**è¨“ç·´æ–¹æ³•ãŒæ ¹æœ¬çš„ã«ç•°ãªã‚‹**ã‹ã‚‰ã ã€‚

| ãƒ¢ãƒ‡ãƒ«+æ¨å®šæ‰‹æ³•ã®åˆ†é¡ | å°¤åº¦ $q_\theta(x)$ | æ¨å®šæ‰‹æ³• | ä»£è¡¨ãƒ¢ãƒ‡ãƒ« |
|:-----|:-------------------|:---------|:-------|
| **æ˜ç¤ºçš„æ¨å®šé‡** (Prescribed) | è¨ˆç®—å¯èƒ½ | ç›´æ¥MLE / å¤‰åˆ†æ¨è«– | Flow, è‡ªå·±å›å¸° |
| **æš—é»™çš„æ¨å®šé‡** (Implicit) | è¨ˆç®—ä¸èƒ½ | æ•µå¯¾çš„è¨“ç·´ / ã‚«ãƒ¼ãƒãƒ«æ³• | GAN |
| **æ˜ç¤ºçš„ + æ½œåœ¨å¤‰æ•°** | å‘¨è¾ºåŒ–ãŒå›°é›£ | ELBO æœ€å¤§åŒ–ï¼ˆå¤‰åˆ†MLEï¼‰ | VAE |
| **ã‚¹ã‚³ã‚¢æ¨å®šé‡** | ä¸è¦ï¼ˆ$\nabla_x \log p$ ã®ã¿ï¼‰ | Score Matching | NCSN, DDPM |

### 3.7 MLEå¤‰å½¢1: å¤‰æ•°å¤‰æ›ã«ã‚ˆã‚‹å°¤åº¦è¨ˆç®—ï¼ˆæ¦‚è¦ã€è©³ç´°ã¯Course IIï¼‰

Normalizing Flow [^7] [^11] [^12] ã¯å¤‰æ•°å¤‰æ›å…¬å¼ã‚’ä½¿ã£ã¦å³å¯†ãªå°¤åº¦è¨ˆç®—ã‚’å¯èƒ½ã«ã™ã‚‹ã€‚

**å®šç† 3.7ï¼ˆå¤‰æ•°å¤‰æ›å…¬å¼ï¼‰**

$z \sim p(z)$ã€$x = f(z)$ ã§ $f$ ãŒå¾®åˆ†åŒç›¸å†™åƒï¼ˆbijection + differentiableï¼‰ã®ã¨ã:

$$q_\theta(x) = p(z) \left|\det \frac{\partial f^{-1}}{\partial x}\right| = p(z) \left|\det \frac{\partial f}{\partial z}\right|^{-1}$$

å¯¾æ•°ã‚’å–ã‚‹ã¨:

$$\log q_\theta(x) = \log p(f^{-1}(x)) + \log \left|\det \frac{\partial f^{-1}}{\partial x}\right|$$

```python
import numpy as np

# Simple 1D flow example: f(z) = z + Î±Â·tanh(z)
alpha = 0.8

def flow_forward(z):
    """x = f(z) = z + Î±Â·tanh(z)"""
    return z + alpha * np.tanh(z)

def flow_log_det_jacobian(z):
    """log |df/dz| = log |1 + Î±Â·(1 - tanhÂ²(z))|"""
    return np.log(np.abs(1 + alpha * (1 - np.tanh(z)**2)))

# Compute log-likelihood
z_samples = np.random.normal(0, 1, 10000)
x_samples = flow_forward(z_samples)

# log p(z) for standard normal
log_pz = -0.5 * z_samples**2 - 0.5 * np.log(2 * np.pi)

# log q(x) = log p(z) - log |df/dz|   (inverse function theorem)
log_qx = log_pz - flow_log_det_jacobian(z_samples)

print(f"Prior: z ~ N(0, 1)")
print(f"Flow: x = z + {alpha}Â·tanh(z)")
print(f"z statistics: mean = {z_samples.mean():.3f}, std = {z_samples.std():.3f}")
print(f"x statistics: mean = {x_samples.mean():.3f}, std = {x_samples.std():.3f}")
print(f"Average log q(x): {log_qx.mean():.4f}")
print(f"â†’ Flow transforms simple distribution into complex one with EXACT likelihood")
```

NICE [^11] ã¨ Real NVP [^12] ã¯ã€ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ãŒä¸‰è§’è¡Œåˆ—ã«ãªã‚‹ã‚ˆã†ã« $f$ ã‚’è¨­è¨ˆã™ã‚‹ã“ã¨ã§ã€è¡Œåˆ—å¼ã®è¨ˆç®—ã‚’ $O(D)$ ã«å‰Šæ¸›ã—ãŸã€‚

### 3.8 MLEå¤‰å½¢2: æš—é»™çš„æ¨å®šé‡ â€” GAN ã®ç›®çš„é–¢æ•°ï¼ˆæ¦‚è¦ã€è©³ç´°ã¯Course IIï¼‰

Goodfellow+ (2014) [^2] ã¯ã€å¯†åº¦ã‚’é™½ã«å®šç¾©ã—ãªã„å…¨ãæ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ãŸã€‚

**å®šç¾© 3.8ï¼ˆGAN ã®ç›®çš„é–¢æ•°ï¼‰**

$$\min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_\text{data}}[\log D(x)] + \mathbb{E}_{z \sim p(z)}[\log(1 - D(G(z)))]$$

ã“ã“ã§ $G: z \to x$ ã¯ç”Ÿæˆå™¨ã€$D: x \to [0, 1]$ ã¯åˆ¤åˆ¥å™¨ã€‚

**å®šç† 3.8aï¼ˆæœ€é©åˆ¤åˆ¥å™¨ï¼‰**

å›ºå®šã•ã‚ŒãŸ $G$ ã«å¯¾ã—ã¦ã€æœ€é©ãªåˆ¤åˆ¥å™¨ã¯:

$$D^*_G(x) = \frac{p_\text{data}(x)}{p_\text{data}(x) + p_g(x)}$$

**å°å‡º:**

$V(D, G)$ ã‚’ $D(x)$ ã«ã¤ã„ã¦æœ€å¤§åŒ–ã™ã‚‹ã€‚$y = D(x)$ ã¨æ›¸ãã¨:

$$f(y) = a \log y + b \log(1 - y)$$

$$f'(y) = \frac{a}{y} - \frac{b}{1-y} = 0 \implies y = \frac{a}{a+b}$$

ã“ã“ã§ $a = p_\text{data}(x)$, $b = p_g(x)$ ãªã®ã§ $D^*(x) = \frac{p_\text{data}(x)}{p_\text{data}(x) + p_g(x)}$ã€‚$\blacksquare$

**å®šç† 3.8bï¼ˆGAN ã¨ JSDï¼‰**

æœ€é©åˆ¤åˆ¥å™¨ $D^*$ ã®ä¸‹ã§:

$$V(D^*, G) = -\log 4 + 2 \cdot D_\text{JS}(p_\text{data} \| p_g)$$

ã“ã“ã§ $D_\text{JS}$ ã¯ Jensen-Shannon ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ï¼ˆç¬¬6å› 3.11bï¼‰ã€‚

ã‚ˆã£ã¦ **GAN ã®è¨“ç·´ã¯ JSD ã®æœ€å°åŒ–**ã«ç­‰ã—ã„ã€‚

```python
import numpy as np

# GAN objective demonstration
def optimal_discriminator(p_data, p_gen):
    """D*(x) = p_data(x) / (p_data(x) + p_gen(x))"""
    return p_data / (p_data + p_gen + 1e-10)

def jsd(p, q, x_grid):
    """Jensen-Shannon divergence"""
    m = 0.5 * (p + q)
    kl_pm = np.trapz(p * np.log(p / (m + 1e-10) + 1e-10) * (p > 1e-10), x_grid)
    kl_qm = np.trapz(q * np.log(q / (m + 1e-10) + 1e-10) * (q > 1e-10), x_grid)
    return 0.5 * (kl_pm + kl_qm)

from scipy import stats
x = np.linspace(-5, 8, 1000)

# True distribution
p = 0.5 * stats.norm.pdf(x, 0, 1) + 0.5 * stats.norm.pdf(x, 4, 1)

# Generator distribution (progressively improving)
stages = [
    ("Random",     stats.norm.pdf(x, 5, 3)),
    ("Learning",   stats.norm.pdf(x, 2, 2)),
    ("Good",       0.5 * stats.norm.pdf(x, 0.2, 1.1) + 0.5 * stats.norm.pdf(x, 3.8, 1.1)),
    ("Converged",  0.5 * stats.norm.pdf(x, 0, 1) + 0.5 * stats.norm.pdf(x, 4, 1)),
]

print(f"{'Stage':>12} {'JSD':>10} {'V(D*,G)':>12} {'D* at x=2':>12}")
print("-" * 50)
for name, q in stages:
    js = jsd(p, q, x)
    v = -np.log(4) + 2 * js
    d_star = optimal_discriminator(p[500], q[500])  # at x â‰ˆ 2
    print(f"{name:>12} {js:10.4f} {v:12.4f} {d_star:12.4f}")
```

### 3.9 MLEå¤‰å½¢3: ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°æ¨å®šé‡ï¼ˆæ¦‚è¦ã€è©³ç´°ã¯Course IIï¼‰

Song & Ermon (2019) [^10] ã¯ã€å¯†åº¦ $p(x)$ ã®ä»£ã‚ã‚Šã«**ã‚¹ã‚³ã‚¢é–¢æ•°**ã‚’å­¦ã¶ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ãŸã€‚

**å®šç¾© 3.9ï¼ˆã‚¹ã‚³ã‚¢é–¢æ•°ï¼‰**

$$s_\theta(x) \approx \nabla_x \log p_\text{data}(x)$$

ã‚¹ã‚³ã‚¢é–¢æ•°ã¯ç¢ºç‡å¯†åº¦ã®å‹¾é…ã§ã‚ã‚Šã€æ­£è¦åŒ–å®šæ•° $Z$ ã«ä¾å­˜ã—ãªã„:

$$\nabla_x \log p(x) = \nabla_x \log \frac{\tilde{p}(x)}{Z} = \nabla_x \log \tilde{p}(x)$$

ã“ã‚ŒãŒç”»æœŸçš„ãªç†ç”±ã¯ã€æ­£è¦åŒ–å®šæ•°ã®è¨ˆç®—ã‚’å®Œå…¨ã«å›é¿ã§ãã‚‹ã“ã¨ã ã€‚

Ho+ (2020) [^5] ã¯ã€ã“ã®ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°ã¨æ‹¡æ•£éç¨‹ã‚’çµ„ã¿åˆã‚ã›ãŸ DDPM ã‚’ææ¡ˆã—ã€ç”»åƒç”Ÿæˆã®å“è³ªã‚’åŠ‡çš„ã«å‘ä¸Šã•ã›ãŸã€‚DDPM ã®æå¤±é–¢æ•°:

$$\mathcal{L}_\text{simple} = \mathbb{E}_{t, x_0, \epsilon}\left[\|\epsilon - \epsilon_\theta(x_t, t)\|^2\right]$$

ã¯ã€denoising score matching ã®é‡ã¿ä»˜ãå¤‰å½¢ã¨ã—ã¦è§£é‡ˆã§ãã‚‹ã€‚

```python
import numpy as np

# Score function demonstration
def gaussian_score(x, mu, sigma):
    """âˆ‡_x log N(x; Î¼, ÏƒÂ²) = -(x - Î¼)/ÏƒÂ²"""
    return -(x - mu) / sigma**2

# Score for mixture is weighted sum
def mixture_score(x, mus, sigmas, weights):
    """Score of Gaussian mixture (not simple weighted average of scores!)"""
    # p(x) = Î£ w_k N(x; Î¼_k, Ïƒ_kÂ²)
    # âˆ‡ log p(x) = (Î£ w_k N(x;Î¼_k,Ïƒ_kÂ²) Â· score_k) / p(x)
    densities = np.array([w * np.exp(-0.5*((x-m)/s)**2) / (s*np.sqrt(2*np.pi))
                          for w, m, s in zip(weights, mus, sigmas)])
    scores = np.array([-(x - m) / s**2 for m, s in zip(mus, sigmas)])
    p_x = densities.sum(axis=0)
    return (densities * scores).sum(axis=0) / (p_x + 1e-10)

x_grid = np.linspace(-5, 8, 200)
mus = [0, 4]
sigmas = [1, 1]
weights = [0.5, 0.5]

scores = mixture_score(x_grid, mus, sigmas, weights)

print("Score function tells you: 'which direction increases density'")
print(f"At x = -3: score = {mixture_score(np.array([-3.0]), mus, sigmas, weights)[0]:.3f} (â†’ positive, go right)")
print(f"At x =  0: score = {mixture_score(np.array([0.0]), mus, sigmas, weights)[0]:.3f} (â†’ near zero, at mode)")
print(f"At x =  2: score = {mixture_score(np.array([2.0]), mus, sigmas, weights)[0]:.3f} (â†’ valley between modes)")
print(f"At x =  4: score = {mixture_score(np.array([4.0]), mus, sigmas, weights)[0]:.3f} (â†’ near zero, at mode)")
print(f"At x =  7: score = {mixture_score(np.array([7.0]), mus, sigmas, weights)[0]:.3f} (â†’ negative, go left)")
```

### 3.10 Mode-Covering vs Mode-Seeking

ç¬¬6å›ã§ KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®éå¯¾ç§°æ€§ã‚’å­¦ã‚“ã ã€‚ã“ã“ã§ã¯ãã®çµæœãŒæ¨å®šé‡ã®æŒ™å‹•ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’æ˜ã‚Šä¸‹ã’ã‚‹ã€‚

**å‰å‘ã KLï¼ˆMode-Coveringï¼‰** â€” MLE / VAE

$$D_\text{KL}(p_\text{data} \| q_\theta) = \mathbb{E}_{p_\text{data}}\left[\log \frac{p_\text{data}(x)}{q_\theta(x)}\right]$$

$p_\text{data}(x) > 0$ ã®å ´æ‰€ã§ $q_\theta(x) \approx 0$ ã ã¨ $\log \frac{p}{q} \to \infty$ â€” **ãƒšãƒŠãƒ«ãƒ†ã‚£å¤§**ã€‚
â†’ $q_\theta$ ã¯ $p_\text{data}$ ã®å…¨ãƒ¢ãƒ¼ãƒ‰ã‚’ã‚«ãƒãƒ¼ã—ã‚ˆã†ã¨ã™ã‚‹ï¼ˆmode-coveringï¼‰ã€‚
â†’ çµæœ: ã¼ã‚„ã‘ã‚‹ãŒã€å…¨ãƒ¢ãƒ¼ãƒ‰ã‚’å«ã‚€ã€‚

**é€†å‘ã KLï¼ˆMode-Seekingï¼‰** â€” GANï¼ˆå®Ÿè³ªçš„ã«ï¼‰

$$D_\text{KL}(q_\theta \| p_\text{data}) = \mathbb{E}_{q_\theta}\left[\log \frac{q_\theta(x)}{p_\text{data}(x)}\right]$$

$q_\theta(x) > 0$ ã®å ´æ‰€ã§ $p_\text{data}(x) \approx 0$ ã ã¨ $\log \frac{q}{p} \to \infty$ â€” **ãƒšãƒŠãƒ«ãƒ†ã‚£å¤§**ã€‚
â†’ $q_\theta$ ã¯ $p_\text{data}$ ã®ãƒ¢ãƒ¼ãƒ‰ã®ä¸Šã ã‘ã«é›†ä¸­ã™ã‚‹ï¼ˆmode-seekingï¼‰ã€‚
â†’ çµæœ: é®®æ˜ã ãŒã€ä¸€éƒ¨ã®ãƒ¢ãƒ¼ãƒ‰ã‚’ç„¡è¦–ã™ã‚‹ï¼ˆmode collapseï¼‰ã€‚

```python
import numpy as np
from scipy import stats

# Demonstration: mode-covering vs mode-seeking
np.random.seed(42)
x = np.linspace(-6, 10, 1000)

# True distribution: bimodal
p_true = 0.5 * stats.norm.pdf(x, 0, 1) + 0.5 * stats.norm.pdf(x, 6, 1)

# Mode-covering (forward KL / MLE): tries to cover both modes
# â†’ single Gaussian spreads wide
q_covering = stats.norm.pdf(x, 3, 3.5)

# Mode-seeking (reverse KL): locks onto one mode
q_seeking = stats.norm.pdf(x, 0, 1.0)

# Compute KLs
def kl_numerical(p, q, x_grid):
    mask = (p > 1e-10) & (q > 1e-10)
    return np.trapz(p[mask] * np.log(p[mask] / q[mask]), x_grid[mask])

kl_forward_covering = kl_numerical(p_true, q_covering, x)
kl_forward_seeking = kl_numerical(p_true, q_seeking, x)
kl_reverse_covering = kl_numerical(q_covering, p_true, x)
kl_reverse_seeking = kl_numerical(q_seeking, p_true, x)

print("Mode-Covering (wide Gaussian, Î¼=3, Ïƒ=3.5):")
print(f"  Forward KL  D(p||q): {kl_forward_covering:.4f}")
print(f"  Reverse KL  D(q||p): {kl_reverse_covering:.4f}")
print()
print("Mode-Seeking (narrow Gaussian, Î¼=0, Ïƒ=1.0):")
print(f"  Forward KL  D(p||q): {kl_forward_seeking:.4f}")
print(f"  Reverse KL  D(q||p): {kl_reverse_seeking:.4f}")
print()
print("â†’ Mode-covering has LOWER forward KL (MLE prefers it)")
print("â†’ Mode-seeking has LOWER reverse KL (GAN-style prefers it)")
```

:::message
**å¼•ã£ã‹ã‹ã‚Šãƒã‚¤ãƒ³ãƒˆ**: GAN ãŒã€Œé€†å‘ã KL ã‚’æœ€å°åŒ–ã™ã‚‹ã€ã¨æ›¸ã„ãŸãŒã€å³å¯†ã«ã¯ GAN ã¯ JSD ã‚’æœ€å°åŒ–ã™ã‚‹ã€‚JSD ã¯ KL ã®å¯¾ç§°åŒ–ç‰ˆã§ã€forward ã¨ reverse ã®ä¸­é–“çš„ãªæŒ¯ã‚‹èˆã„ã‚’ã™ã‚‹ã€‚ãã‚Œã§ã‚‚ GAN ãŒ mode-seeking ã«ãªã‚Šã‚„ã™ã„ã®ã¯ã€åˆ¤åˆ¥å™¨ã®å‹•æ…‹ãŒé€†å‘ã KL çš„ãªåœ§åŠ›ã‚’ç”Ÿã‚€ãŸã‚ã ã€‚ã“ã®å¾®å¦™ãªé•ã„ã¯ç¬¬12å›ï¼ˆGAN ã®ç†è«–ï¼‰ã§è©³ã—ãæ‰±ã†ã€‚
:::

### 3.11 äº‹å¾Œåˆ†å¸ƒã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç†è«–

æ¨å®šé‡ã§å­¦ç¿’ã—ãŸåˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã«ã¯ã€äº‹å¾Œåˆ†å¸ƒã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç†è«–ãŒå¿…è¦ã ã€‚ä¸»è¦ãªæ‰‹æ³•ã‚’æ•´ç†ã™ã‚‹ã€‚

| ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ‰‹æ³• | åŸç† | åˆ©ç”¨ãƒ¢ãƒ‡ãƒ« | è¨ˆç®—ã‚³ã‚¹ãƒˆ |
|:----------------|:-----|:-----------|:-----------|
| **ç¥–å…ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°** | åŒæ™‚åˆ†å¸ƒã‚’æ¡ä»¶ä»˜ãåˆ†è§£ | è‡ªå·±å›å¸°ï¼ˆGPTï¼‰ | $O(T)$ é€æ¬¡ |
| **Rejection Sampling** | ææ¡ˆåˆ†å¸ƒã‹ã‚‰å€™è£œç”Ÿæˆ â†’ æ£„å´ | ç†è«–çš„ | é«˜æ¬¡å…ƒã§æŒ‡æ•°çš„ |
| **Importance Sampling** | é‡ã¿ä»˜ãã‚µãƒ³ãƒ—ãƒ« | VAE ã® IWAE | $O(K \cdot N)$ |
| **MCMC** | Markov Chain ã§å®šå¸¸åˆ†å¸ƒã«åæŸ | ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ¢ãƒ‡ãƒ« | åæŸä¿è¨¼ãªã— |
| **Reparameterization** | $z = \mu + \sigma \cdot \epsilon$ | VAE | $O(1)$ |
| **Langevin Dynamics** | $x_{t+1} = x_t + \eta \nabla_x \log p + \sqrt{2\eta}\epsilon$ | Score Model | $O(T)$ åå¾© |
| **é€†æ‹¡æ•£éç¨‹** | $x_{t-1} \sim p_\theta(x_{t-1}|x_t)$ | Diffusion | $O(T)$ åå¾© |

```python
import numpy as np

# Ancestral sampling from autoregressive model (simplified)
def ancestral_sampling_demo():
    """p(x1, x2, x3) = p(x1) Â· p(x2|x1) Â· p(x3|x1,x2)"""
    x1 = np.random.choice(['A', 'B'], p=[0.7, 0.3])

    # p(x2|x1)
    if x1 == 'A':
        x2 = np.random.choice(['C', 'D'], p=[0.6, 0.4])
    else:
        x2 = np.random.choice(['C', 'D'], p=[0.2, 0.8])

    # p(x3|x1,x2)
    x3 = np.random.choice(['E', 'F'], p=[0.5, 0.5])

    return x1 + x2 + x3

# Reparameterization trick
def reparameterization_demo(mu, sigma, n_samples=5):
    """z = Î¼ + Ïƒ Â· Îµ, Îµ ~ N(0,1) â€” gradient flows through Î¼ and Ïƒ"""
    epsilon = np.random.normal(0, 1, n_samples)
    z = mu + sigma * epsilon
    return z

# Langevin dynamics
def langevin_sampling(score_fn, x_init, step_size=0.01, n_steps=100):
    """x_{t+1} = x_t + Î· Â· âˆ‡_x log p(x_t) + âˆš(2Î·) Â· Îµ"""
    x = x_init.copy()
    trajectory = [x.copy()]
    for _ in range(n_steps):
        noise = np.random.normal(0, 1, x.shape)
        x = x + step_size * score_fn(x) + np.sqrt(2 * step_size) * noise
        trajectory.append(x.copy())
    return np.array(trajectory)

# Demo: Langevin sampling from N(2, 1)
score_fn = lambda x: -(x - 2.0)  # score of N(2, 1)
x_init = np.array([10.0])        # start far away
traj = langevin_sampling(score_fn, x_init, step_size=0.05, n_steps=200)

print(f"Langevin dynamics: start at x = {x_init[0]:.1f}")
print(f"  After 50 steps:  x = {traj[50, 0]:.3f}")
print(f"  After 100 steps: x = {traj[100, 0]:.3f}")
print(f"  After 200 steps: x = {traj[200, 0]:.3f}")
print(f"  Target: N(2, 1)")
```

### 3.12 çµ±è¨ˆçš„è·é›¢ã®å¿œç”¨ â€” æ¨å®šé‡ã®è©•ä¾¡æŒ‡æ¨™

æ¨å®šé‡ã®å“è³ªã‚’æ•°å­¦çš„ã«ã©ã†æ¸¬ã‚‹ã‹ã€‚ã“ã‚Œã¯çµ±è¨ˆçš„è·é›¢ã®å¿œç”¨å•é¡Œã ã€‚ä¸»è¦ãªæŒ‡æ¨™ã‚’æ•°å­¦çš„ã«å®šç¾©ã™ã‚‹ã€‚

**å®šç¾© 3.12aï¼ˆFrechet Inception Distance, FIDï¼‰** [^4]

$$\text{FID} = \|\mu_r - \mu_g\|^2 + \text{Tr}\left(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2}\right)$$

ã“ã“ã§ $(\mu_r, \Sigma_r)$ ã¨ $(\mu_g, \Sigma_g)$ ã¯ãã‚Œãã‚Œå®Ÿç”»åƒã¨ç”Ÿæˆç”»åƒã® Inception-v3 ç‰¹å¾´ç©ºé–“ã§ã®å¹³å‡ã¨å…±åˆ†æ•£ã€‚

FID ã¯2ã¤ã®ã‚¬ã‚¦ã‚¹åˆ†å¸ƒé–“ã® **Frechet è·é›¢**ï¼ˆWasserstein-2 è·é›¢ï¼‰:

$$W_2^2(\mathcal{N}(\mu_1, \Sigma_1), \mathcal{N}(\mu_2, \Sigma_2)) = \|\mu_1 - \mu_2\|^2 + \text{Tr}(\Sigma_1 + \Sigma_2 - 2(\Sigma_1\Sigma_2)^{1/2})$$

```python
import numpy as np

def compute_fid(mu1, sigma1, mu2, sigma2):
    """Frechet Inception Distance between two Gaussian distributions"""
    diff = mu1 - mu2

    # Matrix square root via eigendecomposition
    # (Î£â‚Î£â‚‚)^{1/2}
    product = sigma1 @ sigma2
    eigvals, eigvecs = np.linalg.eigh(product)
    eigvals = np.maximum(eigvals, 0)  # numerical stability
    sqrt_product = eigvecs @ np.diag(np.sqrt(eigvals)) @ eigvecs.T

    fid = np.dot(diff, diff) + np.trace(sigma1 + sigma2 - 2 * sqrt_product)
    return fid
    # NOTE: This computes (Î£â‚Î£â‚‚)^{1/2} via eigh, which assumes the product is
    # symmetric. The exact FrÃ©chet distance uses (Î£â‚^{1/2} Î£â‚‚ Î£â‚^{1/2})^{1/2},
    # which is always symmetric positive semi-definite. When Î£â‚ and Î£â‚‚ commute
    # (or are close), the two coincide. For production use, prefer scipy.linalg.sqrtm.

# Example: 2D feature space
np.random.seed(42)
d = 2

# Real data statistics
mu_r = np.array([1.0, 2.0])
sigma_r = np.array([[1.0, 0.3], [0.3, 0.8]])

# Generated data statistics (progressively improving)
models = {
    "Random":    (np.array([5.0, 5.0]), np.eye(2) * 3),
    "Epoch 10":  (np.array([2.0, 3.0]), np.array([[1.5, 0.2], [0.2, 1.2]])),
    "Epoch 100": (np.array([1.1, 2.1]), np.array([[1.1, 0.35], [0.35, 0.85]])),
    "Converged": (np.array([1.0, 2.0]), np.array([[1.0, 0.3], [0.3, 0.8]])),
}

print(f"{'Model':>12} {'FID':>10}")
print("-" * 25)
for name, (mu_g, sigma_g) in models.items():
    fid = compute_fid(mu_r, sigma_r, mu_g, sigma_g)
    print(f"{name:>12} {fid:10.4f}")
```

**å®šç¾© 3.12bï¼ˆKID: Kernel Inception Distanceï¼‰**

FID ã®ã‚¬ã‚¦ã‚¹ä»®å®šã‚’ç·©å’Œã—ãŸã€ã‚«ãƒ¼ãƒãƒ«ãƒ™ãƒ¼ã‚¹ã®çµ±è¨ˆçš„è·é›¢ã€‚MMDï¼ˆMaximum Mean Discrepancyï¼‰ã‚’ Inception ç‰¹å¾´ç©ºé–“ã§è¨ˆç®—ã™ã‚‹:

$$\text{KID} = \text{MMD}^2_k(\{r_i\}, \{g_j\}) = \frac{1}{\binom{n}{2}}\sum_{i \neq j}k(r_i, r_j) + \frac{1}{\binom{m}{2}}\sum_{i \neq j}k(g_i, g_j) - \frac{2}{nm}\sum_{i,j}k(r_i, g_j)$$

FID ã¨ç•°ãªã‚Šä¸åæ¨å®šé‡ã§ã‚ã‚Šã€ã‚µãƒ³ãƒ—ãƒ«æ•°ã¸ã®ä¾å­˜ãŒå°ã•ã„ã€‚

**å®šç¾© 3.12cï¼ˆCMMDï¼‰** [^9]

Jayasumana+ (2024) ã¯ FID ã®å•é¡Œç‚¹ï¼ˆã‚¬ã‚¦ã‚¹ä»®å®šã€Inception-v3 ã®æ—§ã•ï¼‰ã‚’æŒ‡æ‘˜ã—ã€CLIP ç‰¹å¾´ç©ºé–“ã§ã® **Maximum Mean Discrepancy (MMD)** ã‚’ææ¡ˆã—ãŸ:

$$\text{CMMD}^2 = \frac{1}{n^2}\sum_{i,j}k(r_i, r_j) + \frac{1}{m^2}\sum_{i,j}k(g_i, g_j) - \frac{2}{nm}\sum_{i,j}k(r_i, g_j)$$

ã“ã“ã§ $k$ ã¯ã‚¬ã‚¦ã‚¹ RBF ã‚«ãƒ¼ãƒãƒ«ã€$r_i, g_j$ ã¯ CLIP ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã€‚

çµ±è¨ˆçš„è·é›¢ã®æ¯”è¼ƒ:

| ç‰¹æ€§ | FID [^4] | KID | CMMD [^9] |
|:-----|:---------|:--------|:----------|
| æ•°å­¦çš„åŸºç›¤ | $W_2$ è·é›¢ï¼ˆã‚¬ã‚¦ã‚¹è¿‘ä¼¼ï¼‰ | $\text{MMD}^2$ï¼ˆInceptionç©ºé–“ï¼‰ | $\text{MMD}^2$ï¼ˆCLIPç©ºé–“ï¼‰ |
| åˆ†å¸ƒä»®å®š | ã‚¬ã‚¦ã‚¹ | ãªã—ï¼ˆã‚«ãƒ¼ãƒãƒ«ï¼‰ | ãªã—ï¼ˆã‚«ãƒ¼ãƒãƒ«ï¼‰ |
| ãƒã‚¤ã‚¢ã‚¹ | ã‚ã‚Šï¼ˆ$N$ ã«ä¾å­˜ï¼‰ | **ä¸åæ¨å®šé‡** | **ä¸åæ¨å®šé‡** |
| äººé–“ã®åˆ¤æ–­ã¨ã®ç›¸é–¢ | ä¸­ç¨‹åº¦ | ä¸­ã€œé«˜ | **é«˜ã„** |
| è¨ˆç®—ã‚³ã‚¹ãƒˆ | $O(d^3)$ï¼ˆå…±åˆ†æ•£ã®å›ºæœ‰å€¤ï¼‰ | $O(N^2 d)$ | $O(N^2 d)$ |

### 3.13 LLM ã¨æœ€å°¤æ¨å®š â€” æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬

æœ¬è¬›ç¾©ã® LLM æ¥ç¶šã‚’æ˜ç¢ºã«ã—ã¦ãŠã“ã†ã€‚GPT ç³»ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã¯**è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«**ã§ã‚ã‚Šã€MLE ã§è¨“ç·´ã•ã‚Œã‚‹ï¼ˆæ˜ç¤ºçš„æ¨å®šé‡ã®ä»£è¡¨ä¾‹ï¼‰ã€‚

$$p_\theta(x_1, x_2, \ldots, x_T) = \prod_{t=1}^{T} p_\theta(x_t | x_1, \ldots, x_{t-1})$$

è¨“ç·´ã®æå¤±é–¢æ•°:

$$\mathcal{L}(\theta) = -\frac{1}{T}\sum_{t=1}^{T} \log p_\theta(x_t | x_{<t})$$

ã“ã‚Œã¯**Cross-Entropy æå¤±**ãã®ã‚‚ã®ã§ã‚ã‚Šã€å®šç† 3.2 ã‹ã‚‰ MLE ã¨ç­‰ä¾¡ã€‚

```python
import numpy as np

# Simplified next-token prediction
vocab_size = 50000
sequence = [42, 1337, 7, 256, 99]  # token IDs

# Model output: logits â†’ softmax â†’ p(x_t | x_{<t})
def softmax(logits):
    exp_logits = np.exp(logits - np.max(logits))
    return exp_logits / exp_logits.sum()

def cross_entropy_loss(predictions, targets):
    """CE loss = -mean(log p(x_t | x_{<t}))"""
    total_loss = 0
    for pred_logits, target in zip(predictions, targets):
        probs = softmax(pred_logits)
        total_loss += -np.log(probs[target] + 1e-10)
    return total_loss / len(targets)

# Simulate model predictions (random logits)
np.random.seed(42)
predictions = [np.random.randn(vocab_size) for _ in range(len(sequence) - 1)]
targets = sequence[1:]  # next token at each position

loss = cross_entropy_loss(predictions, targets)
perplexity = np.exp(loss)

print(f"Sequence: {sequence}")
print(f"Cross-Entropy Loss: {loss:.4f}")
print(f"Perplexity: {perplexity:.2f}")
print(f"â†’ PPL = exp(CE) = 2^(CE/log2) = {2**(loss/np.log(2)):.2f}")
print(f"â†’ Random baseline PPL â‰ˆ vocab_size = {vocab_size}")
print(f"\nThis is EXACTLY what GPT training does:")
print(f"  minimize CE = maximize log-likelihood = minimize KL(p_data || q_Î¸)")
```

:::message
**é€²æ—: 50% å®Œäº†** â€” MLE ã®ç†è«–ã€æ¨å®šé‡ã®åˆ†é¡ä½“ç³»ã€è©•ä¾¡æŒ‡æ¨™ã®æ•°å­¦ã‚’æ”»ç•¥ã—ãŸã€‚ã“ã“ã‹ã‚‰å®Ÿè£…ã‚¾ãƒ¼ãƒ³ã«å…¥ã‚‹ã€‚
:::

### 3.14 ãƒœã‚¹æˆ¦ â€” MLE = CE = KL ã®ä¸‰ä½ä¸€ä½“

å…¨ã¦ã‚’çµ±åˆã™ã‚‹ã€‚

$$\underbrace{\hat{\theta}_\text{MLE}}_\text{MLE} = \arg\max_\theta \underbrace{\frac{1}{N}\sum_{i=1}^{N} \log q_\theta(x_i)}_\text{å¹³å‡å¯¾æ•°å°¤åº¦} = \arg\min_\theta \underbrace{H(\hat{p}, q_\theta)}_\text{Cross-Entropy} = \arg\min_\theta \underbrace{D_\text{KL}(\hat{p} \| q_\theta)}_\text{KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹}$$

å„é …ã®æ„å‘³:

| è¡¨ç¾ | è¦–ç‚¹ | ç›´æ„Ÿ |
|:-----|:-----|:-----|
| $\arg\max_\theta \frac{1}{N}\sum \log q_\theta(x_i)$ | **çµ±è¨ˆå­¦** | ãƒ‡ãƒ¼ã‚¿ã‚’æœ€ã‚‚ã€Œã‚‚ã£ã¨ã‚‚ã‚‰ã—ãã€èª¬æ˜ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ |
| $\arg\min_\theta H(\hat{p}, q_\theta)$ | **æƒ…å ±ç†è«–** | ãƒ¢ãƒ‡ãƒ«ã§ãƒ‡ãƒ¼ã‚¿ã‚’ç¬¦å·åŒ–ã™ã‚‹ã‚³ã‚¹ãƒˆã®æœ€å°åŒ– |
| $\arg\min_\theta D_\text{KL}(\hat{p} \| q_\theta)$ | **ç¢ºç‡è«–** | åˆ†å¸ƒé–“ã®æƒ…å ±æå¤±ã®æœ€å°åŒ– |

$$\boxed{\text{LLM è¨“ç·´} = \text{æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã® CE æœ€å°åŒ–} = \text{è¨€èªã® MLE} = \text{KL æœ€å°åŒ–}}$$

```python
import numpy as np

# Boss battle: verify the trinity numerically
np.random.seed(42)

# True distribution: N(3, 2Â²)
true_mu, true_sigma = 3.0, 2.0
N = 100000
data = np.random.normal(true_mu, true_sigma, N)

# Empirical entropy H(pÌ‚)
H_p = 0.5 * np.log(2 * np.pi * np.e * np.var(data))

# Scan Î¸ = (Î¼, Ïƒ=2 fixed)
mus = np.linspace(0, 6, 200)
results = {"mu": [], "avg_ll": [], "CE": [], "KL": []}

for mu in mus:
    sigma = 2.0
    # Average log-likelihood
    ll = np.mean(-0.5 * np.log(2 * np.pi * sigma**2) -
                  0.5 * ((data - mu) / sigma)**2)
    ce = -ll
    kl = ce - H_p

    results["mu"].append(mu)
    results["avg_ll"].append(ll)
    results["CE"].append(ce)
    results["KL"].append(kl)

# Find optima
i_max_ll = np.argmax(results["avg_ll"])
i_min_ce = np.argmin(results["CE"])
i_min_kl = np.argmin(results["KL"])

print("=== The Trinity ===")
print(f"argmax avg-log-likelihood: Î¼ = {results['mu'][i_max_ll]:.4f}")
print(f"argmin Cross-Entropy:      Î¼ = {results['mu'][i_min_ce]:.4f}")
print(f"argmin KL divergence:      Î¼ = {results['mu'][i_min_kl]:.4f}")
print(f"Sample mean (analytical):  Î¼ = {np.mean(data):.4f}")
print(f"\nAll identical: {i_max_ll == i_min_ce == i_min_kl}")
print(f"\nAt optimum:")
print(f"  Max avg log-lik:  {results['avg_ll'][i_max_ll]:.6f}")
print(f"  Min CE:           {results['CE'][i_min_ce]:.6f}")
print(f"  Min KL:           {results['KL'][i_min_kl]:.6f}")
print(f"  H(pÌ‚):            {H_p:.6f}")
print(f"  CE - H(pÌ‚) = KL:  {results['CE'][i_min_ce] - H_p:.6f} = {results['KL'][i_min_kl]:.6f}")
```

:::message
ãƒœã‚¹æ’ƒç ´ã€‚MLE = CE = KL ã®ä¸‰ä½ä¸€ä½“ã‚’æ•°å€¤çš„ã«ç¢ºèªã—ãŸã€‚ã“ã®ç­‰ä¾¡æ€§ã¯ç¢ºç‡ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®å…¨ã¦ã«é€šåº•ã™ã‚‹åŸç†ã ã€‚
:::

---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” MLE å®Ÿè£…ã¨æ¨å®šé‡ã®å®Ÿè·µ

### 4.1 MLE ã®å®Œå…¨å®Ÿè£… â€” ã‚¬ã‚¦ã‚¹æ··åˆãƒ¢ãƒ‡ãƒ«

Zone 0 ã§å˜å³°ã‚¬ã‚¦ã‚¹ã®é™ç•Œã‚’è¦‹ãŸã€‚ã“ã“ã§ã¯æ··åˆãƒ¢ãƒ‡ãƒ«ã® MLE ã‚’å®Ÿè£…ã™ã‚‹ã€‚

```python
import numpy as np

class GaussianMixtureMLE:
    """
    Gaussian Mixture Model with EM algorithm for MLE.
    p(x) = Î£_k Ï€_k Â· N(x; Î¼_k, Ïƒ_kÂ²)
    """
    def __init__(self, n_components):
        self.K = n_components
        self.mus = None
        self.sigmas = None
        self.pis = None

    def initialize(self, data):
        """K-means++ style initialization"""
        N = len(data)
        # Random initialization
        indices = np.random.choice(N, self.K, replace=False)
        self.mus = data[indices].copy()
        self.sigmas = np.full(self.K, np.std(data))
        self.pis = np.full(self.K, 1.0 / self.K)

    def e_step(self, data):
        """E-step: compute responsibilities Î³(z_nk)"""
        N = len(data)
        gamma = np.zeros((N, self.K))
        for k in range(self.K):
            gamma[:, k] = self.pis[k] * self._gaussian(data, self.mus[k], self.sigmas[k])
        # Normalize
        gamma_sum = gamma.sum(axis=1, keepdims=True)
        gamma /= (gamma_sum + 1e-300)
        return gamma

    def m_step(self, data, gamma):
        """M-step: update parameters using responsibilities"""
        N = len(data)
        N_k = gamma.sum(axis=0)  # effective number per component

        for k in range(self.K):
            # Update means
            self.mus[k] = np.sum(gamma[:, k] * data) / (N_k[k] + 1e-10)
            # Update variances
            diff = data - self.mus[k]
            self.sigmas[k] = np.sqrt(np.sum(gamma[:, k] * diff**2) / (N_k[k] + 1e-10))
            self.sigmas[k] = max(self.sigmas[k], 1e-6)  # prevent singularity
            # Update mixing coefficients
            self.pis[k] = N_k[k] / N

    def log_likelihood(self, data):
        """Compute log p(D|Î¸) = Î£_n log Î£_k Ï€_k N(x_n; Î¼_k, Ïƒ_kÂ²)"""
        N = len(data)
        ll = 0
        for n in range(N):
            p_n = sum(self.pis[k] * self._gaussian(data[n:n+1], self.mus[k], self.sigmas[k])[0]
                      for k in range(self.K))
            ll += np.log(p_n + 1e-300)
        return ll

    def fit(self, data, max_iter=100, tol=1e-6):
        """EM algorithm for MLE"""
        self.initialize(data)
        prev_ll = -np.inf
        history = []

        for iteration in range(max_iter):
            # E-step
            gamma = self.e_step(data)
            # M-step
            self.m_step(data, gamma)
            # Log-likelihood
            ll = self.log_likelihood(data)
            history.append(ll)

            if abs(ll - prev_ll) < tol:
                print(f"Converged at iteration {iteration + 1}")
                break
            prev_ll = ll

        return history

    @staticmethod
    def _gaussian(x, mu, sigma):
        return np.exp(-0.5 * ((x - mu) / sigma)**2) / (sigma * np.sqrt(2 * np.pi))


# Demonstration
np.random.seed(42)

# True distribution: 3-component mixture
true_params = {
    'mus': [-3, 0, 4],
    'sigmas': [0.5, 1.0, 0.7],
    'pis': [0.3, 0.4, 0.3]
}

# Sample data
N = 2000
components = np.random.choice(3, size=N, p=true_params['pis'])
data = np.array([np.random.normal(true_params['mus'][c], true_params['sigmas'][c])
                 for c in components])

# Fit GMM
gmm = GaussianMixtureMLE(n_components=3)
history = gmm.fit(data)

print(f"\nTrue parameters:")
for k in range(3):
    print(f"  Component {k}: Ï€={true_params['pis'][k]:.2f}, "
          f"Î¼={true_params['mus'][k]:.2f}, Ïƒ={true_params['sigmas'][k]:.2f}")

print(f"\nEstimated parameters:")
order = np.argsort(gmm.mus)  # sort by mean
for i, k in enumerate(order):
    print(f"  Component {i}: Ï€={gmm.pis[k]:.2f}, "
          f"Î¼={gmm.mus[k]:.2f}, Ïƒ={gmm.sigmas[k]:.2f}")

print(f"\nFinal log-likelihood: {history[-1]:.2f}")
print(f"Iterations: {len(history)}")
```

### 4.2 Mathâ†’Code ç¿»è¨³ãƒ‘ã‚¿ãƒ¼ãƒ³

| æ•°å¼ | Python | æ„å‘³ |
|:-----|:-------|:-----|
| $\prod_{i=1}^{N} q_\theta(x_i)$ | `np.prod(q_theta(data))` | å°¤åº¦ï¼ˆæ•°å€¤çš„ã«ä¸å®‰å®šï¼‰ |
| $\sum_{i=1}^{N} \log q_\theta(x_i)$ | `np.sum(np.log(q_theta(data)))` | å¯¾æ•°å°¤åº¦ï¼ˆã“ã¡ã‚‰ã‚’ä½¿ã†ï¼‰ |
| $\hat{\theta} = \arg\max_\theta$ | `theta[np.argmax(ll)]` or gradient | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®š |
| $\frac{1}{N}\sum \log q_\theta(x_i)$ | `np.mean(np.log(q_theta(data)))` | å¹³å‡å¯¾æ•°å°¤åº¦ |
| $\mathcal{N}(x; \mu, \sigma^2)$ | `np.exp(-0.5*((x-mu)/sigma)**2) / (sigma*np.sqrt(2*np.pi))` | ã‚¬ã‚¦ã‚¹å¯†åº¦ |
| $\gamma_{nk} = \frac{\pi_k q_k(x_n)}{\sum_j \pi_j q_j(x_n)}$ | `gamma[:, k] / gamma.sum(axis=1)` | è²¬ä»»åº¦ |
| $D_\text{KL}(p \| q)$ | `np.sum(p * np.log(p / q))` | KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ |
| $H(p, q) = -\mathbb{E}_p[\log q]$ | `-np.mean(np.log(q_theta(data)))` | Cross-Entropy |
| $\text{FID}$ | `||mu1-mu2||Â² + Tr(Î£1+Î£2-2âˆš(Î£1Î£2))` | ç”Ÿæˆå“è³ª |
| $\text{PPL} = \exp(\mathcal{L})$ | `np.exp(loss)` | Perplexity |

### 4.3 PyTorch å®Ÿè£…ã¨ã®å¯¾å¿œ

:::details PyTorch ã§ã® MLE å®Ÿè£…

```python
import torch
import torch.nn as nn
import torch.optim as optim

class SimpleGenerativeModel(nn.Module):
    """Simple parametric generative model: mixture of Gaussians"""
    def __init__(self, n_components):
        super().__init__()
        self.K = n_components
        self.mus = nn.Parameter(torch.randn(n_components))
        self.log_sigmas = nn.Parameter(torch.zeros(n_components))
        self.logits = nn.Parameter(torch.zeros(n_components))

    def log_prob(self, x):
        """log q_Î¸(x) = log Î£_k Ï€_k N(x; Î¼_k, Ïƒ_kÂ²)"""
        sigmas = torch.exp(self.log_sigmas)
        pis = torch.softmax(self.logits, dim=0)

        # (N, K) matrix of log-probabilities
        x = x.unsqueeze(1)  # (N, 1)
        log_probs = (-0.5 * ((x - self.mus) / sigmas)**2
                     - torch.log(sigmas)
                     - 0.5 * torch.log(torch.tensor(2 * torch.pi)))
        log_pis = torch.log(pis)

        # Log-sum-exp trick for numerical stability
        return torch.logsumexp(log_probs + log_pis, dim=1)

    def sample(self, n):
        """Sample from q_Î¸(x)"""
        with torch.no_grad():
            sigmas = torch.exp(self.log_sigmas)
            pis = torch.softmax(self.logits, dim=0)
            components = torch.multinomial(pis, n, replacement=True)
            samples = torch.randn(n) * sigmas[components] + self.mus[components]
        return samples

# Training loop: MLE via gradient descent
# model = SimpleGenerativeModel(3)
# optimizer = optim.Adam(model.parameters(), lr=0.01)
# for epoch in range(1000):
#     nll = -model.log_prob(data).mean()  # negative log-likelihood
#     optimizer.zero_grad()
#     nll.backward()
#     optimizer.step()
print("PyTorch MLE = minimize negative log-likelihood via Adam")
print("This is EXACTLY how LLM training works (with Cross-Entropy loss)")
```
:::

### 4.4 MLE ã®é€Ÿåº¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ â€” Python ã®é™ç•Œ

:::message alert
ã“ã“ã‹ã‚‰ Python ã®é…ã•ãŒæœ¬æ ¼çš„ã«è¦‹ãˆå§‹ã‚ã‚‹ã€‚ç¬¬9-10å›ã§ã€Œã‚‚ã†é™ç•Œã€ã¨æ„Ÿã˜ã‚‹ä¼ç·šã ã€‚
:::

```python
import numpy as np
import time

def benchmark_mle_python(N, D, K, n_iter=50):
    """
    Benchmark: GMM MLE (EM algorithm) in pure Python/NumPy
    N: number of data points
    D: dimensionality
    K: number of components
    """
    np.random.seed(42)

    # Generate D-dimensional data
    data = np.random.randn(N, D)
    mus = np.random.randn(K, D)
    sigmas = np.ones((K, D))
    pis = np.ones(K) / K

    start = time.perf_counter()

    for iteration in range(n_iter):
        # E-step: compute responsibilities
        gamma = np.zeros((N, K))
        for k in range(K):
            diff = data - mus[k]  # (N, D)
            exponent = -0.5 * np.sum(diff**2 / sigmas[k]**2, axis=1)
            norm_const = np.prod(sigmas[k]) * (2 * np.pi) ** (D / 2)
            gamma[:, k] = pis[k] * np.exp(exponent) / norm_const

        gamma_sum = gamma.sum(axis=1, keepdims=True)
        gamma /= (gamma_sum + 1e-300)

        # M-step
        N_k = gamma.sum(axis=0)
        for k in range(K):
            w = gamma[:, k:k+1]  # (N, 1)
            mus[k] = (w * data).sum(axis=0) / (N_k[k] + 1e-10)
            diff = data - mus[k]
            sigmas[k] = np.sqrt((w * diff**2).sum(axis=0) / (N_k[k] + 1e-10))
            sigmas[k] = np.maximum(sigmas[k], 1e-6)
            pis[k] = N_k[k] / N

    elapsed = time.perf_counter() - start
    return elapsed

# Benchmark across scales
print(f"{'N':>8} {'D':>4} {'K':>4} {'Time (s)':>10} {'iter/s':>10}")
print("-" * 42)

configs = [
    (1000,   10,  3),
    (5000,   10,  3),
    (10000,  10,  5),
    (10000,  50,  5),
    (50000,  10,  5),
    (10000, 100, 10),
]

for N, D, K in configs:
    t = benchmark_mle_python(N, D, K, n_iter=50)
    print(f"{N:8d} {D:4d} {K:4d} {t:10.4f} {50/t:10.1f}")
```

**å‡ºåŠ›ä¾‹:**
```
       N    D    K   Time (s)    iter/s
------------------------------------------
    1000   10    3     0.0321    1557.6
    5000   10    3     0.1205     415.0
   10000   10    5     0.3812     131.2
   10000   50    5     0.7834      63.8
   50000   10    5     1.8921      26.4
   10000  100   10     2.4567      20.4
```

```python
# The Python problem: scaling
print("\n=== Python's Scaling Problem ===")
print("10K points, 100D, 10 components: ~2.5 seconds for 50 iterations")
print("Real-world: 100K+ images, 512D embeddings, 100+ components")
print("Estimated time: ~250 seconds = 4+ minutes per EM run")
print("\nFor neural network-based models (VAE, GAN, Diffusion):")
print("  Training = 1000s of gradient steps Ã— forward + backward")
print("  Python overhead becomes DOMINANT bottleneck")
print("\nâ†’ Lecture 9-10: Julia debut for compute-heavy tasks")
print("â†’ Lecture 11-14: Rust for performance-critical kernels")
```

### 4.5 FIDï¼ˆçµ±è¨ˆçš„è·é›¢ï¼‰è¨ˆç®—ã®å®Ÿè£…

```python
import numpy as np

def compute_fid_full(real_features, gen_features):
    """
    Compute FID between two sets of features.

    Math: FID = ||Î¼_r - Î¼_g||Â² + Tr(Î£_r + Î£_g - 2(Î£_rÂ·Î£_g)^{1/2})

    In practice, features come from Inception-v3's pool3 layer (2048-dim).
    Here we work with arbitrary features for demonstration.
    """
    # Statistics
    mu_r = real_features.mean(axis=0)
    mu_g = gen_features.mean(axis=0)
    sigma_r = np.cov(real_features, rowvar=False)
    sigma_g = np.cov(gen_features, rowvar=False)

    # Mean difference term
    diff = mu_r - mu_g
    mean_term = np.dot(diff, diff)

    # Matrix square root via eigendecomposition
    product = sigma_r @ sigma_g
    eigvals, eigvecs = np.linalg.eigh(product)
    eigvals = np.maximum(eigvals, 0)  # clip negative eigenvalues
    sqrt_product = eigvecs @ np.diag(np.sqrt(eigvals)) @ eigvecs.T

    # Trace term
    trace_term = np.trace(sigma_r + sigma_g - 2 * sqrt_product)

    return mean_term + trace_term

# Demo: simulated features (64-dim instead of 2048-dim for speed)
np.random.seed(42)
D = 64
N = 5000

# Real features
real_features = np.random.multivariate_normal(
    mean=np.zeros(D),
    cov=np.eye(D) + 0.1 * np.random.randn(D, D) @ np.random.randn(D, D).T / D,
    size=N
)

# Generated features at different quality levels
quality_levels = {
    "Random noise": np.random.randn(N, D) * 3 + 2,
    "Poor model":   real_features + np.random.randn(N, D) * 2,
    "Good model":   real_features + np.random.randn(N, D) * 0.5,
    "Great model":  real_features + np.random.randn(N, D) * 0.1,
    "Perfect":      real_features + np.random.randn(N, D) * 0.01,
}

print(f"{'Quality':>15} {'FID':>10}")
print("-" * 28)
for name, gen_features in quality_levels.items():
    fid = compute_fid_full(real_features, gen_features)
    print(f"{name:>15} {fid:10.2f}")
```

### 4.6 è«–æ–‡èª­è§£ãƒ•ãƒ­ãƒ¼ï¼ˆ3-Pass Readingï¼‰

```mermaid
graph TD
    P1[Pass 1: é³¥ç°<br>5-10åˆ†] --> P2[Pass 2: æ§‹é€ <br>30-60åˆ†]
    P2 --> P3[Pass 3: å†ç¾<br>æ•°æ™‚é–“]

    P1 -.-> Q1[ã‚¿ã‚¤ãƒˆãƒ«ãƒ»è¦æ—¨ãƒ»å›³è¡¨]
    P2 -.-> Q2[å°å…¥ãƒ»æ‰‹æ³•ãƒ»å®Ÿé¨“ã‚’ç²¾èª­]
    P3 -.-> Q3[å…¨å°å‡ºã‚’è¿½ã„ã€ã‚³ãƒ¼ãƒ‰ã§å†ç¾]

    style P1 fill:#e8f5e9
    style P2 fill:#fff3e0
    style P3 fill:#fce4ec
```

:::details æœ¬è¬›ç¾©ã®è«–æ–‡: Goodfellow+ (2014) "Generative Adversarial Nets" â€” Pass 1 ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

```python
paper_pass1 = {
    "title": "Generative Adversarial Nets",
    "authors": "Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, Bengio",
    "year": 2014,
    "venue": "NeurIPS 2014",
    "arxiv": "1406.2661",

    "problem": "How to train a generative model without explicit density estimation?",
    "approach": "Adversarial training: Generator G vs Discriminator D in minimax game",
    "key_equation": "min_G max_D E[log D(x)] + E[log(1-D(G(z)))]",
    "key_result": "At Nash equilibrium, p_g = p_data (Theorem 1)",
    "connection_to_this_lecture": {
        "MLE": "GAN avoids MLE entirely â€” no likelihood computation needed",
        "KL": "Optimal GAN minimizes JSD, which is symmetric KL variant",
        "Implicit_model": "GAN = canonical implicit model (Mohamed 2016)",
        "Evaluation": "Early GAN evaluation relied on visual inspection â†’ FID came later",
    },

    "5_minute_summary": (
        "Instead of maximizing likelihood, pit two networks against each other. "
        "The generator tries to fool the discriminator, the discriminator tries to "
        "distinguish real from fake. At convergence, the generator perfectly mimics "
        "the data distribution. Brilliant in simplicity, unstable in practice."
    ),

    "questions_for_pass2": [
        "How is the Nash equilibrium proven? (Theorem 1)",
        "What happens when discriminator is too strong?",
        "Why does mode collapse occur in practice?",
        "How does this relate to f-divergence variational bounds?",
    ]
}

for key, val in paper_pass1.items():
    if isinstance(val, dict):
        print(f"\n{key}:")
        for k, v in val.items():
            print(f"  {k}: {v}")
    elif isinstance(val, list):
        print(f"\n{key}:")
        for item in val:
            print(f"  - {item}")
    else:
        print(f"{key}: {val}")
```
:::

### 4.7 æ¨å®šé‡ã®åˆ†é¡ãƒãƒ£ãƒ¼ãƒˆ â€” å®Ÿè£…ã§ã®åˆ¤æ–­ãƒ•ãƒ­ãƒ¼

```mermaid
flowchart TD
    Start[ç¢ºç‡ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ãŒå¿…è¦] --> Q1{å°¤åº¦ q_Î¸ x<br>ã®è¨ˆç®—ãŒå¿…è¦?}

    Q1 -->|Yes| Q2{æ½œåœ¨å¤‰æ•°<br>ã‚’ä½¿ã†?}
    Q1 -->|No| Q3{ã‚µãƒ³ãƒ—ãƒ«å“è³ª<br>ã‚’é‡è¦–?}

    Q2 -->|Yes| VAE[å¤‰åˆ†MLE<br>ELBOæœ€å¤§åŒ–]
    Q2 -->|No| Q4{å¯é€†å¤‰æ›<br>ãŒå¯èƒ½?}

    Q4 -->|Yes| Flow[å¤‰æ•°å¤‰æ›MLE<br>æ­£ç¢ºãªå°¤åº¦]
    Q4 -->|No| AR[è‡ªå·±å›å¸°MLE<br>GPT, PixelCNN]

    Q3 -->|Yes| Q5{è¨“ç·´ã®å®‰å®šæ€§<br>ã‚‚é‡è¦?}
    Q3 -->|No| GAN[æš—é»™çš„æ¨å®šé‡<br>æ•µå¯¾çš„è¨“ç·´]

    Q5 -->|Yes| Diff[ã‚¹ã‚³ã‚¢æ¨å®šé‡<br>DDPM]
    Q5 -->|No| GAN

    VAE -.->|ã¼ã‚„ã‘ã‚‹| ImproveVAE[VQ-VAE, Hierarchical VAE]
    GAN -.->|ä¸å®‰å®š| ImproveGAN[StyleGAN, WGAN-GP]
    Diff -.->|é…ã„| ImproveDiff[DDIM, Consistency Model]
    Flow -.->|è¡¨ç¾åŠ›| ImproveFlow[Glow, Neural ODE]

    style VAE fill:#e8f5e9
    style GAN fill:#fff3e0
    style Flow fill:#e3f2fd
    style Diff fill:#fce4ec
```

:::message
**é€²æ—: 70% å®Œäº†** â€” MLE ã®å®Œå…¨å®Ÿè£…ã€é€Ÿåº¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€FID è¨ˆç®—ã€è«–æ–‡èª­è§£ãƒ•ãƒ­ãƒ¼ã‚’ç¿’å¾—ã—ãŸã€‚ã“ã“ã‹ã‚‰è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆã«å…¥ã‚‹ã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” è‡ªå·±è¨ºæ–­ã¨å®Ÿé¨“

### 5.1 è¨˜å·èª­è§£ãƒ†ã‚¹ãƒˆ

:::details Q1: $\hat{\theta}_\text{MLE} = \arg\max_\theta \sum_{i=1}^{N} \log q_\theta(x_i)$ ã‚’æ—¥æœ¬èªã§èª­ã¿ä¸Šã’ã¦ãã ã•ã„
ã€Œã‚·ãƒ¼ã‚¿ãƒãƒƒãƒˆ MLE ã¯ã€ã‚·ãƒ¼ã‚¿ã«ã¤ã„ã¦ã€$i = 1$ ã‹ã‚‰ $N$ ã¾ã§ã® $\log q_\theta(x_i)$ ã®ç·å’Œã‚’æœ€å¤§åŒ–ã™ã‚‹å¼•æ•°ã€‚ã€
æ„å‘³: ãƒ‡ãƒ¼ã‚¿ã®å¯¾æ•°å°¤åº¦ã‚’æœ€å¤§åŒ–ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å€¤ãŒ MLEã€‚Fisher (1922) [^1] ãŒä½“ç³»åŒ–ã—ãŸæ¨å®šæ³•ã€‚
:::

:::details Q2: $p_\theta(x_1, \ldots, x_T) = \prod_{t=1}^{T} p_\theta(x_t | x_{<t})$ ã¯ä½•ã‚’è¡¨ã™ï¼Ÿ
è‡ªå·±å›å¸°åˆ†è§£ã€‚åŒæ™‚åˆ†å¸ƒã‚’ã€å„æ™‚åˆ»ã®æ¡ä»¶ä»˜ãåˆ†å¸ƒã®ç©ã«åˆ†è§£ã™ã‚‹ã€‚GPT ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã¯ã“ã®å½¢å¼ã§å®šç¾©ã•ã‚Œã‚‹ã€‚$x_t$ ã¯ $t$ ç•ªç›®ã®ãƒˆãƒ¼ã‚¯ãƒ³ã€$x_{<t}$ ã¯ãã‚Œä»¥å‰ã®å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã€‚
:::

:::details Q3: $D^*_G(x) = \frac{p_\text{data}(x)}{p_\text{data}(x) + p_g(x)}$ ã¯ã©ã†ã„ã†æ„å‘³ï¼Ÿ
GAN ã®æœ€é©åˆ¤åˆ¥å™¨ã€‚$p_\text{data}(x)$ ã¨ $p_g(x)$ ã®æ¯”ç‡ã«åŸºã¥ã„ã¦ã€å…¥åŠ›ãŒæœ¬ç‰©ã‹å½ç‰©ã‹ã‚’åˆ¤å®šã™ã‚‹ã€‚$p_g = p_\text{data}$ ã®ã¨ã $D^* = 0.5$ï¼ˆåŒºåˆ¥ä¸èƒ½ï¼‰ã€‚Goodfellow+ (2014) [^2] ã®å®šç†1ã€‚
:::

:::details Q4: $\text{FID} = \|\mu_r - \mu_g\|^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r\Sigma_g)^{1/2})$ ã®å„é …ã¯ï¼Ÿ
ç¬¬1é … $\|\mu_r - \mu_g\|^2$: å¹³å‡ã®å·®ï¼ˆç‰¹å¾´ç©ºé–“ã§ã®ã€Œä½ç½®ãšã‚Œã€ï¼‰ã€‚ç¬¬2é …: å…±åˆ†æ•£ã®å·®ï¼ˆã€Œå½¢çŠ¶ã®é•ã„ã€ï¼‰ã€‚$\text{Tr}$ ã¯ãƒˆãƒ¬ãƒ¼ã‚¹ï¼ˆå¯¾è§’è¦ç´ ã®å’Œï¼‰ã€‚Heusel+ (2017) [^4] ãŒææ¡ˆã€‚ä½ã„ã»ã©è‰¯ã„ã€‚
:::

:::details Q5: $\nabla_x \log p(x)$ ã¯ãªãœæ­£è¦åŒ–å®šæ•°ã«ä¾å­˜ã—ãªã„ï¼Ÿ
$\log p(x) = \log \tilde{p}(x) - \log Z$ã€‚$\nabla_x$ ã§å¾®åˆ†ã™ã‚‹ã¨ $\log Z$ ã¯å®šæ•°ãªã®ã§æ¶ˆãˆã‚‹: $\nabla_x \log p(x) = \nabla_x \log \tilde{p}(x)$ã€‚ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« [^10] ã®æ ¸å¿ƒã€‚
:::

:::details Q6: $\mathcal{L}_\text{simple} = \mathbb{E}_{t, x_0, \epsilon}[\|\epsilon - \epsilon_\theta(x_t, t)\|^2]$ ã¯ã©ã‚“ãªæå¤±ï¼Ÿ
DDPM [^5] ã® simple lossã€‚æ™‚åˆ» $t$ ã§ãƒã‚¤ã‚º $\epsilon$ ã‚’åŠ ãˆãŸ $x_t$ ã‹ã‚‰ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ $\epsilon_\theta$ ãŒãƒã‚¤ã‚ºã‚’äºˆæ¸¬ã™ã‚‹ã€‚äºˆæ¸¬ã¨çœŸã®ãƒã‚¤ã‚ºã® MSE ã‚’æœ€å°åŒ–ã€‚denoising score matching ã¨ç­‰ä¾¡ã€‚
:::

:::details Q7: $\text{IS} = \exp(\mathbb{E}_{x \sim p_g}[D_\text{KL}(p(y|x) \| p(y))])$ ã®ç›´æ„Ÿã¯ï¼Ÿ
å„ç”Ÿæˆç”»åƒã®åˆ†é¡ç¢ºç‡ $p(y|x)$ ãŒé‹­ãï¼ˆå“è³ªãŒé«˜ã„ï¼‰ã€ã‹ã¤å…¨ä½“ã®å‘¨è¾ºåˆ†å¸ƒ $p(y)$ ãŒä¸€æ§˜ã«è¿‘ã„ï¼ˆå¤šæ§˜æ€§ãŒé«˜ã„ï¼‰ã¨ãã€KL ãŒå¤§ãããªã‚Š IS ãŒé«˜ããªã‚‹ã€‚Salimans+ (2016) [^8]ã€‚æœ€å¤§å€¤ã¯ã‚¯ãƒ©ã‚¹æ•°ã€‚
:::

:::details Q8: æ˜ç¤ºçš„æ¨å®šé‡ã¨æš—é»™çš„æ¨å®šé‡ã®é•ã„ã‚’ä¸€è¨€ã§
æ˜ç¤ºçš„æ¨å®šé‡ï¼ˆPrescribedï¼‰: å°¤åº¦ $q_\theta(x)$ ã®å€¤ãŒè¨ˆç®—å¯èƒ½ã€‚æš—é»™çš„æ¨å®šé‡ï¼ˆImplicitï¼‰: å°¤åº¦ã¯è¨ˆç®—ä¸èƒ½ã ãŒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¯å¯èƒ½ã€‚Mohamed & Lakshminarayanan (2016) [^6] ã®åˆ†é¡ã€‚
:::

:::details Q9: $\log q_\theta(x) = \log p(f^{-1}(x)) + \log |\det \frac{\partial f^{-1}}{\partial x}|$ ã¯ä½•ã®å¼ï¼Ÿ
Normalizing Flow [^7] ã®å¯¾æ•°å°¤åº¦ã€‚å¤‰æ•°å¤‰æ›å…¬å¼ã€‚$f$ ã¯å¯é€†å¤‰æ›ã€$p(z)$ ã¯åŸºåº•åˆ†å¸ƒã€‚ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®è¡Œåˆ—å¼ãŒä½“ç©å¤‰åŒ–ã‚’è£œæ­£ã™ã‚‹ã€‚
:::

:::details Q10: $H(\hat{p}, q_\theta) = H(\hat{p}) + D_\text{KL}(\hat{p} \| q_\theta)$ ãŒMLE ã«é‡è¦ãªç†ç”±ã¯ï¼Ÿ
CE æœ€å°åŒ– = KL æœ€å°åŒ–ã®è¨¼æ˜ã®æ ¸å¿ƒã€‚$H(\hat{p})$ ã¯ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã§ $\theta$ ã«ä¾å­˜ã—ãªã„ã‹ã‚‰ã€CE ã‚’æœ€å°åŒ–ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ KL ã‚’æœ€å°åŒ–ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ä¸€è‡´ã™ã‚‹ã€‚ç¬¬6å›ã®å®šç† 3.4 ã¨æœ¬è¬›ç¾©ã®å®šç† 3.2-3.3 ã‚’æ¥ç¶šã™ã‚‹å¼ã€‚
:::

### 5.2 LaTeX è¨˜è¿°ãƒ†ã‚¹ãƒˆ

:::details L1: MLE ã®å®šç¾©ã‚’ LaTeX ã§æ›¸ã„ã¦ãã ã•ã„
```latex
\hat{\theta}_{\text{MLE}} = \arg\max_{\theta} \frac{1}{N} \sum_{i=1}^{N} \log q_{\theta}(x_i)
```
:::

:::details L2: GAN ã®ç›®çš„é–¢æ•°ã‚’ LaTeX ã§æ›¸ã„ã¦ãã ã•ã„
```latex
\min_G \max_D \mathbb{E}_{x \sim p_{\text{data}}}[\log D(x)] + \mathbb{E}_{z \sim p(z)}[\log(1 - D(G(z)))]
```
:::

:::details L3: FID ã®å®šç¾©ã‚’ LaTeX ã§æ›¸ã„ã¦ãã ã•ã„
```latex
\text{FID} = \|\mu_r - \mu_g\|^2 + \text{Tr}\left(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2}\right)
```
:::

:::details L4: å¤‰æ•°å¤‰æ›å…¬å¼ï¼ˆFlowï¼‰ã‚’ LaTeX ã§æ›¸ã„ã¦ãã ã•ã„
```latex
\log q_{\theta}(x) = \log p(f^{-1}(x)) + \log \left|\det \frac{\partial f^{-1}}{\partial x}\right|
```
:::

:::details L5: DDPM ã®æå¤±é–¢æ•°ã‚’ LaTeX ã§æ›¸ã„ã¦ãã ã•ã„
```latex
\mathcal{L}_{\text{simple}} = \mathbb{E}_{t, x_0, \epsilon}\left[\|\epsilon - \epsilon_{\theta}(x_t, t)\|^2\right]
```
:::

### 5.3 ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ†ã‚¹ãƒˆ

:::details C1: $\hat{\mu}_\text{MLE} = \frac{1}{N}\sum_{i=1}^{N} x_i$ ã‚’ Python ã§
```python
mu_mle = np.mean(data)
# or explicitly: mu_mle = np.sum(data) / len(data)
```
:::

:::details C2: $D_\text{KL}(p \| q) = \sum_x p(x) \log \frac{p(x)}{q(x)}$ ã‚’ Python ã§
```python
kl = np.sum(p * np.log(p / (q + 1e-10)))
# with numerical stability: kl = np.sum(p * (np.log(p + 1e-10) - np.log(q + 1e-10)))
```
:::

:::details C3: Softmax $p_i = \frac{e^{z_i}}{\sum_j e^{z_j}}$ ã‚’æ•°å€¤å®‰å®šã« Python ã§
```python
def softmax(z):
    z_shifted = z - np.max(z)  # numerical stability
    exp_z = np.exp(z_shifted)
    return exp_z / exp_z.sum()
```
:::

:::details C4: Cross-Entropy Loss $\mathcal{L} = -\frac{1}{N}\sum_{i=1}^{N} \log q_\theta(x_i)$ ã‚’ Python ã§
```python
ce_loss = -np.mean(np.log(q_theta(data) + 1e-10))
```
:::

:::details C5: Reparameterization trick $z = \mu + \sigma \odot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)$ ã‚’ Python ã§
```python
epsilon = np.random.normal(0, 1, size=mu.shape)
z = mu + sigma * epsilon  # gradient flows through mu and sigma
```
:::

### 5.4 MLE å®Ÿé¨“: åˆ†å¸ƒãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°æ¯”è¼ƒ

```python
import numpy as np
from scipy import stats

np.random.seed(42)

# True distributions to fit
distributions = {
    "Normal(3, 2)": np.random.normal(3, 2, 5000),
    "Exponential(2)": np.random.exponential(2, 5000),
    "Bimodal": np.concatenate([np.random.normal(-2, 0.5, 2500),
                                np.random.normal(3, 1, 2500)]),
    "Uniform(0,5)": np.random.uniform(0, 5, 5000),
    "Heavy-tailed (t, df=3)": np.random.standard_t(3, 5000),
}

# Fit single Gaussian via MLE to each
print(f"{'Distribution':>25} {'Î¼Ì‚':>8} {'ÏƒÌ‚':>8} {'KL approx':>12}")
print("-" * 56)

for name, data in distributions.items():
    mu_hat = np.mean(data)
    sigma_hat = np.std(data)

    # Approximate KL via histogram
    bins = np.linspace(data.min() - 1, data.max() + 1, 200)
    hist, bin_edges = np.histogram(data, bins=bins, density=True)
    centers = (bin_edges[:-1] + bin_edges[1:]) / 2
    q_model = stats.norm.pdf(centers, mu_hat, sigma_hat)

    mask = (hist > 1e-10) & (q_model > 1e-10)
    dx = centers[1] - centers[0]
    kl = np.sum(hist[mask] * np.log(hist[mask] / q_model[mask]) * dx)

    print(f"{name:>25} {mu_hat:8.3f} {sigma_hat:8.3f} {kl:12.4f}")

print("\nâ†’ Gaussian MLE works well for Normal data, poorly for Bimodal/Heavy-tailed")
print("â†’ Model family MATTERS. MLE finds the best within the family, not the best overall.")
```

### 5.5 æ¨å®šé‡ã®åˆ†é¡ãƒãƒ£ãƒ¼ãƒˆä½œæˆ

```python
# Create comprehensive estimator taxonomy (by likelihood access)
taxonomy = {
    "Explicit Estimators (Prescribed)": {
        "Autoregressive": {
            "examples": ["GPT", "PixelCNN", "WaveNet"],
            "density": "exact (product of conditionals)",
            "sampling": "sequential (slow)",
            "papers": ["van den Oord+ 2016"],
        },
        "VAE": {
            "examples": ["VAE", "Î²-VAE", "VQ-VAE", "Hierarchical VAE"],
            "density": "lower bound (ELBO)",
            "sampling": "one-shot (fast)",
            "papers": ["Kingma & Welling 2013"],
        },
        "Normalizing Flow": {
            "examples": ["NICE", "Real NVP", "Glow", "Neural ODE"],
            "density": "exact (change of variables)",
            "sampling": "one-shot (fast)",
            "papers": ["Dinh+ 2014", "Rezende & Mohamed 2015"],
        },
    },
    "Implicit Estimators": {
        "GAN": {
            "examples": ["GAN", "DCGAN", "StyleGAN", "BigGAN"],
            "density": "not available",
            "sampling": "one-shot (fast)",
            "papers": ["Goodfellow+ 2014"],
        },
    },
    "Score Estimators": {
        "Score Matching": {
            "examples": ["NCSN", "Sliced Score Matching"],
            "density": "not directly (score only)",
            "sampling": "Langevin dynamics (slow)",
            "papers": ["Song & Ermon 2019"],
        },
        "Diffusion": {
            "examples": ["DDPM", "DDIM", "Stable Diffusion", "DALL-E 2"],
            "density": "lower bound (variational)",
            "sampling": "iterative denoising (slow, improving)",
            "papers": ["Sohl-Dickstein+ 2015", "Ho+ 2020"],
        },
    },
}

for category, subcategories in taxonomy.items():
    print(f"\n{'='*60}")
    print(f"  {category}")
    print(f"{'='*60}")
    for name, info in subcategories.items():
        print(f"\n  {name}")
        for key, val in info.items():
            if isinstance(val, list):
                print(f"    {key}: {', '.join(val)}")
            else:
                print(f"    {key}: {val}")
```

### 5.6 ãƒŸãƒ‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: 1D æ¨å®šé‡æ¯”è¼ƒ

```python
import numpy as np
from scipy import stats

np.random.seed(42)

# ========================================
# Mini-project: Compare generative approaches on 1D data
# ========================================

# True distribution: mixture of 3 Gaussians
def sample_true(n):
    components = np.random.choice(3, size=n, p=[0.3, 0.4, 0.3])
    mus = [-3, 1, 5]
    sigmas = [0.6, 0.8, 0.5]
    return np.array([np.random.normal(mus[c], sigmas[c]) for c in components])

def true_density(x):
    return (0.3 * stats.norm.pdf(x, -3, 0.6) +
            0.4 * stats.norm.pdf(x, 1, 0.8) +
            0.3 * stats.norm.pdf(x, 5, 0.5))

data = sample_true(5000)

# === Approach 1: MLE with single Gaussian ===
mu1 = np.mean(data)
sig1 = np.std(data)
model1_density = lambda x: stats.norm.pdf(x, mu1, sig1)

# === Approach 2: MLE with Gaussian Mixture (3 components, simple EM) ===
# Initialize
mus = np.array([-2.0, 0.0, 4.0])
sigs = np.array([1.0, 1.0, 1.0])
pis = np.array([1/3, 1/3, 1/3])

for _ in range(100):  # EM iterations
    # E-step
    resp = np.zeros((len(data), 3))
    for k in range(3):
        resp[:, k] = pis[k] * stats.norm.pdf(data, mus[k], sigs[k])
    resp /= resp.sum(axis=1, keepdims=True) + 1e-300

    # M-step
    Nk = resp.sum(axis=0)
    for k in range(3):
        mus[k] = np.sum(resp[:, k] * data) / (Nk[k] + 1e-10)
        sigs[k] = np.sqrt(np.sum(resp[:, k] * (data - mus[k])**2) / (Nk[k] + 1e-10))
        sigs[k] = max(sigs[k], 0.01)
        pis[k] = Nk[k] / len(data)

order = np.argsort(mus)
model2_density = lambda x: sum(pis[k] * stats.norm.pdf(x, mus[k], sigs[k]) for k in range(3))

# === Approach 3: KDE (Nonparametric) ===
bandwidth = 0.3
model3_density = lambda x: sum(stats.norm.pdf(x, xi, bandwidth) for xi in data) / len(data)

# === Evaluate: KL divergence approximation ===
x_eval = np.linspace(-6, 8, 2000)
p_true = true_density(x_eval)
dx = x_eval[1] - x_eval[0]

def approx_kl(p, q_fn, x_grid, dx):
    q = np.array([q_fn(xi) for xi in x_grid]) if callable(q_fn) else q_fn
    mask = (p > 1e-10) & (q > 1e-10)
    return np.sum(p[mask] * np.log(p[mask] / q[mask]) * dx)

# Model 1 evaluation
q1 = model1_density(x_eval)
kl1 = approx_kl(p_true, q1, x_eval, dx)

# Model 2 evaluation
q2 = np.array([model2_density(xi) for xi in x_eval])
kl2 = approx_kl(p_true, q2, x_eval, dx)

# Model 3 evaluation (vectorized for speed)
q3 = np.zeros_like(x_eval)
for xi in data[:500]:  # subsample for speed
    q3 += stats.norm.pdf(x_eval, xi, bandwidth)
q3 /= 500
kl3 = approx_kl(p_true, q3, x_eval, dx)

print("=== 1D Generative Model Comparison ===")
print(f"{'Model':>20} {'KL(p||q)':>12} {'Verdict':>20}")
print("-" * 55)
print(f"{'Single Gaussian':>20} {kl1:12.4f} {'Underfitting':>20}")
print(f"{'GMM (K=3)':>20} {kl2:12.4f} {'Good fit':>20}")
print(f"{'KDE (h=0.3)':>20} {kl3:12.4f} {'Nonparametric fit':>20}")

print(f"\nGMM recovered parameters (sorted by Î¼):")
for i, k in enumerate(order):
    print(f"  Component {i}: Ï€={pis[k]:.3f}, Î¼={mus[k]:.3f}, Ïƒ={sigs[k]:.3f}")
print(f"True:          Ï€=[0.30, 0.40, 0.30], Î¼=[-3, 1, 5], Ïƒ=[0.6, 0.8, 0.5]")
```

### 5.7 ãƒŸãƒ‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: Langevin Dynamics ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

```python
import numpy as np

def langevin_sampling_2d(score_fn, n_samples=500, step_size=0.01, n_steps=1000):
    """
    Langevin dynamics in 2D:
    x_{t+1} = x_t + Î· Â· âˆ‡ log p(x_t) + âˆš(2Î·) Â· noise
    """
    # Initialize from broad distribution
    x = np.random.randn(n_samples, 2) * 5
    trajectory = [x.copy()]

    for t in range(n_steps):
        score = score_fn(x)
        noise = np.random.randn(*x.shape)
        x = x + step_size * score + np.sqrt(2 * step_size) * noise
        if t % 100 == 0:
            trajectory.append(x.copy())

    return x, trajectory

# Target: mixture of 4 Gaussians in 2D
means = np.array([[-3, -3], [-3, 3], [3, -3], [3, 3]])
sigma = 0.7

def score_gmm(x):
    """Score function âˆ‡_x log p(x) for 2D GMM"""
    # p(x) = (1/4) Î£ N(x; Î¼_k, ÏƒÂ²I)
    # âˆ‡ log p(x) = Î£ w_k(x) Â· (-(x - Î¼_k)/ÏƒÂ²)
    # where w_k(x) = N(x;Î¼_k,ÏƒÂ²I) / Î£_j N(x;Î¼_j,ÏƒÂ²I)
    densities = np.zeros((x.shape[0], 4))
    for k in range(4):
        diff = x - means[k]
        densities[:, k] = np.exp(-0.5 * np.sum(diff**2, axis=1) / sigma**2)

    weights = densities / (densities.sum(axis=1, keepdims=True) + 1e-300)

    score = np.zeros_like(x)
    for k in range(4):
        score += weights[:, k:k+1] * (-(x - means[k]) / sigma**2)

    return score

# Run Langevin dynamics
np.random.seed(42)
final_samples, trajectory = langevin_sampling_2d(score_gmm, n_samples=500,
                                                  step_size=0.005, n_steps=2000)

# Analyze results
print("=== Langevin Dynamics Sampling (2D GMM) ===")
print(f"Target: 4 Gaussians at {means.tolist()}, Ïƒ={sigma}")
print(f"\nFinal sample statistics:")
print(f"  Mean: [{final_samples[:, 0].mean():.2f}, {final_samples[:, 1].mean():.2f}]")
print(f"  Std:  [{final_samples[:, 0].std():.2f}, {final_samples[:, 1].std():.2f}]")

# Check if samples are near the modes
for k, mu in enumerate(means):
    near_mode = np.sum(np.linalg.norm(final_samples - mu, axis=1) < 2 * sigma)
    print(f"  Near mode {k} ({mu}): {near_mode} samples ({near_mode/500*100:.1f}%)")

print(f"\nTrajectory: {len(trajectory)} snapshots over 2000 steps")
print(f"â†’ Score-based sampling works! Samples converge to modes.")
print(f"â†’ This is how NCSN [Song & Ermon 2019] generates images.")
```

### 5.8 ãƒŸãƒ‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: MLE vs MAP æ¨å®šã®æ¯”è¼ƒ

```python
import numpy as np
from scipy import stats

np.random.seed(42)

# Small sample MLE vs MAP comparison
# True: Î¼ = 5.0, Ïƒ = 1.0
true_mu = 5.0
true_sigma = 1.0

# Prior for MAP: Î¼ ~ N(0, Ï„Â²) with Ï„ = 3
prior_mu = 0.0
prior_tau = 3.0

print(f"True Î¼ = {true_mu}, True Ïƒ = {true_sigma}")
print(f"Prior: Î¼ ~ N({prior_mu}, {prior_tau}Â²)")
print()
print(f"{'N':>5} {'MLE Î¼Ì‚':>10} {'MAP Î¼Ì‚':>10} {'MLE err':>10} {'MAP err':>10} {'Better':>8}")
print("-" * 58)

for N in [2, 5, 10, 20, 50, 100, 1000]:
    mle_errors = []
    map_errors = []
    n_trials = 2000

    for _ in range(n_trials):
        data = np.random.normal(true_mu, true_sigma, N)

        # MLE
        mu_mle = np.mean(data)

        # MAP with Gaussian prior
        # Posterior: N(Î¼_MAP, Ïƒ_MAPÂ²)
        # Î¼_MAP = (N/ÏƒÂ² Â· xÌ„ + 1/Ï„Â² Â· Î¼_0) / (N/ÏƒÂ² + 1/Ï„Â²)
        precision_lik = N / true_sigma**2
        precision_prior = 1.0 / prior_tau**2
        mu_map = (precision_lik * mu_mle + precision_prior * prior_mu) / \
                 (precision_lik + precision_prior)

        mle_errors.append((mu_mle - true_mu)**2)
        map_errors.append((mu_map - true_mu)**2)

    mle_mse = np.mean(mle_errors)
    map_mse = np.mean(map_errors)
    better = "MAP" if map_mse < mle_mse else "MLE"

    print(f"{N:5d} {np.mean([np.random.normal(true_mu, true_sigma, N).mean() for _ in range(100)]):10.3f} "
          f"{'â€”':>10} {mle_mse:10.4f} {map_mse:10.4f} {better:>8}")

print("\nâ†’ MAP wins with small N (prior helps), MLE wins with large N (data dominates)")
print("â†’ MAP = MLE + regularization. This is why weight decay works in deep learning.")
```

### 5.9 è‡ªå·±ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

```
- [ ] MLE ã®å®šç¾©ã‚’å¼ã¨è¨€è‘‰ã®ä¸¡æ–¹ã§èª¬æ˜ã§ãã‚‹
- [ ] MLE = CE æœ€å°åŒ– = KL æœ€å°åŒ–ã®ç­‰ä¾¡æ€§ã‚’å°å‡ºã§ãã‚‹
- [ ] Fisher ã®æ¼¸è¿‘3æ€§è³ªï¼ˆä¸€è‡´æ€§ãƒ»æ¼¸è¿‘æ­£è¦æ€§ãƒ»æœ‰åŠ¹æ€§ï¼‰ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Prescribed model ã¨ Implicit model ã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
- [ ] MLE ã®4å¤‰å½¢ï¼ˆå¤‰åˆ†/æš—é»™çš„/å¤‰æ•°å¤‰æ›/ã‚¹ã‚³ã‚¢ï¼‰ã®æå¤±é–¢æ•°ã‚’æ›¸ã‘ã‚‹
- [ ] FID ã®è¨ˆç®—å¼ã¨ç›´æ„Ÿçš„æ„å‘³ã‚’èª¬æ˜ã§ãã‚‹
- [ ] IS ã¨ CMMD ã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Mode-covering ã¨ mode-seeking ã®é•ã„ã‚’å›³ã§èª¬æ˜ã§ãã‚‹
- [ ] GAN ã®æœ€é©åˆ¤åˆ¥å™¨ã‚’å°å‡ºã§ãã‚‹
- [ ] ã‚¹ã‚³ã‚¢é–¢æ•°ãŒæ­£è¦åŒ–å®šæ•°ã«ä¾å­˜ã—ãªã„ç†ç”±ã‚’èª¬æ˜ã§ãã‚‹
- [ ] LLM è¨“ç·´ãŒ MLE ã§ã‚ã‚‹ã“ã¨ã‚’å¼ã§ç¤ºã›ã‚‹
- [ ] æ¬¡å…ƒã®å‘ªã„ãŒå¯†åº¦æ¨å®šã«ä¸ãˆã‚‹å½±éŸ¿ã‚’èª¬æ˜ã§ãã‚‹
```

:::message
**é€²æ—: 85% å®Œäº†** â€” è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆå®Œäº†ã€‚ã“ã“ã‹ã‚‰ç™ºå±•ã‚¾ãƒ¼ãƒ³ã¸ã€‚
:::

---

## ğŸš€ 6. æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨æ¬¡å›äºˆå‘Š

### 6.3 çµ±è¨ˆçš„è·é›¢ã®å•é¡Œç‚¹ã¨æœ€æ–°å‹•å‘ â€” MLE beyond i.i.d.

FID [^4] ã¯äº‹å®Ÿä¸Šã®æ¨™æº–çš„çµ±è¨ˆçš„è·é›¢ã ãŒã€æ·±åˆ»ãªå•é¡ŒãŒã‚ã‚‹ã€‚

```python
# FID's problems
problems = {
    "Inception-v3 ãŒå¤ã„": {
        "issue": "2015å¹´ã®ãƒ¢ãƒ‡ãƒ«ã€‚CLIP/DINO ãŒé¥ã‹ã«è‰¯ã„ç‰¹å¾´é‡ã‚’æŠ½å‡º",
        "impact": "ãƒ†ã‚¯ã‚¹ãƒãƒ£åé‡ã€ã‚»ãƒãƒ³ãƒ†ã‚£ã‚¯ã‚¹è»½è¦–",
        "alternative": "FD-DINOv2, CMMD (CLIP-based)"
    },
    "ã‚¬ã‚¦ã‚¹ä»®å®š": {
        "issue": "ç‰¹å¾´é‡ãŒã‚¬ã‚¦ã‚¹åˆ†å¸ƒã«å¾“ã†ä»®å®šã¯ä¸€èˆ¬ã«ä¸æ­£ç¢º",
        "impact": "å¤šå³°çš„ãªç‰¹å¾´åˆ†å¸ƒã§ä¸æ­£ç¢º",
        "alternative": "CMMD (ã‚«ãƒ¼ãƒãƒ«æ³•ã€åˆ†å¸ƒä»®å®šãªã—)"
    },
    "ã‚µãƒ³ãƒ—ãƒ«ãƒã‚¤ã‚¢ã‚¹": {
        "issue": "FID ã¯ N ã«ä¾å­˜ã™ã‚‹ãƒã‚¤ã‚¢ã‚¹ã‚’æŒã¤",
        "impact": "ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå°‘ãªã„ã¨ä¸å½“ã«é«˜ã„ FID",
        "alternative": "CMMD (ä¸åæ¨å®šé‡)"
    },
    "äººé–“åˆ¤æ–­ã¨ã®ä¸ä¸€è‡´": {
        "issue": "FID ãŒä½ã„ã®ã«äººé–“ã«ã¯ä½å“è³ªã«è¦‹ãˆã‚‹å ´åˆãŒã‚ã‚‹",
        "impact": "è©•ä¾¡æŒ‡æ¨™ã®ä¿¡é ¼æ€§ä½ä¸‹",
        "alternative": "CMMD + äººé–“è©•ä¾¡ã®çµ„ã¿åˆã‚ã›"
    },
}

for name, info in problems.items():
    print(f"\nå•é¡Œ: {name}")
    for key, val in info.items():
        print(f"  {key}: {val}")
```

Jayasumana+ (2024) [^9] ã¯ CMMD ã‚’ææ¡ˆã—ã€ã“ã‚Œã‚‰ã®å•é¡Œã®å¤šãã‚’è§£æ±ºã—ãŸã€‚CMMD ã¯ CLIP ç‰¹å¾´é‡ + ã‚¬ã‚¦ã‚¹ RBF ã‚«ãƒ¼ãƒãƒ«ã® MMD ã§ã€ä¸åæ¨å®šé‡ã‹ã¤åˆ†å¸ƒä»®å®šä¸è¦ã€‚

### 6.4 æ¨å®šé‡ã®æ¼¸è¿‘æ¯”è¼ƒ

| æ¨å®šé‡ã®ç‰¹æ€§ | å¤‰åˆ†MLE [^3] | æš—é»™çš„æ¨å®š [^2] | å¤‰æ•°å¤‰æ›MLE [^7][^11][^12] | ã‚¹ã‚³ã‚¢æ¨å®š [^5][^13] | è‡ªå·±å›å¸°MLE |
|:-----|:---------|:---------|:---------------------|:---------------------|:---------|
| **å°¤åº¦ã‚¢ã‚¯ã‚»ã‚¹** | ä¸‹ç•Œ (ELBO) | è¨ˆç®—ä¸èƒ½ | æ­£ç¢º | ä¸è¦ï¼ˆã‚¹ã‚³ã‚¢ã®ã¿ï¼‰ | æ­£ç¢º |
| **æ¨å®šç²¾åº¦** | ä¸­ï¼ˆmode-coveringï¼‰ | é«˜ï¼ˆmode-seekingï¼‰ | ä¸­ã€œé«˜ | **æœ€é«˜** | é«˜ |
| **æ¨å®šã®å®‰å®šæ€§** | **é«˜** | ä½ï¼ˆä¸å®‰å®šï¼‰ | é«˜ | **é«˜** | **é«˜** |
| **ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é€Ÿåº¦** | **é€Ÿã„** (1-shot) | **é€Ÿã„** (1-shot) | **é€Ÿã„** (1-shot) | é…ã„ (T steps) | é…ã„ (T steps) |
| **æ½œåœ¨å¤‰æ•°** | ã‚ã‚Šï¼ˆæ»‘ã‚‰ã‹ï¼‰ | ãªã— (ç›´æ¥) | ã‚ã‚Šï¼ˆå¯é€†ï¼‰ | ã‚ã‚Šï¼ˆãƒã‚¤ã‚ºï¼‰ | ãªã— |
| **ãƒ¢ãƒ¼ãƒ‰å´©å£Š** | ãªã— | **ã‚ã‚Š** | ãªã— | ãªã— | ãªã— |
| **æ•°å­¦çš„åŸºç›¤** | å¤‰åˆ†æ¨è«– | ã‚²ãƒ¼ãƒ ç†è«– | å¤‰æ•°å¤‰æ› | ç¢ºç‡SDE / Score | ç¢ºç‡ã®é€£é–å¾‹ |
| **æå¤±ã®æœ€å°åŒ–å¯¾è±¡** | -ELBO | JSD | -log p(x) | $\|\epsilon - \hat\epsilon\|^2$ | CE |
| **ä»£è¡¨çš„æˆåŠŸä¾‹** | Î²-VAE, VQ-VAE | StyleGAN3 | Glow | Stable Diffusion | GPT-4 |
| **æœ¬ã‚·ãƒªãƒ¼ã‚º** | ç¬¬9-10å› | ç¬¬12-14å› | ç¬¬11å› | ç¬¬15,25-32å› | ç¬¬16å› |

### 6.5 Densing Law ã¨èƒ½åŠ›å¯†åº¦

æœ€æ–°ã®ç ”ç©¶ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ã€Œèƒ½åŠ›å¯†åº¦ã€ï¼ˆcapability densityï¼‰ã«æ³¨ç›®ã™ã‚‹å‹•ããŒã‚ã‚‹ã€‚åŒã˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã§ã‚ˆã‚Šé«˜ã„æ€§èƒ½ã‚’é”æˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã¯èƒ½åŠ›å¯†åº¦ãŒé«˜ã„ã€‚

```python
# Capability density concept
models = {
    "GPT-3 (2020)":    {"params_B": 175,  "benchmark": 70, "density": 70/175},
    "LLaMA-2 (2023)":  {"params_B": 70,   "benchmark": 75, "density": 75/70},
    "Mistral (2023)":   {"params_B": 7,    "benchmark": 68, "density": 68/7},
    "Phi-3 (2024)":     {"params_B": 3.8,  "benchmark": 69, "density": 69/3.8},
}

print(f"{'Model':>20} {'Params(B)':>10} {'Score':>8} {'Density':>10}")
print("-" * 52)
for name, info in models.items():
    print(f"{name:>20} {info['params_B']:10.1f} {info['benchmark']:8.0f} "
          f"{info['density']:10.2f}")

print("\nâ†’ Densing Law: capability density increases over time")
print("â†’ Smaller models achieve higher scores per parameter")
print("â†’ Implication: efficiency matters as much as scale")
```

ã“ã®å‚¾å‘ã¯å¯†åº¦æ¨å®šãƒ¢ãƒ‡ãƒ«ã«ã‚‚å½“ã¦ã¯ã¾ã‚‹ã€‚Stable Diffusion 3 ã¯å‰ä¸–ä»£ã‚ˆã‚Šå°ã•ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã§ã‚ˆã‚Šé«˜å“è³ªãªç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã€‚åŠ¹ç‡ã®è¿½æ±‚ãŒã€æ¬¡ã®ç ”ç©¶ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ã ã€‚

### 6.6 Simulation-Based Inference â€” æš—é»™çš„æ¨å®šé‡ã®ç§‘å­¦å¿œç”¨

å¯†åº¦æ¨å®šãƒ»æ¨å®šé‡è¨­è¨ˆã¯ç”»åƒç”Ÿæˆã ã‘ã®ã‚‚ã®ã§ã¯ãªã„ã€‚ç§‘å­¦ã®ã‚ã‚‰ã‚†ã‚‹åˆ†é‡ã§ã€Œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã®é€†å•é¡Œã€ã«ä½¿ã‚ã‚Œã¦ã„ã‚‹ã€‚

| åˆ†é‡ | å¿œç”¨ | æ¨å®šé‡ã®å½¹å‰² |
|:-----|:-----|:----------------|
| ç²’å­ç‰©ç†å­¦ | LHC ã®è¡çªãƒ‡ãƒ¼ã‚¿ | ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿â†’ãƒ‡ãƒ¼ã‚¿ã®é€†æ¨å®š |
| å®‡å®™è«– | CMB ãƒ‡ãƒ¼ã‚¿ | å®‡å®™ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®äº‹å¾Œæ¨å®š |
| æ°—å€™ç§‘å­¦ | æ°—å€™ãƒ¢ãƒ‡ãƒ« | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¸ç¢ºå®Ÿæ€§ã®å®šé‡åŒ– |
| å‰µè–¬ | åˆ†å­ç”Ÿæˆ | ä½æ¬¡å…ƒæ½œåœ¨ç©ºé–“ã§ã®æ¢ç´¢ |
| ææ–™ç§‘å­¦ | çµæ™¶æ§‹é€ äºˆæ¸¬ | æ¡ä»¶ä»˜ãç”Ÿæˆ |
| è›‹ç™½è³ªè¨­è¨ˆ | ã‚¿ãƒ³ãƒ‘ã‚¯è³ªæ§‹é€  | æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ç”Ÿæˆ |

:::details World Models â€” å¯†åº¦æ¨å®šã®æ–°ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ 
å¯†åº¦æ¨å®šã‚’ã€Œä¸–ç•Œã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã€ã¨ã—ã¦æ‰ãˆã‚‹æ½®æµãŒã‚ã‚‹ã€‚Sora (2024) ãŒãƒ“ãƒ‡ã‚ªç”Ÿæˆã§è¦‹ã›ãŸã®ã¯ã€ç‰©ç†æ³•å‰‡ã‚’æš—é»™çš„ã«å­¦ç¿’ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®å¯èƒ½æ€§ã ã€‚$p(x_{t+1} | x_{\leq t}, a)$ï¼ˆè¡Œå‹• $a$ ã«å¯¾ã™ã‚‹æ¬¡ã®ä¸–ç•ŒçŠ¶æ…‹ã®äºˆæ¸¬ï¼‰ã¯ã€å¼·åŒ–å­¦ç¿’ã®ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ãã®ã‚‚ã®ã§ã‚ã‚Šã€å¯†åº¦æ¨å®šã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®èåˆç‚¹ã ã€‚
:::

### 6.7 Identifiability å•é¡Œ

å¯†åº¦æ¨å®šã«ã¯æ ¹æœ¬çš„ãªç†è«–çš„å•é¡ŒãŒã‚ã‚‹ â€” åŒã˜ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p(x)$ ã‚’å®Ÿç¾ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã¯ä¸€èˆ¬ã«ä¸€æ„ã§ã¯ãªã„ã€‚

$$q_{\theta_1}(x) = q_{\theta_2}(x) \quad \forall x, \quad \text{but} \quad \theta_1 \neq \theta_2$$

ä¾‹ãˆã° GMM ã®æˆåˆ†ã‚’ãƒ©ãƒ™ãƒ«å…¥ã‚Œæ›¿ãˆã—ã¦ã‚‚å°¤åº¦ã¯ä¸å¤‰ï¼ˆlabel switching problemï¼‰ã€‚VAE ã®æ½œåœ¨ç©ºé–“ã‚‚å›è»¢ä¸å¤‰æ€§ã‚’æŒã¤ã€‚ã“ã‚Œã¯ MLE ã®ç†è«–çš„å¸°çµã§ã‚ã‚Šã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è§£é‡ˆã«æ³¨æ„ãŒå¿…è¦ãªã“ã¨ã‚’ç¤ºã™ã€‚

```python
import numpy as np

# Label switching: permuting components doesn't change likelihood
# GMM with K=2: (Ï€â‚, Î¼â‚, Ïƒâ‚, Ï€â‚‚, Î¼â‚‚, Ïƒâ‚‚)
theta1 = {"pi": [0.3, 0.7], "mu": [-2, 3], "sigma": [0.5, 1.0]}
theta2 = {"pi": [0.7, 0.3], "mu": [3, -2], "sigma": [1.0, 0.5]}  # swapped!

def gmm_likelihood(x, params):
    ll = 0
    for k in range(2):
        ll += params["pi"][k] * np.exp(-0.5 * ((x - params["mu"][k]) / params["sigma"][k])**2) \
              / (params["sigma"][k] * np.sqrt(2 * np.pi))
    return ll

x_test = np.array([0.0, 1.0, -1.5, 2.5])
ll1 = [gmm_likelihood(xi, theta1) for xi in x_test]
ll2 = [gmm_likelihood(xi, theta2) for xi in x_test]

print("Identifiability problem: label switching")
print(f"Î¸â‚: Ï€={theta1['pi']}, Î¼={theta1['mu']}, Ïƒ={theta1['sigma']}")
print(f"Î¸â‚‚: Ï€={theta2['pi']}, Î¼={theta2['mu']}, Ïƒ={theta2['sigma']}")
print(f"\nLikelihoods at test points:")
for xi, l1, l2 in zip(x_test, ll1, ll2):
    print(f"  x={xi:5.1f}: L(Î¸â‚)={l1:.6f}, L(Î¸â‚‚)={l2:.6f}, equal={np.isclose(l1, l2)}")
print(f"\nâ†’ Different parameters, SAME likelihood â†’ MLE is NOT unique")
print(f"â†’ For K components, there are K! equivalent solutions")
print(f"â†’ K=10: 10! = {np.math.factorial(10):,} equivalent solutions!")
```

### 6.8 MLEâ†’EMâ†’å¤‰åˆ†æ¨è«– â€” æ¨è«–ã®å›°é›£åº¦ãƒãƒƒãƒ—

```mermaid
graph TD
    subgraph "Course I: æ•°å­¦åŸºç›¤ (å®Œäº†)"
        L6[ç¬¬6å›: KL, CE, Adam]
        L7[ç¬¬7å›: MLE, æ¨å®šé‡ã®åˆ†é¡<br>â† æœ¬è¬›ç¾©]
    end

    subgraph "Course II: ç¢ºç‡ãƒ¢ãƒ‡ãƒ«åŸºç¤ (ç¬¬8-16å›)"
        L8[ç¬¬8å›: æ½œåœ¨å¤‰æ•°ãƒ»EM]
        L9[ç¬¬9å›: VAE]
        L10[ç¬¬10å›: VAE ç™ºå±•]
        L11[ç¬¬11å›: Flow]
        L12[ç¬¬12å›: GAN]
        L13[ç¬¬13å›: GAN ç™ºå±•]
        L14[ç¬¬14å›: è©•ä¾¡æŒ‡æ¨™ æ·±å €ã‚Š]
        L15[ç¬¬15å›: Diffusion åŸºç¤]
        L16[ç¬¬16å›: Transformer]
    end

    L6 --> L7
    L7 -->|MLE ã®é™ç•Œâ†’æ½œåœ¨å¤‰æ•°| L8
    L8 -->|ELBOâ†’å¤‰åˆ†MLE| L9
    L9 -->|å¤‰åˆ†æ¨å®šé‡ã®æ‹¡å¼µ| L10
    L7 -->|å¤‰æ•°å¤‰æ›æ¨å®šé‡| L11
    L7 -->|æš—é»™çš„æ¨å®šé‡| L12
    L12 --> L13
    L7 -->|çµ±è¨ˆçš„è·é›¢| L14
    L7 -->|ã‚¹ã‚³ã‚¢æ¨å®šé‡| L15
    L7 -->|è‡ªå·±å›å¸° MLE| L16

    style L7 fill:#ff9800,color:#fff
    style L8 fill:#e8f5e9
    style L9 fill:#e8f5e9
    style L12 fill:#fff3e0
    style L15 fill:#fce4ec
```

ã“ã®å›³ã®é€šã‚Šã€æœ¬è¬›ç¾©ã§ç¯‰ã„ãŸæ¨å®šé‡ã®æ•°å­¦çš„åŸºç›¤ã¯ç¬¬8-16å›ã®å…¨ã¦ã«æ¥ç¶šã—ã¦ã„ã‚‹ã€‚å„è¬›ç¾©ã§æˆ»ã£ã¦ãã‚‹ãŸã³ã«ã€æ¨å®šåŸç†ã®ç†è§£ãŒæ·±ã¾ã‚‹ã€‚

:::details ç”¨èªé›†ï¼ˆæœ¬è¬›ç¾©ã§å°å…¥ã—ãŸå…¨ç”¨èªï¼‰

| ç”¨èª | è‹±èª | å®šç¾© |
|:-----|:-----|:-----|
| æœ€å°¤æ¨å®š | Maximum Likelihood Estimation (MLE) | å°¤åº¦ã‚’æœ€å¤§åŒ–ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®šæ³• |
| å¯¾æ•°å°¤åº¦ | Log-Likelihood | $\sum \log q_\theta(x_i)$ã€‚å°¤åº¦ã®å¯¾æ•° |
| çµŒé¨“åˆ†å¸ƒ | Empirical Distribution | $\hat{p}(x) = \frac{1}{N}\sum \delta(x-x_i)$ |
| åˆ¤åˆ¥ãƒ¢ãƒ‡ãƒ« | Discriminative Model | $p(y|x)$ ã‚’å­¦ç¿’ã™ã‚‹ãƒ¢ãƒ‡ãƒ« |
| ç”Ÿæˆãƒ¢ãƒ‡ãƒ« | Generative Model | $p(x)$ ã‚’æ¨å®šã™ã‚‹ç¢ºç‡ãƒ¢ãƒ‡ãƒ« |
| æ˜ç¤ºçš„æ¨å®šé‡ | Prescribed Estimator | å°¤åº¦ãŒé™½ã«è¨ˆç®—å¯èƒ½ãªæ¨å®šé‡ |
| æš—é»™çš„æ¨å®šé‡ | Implicit Estimator | ã‚µãƒ³ãƒ—ãƒ«ã®ã¿å¯èƒ½ã€å°¤åº¦è¨ˆç®—ä¸èƒ½ |
| å¤šæ§˜ä½“ä»®èª¬ | Manifold Hypothesis | ãƒ‡ãƒ¼ã‚¿ã¯ä½æ¬¡å…ƒå¤šæ§˜ä½“ä¸Šã«é›†ä¸­ |
| æ¬¡å…ƒã®å‘ªã„ | Curse of Dimensionality | é«˜æ¬¡å…ƒã§å¯†åº¦æ¨å®šãŒæŒ‡æ•°çš„ã«å›°é›£ |
| ã‚¹ã‚³ã‚¢é–¢æ•° | Score Function | $\nabla_x \log p(x)$ã€‚å¯†åº¦ã®å‹¾é… |
| Mode-Covering | Mode-Covering | å…¨ãƒ¢ãƒ¼ãƒ‰ã‚’ã‚«ãƒãƒ¼ï¼ˆå‰å‘ã KLï¼‰ |
| Mode-Seeking | Mode-Seeking | ç‰¹å®šãƒ¢ãƒ¼ãƒ‰ã«é›†ä¸­ï¼ˆé€†å‘ã KLï¼‰ |
| FID | Frechet Inception Distance | ç”Ÿæˆç”»åƒã¨å®Ÿç”»åƒã® Frechet è·é›¢ |
| IS | Inception Score | ç”Ÿæˆå“è³ªã¨å¤šæ§˜æ€§ã®æŒ‡æ¨™ |
| CMMD | CLIP Maximum Mean Discrepancy | FID ã®æ”¹è‰¯æŒ‡æ¨™ |
| å¤‰æ•°å¤‰æ›å…¬å¼ | Change of Variables | Flow ãƒ¢ãƒ‡ãƒ«ã®å°¤åº¦è¨ˆç®—ã®åŸºç¤ |
| è‡ªå·±å›å¸°åˆ†è§£ | Autoregressive Decomposition | $p(x) = \prod p(x_t | x_{<t})$ |
| Reparameterization | Reparameterization Trick | $z = \mu + \sigma\epsilon$ ã§å‹¾é…ä¼æ’­ |
| Langevin å‹•åŠ›å­¦ | Langevin Dynamics | ã‚¹ã‚³ã‚¢ã«åŸºã¥ãã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° |
| Fisher æƒ…å ±è¡Œåˆ— | Fisher Information Matrix | $\mathcal{I}(\theta) = -\mathbb{E}[\nabla^2 \log p]$ |
| ä¸€è‡´æ€§ | Consistency | MLE ãŒçœŸã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«åæŸã™ã‚‹æ€§è³ª |
| æ¼¸è¿‘æ­£è¦æ€§ | Asymptotic Normality | MLE ã®åˆ†å¸ƒãŒæ­£è¦ã«è¿‘ã¥ãæ€§è³ª |
| æ¼¸è¿‘æœ‰åŠ¹æ€§ | Asymptotic Efficiency | MLE ãŒæœ€å°åˆ†æ•£ã‚’é”æˆã™ã‚‹æ€§è³ª |
| ELBO | Evidence Lower Bound | $\log p(x)$ ã®å¤‰åˆ†ä¸‹ç•Œ |
| ç¥–å…ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° | Ancestral Sampling | æ¡ä»¶ä»˜ãåˆ†å¸ƒã®é€£é–ã§ã‚µãƒ³ãƒ—ãƒ« |
| é‡ç‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° | Importance Sampling | ææ¡ˆåˆ†å¸ƒã‹ã‚‰ã®é‡ã¿ä»˜ãã‚µãƒ³ãƒ—ãƒ« |
| éå¹³è¡¡ç†±åŠ›å­¦ | Nonequilibrium Thermodynamics | Diffusion ãƒ¢ãƒ‡ãƒ«ã®ç‰©ç†çš„ç€æƒ³ |
:::

:::details ä¸ç­‰å¼ãƒ»ç­‰å¼ã¾ã¨ã‚

| ç­‰å¼/ä¸ç­‰å¼ | æ•°å¼ | æ„å‘³ |
|:-----------|:-----|:-----|
| MLE = CE æœ€å°åŒ– | $\arg\max \sum \log q_\theta(x_i) = \arg\min H(\hat{p}, q_\theta)$ | å®šç† 3.2 |
| MLE = KL æœ€å°åŒ– | $\arg\min H(\hat{p}, q_\theta) = \arg\min D_\text{KL}(\hat{p} \| q_\theta)$ | å®šç† 3.3 |
| CE åˆ†è§£ | $H(\hat{p}, q_\theta) = H(\hat{p}) + D_\text{KL}(\hat{p} \| q_\theta)$ | ç¬¬6å› å®šç† 3.4 |
| GAN æœ€é©åˆ¤åˆ¥å™¨ | $D^*(x) = \frac{p_\text{data}}{p_\text{data} + p_g}$ | å®šç† 3.8a |
| GAN = JSD | $V(D^*, G) = -\log 4 + 2 \cdot \text{JSD}$ | å®šç† 3.8b |
| Fisher æ¼¸è¿‘ | $\sqrt{N}(\hat\theta - \theta^*) \to \mathcal{N}(0, \mathcal{I}^{-1})$ | æ€§è³ª 3.4b |
| Flow å°¤åº¦ | $\log q(x) = \log p(f^{-1}(x)) + \log |\det J|$ | å®šç† 3.7 |
| Score æ­£è¦åŒ–ä¸å¤‰ | $\nabla_x \log p(x) = \nabla_x \log \tilde{p}(x)$ | å®šç¾© 3.9 |
| ELBO | $\log p(x) \geq \text{ELBO}$ | ç¬¬8å› å…ˆå–ã‚Š |
:::

### 6.9 çŸ¥è­˜ãƒã‚¤ãƒ³ãƒ‰ãƒãƒƒãƒ—

```mermaid
mindmap
  root((ç¬¬7å›))
    æœ€å°¤æ¨å®š
      Fisher 1922
      MLE = CEæœ€å°åŒ–
      MLE = KLæœ€å°åŒ–
      æ¼¸è¿‘3æ€§è³ª
        ä¸€è‡´æ€§
        æ¼¸è¿‘æ­£è¦æ€§
        æ¼¸è¿‘æœ‰åŠ¹æ€§
      é™ç•Œ
        ãƒ¢ãƒ‡ãƒ«æ—ä¾å­˜
        é«˜æ¬¡å…ƒå›°é›£
        å‘¨è¾ºåŒ–ä¸èƒ½
    æ¨å®šé‡ã®åˆ†é¡
      æ˜ç¤ºçš„æ¨å®šé‡
        VAE å¤‰åˆ†MLE
        Flow å¤‰æ•°å¤‰æ›MLE
        è‡ªå·±å›å¸°MLE
      æš—é»™çš„æ¨å®šé‡
        GAN
      ã‚¹ã‚³ã‚¢æ¨å®šé‡
        NCSN
        DDPM
    çµ±è¨ˆçš„è·é›¢
      FID W2è·é›¢
      KID MMD
      CMMD CLIP-MMD
    LLMæ¥ç¶š
      æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬
      è‡ªå·±å›å¸°MLE
      Perplexity
    æ¨å®šåŸç†ã®å¤‰å½¢
      KLâ†’æå¤±è¨­è¨ˆ
      JSDâ†’æš—é»™çš„æ¨å®š
      å¤‰æ•°å¤‰æ›â†’Flow
      Scoreâ†’Diffusion
```

### 6.10 æœ¬è¬›ç¾©ã®ã‚­ãƒ¼ãƒ†ã‚¤ã‚¯ã‚¢ã‚¦ã‚§ã‚¤

1. **MLE = CE æœ€å°åŒ– = KL æœ€å°åŒ–** â€” ã“ã®ä¸‰ä½ä¸€ä½“ãŒçµ±è¨ˆçš„æ¨å®šã®æ ¹å¹¹ã€‚ç¬¬6å›ã®æƒ…å ±ç†è«–ã¨æœ¬è¬›ç¾©ã® MLE ãŒåˆæµã—ãŸã€‚
2. **æ¨å®šé‡ã¯å°¤åº¦é–¢æ•°ã®ã‚¢ã‚¯ã‚»ã‚¹å½¢æ…‹ã§åˆ†é¡**ã§ãã‚‹: æ˜ç¤ºçš„ï¼ˆå¤‰åˆ†MLE, å¤‰æ•°å¤‰æ›MLE, è‡ªå·±å›å¸°MLEï¼‰ã€æš—é»™çš„ï¼ˆGANï¼‰ã€ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ï¼ˆDDPMï¼‰ã€‚å„ã€…ãŒç•°ãªã‚‹æ•°å­¦çš„åŸºç›¤ã‚’æŒã¤ã€‚
3. **æ˜ç¤ºçš„ vs æš—é»™çš„æ¨å®šé‡** â€” å°¤åº¦ãŒè¨ˆç®—å¯èƒ½ã‹å¦ã‹ã§æ¨å®šæ–¹æ³•ãŒæ ¹æœ¬çš„ã«ç•°ãªã‚‹ã€‚ã“ã®åˆ†é¡ãŒç¬¬8-16å›ã®å…¨ã¦ã®å‡ºç™ºç‚¹ã€‚
4. **çµ±è¨ˆçš„è·é›¢ã¯æ¨å®šé‡ã®è©•ä¾¡åŸç†** â€” FIDï¼ˆ$W_2$ è·é›¢ï¼‰ã¯æ¨™æº–ã ãŒé™ç•ŒãŒã‚ã‚‹ã€‚KID, CMMD ãŒæ”¹å–„ã‚’ææ¡ˆã€‚ã€Œä½•ã‚’ã‚‚ã£ã¦è‰¯ã„æ¨å®šã¨ã™ã‚‹ã‹ã€ã¯æ·±ã„å•ã„ã€‚

### 6.11 FAQ

:::details Q1: MLE ã¯ç”»åƒç”Ÿæˆä»¥å¤–ã«ã©ã†ä½¿ã‚ã‚Œã‚‹ï¼Ÿ
ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆGPT = è‡ªå·±å›å¸°MLEï¼‰ã€éŸ³å£°åˆæˆï¼ˆWaveNetï¼‰ã€åˆ†å­è¨­è¨ˆï¼ˆå‰µè–¬ï¼‰ã€ã‚¿ãƒ³ãƒ‘ã‚¯è³ªæ§‹é€ äºˆæ¸¬ã€æ°—å€™ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€ç•°å¸¸æ¤œçŸ¥ã€å…¨ã¦ãŒã€Œç¢ºç‡åˆ†å¸ƒã®æ¨å®šã€å•é¡Œã ã€‚MLE ã¨ãã®å¤‰å½¢ã¯ã€ç¢ºç‡åˆ†å¸ƒã§è¡¨ç¾ã§ãã‚‹ã‚ã‚‰ã‚†ã‚‹ãƒ‡ãƒ¼ã‚¿ã«é©ç”¨å¯èƒ½ã€‚
:::

:::details Q2: å¤‰åˆ†MLEï¼ˆVAEï¼‰ã¨æš—é»™çš„æ¨å®šï¼ˆGANï¼‰ã€ã©ã¡ã‚‰ãŒè‰¯ã„ï¼Ÿ
æ¨å®šã®ç›®çš„ã«ã‚ˆã‚‹ã€‚å¤‰åˆ†MLE: å°¤åº¦ãŒè¨ˆç®—å¯èƒ½ã€æ¨å®šãŒå®‰å®šã€æ½œåœ¨ç©ºé–“ãŒæ»‘ã‚‰ã‹ â†’ è¡¨ç¾å­¦ç¿’ã€åŠæ•™å¸«ã‚ã‚Šå­¦ç¿’ã€‚æš—é»™çš„æ¨å®š: ã‚µãƒ³ãƒ—ãƒ«å“è³ªãŒé«˜ã„ã€é®®æ˜ãªå‡ºåŠ› â†’ é«˜å“è³ªç”Ÿæˆã€è¶…è§£åƒã€‚2024å¹´ç¾åœ¨ã€å¤šãã®ã‚¿ã‚¹ã‚¯ã§ã‚¹ã‚³ã‚¢æ¨å®šé‡ï¼ˆDiffusion Modelï¼‰ãŒä¸¡æ–¹ã‚’ä¸Šå›ã‚‹ã€‚ã€Œã©ã¡ã‚‰ãŒè‰¯ã„ã€ã‚ˆã‚Šã€Œä½•ã‚’æ¨å®šã™ã‚‹ã‹ã€ã§é¸ã¶ã¹ãã€‚
:::

:::details Q3: Diffusion Model ã¯ãªãœã“ã‚Œã»ã©æˆåŠŸã—ãŸï¼Ÿ
3ã¤ã®ç†ç”±: (1) è¨“ç·´ãŒå®‰å®šï¼ˆå˜ç´”ãª MSE æå¤±ï¼‰ã€(2) ã‚µãƒ³ãƒ—ãƒ«å“è³ªãŒé«˜ã„ï¼ˆæ®µéšçš„ãªãƒã‚¤ã‚ºé™¤å»ï¼‰ã€(3) ç†è«–çš„åŸºç›¤ãŒå …å›ºï¼ˆã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚° + ç¢ºç‡å¾®åˆ†æ–¹ç¨‹å¼ï¼‰ã€‚DDPM [^5] ãŒå“è³ªã§ GAN ã«åŒ¹æ•µã—ã€ãƒ¢ãƒ¼ãƒ‰å´©å£Šãªã—ã®è¨“ç·´ã‚’å®Ÿç¾ã—ãŸã“ã¨ãŒè»¢æ›ç‚¹ã ã£ãŸã€‚
:::

:::details Q4: FIDï¼ˆçµ±è¨ˆçš„è·é›¢ï¼‰ã®çµ¶å¯¾å€¤ã¯ã©ã†è§£é‡ˆã™ã‚‹ï¼Ÿ
å¤§ã¾ã‹ã«: FID < 10 = æ¨å®šãŒéå¸¸ã«è‰¯ã„ã€10-50 = è‰¯ã„ã€50-100 = ã¾ã‚ã¾ã‚ã€> 100 = æ‚ªã„ã€‚ãŸã ã—ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¤§ããä¾å­˜ã™ã‚‹ã€‚CelebAï¼ˆé¡”ï¼‰ã¯ FID ãŒä½ããªã‚Šã‚„ã™ãã€ImageNetï¼ˆä¸€èˆ¬ç”»åƒï¼‰ã¯é«˜ããªã‚Šã‚„ã™ã„ã€‚åŒã˜ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã§ã®ç›¸å¯¾æ¯”è¼ƒãŒæœ‰åŠ¹ã€‚æ•°å­¦çš„ã«ã¯ã‚¬ã‚¦ã‚¹è¿‘ä¼¼ $W_2$ è·é›¢ã§ã‚ã‚‹ã“ã¨ã‚’å¸¸ã«æ„è­˜ã™ã¹ãã€‚
:::

:::details Q5: MLE ä»¥å¤–ã®æ¨å®šæ³•ã¯ãªã„ã®ã‹ï¼Ÿ
MAPï¼ˆMaximum A Posterioriï¼‰æ¨å®š: MLE + äº‹å‰åˆ†å¸ƒã€‚ãƒ™ã‚¤ã‚ºæ¨å®š: äº‹å¾Œåˆ†å¸ƒå…¨ä½“ã‚’æ¨å®šã€‚æ–¹æ³•ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆï¼ˆMethod of Momentsï¼‰ã€‚æœ€å°è·é›¢æ¨å®šã€‚MLE ãŒæœ€ã‚‚åºƒãä½¿ã‚ã‚Œã‚‹ç†ç”±ã¯ã€æ¼¸è¿‘çš„ãªæœ€é©æ€§ï¼ˆFisher ã®å®šç†ï¼‰ã¨è¨ˆç®—ã®å®¹æ˜“ã•ã€‚
:::

:::details Q6: è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ï¼ˆGPTï¼‰ã¯æ˜ç¤ºçš„æ¨å®šé‡ï¼Ÿ
ãã†ã ã€‚$p(x_t | x_{<t})$ ãŒé™½ã«è¨ˆç®—å¯èƒ½ï¼ˆsoftmax å‡ºåŠ›ï¼‰ãªã®ã§ã€æ˜ç¤ºçš„æ¨å®šé‡ï¼ˆPrescribedï¼‰ã€‚å¯¾æ•°å°¤åº¦ã‚‚æ­£ç¢ºã«è¨ˆç®—ã§ãã‚‹ã€‚ã“ã‚ŒãŒ LLM ã® Perplexity = $2^{H}$ ã‚’è©•ä¾¡æŒ‡æ¨™ã¨ã—ã¦ä½¿ãˆã‚‹ç†ç”±ã€‚
:::

:::details Q7: æ¬¡å…ƒã®å‘ªã„ã¯å›é¿ã§ããªã„ã®ã‹ï¼Ÿ
å®Œå…¨ã«ã¯å›é¿ã§ããªã„ãŒã€ç·©å’Œç­–ãŒã‚ã‚‹: (1) å¤šæ§˜ä½“ä»®èª¬ã‚’åˆ©ç”¨ï¼ˆä½æ¬¡å…ƒæ½œåœ¨ç©ºé–“ï¼‰ã€(2) åˆ†å‰²çµ±æ²»ï¼ˆè‡ªå·±å›å¸°ã¯1æ¬¡å…ƒãšã¤ï¼‰ã€(3) éšå±¤çš„æ§‹é€ ï¼ˆHierarchical VAEï¼‰ã€(4) ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆDiffusion ã¯æ®µéšçš„ï¼‰ã€‚å…¨ã¦ã®æˆåŠŸã—ãŸæ¨å®šé‡ã¯ã€ä½•ã‚‰ã‹ã®å½¢ã§æ¬¡å…ƒã®å‘ªã„ã‚’å›é¿ã—ã¦ã„ã‚‹ã€‚
:::

:::details Q8: KL æœ€å°åŒ–ã¨ Wasserstein è·é›¢æœ€å°åŒ–ã®é•ã„ã¯ï¼Ÿ
KL: å¯†åº¦æ¯” $p/q$ ã«åŸºã¥ãã€‚$q = 0$ ã®å ´æ‰€ã§ $p > 0$ ãªã‚‰ $\infty$ã€‚æ”¯æŒé›†åˆãŒç•°ãªã‚‹ã¨ä½¿ãˆãªã„ã€‚Wasserstein: ã€Œè³ªé‡ã‚’ç§»å‹•ã™ã‚‹ã‚³ã‚¹ãƒˆã€ã«åŸºã¥ãã€‚æ”¯æŒé›†åˆãŒç•°ãªã£ã¦ã‚‚å®šç¾©ã§ãã‚‹ã€‚WGAN [Arjovsky+ 2017] ãŒ Wasserstein è·é›¢ã§ GAN ã‚’å®‰å®šåŒ–ã•ã›ãŸã€‚ç¬¬13å›ã§è©³ã—ãæ‰±ã†ã€‚
:::

:::details Q9: ã€Œæ¨å®šé‡ã®è¨­è¨ˆãŒå…¨ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã®æ ¹åº•ã«ã‚ã‚‹ã€ã¨ã¯ã©ã†ã„ã†æ„å‘³ï¼Ÿ
ç”»åƒç”Ÿæˆã¯ã€Œç”»åƒã®ç¢ºç‡åˆ†å¸ƒ $p(\text{image})$ ã®æ¨å®šã€ã€ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã¯ã€Œæ–‡ã®ç¢ºç‡åˆ†å¸ƒ $p(\text{text})$ ã®æ¨å®šã€ã€‚å¿œç”¨ã¯é•ã†ãŒã€æ•°å­¦ã¯åŒã˜ â€” å°¤åº¦é–¢æ•°ã‚’æœ€å¤§åŒ–ã™ã‚‹æ¨å®šé‡ã®è¨­è¨ˆã ã€‚ã ã‹ã‚‰ã“ãã€MLE ã‚„ KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¨ã„ã†å…±é€šã®æ¨å®šåŸç†ãŒå…¨ã¦ã«é€šç”¨ã™ã‚‹ã€‚
:::

:::details Q10: ã“ã®è¬›ç¾©ã®å†…å®¹ã¯ã€å®Ÿå‹™ã§ã©ã®ç¨‹åº¦å¿…è¦ï¼Ÿ
MLE = CE = KL ã®ç­‰ä¾¡æ€§ã¯ LLM ã‚’ä½¿ã†å…¨ã¦ã®äººã«å¿…é ˆã€‚æ¨å®šé‡ã®åˆ†é¡ä½“ç³»ã®ç†è§£ã¯ã€é©åˆ‡ãªãƒ¢ãƒ‡ãƒ«é¸æŠã«ä¸å¯æ¬ ã€‚FID/KID/CMMD ã®æ•°å­¦çš„ç†è§£ã¯è«–æ–‡ã‚’èª­ã‚€éš›ã«å¿…è¦ã€‚ã€Œã¨ã‚Šã‚ãˆãš Diffusionã€ã§ã¯ãªãã€Œãªãœã‚¹ã‚³ã‚¢æ¨å®šé‡ãŒé©åˆ‡ã‹ã€ã‚’ç†è§£ã™ã‚‹åŠ›ã‚’é¤Šã†å›ã€‚
:::

### 6.12 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ1é€±é–“ãƒ—ãƒ©ãƒ³ï¼‰

| æ—¥ | å†…å®¹ | ç›®å®‰æ™‚é–“ |
|:---|:-----|:---------|
| Day 1 | Zone 0-2 ã‚’é€šèª­ + æ¬¡å…ƒã®å‘ªã„ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ | 45åˆ† |
| Day 2 | Zone 3 ã® 3.1-3.5ï¼ˆMLE ç†è«–ãƒ‘ãƒ¼ãƒˆï¼‰ã‚’ç´™ã§å°å‡º | 90åˆ† |
| Day 3 | Zone 3 ã® 3.6-3.10ï¼ˆæ¨å®šé‡åˆ†é¡ãƒ»æš—é»™çš„æ¨å®šãƒ»Scoreï¼‰ã‚’ç²¾èª­ | 60åˆ† |
| Day 4 | Zone 3 ã® 3.12-3.14ï¼ˆçµ±è¨ˆçš„è·é›¢ + ãƒœã‚¹æˆ¦ï¼‰+ Zone 4 ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œ | 90åˆ† |
| Day 5 | Zone 5 ã®è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ + Goodfellow (2014) è«–æ–‡ Pass 1 | 60åˆ† |
| Day 6 | ãƒœã‚¹æˆ¦ã®ä¸‰ä½ä¸€ä½“ã‚’ç´™ã§å†ç¾ + åˆ†é¡ãƒãƒ£ãƒ¼ãƒˆä½œæˆ | 45åˆ† |
| Day 7 | ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆæœ€çµ‚ç¢ºèª + Zone 6 ã®æ¥ç¶šãƒãƒƒãƒ—ã§å…¨ä½“ã‚’ä¿¯ç° | 30åˆ† |

### 6.13 é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼

```python
lecture7_progress = {
    "zone0_quickstart": True,
    "zone1_experience": True,
    "zone2_intuition": True,
    "zone3_math": {
        "mle_definition": False,       # Can you define MLE?
        "mle_ce_equivalence": False,    # Can you prove MLE = CE?
        "mle_kl_equivalence": False,    # Can you prove MLE = KL?
        "fisher_asymptotics": False,    # Can you state 3 properties?
        "mle_limitations": False,       # Can you list 3 limitations?
        "estimator_classification": False, # Can you classify estimators?
        "flow_change_of_var": False,    # Can you write the formula?
        "gan_objective": False,         # Can you write min-max?
        "optimal_discriminator": False, # Can you derive D*?
        "score_function": False,        # Can you explain score?
        "mode_cover_seek": False,       # Can you explain both?
        "fid_formula": False,           # Can you write FID?
        "llm_mle": False,              # Can you show LLM = MLE?
        "boss_trinity": False,          # Can you show MLE=CE=KL?
    },
    "zone4_implementation": False,
    "zone5_experiment": False,
}

completed = sum(1 for v in lecture7_progress["zone3_math"].values() if v)
total = len(lecture7_progress["zone3_math"])
print(f"Zone 3 progress: {completed}/{total} ({completed/total:.0%})")
print(f"Mark each as True when you can do it WITHOUT looking at notes.")
```

### 6.14 æ¬¡å›äºˆå‘Š â€” ç¬¬8å›: æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ« & EMç®—æ³•

ç¬¬7å›ã§ã€ŒMLE ã®é™ç•Œã€ã‚’æ˜ç¢ºã«ã—ãŸã€‚å˜ç´”ãªãƒ¢ãƒ‡ãƒ«æ—ã§ã¯è¤‡é›‘ãªãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã‚’æ‰ãˆã‚‰ã‚Œãªã„ã€‚

ç¬¬8å›ã¯ã“ã®é™ç•Œã‚’æ‰“ç ´ã™ã‚‹ã€‚

- **æ½œåœ¨å¤‰æ•°ã®å°å…¥**: $p(x) = \int p(x|z) p(z) dz$ â€” è¦³æ¸¬ã®èƒŒå¾Œã«ã€Œéš ã‚ŒãŸå¤‰æ•°ã€ã‚’ä»®å®šã™ã‚‹
- **EMç®—æ³•**: å‘¨è¾ºå°¤åº¦ãŒè¨ˆç®—ä¸èƒ½ã§ã‚‚ã€E-step ã¨ M-step ã®äº¤äº’æœ€é©åŒ–ã§ MLE ã‚’è¿‘ä¼¼ã™ã‚‹
- **ELBO ã®å°å‡º**: Jensen ã®ä¸ç­‰å¼ã‹ã‚‰ $\log p(x) \geq \text{ELBO}$ ã‚’å°å‡º â€” ã“ã‚ŒãŒ VAE ã®æ•°å­¦çš„åŸºç›¤
- **GMM ã®å®Œå…¨å®Ÿè£…**: æœ¬è¬›ç¾©ã® GMM ã‚’ EM ã§è¨“ç·´ã—ã€å¤šå³°åˆ†å¸ƒã‚’æ­£ã—ãæ‰ãˆã‚‹
- **Python ã®é€Ÿåº¦å•é¡Œ**: EM ã®åå¾©è¨ˆç®—ãŒ Python ã®é™ç•Œã‚’éœ²å‘ˆã™ã‚‹

ç¬¬6å›ã® KL + ç¬¬7å›ã® MLE + ç¬¬8å›ã® ELBOã€‚ã“ã®3ã¤ãŒåˆæµã™ã‚‹ã¨ãã€ç¬¬9å›ã§ VAE ãŒè‡ªç„¶ã«èª•ç”Ÿã™ã‚‹ã€‚

:::message
**é€²æ—: 100% å®Œäº†** ç¬¬7å›ã€Œæœ€å°¤æ¨å®šã¨çµ±è¨ˆçš„æ¨è«–ã€å®Œäº†ã€‚Course I ã®æ•°å­¦çš„æ­¦è£…ã¯ 7/8ã€‚æ¬¡å›ã¯æ½œåœ¨å¤‰æ•°ã§ MLE ã®é™ç•Œã‚’æ‰“ç ´ã™ã‚‹ã€‚
:::

### 6.15 ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **æ¨å®šé‡ã®è¨­è¨ˆãŒå…¨ã¦ã‚’æ±ºã‚ã‚‹ã€‚VAE/GAN/Flow/Diffusion ã¯ã€MLE ã®100å¹´ã®æ•°å­¦ãŒç”Ÿã‚“ã å¤‰å½¢ã«éããªã„ã®ã§ã¯ï¼Ÿ**

ã“ã®å•ã„ã‚’3ã¤ã®è§’åº¦ã‹ã‚‰è€ƒãˆã¦ã¿ã¦ã»ã—ã„ã€‚

1. **æ•°å­¦çš„ç­‰ä¾¡æ€§**: ç”»åƒç”Ÿæˆã‚‚ã€ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚‚ã€åˆ†å­ç”Ÿæˆã‚‚ã€æ•°å­¦çš„ã«ã¯å…¨ã¦åŒã˜ â€” é«˜æ¬¡å…ƒç¢ºç‡åˆ†å¸ƒ $p(x)$ ã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚VAE ã® ELBO ã¯ç”»åƒã«ã‚‚ãƒ†ã‚­ã‚¹ãƒˆã«ã‚‚é©ç”¨ã§ãã‚‹ã€‚GAN ã®æ•µå¯¾çš„è¨“ç·´ã¯ã€ãƒ‡ãƒ¼ã‚¿ã®ç¨®é¡ã‚’å•ã‚ãªã„ã€‚æ¨å®šé‡è¨­è¨ˆã®ã€ŒçœŸã®å§¿ã€ã¯ã€ç‰¹å®šã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã«ç¸›ã‚‰ã‚Œãªã„**æ±ç”¨çš„ãªç¢ºç‡åˆ†å¸ƒå­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**ã ã€‚

2. **ç§‘å­¦çš„ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ**: AlphaFold 2 ã¯ã‚¿ãƒ³ãƒ‘ã‚¯è³ªæ§‹é€ ã‚’ã€Œç”Ÿæˆã€ã—ã€æ°—å€™ç§‘å­¦è€…ã¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã®å‡ºåŠ›ã‹ã‚‰ã€Œäº‹å¾Œåˆ†å¸ƒã‚’æ¨å®šã€ã™ã‚‹ã€‚ã“ã‚Œã‚‰ã¯ç”»åƒç”Ÿæˆã¨ã¯ç„¡é–¢ä¿‚ã ãŒã€åŒã˜æ•°å­¦çš„é“å…·ï¼ˆMLEã€å¤‰åˆ†æ¨è«–ã€ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°ï¼‰ã‚’ä½¿ã£ã¦ã„ã‚‹ã€‚çµ±è¨ˆçš„æ¨è«–ã®æœ€å¤§ã®ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã¯ã€ç”»åƒç”Ÿæˆã§ã¯ãªã**ç§‘å­¦çš„ç™ºè¦‹**ã«ã‚ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã€‚

3. **èªçŸ¥ã®åã‚Š**: ç”»åƒç”ŸæˆãŒæ³¨ç›®ã•ã‚Œã‚‹ã®ã¯ã€äººé–“ã«ã¨ã£ã¦ã€Œè¦–è¦šçš„ã«åˆ†ã‹ã‚Šã‚„ã™ã„ã€ã‹ã‚‰ã«éããªã„ã€‚ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆGPTï¼‰ã¯è¨€èªã®ç¢ºç‡åˆ†å¸ƒã®å­¦ç¿’ã§ã‚ã‚Šã€éŸ³å£°åˆæˆã¯æ³¢å½¢ã®ç¢ºç‡åˆ†å¸ƒã®å­¦ç¿’ã§ã‚ã‚Šã€åˆ†å­è¨­è¨ˆã¯åŒ–å­¦ç©ºé–“ã®ç¢ºç‡åˆ†å¸ƒã®å­¦ç¿’ã ã€‚ã€Œç”»åƒç”Ÿæˆ AIã€ã¨å‘¼ã¶ã®ã¯ã€æœ¨ã‚’è¦‹ã¦æ£®ã‚’è¦‹ãªã„ã“ã¨ã ã€‚

:::details æ­´å²çš„æ–‡è„ˆ: Fisher ã®ã€Œæœ€å°¤æ³•ã€ã¨ç”Ÿæˆ AI ã®100å¹´
Fisher ãŒ 1922å¹´ã«æœ€å°¤æ¨å®šã‚’ä½“ç³»åŒ–ã—ãŸã¨ã [^1]ã€å½¼ã¯ã€Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®šã®ä¸€èˆ¬ç†è«–ã€ã‚’æ§‹ç¯‰ã—ã‚ˆã†ã¨ã—ã¦ã„ãŸã€‚100å¹´å¾Œã€ãã®ã€Œä¸€èˆ¬ç†è«–ã€ãŒ DALL-E ã‚„ Stable Diffusion ã®æ•°å­¦çš„åŸºç›¤ã«ãªã£ã¦ã„ã‚‹ã€‚Fisher ãŒ MLE ã‚’ã€ŒOn the mathematical foundations of theoretical statisticsã€ã¨é¡Œã—ãŸã®ã¯ã€ã€ŒåŸºç›¤ã€ï¼ˆfoundationsï¼‰ã‚’ä½œã‚ã†ã¨ã—ãŸã‹ã‚‰ã ã€‚å®Ÿéš›ã«ãã†ãªã£ãŸ â€” MLE ã¯çµ±è¨ˆå­¦ã®åŸºç›¤ã§ã‚ã‚‹ã ã‘ã§ãªãã€ç”Ÿæˆ AI ã®åŸºç›¤ã§ã‚‚ã‚ã‚‹ã€‚
:::

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Fisher, R. A. (1922). "On the mathematical foundations of theoretical statistics." *Philosophical Transactions of the Royal Society of London, Series A*, 222, 309-368.
@[card](https://doi.org/10.1098/rsta.1922.0009)

[^2]: Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., et al. (2014). "Generative Adversarial Nets." *NeurIPS 2014*.
@[card](https://arxiv.org/abs/1406.2661)

[^3]: Kingma, D. P. & Welling, M. (2013). "Auto-Encoding Variational Bayes." *ICLR 2014*.
@[card](https://arxiv.org/abs/1312.6114)

[^4]: Heusel, M., Ramsauer, H., Unterthiner, T., et al. (2017). "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium." *NeurIPS 2017*.
@[card](https://arxiv.org/abs/1706.08500)

[^5]: Ho, J., Jain, A. & Abbeel, P. (2020). "Denoising Diffusion Probabilistic Models." *NeurIPS 2020*.
@[card](https://arxiv.org/abs/2006.11239)

[^6]: Mohamed, S. & Lakshminarayanan, B. (2016). "Learning in Implicit Generative Models." *arXiv:1610.03483*.
@[card](https://arxiv.org/abs/1610.03483)

[^7]: Rezende, D. J. & Mohamed, S. (2015). "Variational Inference with Normalizing Flows." *ICML 2015*.
@[card](https://arxiv.org/abs/1505.05770)

[^8]: Salimans, T., Goodfellow, I., Zaremba, W., et al. (2016). "Improved Techniques for Training GANs." *NeurIPS 2016*.
@[card](https://arxiv.org/abs/1606.03498)

[^9]: Jayasumana, S., Ramalingam, S., Veit, A., et al. (2024). "Rethinking FID: Towards a Better Evaluation Metric for Image Generation." *CVPR 2024*.
@[card](https://arxiv.org/abs/2401.09603)

[^10]: Song, Y. & Ermon, S. (2019). "Generative Modeling by Estimating Gradients of the Data Distribution." *NeurIPS 2019*.
@[card](https://arxiv.org/abs/1907.05600)

[^11]: Dinh, L., Krueger, D. & Bengio, Y. (2014). "NICE: Non-linear Independent Components Estimation." *ICLR 2015 Workshop*.
@[card](https://arxiv.org/abs/1410.8516)

[^12]: Dinh, L., Sohl-Dickstein, J. & Bengio, S. (2016). "Density estimation using Real NVP." *ICLR 2017*.
@[card](https://arxiv.org/abs/1605.08803)

[^13]: Sohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N. & Ganguli, S. (2015). "Deep Unsupervised Learning using Nonequilibrium Thermodynamics." *ICML 2015*.
@[card](https://arxiv.org/abs/1503.03585)

[^14]: CramÃ©r, H. (1946). *Mathematical Methods of Statistics*. Princeton University Press.

[^15]: Rao, C. R. (1945). "Information and the accuracy attainable in the estimation of statistical parameters." *Bulletin of the Calcutta Mathematical Society*, 37, 81-91.

### æ•™ç§‘æ›¸

- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.
- Goodfellow, I., Bengio, Y. & Courville, A. (2016). *Deep Learning*. MIT Press. [Free: deeplearningbook.org]
- Murphy, K. P. (2023). *Probabilistic Machine Learning: Advanced Topics*. MIT Press. [Free: probml.github.io]
- Cover, T. M. & Thomas, J. A. (2006). *Elements of Information Theory*. 2nd ed. Wiley.

---

## è¨˜æ³•è¦ç´„

| è¨˜å· | èª­ã¿æ–¹ | æ„å‘³ | åˆå‡º |
|:-----|:-------|:-----|:-----|
| $\hat{\theta}_\text{MLE}$ | ã‚·ãƒ¼ã‚¿ãƒãƒƒãƒˆ ã‚¨ãƒ ã‚¨ãƒ«ã‚¤ãƒ¼ | æœ€å°¤æ¨å®šé‡ | å®šç¾© 3.1 |
| $q_\theta(x)$ | ã‚­ãƒ¥ãƒ¼ ã‚·ãƒ¼ã‚¿ ã‚¨ãƒƒã‚¯ã‚¹ | ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã®å¯†åº¦ | å®šç¾© 3.1 |
| $p_\text{data}(x)$ | ãƒ”ãƒ¼ ãƒ‡ãƒ¼ã‚¿ | ãƒ‡ãƒ¼ã‚¿ã®çœŸã®åˆ†å¸ƒ | Zone 0 |
| $\hat{p}(x)$ | ãƒ”ãƒ¼ãƒãƒƒãƒˆ | çµŒé¨“åˆ†å¸ƒ | å®šç† 3.2 |
| $H(\hat{p}, q_\theta)$ | ã‚¨ã‚¤ãƒ | Cross-Entropy | å®šç† 3.2 |
| $D_\text{KL}(\hat{p} \| q_\theta)$ | ã‚±ãƒ¼ã‚¨ãƒ« | KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ | å®šç† 3.3 |
| $\mathcal{I}(\theta)$ | ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼ ã‚¢ã‚¤ | Fisher æƒ…å ±è¡Œåˆ— | æ€§è³ª 3.4b |
| $G_\theta(z)$ | ã‚¸ãƒ¼ ã‚·ãƒ¼ã‚¿ | GAN ã®ç”Ÿæˆå™¨ | å®šç¾© 3.8 |
| $D_\phi(x)$ | ãƒ‡ã‚£ãƒ¼ ãƒ•ã‚¡ã‚¤ | GAN ã®åˆ¤åˆ¥å™¨ | å®šç¾© 3.8 |
| $D_\text{JS}$ | ã‚¸ã‚§ãƒ¼ã‚¨ã‚¹ | Jensen-Shannon ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ | å®šç† 3.8b |
| $s_\theta(x)$ | ã‚¨ã‚¹ ã‚·ãƒ¼ã‚¿ | ã‚¹ã‚³ã‚¢é–¢æ•°ã®æ¨å®š | å®šç¾© 3.9 |
| $\nabla_x \log p(x)$ | ãƒŠãƒ–ãƒ© ã‚¨ãƒƒã‚¯ã‚¹ | ã‚¹ã‚³ã‚¢é–¢æ•°ï¼ˆçœŸï¼‰ | å®šç¾© 3.9 |
| $\epsilon_\theta(x_t, t)$ | ã‚¤ãƒ—ã‚·ãƒ­ãƒ³ ã‚·ãƒ¼ã‚¿ | DDPM ã®ãƒã‚¤ã‚ºäºˆæ¸¬å™¨ | 3.9 |
| $\text{FID}$ | ã‚¨ãƒ•ã‚¢ã‚¤ãƒ‡ã‚£ãƒ¼ | Frechet Inception Distance | å®šç¾© 3.12a |
| $\text{IS}$ | ã‚¢ã‚¤ã‚¨ã‚¹ | Inception Score | å®šç¾© 3.12b |
| $\text{CMMD}$ | ã‚·ãƒ¼ã‚¨ãƒ ã‚¨ãƒ ãƒ‡ã‚£ãƒ¼ | CLIP MMD | å®šç¾© 3.12c |
| $f^{-1}$ | ã‚¨ãƒ• ã‚¤ãƒ³ãƒãƒ¼ã‚¹ | Flow ã®é€†å¤‰æ› | å®šç† 3.7 |
| $\det J$ | ãƒ‡ãƒƒãƒˆ ã‚¸ã‚§ãƒ¼ | ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—å¼ | å®šç† 3.7 |
| $p(z)$ | ãƒ”ãƒ¼ ã‚¼ãƒƒãƒˆ | æ½œåœ¨ç©ºé–“ã®äº‹å‰åˆ†å¸ƒ | 3.6 |
| $x_t$ | ã‚¨ãƒƒã‚¯ã‚¹ ãƒ†ã‚£ãƒ¼ | æ‹¡æ•£éç¨‹ã®æ™‚åˆ» $t$ ã®çŠ¶æ…‹ | 3.9 |
| $\text{ELBO}$ | ã‚¨ãƒ«ãƒœ | å¤‰åˆ†ä¸‹ç•Œï¼ˆç¬¬8å›ã§å°å‡ºï¼‰ | 3.5 |
| $\pi_k$ | ãƒ‘ã‚¤ ã‚±ãƒ¼ | æ··åˆä¿‚æ•°ï¼ˆGMMï¼‰ | 4.1 |
| $\gamma_{nk}$ | ã‚¬ãƒ³ãƒ | è²¬ä»»åº¦ï¼ˆEM ã® E-stepï¼‰ | 4.1 |
| $G_{\theta\#}\mu$ | ãƒ—ãƒƒã‚·ãƒ¥ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ | Pushforward æ¸¬åº¦ | 2.7 |
| $\mathcal{M}$ | ã‚¨ãƒ  | ãƒ‡ãƒ¼ã‚¿å¤šæ§˜ä½“ | 2.5 |
| $D^*_G(x)$ | ãƒ‡ã‚£ãƒ¼ã‚¹ã‚¿ãƒ¼ | GAN ã®æœ€é©åˆ¤åˆ¥å™¨ | å®šç† 3.8a |

---

## å®Ÿè·µãƒãƒ¼ãƒˆã‚·ãƒ¼ãƒˆ

:::details æ¨å®šé‡é¸æŠãƒãƒ¼ãƒˆã‚·ãƒ¼ãƒˆï¼ˆå°åˆ·ç”¨ï¼‰

**å•é¡Œåˆ¥æ¨å®šé‡é¸æŠã‚¬ã‚¤ãƒ‰**

| æ¨å®šã®ç›®çš„ | ç¬¬ä¸€é¸æŠ | ç¬¬äºŒé¸æŠ | ç†ç”± |
|:-----|:---------|:---------|:-----|
| é«˜å“è³ªå¯†åº¦æ¨å®š | ã‚¹ã‚³ã‚¢æ¨å®šé‡ï¼ˆDiffusionï¼‰ | æš—é»™çš„æ¨å®šï¼ˆGANï¼‰ | æ¨å®šç²¾åº¦ + å®‰å®šæ€§ |
| é›¢æ•£ç³»åˆ—æ¨å®š | è‡ªå·±å›å¸°MLEï¼ˆGPTï¼‰ | - | é›¢æ•£ãƒ‡ãƒ¼ã‚¿ã«æœ€é© |
| æ½œåœ¨è¡¨ç¾å­¦ç¿’ | å¤‰åˆ†MLEï¼ˆVAEï¼‰ | å¤‰æ•°å¤‰æ›MLEï¼ˆFlowï¼‰ | æ»‘ã‚‰ã‹ãªæ½œåœ¨ç©ºé–“ |
| ç•°å¸¸æ¤œçŸ¥ | Flow / VAE | - | å°¤åº¦è¨ˆç®—ãŒå¿…è¦ |
| æ­£ç¢ºãªå¯†åº¦æ¨å®š | å¤‰æ•°å¤‰æ›MLEï¼ˆFlowï¼‰ | è‡ªå·±å›å¸°MLE | æ­£ç¢ºãªå°¤åº¦ |
| é«˜é€Ÿã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° | æš—é»™çš„æ¨å®š / å¤‰åˆ†MLE | Consistency Model | 1-shotç”Ÿæˆ |
| æ¡ä»¶ä»˜ãæ¨å®š | ã‚¹ã‚³ã‚¢æ¨å®šé‡ | æš—é»™çš„æ¨å®š | Classifier-free guidance |
| æ™‚ç³»åˆ—æ¨å®š | ã‚¹ã‚³ã‚¢æ¨å®šé‡ | - | æ™‚é–“æ•´åˆæ€§ |

**MLE ã®å…¬å¼é›†**

$$\hat{\theta}_\text{MLE} = \arg\max_\theta \frac{1}{N}\sum_{i=1}^{N} \log q_\theta(x_i) = \arg\min_\theta H(\hat{p}, q_\theta) = \arg\min_\theta D_\text{KL}(\hat{p} \| q_\theta)$$

**ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã® MLEï¼ˆè¦šãˆã‚‹ã¹ãï¼‰:**

$$\hat{\mu} = \frac{1}{N}\sum_{i=1}^{N} x_i, \quad \hat{\sigma}^2 = \frac{1}{N}\sum_{i=1}^{N}(x_i - \hat{\mu})^2$$

**MLEã®4å¤‰å½¢ã®æå¤±é–¢æ•°**

```
VAE:       L = -E_q[log p(x|z)] + KL[q(z|x) || p(z)]
GAN:       L_D = -E[log D(x)] - E[log(1-D(G(z)))]
           L_G = -E[log D(G(z))]
Flow:      L = -E[log p(fâ»Â¹(x)) + log|det(âˆ‚fâ»Â¹/âˆ‚x)|]
Diffusion: L = E[||Îµ - Îµ_Î¸(âˆšá¾±â‚œxâ‚€ + âˆš(1-á¾±â‚œ)Îµ, t)||Â²]
```

**çµ±è¨ˆçš„è·é›¢ãƒ¯ãƒ³ãƒ©ã‚¤ãƒŠãƒ¼**

```python
# FID: Wâ‚‚ distance with Gaussian approximation
FID = np.dot(mu_r-mu_g, mu_r-mu_g) + np.trace(sigma_r + sigma_g - 2*sqrtm(sigma_r@sigma_g))

# CMMD: MMD in CLIP space
CMMD2 = mean_k(r,r) + mean_k(g,g) - 2*mean_k(r,g)  # k = RBF kernel

# Perplexity: exponentiated cross-entropy
PPL = np.exp(cross_entropy_loss)
```

**é‡è¦ãªç­‰ä¾¡é–¢ä¿‚**

```
MLE â‰¡ Cross-Entropyæœ€å°åŒ– â‰¡ KLæœ€å°åŒ– â‰¡ å‰å‘ãKLæœ€å°åŒ–
GAN â‰¡ JSDæœ€å°åŒ– â‰¡ å¯†åº¦æ¯”æ¨å®š
LLMè¨“ç·´ â‰¡ è‡ªå·±å›å¸°MLE â‰¡ æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³CEæœ€å°åŒ–
Score Matching â‰¡ Denoising â‰¡ Diffusion (ç°¡æ˜“ç‰ˆ)
MAP â‰¡ MLE + L2æ­£å‰‡åŒ– (ã‚¬ã‚¦ã‚¹äº‹å‰åˆ†å¸ƒã®å ´åˆ)
```

**Mode-Covering vs Mode-Seeking è¦šãˆæ–¹**

```
Forward KL: D(p_data || q_model)
  â†’ q must cover where p > 0
  â†’ "Cover all modes" â†’ blurry but complete
  â†’ Used by: MLE, VAE

Reverse KL: D(q_model || p_data)
  â†’ q must stay where p > 0
  â†’ "Seek one mode" â†’ sharp but incomplete
  â†’ Used by: GAN (approximately via JSD)
```
:::

:::details çµ±è¨ˆçš„æ¨å®šã®å¹´ä»£è¨˜ï¼ˆè¦šãˆã‚‹ã¹ãè«–æ–‡ Top 13ï¼‰

| å¹´ | è«–æ–‡ | è²¢çŒ® | arXiv |
|:---|:-----|:-----|:------|
| 1922 | Fisher | MLE ã®ä½“ç³»åŒ– | - |
| 2013 | Kingma & Welling | VAE | 1312.6114 |
| 2014 | Goodfellow+ | GAN | 1406.2661 |
| 2014 | Dinh+ | NICE (Flow ã®å§‹ç¥–) | 1410.8516 |
| 2015 | Sohl-Dickstein+ | Diffusion ã®ç€æƒ³ | 1503.03585 |
| 2015 | Rezende & Mohamed | Normalizing Flows | 1505.05770 |
| 2016 | Salimans+ | Inception Score | 1606.03498 |
| 2016 | Dinh+ | Real NVP | 1605.08803 |
| 2016 | Mohamed+ | Prescribed vs Implicit | 1610.03483 |
| 2017 | Heusel+ | FID | 1706.08500 |
| 2019 | Song & Ermon | Score Matching ç”Ÿæˆ | 1907.05600 |
| 2020 | Ho+ | DDPM | 2006.11239 |
| 2024 | Jayasumana+ | CMMD (FID æ”¹å–„) | 2401.09603 |
:::

:::details æ¨å®šé‡ã®æ•°å­¦çš„å‰ææ¡ä»¶ãƒãƒƒãƒ—

```
ç¬¬2å› ç·šå½¢ä»£æ•°
  â”œâ”€â”€ å›ºæœ‰å€¤åˆ†è§£ â†’ FID ã®è¡Œåˆ—å¹³æ–¹æ ¹
  â”œâ”€â”€ è¡Œåˆ—å¼ â†’ Flow ã®ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³
  â””â”€â”€ å†…ç©ç©ºé–“ â†’ Fisher æƒ…å ±è¡Œåˆ—

ç¬¬3å› å¾®åˆ†ç©åˆ†
  â”œâ”€â”€ åå¾®åˆ† â†’ MLE ã®å‹¾é…
  â”œâ”€â”€ ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ â†’ å¤‰æ•°å¤‰æ›å…¬å¼
  â””â”€â”€ é€£é–å¾‹ â†’ Backpropagation

ç¬¬4å› ç¢ºç‡çµ±è¨ˆ
  â”œâ”€â”€ ç¢ºç‡åˆ†å¸ƒ â†’ å¯†åº¦æ¨å®šã®å®šç¾©
  â”œâ”€â”€ ãƒ™ã‚¤ã‚ºã®å®šç† â†’ äº‹å¾Œæ¨è«–
  â””â”€â”€ æ¡ä»¶ä»˜ãç¢ºç‡ â†’ è‡ªå·±å›å¸°åˆ†è§£

ç¬¬5å› æ¸¬åº¦è«–
  â”œâ”€â”€ Lebesgue ç©åˆ† â†’ æœŸå¾…å€¤ã®å³å¯†å®šç¾©
  â”œâ”€â”€ Radon-Nikodym â†’ å¯†åº¦æ¯”æ¨å®š
  â””â”€â”€ Pushforward æ¸¬åº¦ â†’ GAN ã®ç”Ÿæˆå™¨

ç¬¬6å› æƒ…å ±ç†è«–ãƒ»æœ€é©åŒ–
  â”œâ”€â”€ KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ â†’ MLE ç­‰ä¾¡æ€§
  â”œâ”€â”€ Cross-Entropy â†’ æå¤±é–¢æ•°
  â”œâ”€â”€ Adam â†’ è¨“ç·´ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
  â””â”€â”€ Jensen ä¸ç­‰å¼ â†’ ELBO (ç¬¬8å›)

ç¬¬7å› æœ¬è¬›ç¾©
  â”œâ”€â”€ MLE â†’ å…¨æ¨å®šé‡ã®åŸºç›¤
  â”œâ”€â”€ åˆ†é¡ä½“ç³» â†’ ãƒ¢ãƒ‡ãƒ«é¸æŠã®æŒ‡é‡
  â””â”€â”€ è©•ä¾¡æŒ‡æ¨™ â†’ å“è³ªæ¸¬å®š
```
:::

:::details æ•°å€¤ã®ç›´æ„Ÿï¼ˆè¦šãˆã¦ãŠãã¨ä¾¿åˆ©ï¼‰

| é‡ | å…¸å‹å€¤ | æ„å‘³ |
|:---|:-------|:-----|
| CIFAR-10 FID (DDPM) | 3.17 | ç”»åƒç”Ÿæˆã® SOTA ãƒ¬ãƒ™ãƒ« |
| ImageNet FID (Diffusion) | ~2-5 | å¤§è¦æ¨¡ç”»åƒç”Ÿæˆ |
| GPT-4 Perplexity | ~10-20 (æ¨å®š) | éå¸¸ã«è‰¯ã„è¨€èªãƒ¢ãƒ‡ãƒ« |
| Random baseline PPL | vocab_size (~50K) | å­¦ç¿’å‰ã®çŠ¶æ…‹ |
| é¡”ç”»åƒã®å†…åœ¨æ¬¡å…ƒ | ~100 | 12,288æ¬¡å…ƒä¸­ |
| MNIST ã®å†…åœ¨æ¬¡å…ƒ | ~10-15 | 784æ¬¡å…ƒä¸­ |
| IS (CIFAR-10, æœ€è‰¯) | ~9.5 | æœ€å¤§å€¤ã¯10ï¼ˆã‚¯ãƒ©ã‚¹æ•°ï¼‰ |
| ã‚¬ã‚¦ã‚¹ MLE ã®åæŸ | $O(1/\sqrt{N})$ | Fisher æƒ…å ±ã‹ã‚‰ |
:::

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
