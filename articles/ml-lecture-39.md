---
title: "ç¬¬39å›: Latent Diffusion Models: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸ–¼ï¸"
type: "tech"
topics: ["machinelearning", "deeplearning", "ldm", "julia", "stablediffusion"]
published: true
---

# ç¬¬39å›: ğŸ–¼ï¸ Latent Diffusion Models

:::message
**å‰å›ã®åˆ°é”ç‚¹**: ç¬¬38å›ã§Score/Flow/Diffusionã®æ•°å­¦çš„ç­‰ä¾¡æ€§ã‚’è¨¼æ˜ã—ã€çµ±ä¸€ç†è«–ãŒå®Œæˆã—ãŸã€‚ç†è«–ã ã‘ã§ã¯ç”»åƒã¯ç”Ÿæˆã§ããªã„ â€” ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“æ‹¡æ•£ã®è¨ˆç®—é™ç•Œã‚’è¶…ãˆã‚‹æ½œåœ¨ç©ºé–“æ‹¡æ•£ã¨ã€ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ä»˜ãç”Ÿæˆã¸ã€‚
:::

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” ãƒ”ã‚¯ã‚»ãƒ« vs æ½œåœ¨ç©ºé–“ã®è¡æ’ƒ

```julia
using Lux, Random

# ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“æ‹¡æ•£: 512x512x3 = 786,432æ¬¡å…ƒ
pixel_dim = 512 * 512 * 3
pixel_diffusion_params = pixel_dim * 1000  # 7å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿...

# VAE latent space: 64x64x4 = 16,384æ¬¡å…ƒ (48xåœ§ç¸®!)
latent_dim = 64 * 64 * 4
latent_diffusion_params = latent_dim * 1000  # 1640ä¸‡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

compression_ratio = pixel_dim / latent_dim
speedup = compression_ratio^2  # è¨ˆç®—é‡ã¯O(NÂ²)

println("Compression: $(round(compression_ratio, digits=1))x")
println("Theoretical speedup: $(round(speedup, digits=1))x")
# Output:
# Compression: 48.0x
# Theoretical speedup: 2304.0x
```

**æ•°å¼ã®æ­£ä½“**:
$$
\begin{aligned}
\text{Pixel Diffusion: } &x \in \mathbb{R}^{512 \times 512 \times 3} \quad (\approx 786\text{Kæ¬¡å…ƒ}) \\
\text{Latent Diffusion: } &z \in \mathbb{R}^{64 \times 64 \times 4} \quad (\approx 16\text{Kæ¬¡å…ƒ}) \\
\text{Compression: } &f = \frac{512^2 \times 3}{64^2 \times 4} = 48\times \\
\text{Speedup: } &\mathcal{O}(f^2) \approx 2304\times
\end{aligned}
$$

**ã“ã®30ç§’ã§ä½“æ„Ÿã—ãŸã“ã¨**: æ¬¡å…ƒå‰Šæ¸›ãŒè¨ˆç®—é‡ã‚’ **2000å€** å‰Šæ¸›ã€‚Stable DiffusionãŒæ¶ˆè²»è€…GPUã§å‹•ãç†ç”±ã€‚

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®3%å®Œäº†ï¼** ã“ã‚Œã‹ã‚‰æ½œåœ¨ç©ºé–“ã®æ•°å­¦çš„åŸºç›¤ã¨ã€ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ä»˜ãç”Ÿæˆã®å®Œå…¨ç†è«–ã¸ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” ãªãœæ½œåœ¨ç©ºé–“ã‹

### ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“æ‹¡æ•£ã®é™ç•Œ

ç¬¬36å›ã§å­¦ã‚“ã DDPMã¯ç¾ã—ã„ç†è«–ã ãŒã€è¨ˆç®—é™ç•ŒãŒã‚ã‚‹:

| é …ç›® | 256Ã—256 DDPM | 512Ã—512 DDPM | 1024Ã—1024 DDPM |
|:-----|:-------------|:-------------|:----------------|
| **å…¥åŠ›æ¬¡å…ƒ** | 196,608 | 786,432 | 3,145,728 |
| **U-Net params** | ~100M | ~500M | ~2B |
| **è¨“ç·´æ™‚é–“/iter** | ~1ç§’ | ~5ç§’ | ~20ç§’ |
| **V100 VRAM** | 12GB | 32GB | 80GB (ä¸å¯èƒ½) |
| **åæŸã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³** | 500K | 1M | 2M+ |
| **ç·è¨“ç·´æ™‚é–“** | 6æ—¥ | 58æ—¥ | **å¹´å˜ä½** |

**å•é¡Œã®æœ¬è³ª**: ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ã®æ¬¡å…ƒ $d = H \times W \times C$ ãŒå¤§ãã™ãã‚‹ã€‚U-Netã®self-attentionã¯ $\mathcal{O}(d^2)$ ã®è¨ˆç®—é‡ â€” è§£åƒåº¦ã‚’2å€ã«ã™ã‚‹ã¨è¨ˆç®—é‡ã¯ **16å€**ã€‚

### æ½œåœ¨ç©ºé–“ã¸ã®å¿…ç„¶æ€§

**éµã¨ãªã‚‹è¦³å¯Ÿ**: è‡ªç„¶ç”»åƒã¯é«˜æ¬¡å…ƒç©ºé–“ã«åŸ‹ã‚è¾¼ã¾ã‚Œã¦ã„ã‚‹ãŒã€å®Ÿéš›ã«ã¯ä½æ¬¡å…ƒå¤šæ§˜ä½“ä¸Šã«åˆ†å¸ƒã—ã¦ã„ã‚‹ï¼ˆå¤šæ§˜ä½“ä»®èª¬ï¼‰ã€‚

$$
\begin{aligned}
\text{Pixel space: } &\mathbb{R}^{H \times W \times C} \quad \text{(é«˜æ¬¡å…ƒãƒ»å†—é•·)} \\
\text{Manifold: } &\mathcal{M} \subset \mathbb{R}^{H \times W \times C}, \quad \dim(\mathcal{M}) \ll H \times W \times C \\
\text{Latent space: } &\mathbb{R}^{h \times w \times c}, \quad h \ll H, w \ll W
\end{aligned}
$$

**è§£æ±ºç­–**: VAEã§ä½æ¬¡å…ƒæ½œåœ¨ç©ºé–“ $z$ ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã€ãã“ã§æ‹¡æ•£éç¨‹ã‚’å®Ÿè¡Œã€‚

```mermaid
graph LR
    X["Pixel x âˆˆ R^(HxWxC)"] -->|Encoder E| Z["Latent z âˆˆ R^(hxwxc)"]
    Z -->|Diffusion| Z2["Denoised z' âˆˆ R^(hxwxc)"]
    Z2 -->|Decoder D| X2["Pixel x' âˆˆ R^(HxWxC)"]

    style X fill:#ffcccc
    style Z fill:#ccffcc
    style Z2 fill:#ccffcc
    style X2 fill:#ffcccc
```

### LDM vs ãƒ”ã‚¯ã‚»ãƒ«æ‹¡æ•£: æ•°å€¤æ¯”è¼ƒ

Stable Diffusion 1.5ã®å®Ÿæ¸¬å€¤:

| ãƒ¡ãƒˆãƒªãƒƒã‚¯ | DDPM (512Â²) | LDM (SD 1.5) | æ”¹å–„ç‡ |
|:-----------|:------------|:-------------|:-------|
| **æ½œåœ¨ç©ºé–“æ¬¡å…ƒ** | 786,432 | 16,384 | **48xåœ§ç¸®** |
| **U-Net params** | ~500M | ~860M | 1.7x (ã§ã‚‚GPUã«ä¹—ã‚‹) |
| **è¨“ç·´æ™‚é–“/iter** | ~5ç§’ | ~0.8ç§’ | **6.25xé«˜é€ŸåŒ–** |
| **VRAM (fp16)** | 32GB | 10GB | **3.2xå‰Šæ¸›** |
| **åæŸã‚¹ãƒ†ãƒƒãƒ—** | 1M | 500K | **2xé«˜é€Ÿ** |
| **FID (COCO)** | 12.6 | **10.4** | å“è³ªå‘ä¸Š |

**ãªãœé«˜é€ŸåŒ–ã¨å“è³ªå‘ä¸ŠãŒä¸¡ç«‹ï¼Ÿ**

1. **Inductive bias**: æ½œåœ¨ç©ºé–“ã¯ã€Œæ„å‘³ã®ã‚ã‚‹ç‰¹å¾´ã€ã«åœ§ç¸®æ¸ˆã¿ â†’ æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã—ã‚„ã™ã„
2. **Perceptual compression**: VAEãŒçŸ¥è¦šçš„ã«é‡è¦ãªç‰¹å¾´ã‚’ä¿å­˜ â†’ å“è³ªç¶­æŒ
3. **Computational efficiency**: æ¬¡å…ƒå‰Šæ¸›ã§è¨ˆç®—é‡å‰Šæ¸› â†’ ã‚ˆã‚Šæ·±ã„U-Netãƒ»é•·æ™‚é–“è¨“ç·´ãŒå¯èƒ½

:::message alert
**ã‚ˆãã‚ã‚‹èª¤è§£**: ã€Œæ½œåœ¨ç©ºé–“ã§æ‹¡æ•£ã™ã‚‹ã‹ã‚‰å“è³ªãŒä¸‹ãŒã‚‹ã€â€” å®Ÿéš›ã¯VAEã®çŸ¥è¦šçš„æå¤±é–¢æ•°ã§ **å“è³ªã¯å‘ä¸Š**ã€‚
:::

### æ•°å¼ã§è¦‹ã‚‹LDM

ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“DDPMã®ç›®çš„é–¢æ•°ï¼ˆç¬¬36å›ã®å¾©ç¿’ï¼‰:
$$
\mathcal{L}_\text{DDPM} = \mathbb{E}_{x_0, \epsilon, t} \left[ \|\epsilon - \epsilon_\theta(x_t, t)\|^2 \right]
$$

Latent Diffusion Modelã®ç›®çš„é–¢æ•°:
$$
\mathcal{L}_\text{LDM} = \mathbb{E}_{z_0, \epsilon, t} \left[ \|\epsilon - \epsilon_\theta(z_t, t)\|^2 \right], \quad z_0 = \mathcal{E}(x_0)
$$

ã“ã“ã§:
- $\mathcal{E}: \mathbb{R}^{H \times W \times C} \to \mathbb{R}^{h \times w \times c}$ ãŒVAE Encoder
- $z_t = \sqrt{\bar{\alpha}_t} z_0 + \sqrt{1-\bar{\alpha}_t} \epsilon$ ã¯æ½œåœ¨ç©ºé–“ã§ã®forward process
- $\epsilon_\theta(z_t, t)$ ã¯æ½œåœ¨ç©ºé–“ã§å‹•ä½œã™ã‚‹U-Net

**éµ**: $x$ ã‚’ $z$ ã«ç½®ãæ›ãˆãŸã ã‘ã€‚DDPMã®ç†è«–ãŒãã®ã¾ã¾ä½¿ãˆã‚‹!

```julia
# ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“DDPM
xâ‚€ = randn(512, 512, 3)  # 786Kæ¬¡å…ƒ
Îµâ‚œ = unet_pixel(xâ‚œ, t)    # 786Kæ¬¡å…ƒã®é€†æ‹¡æ•£

# æ½œåœ¨ç©ºé–“LDM
zâ‚€ = encoder(xâ‚€)         # 64Ã—64Ã—4 = 16Kæ¬¡å…ƒ
Îµâ‚œ = unet_latent(zâ‚œ, t)  # 16Kæ¬¡å…ƒã®é€†æ‹¡æ•£ (48xå‰Šæ¸›!)
```

### LDMã®è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

2æ®µéšã®è¨“ç·´:

```mermaid
graph TD
    A[Stage 1: VAEäº‹å‰è¨“ç·´] -->|å›ºå®š| B[Stage 2: æ½œåœ¨ç©ºé–“æ‹¡æ•£è¨“ç·´]

    A1[VAEè¨“ç·´<br>å†æ§‹æˆæå¤± + KLæ­£å‰‡åŒ–] --> A2[Encoder E & Decoder D]
    A2 --> B1[Diffusion U-Netè¨“ç·´<br>Îµ-prediction on z]
    B1 --> B2[å®Œæˆ: LDM]

    style A fill:#ffffcc
    style B fill:#ccffff
```

**Stage 1: VAEè¨“ç·´**
$$
\mathcal{L}_\text{VAE} = \underbrace{\mathbb{E}_{q(z|x)}[\log p(x|z)]}_{\text{å†æ§‹æˆé …}} - \underbrace{\beta \cdot \text{KL}[q(z|x) \| p(z)]}_{\text{æ­£å‰‡åŒ–é …}}
$$

- ç¬¬10å›ã§å­¦ã‚“ã VAEãã®ã‚‚ã®
- $\beta$-VAE ($\beta < 1$) ã§å†æ§‹æˆå“è³ªã‚’é‡è¦–
- ã¾ãŸã¯ VQ-VAE/FSQ ã§é›¢æ•£è¡¨ç¾å­¦ç¿’

**Stage 2: Diffusionè¨“ç·´**
$$
\mathcal{L}_\text{Diffusion} = \mathbb{E}_{z_0 \sim q(z_0), \epsilon \sim \mathcal{N}(0,I), t \sim \mathcal{U}(1,T)} \left[ \|\epsilon - \epsilon_\theta(z_t, t, c)\|^2 \right]
$$

- $c$ ã¯æ¡ä»¶ä»˜ã‘æƒ…å ±ï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒ»ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ç­‰ï¼‰
- VAEã¯ **å›ºå®š** ï¼ˆå‹¾é…ã‚’æµã•ãªã„ï¼‰
- U-Netã ã‘ã‚’è¨“ç·´

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®10%å®Œäº†ï¼** VAEã§åœ§ç¸®ã€æ½œåœ¨ç©ºé–“ã§æ‹¡æ•£ã¨ã„ã†2æ®µéšè¨­è¨ˆã®ç†è«–çš„æ ¹æ‹ ã‚’ç†è§£ã—ãŸã€‚æ¬¡ã¯æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ã¸ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” æ½œåœ¨ç©ºé–“ã®å¹¾ä½•å­¦

### ãªãœã“ã®è¬›ç¾©ã‚’å­¦ã¶ã‹

**åˆ°é”ç›®æ¨™**:
- Stable Diffusionã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å®Œå…¨ç†è§£
- Classifier-Free Guidanceã®æ•°å­¦çš„å°å‡ºã‚’è‡ªåŠ›ã§è¡Œãˆã‚‹
- FLUX/SD3ç­‰ã®æœ€æ–°ãƒ¢ãƒ‡ãƒ«ã®ç†è«–çš„åŸºç›¤ã‚’æŠŠæ¡
- ãƒ†ã‚­ã‚¹ãƒˆâ†’ç”»åƒç”Ÿæˆã®å…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè£…ã§ãã‚‹

**Course IVã§ã®ä½ç½®ã¥ã‘**:

```mermaid
graph LR
    L33[L33: Normalizing Flow] --> L34[L34: EBM]
    L34 --> L35[L35: Score Matching]
    L35 --> L36[L36: DDPM]
    L36 --> L37[L37: SDE/ODE]
    L37 --> L38[L38: Flow Matchingçµ±ä¸€ç†è«–]
    L38 --> L39[â˜…L39: Latent Diffusionâ˜…]
    L39 --> L40[L40: Consistency Models]
    L40 --> L41[L41: World Models]
    L41 --> L42[L42: çµ±ä¸€ç†è«–]

    style L39 fill:#ffcccc
```

**å‰æçŸ¥è­˜**:
- ç¬¬10å›: VAEç†è«–ï¼ˆELBOãƒ»å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ï¼‰
- ç¬¬36å›: DDPMå®Œå…¨å°å‡º
- ç¬¬38å›: Flow Matchingçµ±ä¸€ç†è«–

### æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤ã¨ã®å·®åˆ¥åŒ–

| ãƒˆãƒ”ãƒƒã‚¯ | æ¾å°¾ãƒ»å²©æ¾¤ç ” | æœ¬è¬›ç¾© |
|:---------|:-------------|:-------|
| **LDMç†è«–** | æ¦‚è¦èª¬æ˜ | VAEåœ§ç¸®ç‡ã¨DDPMè¨ˆç®—é‡ã® **å®šé‡çš„é–¢ä¿‚** å°å‡º |
| **CFGå°å‡º** | ã‚¹ã‚­ãƒƒãƒ— | Îµ-prediction / score / æ¸©åº¦ã® **3è¦–ç‚¹ã‹ã‚‰å®Œå…¨å°å‡º** |
| **Text Conditioning** | CLIPç´¹ä»‹ | Cross-Attention / Self-Attention / Positional Encodingã® **å®Ÿè£…ãƒ¬ãƒ™ãƒ«è©³ç´°** |
| **FLUXè§£èª¬** | ãªã— | Rectified Flowçµ±åˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã® **æ•°å­¦çš„è§£æ** (2025) |
| **å®Ÿè£…** | ãªã— | âš¡Juliaè¨“ç·´ + ğŸ¦€Rustæ¨è«– + CFGå®Ÿé¨“ (3,000è¡Œè¶…) |

### LDMã®3ã¤ã®ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼

**ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼1: åœ°å›³å¸³**
- ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ = ä¸–ç•Œä¸­ã®è¡—ã®è©³ç´°åœ°å›³ï¼ˆè†¨å¤§ï¼‰
- æ½œåœ¨ç©ºé–“ = åœ°å›³å¸³ã®ç›®æ¬¡ï¼ˆã‚³ãƒ³ãƒ‘ã‚¯ãƒˆã ãŒä½ç½®é–¢ä¿‚ã¯ä¿å­˜ï¼‰
- æ‹¡æ•£éç¨‹ = ç›®æ¬¡ã‚’ã¼ã‹ã—ã¦å¾©å…ƒï¼ˆç›®æ¬¡ãƒ¬ãƒ™ãƒ«ã§ä½œæ¥­ï¼‰

**ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼2: åœ§ç¸®ãƒ•ã‚¡ã‚¤ãƒ«**
- VAE Encoder = ZIPåœ§ç¸®
- æ½œåœ¨ç©ºé–“ = .zip ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ48å€åœ§ç¸®ï¼‰
- æ‹¡æ•£U-Net = .zipå†…ã§ç›´æ¥ç·¨é›†
- VAE Decoder = è§£å‡

**ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼3: ã‚¹ã‚±ãƒƒãƒâ†’è©³ç´°åŒ–**
- æ½œåœ¨ç©ºé–“ = ãƒ©ãƒ•ã‚¹ã‚±ãƒƒãƒï¼ˆæ§‹å›³ãƒ»é…ç½®ã ã‘ï¼‰
- VAE Decoder = ãƒ‡ã‚£ãƒ†ãƒ¼ãƒ«è¿½åŠ ï¼ˆãƒ†ã‚¯ã‚¹ãƒãƒ£ãƒ»è‰²ãƒ»ç´°éƒ¨ï¼‰
- æ‹¡æ•£éç¨‹ = ã‚¹ã‚±ãƒƒãƒã®ãƒã‚¤ã‚ºé™¤å»

### Course I-IIIæ•°å­¦ã®æ´»ç”¨

| Course | æ´»ç”¨ç®‡æ‰€ |
|:-------|:---------|
| **ç¬¬10å› VAE** | Encoder/Decoderè¨­è¨ˆ / KLæ­£å‰‡åŒ– / å†æ§‹æˆæå¤± |
| **ç¬¬13å› OT** | Wassersteinè·é›¢ã§ã®å“è³ªè©•ä¾¡ / OT-CFMã¨ã®æ¥ç¶š |
| **ç¬¬36å› DDPM** | æ½œåœ¨ç©ºé–“ã§ã®æ‹¡æ•£éç¨‹ / Îµ-prediction / VLB |
| **ç¬¬38å› Flow Matching** | FLUX architecture / Rectified Flowçµ±åˆ |
| **ç¬¬14å› Attention** | Cross-Attention text conditioning / Self-Attention in U-Net |

### ãƒ¬ãƒ™ãƒ«ã‚¢ãƒƒãƒ—ãƒãƒƒãƒ—

```mermaid
graph TD
    A[Lv1: LDMã®å‹•æ©Ÿç†è§£<br>ãªãœæ½œåœ¨ç©ºé–“ã‹] --> B[Lv2: VAEåœ§ç¸®ã®æ•°å­¦<br>åœ§ç¸®ç‡vså“è³ª]
    B --> C[Lv3: æ½œåœ¨ç©ºé–“æ‹¡æ•£<br>DDPMã®æ‹¡å¼µ]
    C --> D[Lv4: Classifier Guidance<br>å‹¾é…ãƒ™ãƒ¼ã‚¹èª˜å°]
    D --> E[Lv5: Classifier-Free Guidance<br>CFGå®Œå…¨å°å‡º]
    E --> F[Lv6: Text Conditioning<br>Cross-Attention]
    F --> G[Lv7: FLUX Architecture<br>Flow Matchingçµ±åˆ]
    G --> H[Boss: Mini LDMå®Ÿè£…<br>Juliaè¨“ç·´+CFGå®Ÿé¨“]
```

### Trojan Horse â€” ğŸâ†’âš¡ğŸ¦€ğŸ”®ã®å¿…ç„¶æ€§

**ç¬¬39å›ã®è¨€èªæ§‹æˆ**: âš¡Julia 70% / ğŸ¦€Rust 20% / ğŸ”®Elixir 10%

**ãªãœJuliaä¸»å½¹ï¼Ÿ**
- VAEè¨“ç·´: Lux.jl + Reactant â†’ JAXä¸¦ã®é€Ÿåº¦
- Diffusionè¨“ç·´: å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã§æå¤±é–¢æ•°ãŒè‡ªå‹•æœ€é©åŒ–
- CFGå®Ÿé¨“: Guidance scaleæƒå¼•ãŒ1è¡Œ

**ãªãœRustï¼Ÿ**
- ONNXæ¨è«–: Candle/Burn â†’ PyTorchæ¯”35-47%é«˜é€Ÿ
- ãƒãƒƒãƒå‡¦ç†: ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ + SIMDæœ€é©åŒ–

**ãªãœElixirï¼Ÿ**
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚µãƒ¼ãƒ“ãƒ³ã‚°: GenStage + Broadway
- åˆ†æ•£æ¨è«–: Supervisor Treeè€éšœå®³æ€§

ã“ã®3è¨€èªã‚¹ã‚¿ãƒƒã‚¯ã§ **è¨“ç·´â†’æ¨è«–â†’é…ä¿¡** ã®å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®20%å®Œäº†ï¼** ç›´æ„Ÿã‚’å›ºã‚ãŸã€‚æ¬¡ã¯æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ã§å®Œå…¨ç†è«–ã¸ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” LDMå®Œå…¨ç†è«–

### 3.1 VAE Encoder/Decoderã®æ•°å­¦

**Encoder $\mathcal{E}: \mathbb{R}^{H \times W \times C} \to \mathbb{R}^{h \times w \times c}$**

| é …ç›® | è©³ç´° |
|:-----|:-----|
| **åœ§ç¸®æ¯”** | $f = \frac{H \times W}{h \times w}$ (å…¸å‹çš„ã« $f=8$ or $f=16$) |
| **ãƒãƒ£ãƒãƒ«æ•°** | $c$ ã¯é€šå¸¸4 or 8 (RGB 3chã‚ˆã‚Šå¢—ãˆã‚‹) |
| **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–** | ResNet blocks + Downsampling + Attention |
| **å‡ºåŠ›** | Deterministic $z = \mathcal{E}(x)$ or Stochastic $z \sim q(z\|x) = \mathcal{N}(\mu_\phi(x), \sigma_\phi^2(x))$ |

**Decoder $\mathcal{D}: \mathbb{R}^{h \times w \times c} \to \mathbb{R}^{H \times W \times C}$**

$$
\tilde{x} = \mathcal{D}(z), \quad \text{s.t.} \quad \|\tilde{x} - x\|_\text{perceptual} < \epsilon
$$

çŸ¥è¦šçš„æå¤±ï¼ˆPerceptual Lossï¼‰ã‚’ä½¿ã†:
$$
\mathcal{L}_\text{rec} = \|\Phi(x) - \Phi(\mathcal{D}(\mathcal{E}(x)))\|^2
$$

ã“ã“ã§ $\Phi$ ã¯VGGã‚„LPIPSç‰¹å¾´æŠ½å‡ºå™¨ã€‚

**2ã¤ã®VAEæ­£å‰‡åŒ–æ–¹å¼**:

| æ–¹å¼ | æ­£å‰‡åŒ–é … | ç‰¹å¾´ |
|:-----|:---------|:-----|
| **KL-regularization** | $\mathcal{L}_\text{KL} = \text{KL}[q(z\|x) \| \mathcal{N}(0,I)]$ | é€£ç¶šæ½œåœ¨ç©ºé–“ / $\beta$-VAE |
| **VQ-regularization** | Codebook loss + Commitment loss | é›¢æ•£æ½œåœ¨ç©ºé–“ / VQ-VAE/FSQ |

**Stable Diffusion 1.x/2.xã®é¸æŠ**: KL-reg VAE with $f=8$ compressionã€‚

**åœ§ç¸®ç‡ vs å†æ§‹æˆå“è³ªã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**:

| åœ§ç¸®ç‡ $f$ | Latent size (512Â² input) | LPIPSâ†“ | FIDâ†“ | è¨“ç·´é€Ÿåº¦ |
|:-----------|:------------------------|:-------|:-----|:---------|
| $f=4$ | 128Ã—128Ã—4 | 0.05 | 1.2 | 1x |
| $f=8$ | 64Ã—64Ã—4 | 0.08 | 2.1 | **4x** |
| $f=16$ | 32Ã—32Ã—4 | 0.15 | 5.6 | 16x |

**çµè«–**: $f=8$ ãŒå“è³ªã¨é€Ÿåº¦ã®æœ€é©ãƒãƒ©ãƒ³ã‚¹ã€‚

```julia
# VAE Encoder/Decoderå®šç¾© (Lux.jl)
function create_vae_encoder(; img_size=512, latent_size=64, latent_channels=4)
    # Downsampling path: 512 -> 256 -> 128 -> 64
    encoder = Chain(
        Conv((3, 3), 3 => 128, pad=1),
        ResBlock(128),
        Downsample(128 => 256),  # 512 -> 256
        ResBlock(256),
        Downsample(256 => 512),  # 256 -> 128
        ResBlock(512),
        Downsample(512 => 512),  # 128 -> 64
        SelfAttention(512),
        ResBlock(512),
        Conv((3, 3), 512 => latent_channels, pad=1)  # Output z
    )
    return encoder
end

# ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
x = randn(Float32, 512, 512, 3, 1)  # [H, W, C, B]
z = encoder(x, ps, st)[1]           # [64, 64, 4, 1] (48xåœ§ç¸®!)
```

:::details VAE Decoderã®ãƒŸãƒ©ãƒ¼æ§‹é€ 
```julia
function create_vae_decoder(; latent_size=64, img_size=512, latent_channels=4)
    decoder = Chain(
        Conv((3, 3), latent_channels => 512, pad=1),
        ResBlock(512),
        SelfAttention(512),
        ResBlock(512),
        Upsample(512 => 512),  # 64 -> 128
        ResBlock(512),
        Upsample(512 => 256),  # 128 -> 256
        ResBlock(256),
        Upsample(256 => 128),  # 256 -> 512
        ResBlock(128),
        Conv((3, 3), 128 => 3, pad=1, activation=tanh)  # Output x
    )
    return decoder
end

xÌƒ = decoder(z, ps, st)[1]  # [512, 512, 3, 1] å†æ§‹æˆ
```
:::

### 3.2 æ½œåœ¨ç©ºé–“ã§ã®æ‹¡æ•£ãƒ—ãƒ­ã‚»ã‚¹

**Forward process** (DDPMç¬¬36å›ã®å¾©ç¿’):
$$
q(z_t | z_0) = \mathcal{N}(z_t; \sqrt{\bar{\alpha}_t} z_0, (1-\bar{\alpha}_t) I)
$$

**é–‰å½¢å¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**:
$$
z_t = \sqrt{\bar{\alpha}_t} z_0 + \sqrt{1-\bar{\alpha}_t} \epsilon, \quad \epsilon \sim \mathcal{N}(0,I)
$$

**Reverse process**:
$$
p_\theta(z_{t-1} | z_t) = \mathcal{N}(z_{t-1}; \mu_\theta(z_t, t), \Sigma_\theta(z_t, t))
$$

**è¨“ç·´ç›®çš„é–¢æ•°** (Îµ-prediction):
$$
\mathcal{L}_\text{simple} = \mathbb{E}_{z_0, \epsilon, t} \left[ \|\epsilon - \epsilon_\theta(z_t, t)\|^2 \right]
$$

**éµ**: ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ã®DDPMã¨æ•°å¼ã¯ **å®Œå…¨ã«åŒã˜**ã€‚$x \to z$ ã®ç½®ãæ›ãˆã ã‘ã€‚

```julia
# Forward process: zâ‚€ã«ãƒã‚¤ã‚ºä»˜åŠ 
function forward_diffusion(zâ‚€, t, Î±â‚œ_bar, rng)
    Îµ = randn(rng, Float32, size(zâ‚€))
    z_t = sqrt(Î±â‚œ_bar) .* zâ‚€ .+ sqrt(1 - Î±â‚œ_bar) .* Îµ
    return z_t, Îµ
end

# è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—
function train_step!(model, zâ‚€, t, Î±â‚œ_bar, ps, st, opt_state)
    z_t, Îµ_true = forward_diffusion(zâ‚€, t, Î±â‚œ_bar, rng)

    # Îµ-prediction
    Îµ_pred, st = model((z_t, t), ps, st)

    # MSE loss
    loss = mean((Îµ_pred .- Îµ_true).^2)

    # Backprop
    gs = gradient(ps -> loss, ps)[1]
    opt_state, ps = Optimisers.update(opt_state, ps, gs)

    return loss, ps, st, opt_state
end
```

**Noise scheduleã®é¸æŠ**:

ç¬¬36å›ã§å­¦ã‚“ã ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒãã®ã¾ã¾ä½¿ãˆã‚‹:
- Linear: $\beta_t = \beta_\text{start} + t \cdot (\beta_\text{end} - \beta_\text{start}) / T$
- Cosine: $\bar{\alpha}_t = \cos\left(\frac{t/T + s}{1+s} \cdot \frac{\pi}{2}\right)^2 / \cos\left(\frac{s}{1+s} \cdot \frac{\pi}{2}\right)^2$
- **Zero Terminal SNR**: $\bar{\alpha}_T = 0$ ã‚’å¼·åˆ¶ï¼ˆå¾Œè¿°ï¼‰

Stable Diffusion 1.x/2.xã¯ **Linear schedule** with 1000 stepsã€‚

### 3.3 LDMè¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

**2æ®µéšè¨“ç·´**:

```mermaid
sequenceDiagram
    participant Data as Dataset
    participant VAE as VAE (fixed)
    participant UNet as Diffusion U-Net
    participant Loss as Loss

    Note over Data,Loss: Stage 1: VAEè¨“ç·´ (åˆ¥é€”å®Œäº†)

    Note over Data,Loss: Stage 2: Diffusionè¨“ç·´
    Data->>VAE: xâ‚€ (512Ã—512Ã—3)
    VAE->>UNet: zâ‚€ = E(xâ‚€) (64Ã—64Ã—4)
    UNet->>UNet: Forward: z_t ~ q(z_t|zâ‚€)
    UNet->>UNet: Predict: Îµ_Î¸(z_t, t)
    UNet->>Loss: ||Îµ - Îµ_Î¸(z_t, t)||Â²
    Loss->>UNet: Backprop (VAEã¯å›ºå®š)
```

**Stage 2ã®å®Œå…¨ãªè¨“ç·´ãƒ«ãƒ¼ãƒ—**:

$$
\begin{aligned}
&\text{for epoch in 1:N} \\
&\quad \text{for } (x_0, c) \in \text{DataLoader} \\
&\quad\quad z_0 \leftarrow \mathcal{E}(x_0) \quad \text{(Encoder forward, no grad)} \\
&\quad\quad t \sim \mathcal{U}(1, T) \\
&\quad\quad \epsilon \sim \mathcal{N}(0, I) \\
&\quad\quad z_t \leftarrow \sqrt{\bar{\alpha}_t} z_0 + \sqrt{1-\bar{\alpha}_t} \epsilon \\
&\quad\quad \epsilon_\theta \leftarrow \text{UNet}(z_t, t, c) \\
&\quad\quad \mathcal{L} \leftarrow \|\epsilon - \epsilon_\theta\|^2 \\
&\quad\quad \theta \leftarrow \theta - \eta \nabla_\theta \mathcal{L} \quad \text{(UNetã®ã¿æ›´æ–°)}
\end{aligned}
$$

```julia
# å®Œå…¨ãªè¨“ç·´ãƒ«ãƒ¼ãƒ—
function train_ldm!(unet, vae_encoder, dataloader, epochs; lr=1e-4)
    opt = Adam(lr)
    opt_state = Optimisers.setup(opt, ps_unet)

    for epoch in 1:epochs
        for (x, c) in dataloader
            # VAE encode (no grad)
            zâ‚€ = vae_encoder(x, ps_vae, st_vae)[1]  # å‹¾é…ãªã—

            # Random timestep
            t = rand(1:T)
            Î±â‚œ_bar = alpha_bar_schedule[t]

            # Forward diffusion
            z_t, Îµ = forward_diffusion(zâ‚€, t, Î±â‚œ_bar, rng)

            # Predict noise
            Îµ_pred, st_unet = unet((z_t, t, c), ps_unet, st_unet)

            # Loss & update
            loss = mse_loss(Îµ_pred, Îµ)
            gs = gradient(ps -> loss, ps_unet)[1]
            opt_state, ps_unet = Optimisers.update(opt_state, ps_unet, gs)
        end
    end
    return ps_unet
end
```

:::message alert
**ã‚ˆãã‚ã‚‹ãƒŸã‚¹**: VAEã«å‹¾é…ã‚’æµã—ã¦ã—ã¾ã†ã€‚Encoderã¯ **å®Œå…¨ã«å›ºå®š** ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„ã€‚
:::

### 3.4 Classifier Guidanceå®Œå…¨ç‰ˆ

**å‹•æ©Ÿ**: æ¡ä»¶ä»˜ãç”Ÿæˆ $p(z_0 | c)$ ã§ã€æ¡ä»¶ $c$ (ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«)ã¸ã®å¿ å®Ÿåº¦ã‚’é«˜ã‚ãŸã„ã€‚

**ãƒ™ã‚¤ã‚ºã®å®šç†**:
$$
\nabla_{z_t} \log p(z_t | c) = \nabla_{z_t} \log p(z_t) + \nabla_{z_t} \log p(c | z_t)
$$

ã“ã“ã§:
- $\nabla_{z_t} \log p(z_t)$ ã¯ç„¡æ¡ä»¶ã‚¹ã‚³ã‚¢ï¼ˆU-NetãŒå­¦ç¿’ï¼‰
- $\nabla_{z_t} \log p(c | z_t)$ ã¯åˆ†é¡å™¨ã®å‹¾é…

**Classifier Guidanceã®ä¿®æ­£ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**:
$$
\tilde{\epsilon}_\theta(z_t, t, c) = \epsilon_\theta(z_t, t) - \sqrt{1-\bar{\alpha}_t} \cdot w \nabla_{z_t} \log p_\phi(c | z_t)
$$

ã“ã“ã§:
- $\epsilon_\theta(z_t, t)$ ã¯ç„¡æ¡ä»¶ãƒã‚¤ã‚ºäºˆæ¸¬
- $p_\phi(c | z_t)$ ã¯ **åˆ¥é€”è¨“ç·´ã—ãŸåˆ†é¡å™¨**
- $w$ ã¯guidance scale

**å•é¡Œç‚¹**:
1. åˆ†é¡å™¨ $p_\phi(c | z_t)$ ã‚’åˆ¥é€”è¨“ç·´ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
2. å„timestep $t$ ã§ç•°ãªã‚‹åˆ†é¡å™¨ãŒå¿…è¦
3. è¨“ç·´ã‚³ã‚¹ãƒˆãŒ2å€

â†’ Classifier-Free Guidanceã§è§£æ±º!

```julia
# Classifier Guidance (å‚è€ƒå®Ÿè£…)
function classifier_guidance_sample(unet, classifier, z_T, c, w)
    z_t = z_T
    for t in T:-1:1
        # ç„¡æ¡ä»¶ã‚¹ã‚³ã‚¢
        Îµ_uncond = unet(z_t, t, nothing)

        # åˆ†é¡å™¨å‹¾é…
        grad_log_p_c = gradient(z -> log_prob(classifier(z), c), z_t)[1]

        # ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹é©ç”¨
        Îµ_guided = Îµ_uncond .- sqrt(1 - Î±_bar[t]) .* w .* grad_log_p_c

        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¹ãƒ†ãƒƒãƒ—
        z_t = reverse_step(z_t, Îµ_guided, t)
    end
    return decoder(z_t)
end
```

### 3.5 Classifier-Free Guidanceå®Œå…¨å°å‡º

**éµã¨ãªã‚‹ã‚¢ã‚¤ãƒ‡ã‚¢**: æ¡ä»¶ä»˜ããƒ¢ãƒ‡ãƒ« $\epsilon_\theta(z_t, t, c)$ ã¨ç„¡æ¡ä»¶ãƒ¢ãƒ‡ãƒ« $\epsilon_\theta(z_t, t, \emptyset)$ ã‚’ **åŒæ™‚ã«è¨“ç·´** ã—ã€æ¨è«–æ™‚ã«ç·šå½¢çµåˆã™ã‚‹ã€‚

**è¨“ç·´æ™‚**: ãƒ©ãƒ³ãƒ€ãƒ ã«conditionã‚’drop
$$
c_\text{input} = \begin{cases}
c & \text{with probability } p_\text{uncond} \text{ (e.g. 0.1)} \\
\emptyset & \text{with probability } 1 - p_\text{uncond}
\end{cases}
$$

ã“ã‚Œã§å˜ä¸€ãƒ¢ãƒ‡ãƒ«ãŒæ¡ä»¶ä»˜ããƒ»ç„¡æ¡ä»¶ä¸¡æ–¹ã‚’å­¦ç¿’ã€‚

**æ¨è«–æ™‚**: 2ã¤ã®äºˆæ¸¬ã®ç·šå½¢çµåˆ
$$
\tilde{\epsilon}_\theta(z_t, t, c, w) = \epsilon_\theta(z_t, t, \emptyset) + w \cdot \left( \epsilon_\theta(z_t, t, c) - \epsilon_\theta(z_t, t, \emptyset) \right)
$$

ã“ã“ã§:
- $w$ ã¯guidance scale ($w=0$: ç„¡æ¡ä»¶, $w=1$: æ¡ä»¶ä»˜ã, $w>1$: over-guidance)
- $\epsilon_\theta(z_t, t, c) - \epsilon_\theta(z_t, t, \emptyset)$ ãŒæ¡ä»¶ã®"æ–¹å‘"

**ãªãœã“ã‚ŒãŒæ©Ÿèƒ½ã™ã‚‹ã‹ï¼Ÿã‚¹ã‚³ã‚¢ã®è¦–ç‚¹ã‹ã‚‰å°å‡º**:

ã‚¹ã‚³ã‚¢é–¢æ•° $s_\theta(z_t, t, c) = -\frac{\epsilon_\theta(z_t, t, c)}{\sqrt{1-\bar{\alpha}_t}}$ ã¨ã™ã‚‹ã¨:

$$
\begin{aligned}
\tilde{s}_\theta(z_t, t, c, w) &= -\frac{\tilde{\epsilon}_\theta(z_t, t, c, w)}{\sqrt{1-\bar{\alpha}_t}} \\
&= -\frac{1}{\sqrt{1-\bar{\alpha}_t}} \left[ \epsilon_\theta(z_t, t, \emptyset) + w(\epsilon_\theta(z_t, t, c) - \epsilon_\theta(z_t, t, \emptyset)) \right] \\
&= s_\theta(z_t, t, \emptyset) + w \cdot (s_\theta(z_t, t, c) - s_\theta(z_t, t, \emptyset)) \\
&= (1-w) \cdot s_\theta(z_t, t, \emptyset) + w \cdot s_\theta(z_t, t, c)
\end{aligned}
$$

ã“ã‚Œã¯ **ç„¡æ¡ä»¶ã‚¹ã‚³ã‚¢ã¨æ¡ä»¶ä»˜ãã‚¹ã‚³ã‚¢ã®åŠ é‡å¹³å‡** !

**$w > 1$ ã®å ´åˆ**:
$$
\tilde{s}_\theta = s_\theta(z_t, t, \emptyset) + w \cdot (s_\theta(z_t, t, c) - s_\theta(z_t, t, \emptyset))
$$

æ¡ä»¶ã®æ–¹å‘ã« **$w$å€å¼·ã** æŠ¼ã™ â†’ mode-seekingè¡Œå‹•ã€‚

**åˆ¥ã®è¦–ç‚¹: æš—é»™çš„ãªæ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:

$w > 1$ ã®ã¨ãã€å®ŸåŠ¹çš„ãªç¢ºç‡åˆ†å¸ƒã¯:
$$
p_w(z_t | c) \propto p(z_t | c)^w \cdot p(z_t)^{1-w}
$$

ã“ã‚Œã¯ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«è¦–ç‚¹ã§:
$$
E_w(z_t) = -w \log p(z_t | c) - (1-w) \log p(z_t)
$$

$w \to \infty$ ã§æ¡ä»¶ä»˜ãåˆ†å¸ƒã® **æœ€é »å€¤** (mode)ã«ãƒ”ãƒ¼ã‚¯ â†’ å“è³ªå‘ä¸Šã ãŒå¤šæ§˜æ€§ä½ä¸‹ã€‚

:::details CFGã®ç†è«–çš„ç†è§£: Mode-Seeking vs Mode-Covering
**$w$ã®åŠ¹æœ**:

| Guidance Scale $w$ | æŒ™å‹• | å“è³ª | å¤šæ§˜æ€§ |
|:-------------------|:-----|:-----|:-------|
| $w = 0$ | ç„¡æ¡ä»¶ç”Ÿæˆ | ä½ | é«˜ |
| $w = 1$ | æ¨™æº–æ¡ä»¶ä»˜ã | ä¸­ | ä¸­ |
| $w \in (1, 7]$ | Mode-seeking | **é«˜** | ä¸­ |
| $w > 7$ | Over-guidance | éé£½å’Œ | **ä½** |

**å…¸å‹çš„ãªå€¤**:
- Stable Diffusion 1.x: $w = 7.5$
- DALL-E 2: $w = 3.0$
- Imagen: $w = 5.0$

**æ•°å­¦çš„è¦–ç‚¹**:
- $w < 1$: Mode-covering (KL[qâ€–p]æœ€å°åŒ–é¢¨)
- $w > 1$: Mode-seeking (KL[pâ€–q]æœ€å°åŒ–é¢¨)
:::

```julia
# Classifier-Free Guidanceå®Ÿè£…
function cfg_sample(unet, z_T, c, w; steps=50)
    z_t = z_T
    timesteps = reverse(1:steps)

    for t in timesteps
        # 2å›ã®forward pass
        Îµ_uncond = unet((z_t, t, nothing), ps, st)[1]  # ç„¡æ¡ä»¶
        Îµ_cond = unet((z_t, t, c), ps, st)[1]           # æ¡ä»¶ä»˜ã

        # CFGçµåˆ
        Îµ_guided = Îµ_uncond .+ w .* (Îµ_cond .- Îµ_uncond)

        # DDIMã‚¹ãƒ†ãƒƒãƒ— (é«˜é€Ÿã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°)
        z_t = ddim_step(z_t, Îµ_guided, t, t-1, Î±â‚œ_bar)
    end

    return vae_decoder(z_t)
end

# è¨“ç·´æ™‚ã®Condition Drop
function train_step_with_cfg!(unet, zâ‚€, c, t, p_uncond=0.1)
    # Randomly drop condition
    if rand() < p_uncond
        c = nothing  # ç„¡æ¡ä»¶åŒ–
    end

    z_t, Îµ = forward_diffusion(zâ‚€, t, Î±â‚œ_bar, rng)
    Îµ_pred = unet((z_t, t, c), ps, st)[1]

    loss = mse_loss(Îµ_pred, Îµ)
    # ... backprop
end
```

### 3.6 CFGã®ç†è«–çš„ç†è§£

**ãªãœå“è³ªãŒå‘ä¸Šã™ã‚‹ã‹ï¼Ÿ**

1. **æ¡ä»¶ã®å¼·èª¿**: $w > 1$ ã§æ¡ä»¶æƒ…å ±ã‚’å¢—å¹… â†’ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¿ å®Ÿåº¦â†‘
2. **åˆ†æ•£å‰Šæ¸›**: ç„¡æ¡ä»¶ã¨ã®å·®åˆ†ãŒã€Œæ¡ä»¶ã®ç´”ç²‹ãªåŠ¹æœã€ â†’ ãƒã‚¤ã‚ºé™¤å»
3. **Mode-seeking**: é«˜ç¢ºç‡é ˜åŸŸã«é›†ä¸­ â†’ é®®æ˜ãªç”»åƒ

**ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**:
$$
\text{Quality} \uparrow, \quad \text{Diversity} \downarrow, \quad \text{Compute} \times 2
$$

æ¨è«–æ™‚ã«2å›ã®U-Net forward passãŒå¿…è¦ â†’ é€Ÿåº¦åŠæ¸›ã€‚

**æœ€é©åŒ–**: Negative Promptï¼ˆæ¬¡ç¯€ï¼‰ã§1.5å›ã«å‰Šæ¸›å¯èƒ½ã€‚

### 3.7 Negative Prompt

**å‹•æ©Ÿ**: CFGã§ $\epsilon_\theta(z_t, t, \emptyset)$ ã®ä»£ã‚ã‚Šã«ã€Œé¿ã‘ãŸã„æ¦‚å¿µã€ã‚’æŒ‡å®šã—ãŸã„ã€‚

**å®šå¼åŒ–**:
$$
\tilde{\epsilon}_\theta(z_t, t, c_\text{pos}, c_\text{neg}, w) = \epsilon_\theta(z_t, t, c_\text{neg}) + w \cdot \left( \epsilon_\theta(z_t, t, c_\text{pos}) - \epsilon_\theta(z_t, t, c_\text{neg}) \right)
$$

ã“ã“ã§:
- $c_\text{pos}$ ã¯æ­£ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ("a beautiful mountain")
- $c_\text{neg}$ ã¯è² ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ("blurry, low quality")
- $w$ ã¯guidance scale

**è§£é‡ˆ**: ã€Œ$c_\text{neg}$ã‹ã‚‰é›¢ã‚Œã€$c_\text{pos}$ã«è¿‘ã¥ãã€æ–¹å‘ã«ã‚¬ã‚¤ãƒ‰ã€‚

**å…¸å‹çš„ãªNegative Prompt**:
```
"blurry, low quality, watermark, signature, jpeg artifacts,
 worst quality, low resolution, bad anatomy"
```

```julia
# Negative Promptå®Ÿè£…
function cfg_with_negative(unet, z_T, c_pos, c_neg, w)
    z_t = z_T
    for t in T:-1:1
        Îµ_neg = unet((z_t, t, c_neg), ps, st)[1]
        Îµ_pos = unet((z_t, t, c_pos), ps, st)[1]

        # Negative Prompté©ç”¨
        Îµ_guided = Îµ_neg .+ w .* (Îµ_pos .- Îµ_neg)

        z_t = reverse_step(z_t, Îµ_guided, t)
    end
    return decoder(z_t)
end
```

**åŠ¹æœ**:
- å“è³ªå‘ä¸Š: ã€Œblurryã€ã‚’é¿ã‘ã‚‹ â†’ é®®æ˜
- ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆå‰Šæ¸›: ã€Œwatermarkã€ã‚’é¿ã‘ã‚‹
- è§£å‰–å­¦çš„ã‚¨ãƒ©ãƒ¼å‰Šæ¸›: ã€Œbad anatomyã€ã‚’é¿ã‘ã‚‹

### 3.8 Text Conditioning

**å•é¡Œè¨­å®š**: ãƒ†ã‚­ã‚¹ãƒˆ $\mathbf{t}$ â†’ ç”»åƒ $x$ ã®ç”Ÿæˆã€‚æ¡ä»¶ $c = \text{Encoder}_\text{text}(\mathbf{t})$ã€‚

**Text Encoderé¸æŠ**:

| Encoder | æ¬¡å…ƒ | ç‰¹å¾´ | SDæ¡ç”¨ |
|:--------|:-----|:-----|:-------|
| **CLIP Text** | 768 | Vision-languageã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ | SD 1.x |
| **OpenCLIP** | 1024 | ã‚ˆã‚Šå¤§è¦æ¨¡CLIP | SD 2.x |
| **T5** | 4096 | ç´”ç²‹è¨€èªãƒ¢ãƒ‡ãƒ« | Imagen/SD3 |
| **CLIP+T5** | 768+4096 | ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ | SDXL/SD3 |

**CLIP vs T5ã®é•ã„**:

| é …ç›® | CLIP | T5 |
|:-----|:-----|:---|
| **è¨“ç·´** | Image-Text contrastive | Language modeling |
| **èªå½™** | 49K | 32K |
| **æ–‡ç†è§£** | æµ…ã„ | æ·±ã„ (Transformer) |
| **ç”»åƒã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ** | å¼·ã„ | å¼±ã„ |
| **é•·æ–‡** | 77 tokens max | 512 tokens |

**Stable Diffusion 3ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰**:
- CLIP: ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªæ„å‘³ï¼ˆã€ŒçŠ¬ã€ã€Œå±±ã€ï¼‰
- T5: è©³ç´°ãªé–¢ä¿‚æ€§ï¼ˆã€ŒçŠ¬ãŒå±±ã®ä¸Šã«åº§ã£ã¦ã„ã‚‹ã€ï¼‰

:::details CLIP Text Encoderã®ä»•çµ„ã¿
```python
# CLIPãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° (PyTorchæ“¬ä¼¼ã‚³ãƒ¼ãƒ‰)
text = "A beautiful mountain landscape"
tokens = tokenizer(text)  # [BOS, 320, 1215, 5270, 5677, EOS, PAD, ...]  # 77 tokens

# Transformer Encoder
embeddings = text_embedding(tokens)  # [77, 768]
hidden = transformer_layers(embeddings)  # [77, 768]

# Pooling
pooled = hidden[0]  # [BOS]ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨ (BERTé¢¨)
# ã¾ãŸã¯
pooled = hidden.mean(dim=0)  # å¹³å‡ãƒ—ãƒ¼ãƒªãƒ³ã‚°

# Output: [768] vector
```

SD 1.xã¯CLIPã®æœ€çµ‚å±¤hidden statesã‚’ **å…¨ã¦ä½¿ç”¨**:
- $c = [\mathbf{h}_0, \mathbf{h}_1, \ldots, \mathbf{h}_{76}]$ : shape [77, 768]
- ã“ã‚Œã‚’Cross-Attentionã«å…¥åŠ›
:::

### 3.9 Cross-Attention Text Conditioning

**U-Netã¸ã®ãƒ†ã‚­ã‚¹ãƒˆæ³¨å…¥æ–¹æ³•**:

```mermaid
graph TD
    T[Text: "mountain landscape"] --> TE[Text Encoder<br>CLIP/T5]
    TE --> C[c âˆˆ R^(LÃ—D)]

    Z[z_t âˆˆ R^(hÃ—wÃ—c)] --> SA[Self-Attention]
    SA --> CA[Cross-Attention]
    C --> CA
    CA --> FF[FeedForward]
    FF --> Out[Output]

    style CA fill:#ffcccc
```

**Cross-Attentionã®å®šå¼åŒ–**:

U-Netã®ä¸­é–“ç‰¹å¾´ $\mathbf{f} \in \mathbb{R}^{(h \times w) \times d}$ ã«å¯¾ã—ã¦:

$$
\begin{aligned}
Q &= W_Q \mathbf{f} \quad \in \mathbb{R}^{(h \times w) \times d_k} \\
K &= W_K \mathbf{c} \quad \in \mathbb{R}^{L \times d_k} \\
V &= W_V \mathbf{c} \quad \in \mathbb{R}^{L \times d_v} \\
\text{Attention}(Q, K, V) &= \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
\end{aligned}
$$

ã“ã“ã§:
- $\mathbf{f}$: U-Netä¸­é–“ç‰¹å¾´ï¼ˆç”»åƒå´ï¼‰
- $\mathbf{c}$: ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆæ¡ä»¶å´ï¼‰
- $Q$ from image, $K, V$ from text â†’ **Cross**-Attention

**Multi-Head Cross-Attention**:
$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W^O
$$

å„headãŒç•°ãªã‚‹æ„å‘³çš„å´é¢ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ï¼ˆä¾‹: è‰²ã€å½¢ã€é…ç½®ï¼‰ã€‚

```julia
# Cross-Attention Layer (Lux.jl)
struct CrossAttention
    num_heads::Int
    head_dim::Int
    W_Q::Dense
    W_K::Dense
    W_V::Dense
    W_O::Dense
end

function (ca::CrossAttention)(f, c)
    # f: [h*w, d], c: [L, d_text]
    Q = ca.W_Q(f)      # [h*w, num_heads * head_dim]
    K = ca.W_K(c)      # [L, num_heads * head_dim]
    V = ca.W_V(c)      # [L, num_heads * head_dim]

    # Reshape for multi-head
    Q = reshape(Q, :, ca.num_heads, ca.head_dim)  # [h*w, heads, dim]
    K = reshape(K, :, ca.num_heads, ca.head_dim)  # [L, heads, dim]
    V = reshape(V, :, ca.num_heads, ca.head_dim)

    # Attention
    scores = batched_mul(Q, permutedims(K, (2, 1, 3))) / sqrt(ca.head_dim)
    attn = softmax(scores, dims=2)  # [h*w, L, heads]
    out = batched_mul(attn, V)      # [h*w, heads, dim]

    # Concat + projection
    out = reshape(out, :, ca.num_heads * ca.head_dim)
    return ca.W_O(out)
end
```

**SpatialTransformer Block** (SD U-Net):
```
Input z_t
  â†“
GroupNorm
  â†“
Self-Attention (spatial)
  â†“
Cross-Attention (with text c)
  â†“
FeedForward
  â†“
Output
```

ã“ã‚Œã‚’å„è§£åƒåº¦ã§ç¹°ã‚Šè¿”ã—ã€‚

### 3.10 Stable Diffusion 1.x/2.x ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©³ç´°

**å…¨ä½“æ§‹æˆ**:

```mermaid
graph LR
    Input[Text Prompt] --> CLIP[CLIP Text Encoder]
    CLIP --> C[c: [77, 768]]

    Noise[Random Noise z_T] --> UNet[U-Net with<br>Cross-Attention]
    C --> UNet
    UNet --> z0[Denoised z_0]
    z0 --> VAE[VAE Decoder]
    VAE --> Output[Image 512Ã—512]
```

**U-Netè©³ç´°**:

| ãƒ¬ãƒ™ãƒ« | è§£åƒåº¦ | Channels | Blocks | Cross-Attn |
|:-------|:-------|:---------|:-------|:-----------|
| **Input** | 64Ã—64 | 4 | - | - |
| **Down 1** | 64â†’32 | 320 | ResBlockÃ—2 | âœ“ |
| **Down 2** | 32â†’16 | 640 | ResBlockÃ—2 | âœ“ |
| **Down 3** | 16â†’8 | 1280 | ResBlockÃ—2 | âœ“ |
| **Middle** | 8 | 1280 | ResBlockÃ—1 | âœ“ |
| **Up 1** | 8â†’16 | 1280 | ResBlockÃ—3 | âœ“ |
| **Up 2** | 16â†’32 | 640 | ResBlockÃ—3 | âœ“ |
| **Up 3** | 32â†’64 | 320 | ResBlockÃ—3 | âœ“ |
| **Output** | 64Ã—64 | 4 | - | - |

**ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°**: ~860M

**ResBlockæ§‹æˆ**:
```
Input
  â†“
GroupNorm â†’ SiLU â†’ Conv (3Ã—3)
  â†“
Timestep Embedding (broadcast add)
  â†“
GroupNorm â†’ SiLU â†’ Conv (3Ã—3)
  â†“
Residual Connection
  â†“
Output
```

**Timestep Embedding**:
$$
\gamma(t) = [\sin(t \omega_1), \cos(t \omega_1), \ldots, \sin(t \omega_{d/2}), \cos(t \omega_{d/2})]
$$

ã“ã“ã§ $\omega_k = 10000^{-2k/d}$ (Transformeré¢¨)ã€‚

```julia
# Timestep Embedding
function sinusoidal_embedding(t, dim)
    half_dim = dim Ã· 2
    emb = log(10000) / (half_dim - 1)
    emb = exp.(-emb .* (0:(half_dim-1)))
    emb = t .* emb'
    return hcat(sin.(emb), cos.(emb))
end
```

### 3.11 LDMå›ºæœ‰ã®U-Netæ‹¡å¼µ: SpatialTransformer

**æ¨™æº–U-Net vs LDM U-Net**:

| é …ç›® | æ¨™æº–U-Net | LDM U-Net |
|:-----|:----------|:----------|
| **Attention** | Self-Attention ã®ã¿ | Self + **Cross**-Attention |
| **æ¡ä»¶ä»˜ã‘** | Timestep $t$ ã®ã¿ | $t$ + **text $c$** |
| **å±¤æ§‹æˆ** | ResBlockâ†’Attn | ResBlockâ†’**SpatialTransformer** |

**SpatialTransformer Block**:
```
Input: [B, H, W, C]
  â†“
Reshape: [B, H*W, C]
  â†“
LayerNorm
  â†“
Self-Attention: Attention(Q,K,V) where Q=K=V=features
  â†“
LayerNorm
  â†“
Cross-Attention: Attention(Q_img, K_text, V_text)
  â†“
LayerNorm
  â†“
FeedForward (MLP)
  â†“
Reshape: [B, H, W, C]
  â†“
Residual Add
  â†“
Output
```

**ãªãœSpatial Transformerï¼Ÿ**

1. **Self-Attention**: ç”»åƒå†…ã®é•·è·é›¢ä¾å­˜ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£
2. **Cross-Attention**: ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã‚’æ•´åˆ—
3. **FeedForward**: éç·šå½¢å¤‰æ›ã§è¡¨ç¾åŠ›å‘ä¸Š

**è¨ˆç®—é‡**: $\mathcal{O}((HW)^2 + HW \cdot L)$ where $L$ ã¯ãƒ†ã‚­ã‚¹ãƒˆé•·ã€‚

### 3.12 FLUX Architectureè©³è§£

**FLUXã®é©æ–°**: Rectified Flowï¼ˆç¬¬38å›ï¼‰+ DiTé¢¨Transformerè¨­è¨ˆã€‚

**FLUX vs Stable Diffusion**:

| é …ç›® | SD 1.x/2.x | FLUX.1 |
|:-----|:-----------|:-------|
| **ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³** | U-Net | **Transformer (DiTé¢¨)** |
| **Flow** | DDPM | **Rectified Flow** |
| **Text Encoder** | CLIP | **CLIP + T5** |
| **Latent Size** | 64Ã—64Ã—4 | 64Ã—64Ã—16 (é«˜æ¬¡å…ƒåŒ–) |
| **Params** | 860M | **12B** |
| **é€Ÿåº¦** | 50 steps | **20 steps** (ç›´ç·šåŒ–) |

**Rectified Flowçµ±åˆ**:

ç¬¬38å›ã§å­¦ã‚“ã ã‚ˆã†ã«ã€Rectified Flowã¯ODEã§ç›´ç·šçš„ãªè¼¸é€çµŒè·¯:
$$
\frac{dz_t}{dt} = v_\theta(z_t, t, c)
$$

FLUXã¯ $v_\theta$ ã‚’Transformerã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–:
$$
v_\theta(z_t, t, c) = \text{Transformer}(z_t, t, \text{CLIP}(c), \text{T5}(c))
$$

**DiT (Diffusion Transformer) é¢¨è¨­è¨ˆ**:

```mermaid
graph TD
    Z[z_t patches] --> PE[Positional Encoding]
    PE --> TB1[Transformer Block 1]
    TB1 --> TB2[Transformer Block 2]
    TB2 --> TBn[Transformer Block N]
    TBn --> Out[v_Î¸ prediction]

    T[Text: CLIP+T5] --> Cond[Conditioning]
    Cond --> TB1
    Cond --> TB2
    Cond --> TBn
```

**Transformer Blockã®æ§‹æˆ**:
```
Input: z_t patches + timestep t + text c
  â†“
Adaptive LayerNorm (conditioned on t, c)
  â†“
Self-Attention (å…¨patché–“)
  â†“
Adaptive LayerNorm
  â†“
Cross-Attention (patch â†” text)
  â†“
Adaptive LayerNorm
  â†“
MLP
  â†“
Output
```

**ãªãœFLUXãŒé€Ÿã„ï¼Ÿ**

1. **Rectified Flow**: ç›´ç·šçš„è¼¸é€ â†’ å°‘ã‚¹ãƒ†ãƒƒãƒ—ã§åæŸ
2. **é«˜æ¬¡å…ƒlatent**: 16ch â†’ ã‚ˆã‚Šè±Šã‹ãªè¡¨ç¾
3. **Transformer**: é•·è·é›¢ä¾å­˜ã‚’åŠ¹ç‡çš„ã«ã‚­ãƒ£ãƒ—ãƒãƒ£

**FLUX.1ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³**:

| ãƒ¢ãƒ‡ãƒ« | Params | é€Ÿåº¦ | å“è³ª | ç”¨é€” |
|:-------|:-------|:-----|:-----|:-----|
| **FLUX.1-pro** | 12B | 20 steps | æœ€é«˜ | APIå°‚ç”¨ |
| **FLUX.1-dev** | 12B | 20 steps | é«˜ | é–‹ç™ºç”¨ |
| **FLUX.1-schnell** | 12B | **4 steps** | ä¸­ | é«˜é€Ÿç”Ÿæˆ |

:::details FLUX Transformerã®å®Ÿè£…æ¦‚è¦
```julia
# FLUX Transformer Block (æ¦‚å¿µçš„)
struct FLUXTransformerBlock
    self_attn::MultiHeadAttention
    cross_attn::MultiHeadAttention
    mlp::MLP
    adaLN::AdaptiveLayerNorm
end

function (block::FLUXTransformerBlock)(z_patches, t_emb, text_emb)
    # Adaptive LayerNorm (timestep & text conditioned)
    z = block.adaLN(z_patches, t_emb, text_emb)

    # Self-Attention (å…¨patché–“)
    z = z + block.self_attn(z, z, z)

    # Cross-Attention (patch â†” text)
    z = block.adaLN(z, t_emb, text_emb)
    z = z + block.cross_attn(z, text_emb, text_emb)

    # MLP
    z = block.adaLN(z, t_emb, text_emb)
    z = z + block.mlp(z)

    return z
end
```
:::

### 3.13 å­¦ç¿’ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯

**Noise Offset**:

Forward processã«ãƒã‚¤ã‚¢ã‚¹ã‚’åŠ ãˆã‚‹:
$$
z_t = \sqrt{\bar{\alpha}_t} z_0 + \sqrt{1-\bar{\alpha}_t} (\epsilon + \text{offset})
$$

**åŠ¹æœ**: æš—ã„ç”»åƒãƒ»æ˜ã‚‹ã„ç”»åƒã®ç”Ÿæˆå“è³ªå‘ä¸Šã€‚

**Min-SNR Weighting**:

Loss weightingã‚’SNRãƒ™ãƒ¼ã‚¹ã§èª¿æ•´:
$$
\mathcal{L}_\text{Min-SNR} = \mathbb{E}_{t} \left[ w(t) \|\epsilon - \epsilon_\theta(z_t, t)\|^2 \right]
$$

$$
w(t) = \min\left(\text{SNR}(t), \gamma\right), \quad \text{SNR}(t) = \frac{\bar{\alpha}_t}{1-\bar{\alpha}_t}
$$

å…¸å‹çš„ã« $\gamma = 5$ã€‚

**åŠ¹æœ**: 3.4å€è¨“ç·´é«˜é€ŸåŒ– [^min_snr]ã€‚

**v-prediction**:

Îµ-predictionã®ä»£ã‚ã‚Šã«velocity prediction:
$$
v_t = \sqrt{\bar{\alpha}_t} \epsilon - \sqrt{1-\bar{\alpha}_t} z_0
$$

$$
\mathcal{L}_v = \mathbb{E}_{t} \left[ \|v_t - v_\theta(z_t, t)\|^2 \right]
$$

**åŠ¹æœ**: æ•°å€¤å®‰å®šæ€§å‘ä¸Šãƒ»åæŸæ€§æ”¹å–„ã€‚

**Zero Terminal SNR**:

Noise scheduleã‚’å¼·åˆ¶çš„ã« $\bar{\alpha}_T = 0$ ã« rescale:
$$
\tilde{\alpha}_t = \frac{\alpha_t}{\alpha_T}
$$

**åŠ¹æœ**: éå¸¸ã«æ˜ã‚‹ã„/æš—ã„ç”»åƒã®ç”Ÿæˆå“è³ªå‘ä¸Š [^zero_snr]ã€‚

```julia
# Zero Terminal SNR rescaling
function rescale_to_zero_terminal_snr(alphas)
    alphas_cumprod = cumprod(alphas)
    sqrt_alphas_cumprod = sqrt.(alphas_cumprod)

    # Rescale
    sqrt_alphas_cumprod_final = sqrt_alphas_cumprod[end]
    sqrt_alphas_cumprod .= sqrt_alphas_cumprod ./ sqrt_alphas_cumprod_final

    return sqrt_alphas_cumprod
end
```

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®50%å®Œäº†ï¼ ãƒœã‚¹æˆ¦å‰ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã€‚**

æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³å®Œäº†:
- VAE Encoder/Decoderåœ§ç¸®ã®æ•°å­¦
- æ½œåœ¨ç©ºé–“æ‹¡æ•£ãƒ—ãƒ­ã‚»ã‚¹
- Classifier Guidanceå®Œå…¨ç‰ˆ
- Classifier-Free Guidanceå®Œå…¨å°å‡º
- CFGç†è«–çš„ç†è§£ï¼ˆMode-Seeking / æ¸©åº¦ï¼‰
- Negative Prompt
- Text Conditioningï¼ˆCLIP / T5ï¼‰
- Cross-Attentionå®Œå…¨ç‰ˆ
- SD 1.x/2.x ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- SpatialTransformer
- FLUX Architectureè©³è§£
- å­¦ç¿’ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ï¼ˆNoise Offset / Min-SNR / v-prediction / Zero Terminal SNRï¼‰

æ¬¡ã¯å®Ÿè£…ã‚¾ãƒ¼ãƒ³ã¸ï¼
:::

### 3.14 SD vs FLUX: ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©³ç´°æ¯”è¼ƒ

**Stable Diffusion 1.x/2.x/SDXL Architecture**:

```
Input: Text prompt
  â†“
CLIP Text Encoder: [77, 768]
  â†“
Random Noise z_T: [64, 64, 4]
  â†“
U-Net (ResBlock + SpatialTransformer):
  - Down1: 64â†’32 (320ch, Cross-Attn)
  - Down2: 32â†’16 (640ch, Cross-Attn)
  - Down3: 16â†’8 (1280ch, Cross-Attn)
  - Middle: 8 (1280ch, Cross-Attn)
  - Up1: 8â†’16 (1280ch, Cross-Attn)
  - Up2: 16â†’32 (640ch, Cross-Attn)
  - Up3: 32â†’64 (320ch, Cross-Attn)
  â†“
Denoised z_0: [64, 64, 4]
  â†“
VAE Decoder (f=8)
  â†“
Image: [512, 512, 3]
```

**FLUX.1 Architecture**:

```
Input: Text prompt
  â†“
Dual Encoders:
  - CLIP ViT-L: [77, 768]
  - T5-XXL: [512, 4096]
  â†“
Random Noise z_T: [64, 64, 16]  # 4å€ã®ãƒãƒ£ãƒãƒ«!
  â†“
Patchify: [64, 64, 16] â†’ [1024, 768]  # 4Ã—4ãƒ‘ãƒƒãƒ
  â†“
Positional Encoding (RoPE)
  â†“
Transformer Blocks (N=24):
  - Adaptive LayerNorm (t, c conditioned)
  - Self-Attention (å…¨patché–“)
  - Cross-Attention (patch â†” CLIP+T5)
  - Gated FFN
  â†“
Unpatchify: [1024, 768] â†’ [64, 64, 16]
  â†“
VAE Decoder (f=8, 16ch input)
  â†“
Image: [512, 512, 3]
```

**è©³ç´°æ¯”è¼ƒè¡¨**:

| é …ç›® | SD 1.x/2.x | SDXL | FLUX.1 |
|:-----|:-----------|:-----|:-------|
| **Backbone** | U-Net | U-Net | **Transformer** |
| **Total Params** | 860M | 2.6B | **12B** |
| **Text Encoder** | CLIP (768) | CLIP+OpenCLIP (2048) | **CLIP+T5 (4864)** |
| **Latent Channels** | 4 | 4 | **16** |
| **Latent Res** | 64Ã—64 (f=8) | 128Ã—128 (f=8) | 64Ã—64 (f=8) |
| **Attention Type** | Cross-Attn in U-Net | Cross-Attn in U-Net | **Self+Cross in Transformer** |
| **Position Enc** | ãªã— (CNN) | ãªã— (CNN) | **RoPE** |
| **Flow Type** | DDPM | DDPM | **Rectified Flow** |
| **Timesteps** | 1000 | 1000 | 1000 (è¨“ç·´), 20-50 (æ¨è«–) |
| **CFG Default** | 7.5 | 7.5 | 3.5 (ä½ã‚ã§OK) |
| **Memory (fp16)** | 10GB | 16GB | **24GB** |
| **Speed (50 steps)** | ~5s (A100) | ~8s (A100) | **~3s (20 steps, A100)** |

### 3.15 Cross-Attentionè©³ç´°å®Ÿè£…

**Multi-Head Cross-Attentionå®Œå…¨ç‰ˆ**:

```julia
struct MultiHeadCrossAttention{F}
    num_heads::Int
    head_dim::Int
    qkv_dim::Int
    W_Q::Dense
    W_K::Dense
    W_V::Dense
    W_O::Dense
    dropout::Dropout
end

function MultiHeadCrossAttention(qkv_dim::Int, num_heads::Int; dropout_rate=0.1)
    head_dim = qkv_dim Ã· num_heads
    @assert qkv_dim == num_heads * head_dim "qkv_dim must be divisible by num_heads"

    return MultiHeadCrossAttention(
        num_heads,
        head_dim,
        qkv_dim,
        Dense(qkv_dim => qkv_dim),  # W_Q
        Dense(qkv_dim => qkv_dim),  # W_K
        Dense(qkv_dim => qkv_dim),  # W_V
        Dense(qkv_dim => qkv_dim),  # W_O
        Dropout(dropout_rate)
    )
end

function (mha::MultiHeadCrossAttention)(q, k, v, mask=nothing)
    # q: [N_q, d], k: [N_k, d], v: [N_k, d]
    batch_size = size(q, 1)

    # Linear projections
    Q = mha.W_Q(q)  # [N_q, d]
    K = mha.W_K(k)  # [N_k, d]
    V = mha.W_V(v)  # [N_k, d]

    # Reshape to multi-head: [N, d] â†’ [N, num_heads, head_dim]
    Q = reshape(Q, :, mha.num_heads, mha.head_dim)
    K = reshape(K, :, mha.num_heads, mha.head_dim)
    V = reshape(V, :, mha.num_heads, mha.head_dim)

    # Transpose for batch matrix multiply: [num_heads, N, head_dim]
    Q = permutedims(Q, (2, 1, 3))
    K = permutedims(K, (2, 1, 3))
    V = permutedims(V, (2, 1, 3))

    # Scaled dot-product attention
    scores = batched_mul(Q, batched_transpose(K)) / sqrt(Float32(mha.head_dim))
    # scores: [num_heads, N_q, N_k]

    # Apply mask if provided
    if mask !== nothing
        scores = scores .+ mask
    end

    # Softmax
    attn_weights = softmax(scores, dims=3)  # Over N_k
    attn_weights = mha.dropout(attn_weights)

    # Apply attention to values
    out = batched_mul(attn_weights, V)  # [num_heads, N_q, head_dim]

    # Transpose back: [num_heads, N_q, head_dim] â†’ [N_q, num_heads, head_dim]
    out = permutedims(out, (2, 1, 3))

    # Concat heads: [N_q, num_heads, head_dim] â†’ [N_q, d]
    out = reshape(out, :, mha.qkv_dim)

    # Final linear
    return mha.W_O(out)
end
```

**ä½¿ç”¨ä¾‹**:

```julia
# åˆæœŸåŒ–
d_model = 768
num_heads = 12
mha = MultiHeadCrossAttention(d_model, num_heads)

# U-Netä¸­é–“ç‰¹å¾´: [h*w, d_model]
f = randn(Float32, 64*64, d_model)

# Text embeddings: [77, d_model]
c = randn(Float32, 77, d_model)

# Cross-Attention
out = mha(f, c, c)  # Q from image, K/V from text
# out: [64*64, d_model]
```

### 3.16 VAEè¨“ç·´ã®è©³ç´°

**VAEæå¤±é–¢æ•°ã®å®Œå…¨å±•é–‹**:

$$
\begin{aligned}
\mathcal{L}_\text{VAE} &= \mathcal{L}_\text{rec} + \beta \cdot \mathcal{L}_\text{KL} \\
\mathcal{L}_\text{rec} &= \mathbb{E}_{q(z|x)}[-\log p_\theta(x|z)] \\
&\approx -\log p_\theta(x | \mathcal{E}(x)) \\
&= \text{MSE}(x, \mathcal{D}(\mathcal{E}(x))) \quad \text{or} \quad \text{LPIPS}(x, \mathcal{D}(\mathcal{E}(x))) \\
\mathcal{L}_\text{KL} &= \text{KL}[q_\phi(z|x) \| p(z)] \\
&= \text{KL}[\mathcal{N}(\mu_\phi(x), \sigma_\phi^2(x)) \| \mathcal{N}(0, I)] \\
&= \frac{1}{2} \sum_{i=1}^d \left( \mu_i^2 + \sigma_i^2 - \log \sigma_i^2 - 1 \right)
\end{aligned}
$$

**Perceptual Loss (LPIPS)**:

LPIPSã¯VGGç‰¹å¾´ç©ºé–“ã§ã®è·é›¢:
$$
\mathcal{L}_\text{LPIPS}(x, \tilde{x}) = \sum_l w_l \|\Phi_l(x) - \Phi_l(\tilde{x})\|^2
$$

ã“ã“ã§ $\Phi_l$ ã¯VGGã®ç¬¬$l$å±¤ç‰¹å¾´ã€‚

```julia
# LPIPSæå¤± (ç°¡ç•¥ç‰ˆ)
function lpips_loss(x, x_recon, vgg_model, layers=[3, 8, 15, 22])
    loss = 0.0
    for layer in layers
        feat_x = vgg_model[1:layer](x)
        feat_recon = vgg_model[1:layer](x_recon)
        loss += mean((feat_x .- feat_recon).^2)
    end
    return loss / length(layers)
end

# VAEè¨“ç·´with LPIPS
function train_vae_lpips!(encoder, decoder, vgg, dataloader; Î²=0.1)
    for (x,) in dataloader
        # Encode
        Î¼, logÏƒÂ² = encoder(x)
        Ïƒ = exp.(0.5 .* logÏƒÂ²)
        Îµ = randn(size(Î¼))
        z = Î¼ .+ Ïƒ .* Îµ

        # Decode
        x_recon = decoder(z)

        # Losses
        recon_loss = lpips_loss(x, x_recon, vgg)
        kl_loss = 0.5 * mean(Î¼.^2 .+ Ïƒ.^2 .- logÏƒÂ² .- 1)

        loss = recon_loss + Î² * kl_loss
        # ... backprop
    end
end
```

### 3.17 Noise Scheduleã®è¨­è¨ˆç†è«–

**3ã¤ã®ä¸»è¦ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«**:

**1. Linear Schedule** (DDPM original):
$$
\beta_t = \beta_\text{min} + \frac{t-1}{T-1} (\beta_\text{max} - \beta_\text{min})
$$

å…¸å‹å€¤: $\beta_\text{min} = 0.0001$, $\beta_\text{max} = 0.02$, $T = 1000$

**2. Cosine Schedule** (Improved DDPM):
$$
\bar{\alpha}_t = \frac{f(t)}{f(0)}, \quad f(t) = \cos\left( \frac{t/T + s}{1+s} \cdot \frac{\pi}{2} \right)^2
$$

$s = 0.008$ ãŒæ¨™æº–ã€‚

**3. Learned Schedule**:

$\beta_t$ ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã—ã¦å­¦ç¿’ã€‚

```julia
# 3ã¤ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè£…
function linear_beta_schedule(T::Int; Î²_start=1e-4, Î²_end=0.02)
    return range(Î²_start, Î²_end, length=T)
end

function cosine_alpha_bar_schedule(T::Int; s=0.008)
    t = 0:T
    f_t = cos.(((t ./ T) .+ s) ./ (1 + s) .* Ï€ ./ 2).^2
    Î±_bar = f_t ./ f_t[1]
    return Î±_bar[2:end]
end

function learned_beta_schedule(T::Int; init_Î²_start=1e-4, init_Î²_end=0.02)
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–Î²ã‚’å­¦ç¿’
    logit_Î² = range(logit(init_Î²_start), logit(init_Î²_end), length=T)
    return logit_Î²  # è¨“ç·´ä¸­ã«æœ€é©åŒ–
end

# Zero Terminal SNR rescaling
function rescale_zero_terminal_snr(Î±_bar)
    # Î±_bar[T] = 0 ã‚’å¼·åˆ¶
    return Î±_bar ./ Î±_bar[end]
end
```

**SNRã®å¯è¦–åŒ–**:

```julia
using Plots

T = 1000
betas_linear = linear_beta_schedule(T)
Î±_bar_cosine = cosine_alpha_bar_schedule(T)

# SNRè¨ˆç®—
snr_linear = cumprod(1 .- betas_linear) ./ (1 .- cumprod(1 .- betas_linear))
snr_cosine = Î±_bar_cosine ./ (1 .- Î±_bar_cosine)

# Plot
plot(1:T, snr_linear, label="Linear", yscale=:log10, xlabel="Timestep", ylabel="SNR")
plot!(1:T, snr_cosine, label="Cosine")
title!("SNR Schedule Comparison")
```

### 3.18 Sampling Algorithmså®Œå…¨æ¯”è¼ƒ

| ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  | ã‚¹ãƒ†ãƒƒãƒ—æ•° | å“è³ª | é€Ÿåº¦ | æ±ºå®šè«–çš„ | å‚™è€ƒ |
|:-------------|:-----------|:-----|:-----|:---------|:-----|
| **DDPM** | 1000 | æœ€é«˜ | æœ€é… | âœ— | ç¢ºç‡çš„ã€å…¨ã‚¹ãƒ†ãƒƒãƒ—å¿…è¦ |
| **DDIM** | 50-100 | é«˜ | ä¸­ | âœ“ | æ±ºå®šè«–çš„ã€ã‚¹ã‚­ãƒƒãƒ—å¯èƒ½ |
| **DPM-Solver++** | 20-30 | é«˜ | é«˜ | âœ“ | é«˜æ¬¡ODE solver |
| **UniPC** | 10-20 | ä¸­ | æœ€é«˜ | âœ“ | Predictor-Corrector |
| **PNDM** | 50 | é«˜ | ä¸­ | âœ— | Pseudo Numerical |
| **LMS** | 50 | ä¸­ | ä¸­ | âœ“ | Linear Multi-Step |

**DDIMå®Œå…¨ç‰ˆ**:

$$
\begin{aligned}
z_{t-\Delta t} &= \sqrt{\bar{\alpha}_{t-\Delta t}} \underbrace{\left( \frac{z_t - \sqrt{1-\bar{\alpha}_t} \epsilon_\theta(z_t, t)}{\sqrt{\bar{\alpha}_t}} \right)}_{\text{pred } x_0} \\
&\quad + \underbrace{\sqrt{1-\bar{\alpha}_{t-\Delta t} - \sigma_t^2} \cdot \epsilon_\theta(z_t, t)}_{\text{direction to } z_{t-\Delta t}} \\
&\quad + \underbrace{\sigma_t \epsilon}_{\text{random noise}}
\end{aligned}
$$

$\sigma_t = 0$ ã§å®Œå…¨æ±ºå®šè«–çš„ã€$\sigma_t = \sqrt{(1-\bar{\alpha}_{t-\Delta t})/(1-\bar{\alpha}_t)} \sqrt{1-\bar{\alpha}_t/\bar{\alpha}_{t-\Delta t}}$ ã§DDPMã¨åŒç­‰ã€‚

```julia
function ddim_step(z_t, Îµ_Î¸, t, t_prev, Î±_bar; Î·=0.0)
    # Predict xâ‚€
    Î±_t = Î±_bar[t]
    Î±_prev = t_prev > 0 ? Î±_bar[t_prev] : 1.0

    pred_xâ‚€ = (z_t .- sqrt(1 - Î±_t) .* Îµ_Î¸) ./ sqrt(Î±_t)

    # Direction
    Ïƒ_t = Î· * sqrt((1 - Î±_prev) / (1 - Î±_t)) * sqrt(1 - Î±_t / Î±_prev)
    dir_z = sqrt(1 - Î±_prev - Ïƒ_t^2) .* Îµ_Î¸

    # Noise
    noise = Ïƒ_t .* randn(Float32, size(z_t))

    # Combine
    z_prev = sqrt(Î±_prev) .* pred_xâ‚€ .+ dir_z .+ noise
    return z_prev
end
```

**DPM-Solver++ (æ¦‚è¦)**:

é«˜æ¬¡ODE solverã§DDIMã‚’æ”¹å–„:
$$
z_{t-\Delta t} = z_t + \int_t^{t-\Delta t} f(z_s, s) ds
$$

3æ¬¡Adams-Bashforthæ³•ã§è¿‘ä¼¼ â†’ 20ã‚¹ãƒ†ãƒƒãƒ—ã§é«˜å“è³ªã€‚

### âš”ï¸ Boss Battle: CFGã®å®Œå…¨åˆ†è§£

**èª²é¡Œ**: Classifier-Free Guidanceã®æ•°å¼ã‚’ã€3ã¤ã®è¦–ç‚¹ã‹ã‚‰å®Œå…¨ã«å°å‡ºã›ã‚ˆã€‚

**è¦–ç‚¹1: Îµ-prediction**

ç›®æ¨™: ä¿®æ­£ã•ã‚ŒãŸãƒã‚¤ã‚ºäºˆæ¸¬ $\tilde{\epsilon}_\theta(z_t, t, c, w)$ ã‚’æ±‚ã‚ã‚ˆã€‚

**è§£ç­”**:
$$
\begin{aligned}
\tilde{\epsilon}_\theta(z_t, t, c, w) &= \epsilon_\theta(z_t, t, \emptyset) + w \cdot \left( \epsilon_\theta(z_t, t, c) - \epsilon_\theta(z_t, t, \emptyset) \right) \\
&= (1-w) \epsilon_\theta(z_t, t, \emptyset) + w \cdot \epsilon_\theta(z_t, t, c)
\end{aligned}
$$

**è¦–ç‚¹2: ã‚¹ã‚³ã‚¢é–¢æ•°**

ç›®æ¨™: ä¿®æ­£ã•ã‚ŒãŸã‚¹ã‚³ã‚¢ $\tilde{s}_\theta(z_t, t, c, w)$ ã‚’æ±‚ã‚ã‚ˆã€‚

**è§£ç­”**:

ã‚¹ã‚³ã‚¢ $s_\theta = -\frac{\epsilon_\theta}{\sqrt{1-\bar{\alpha}_t}}$ ã‚ˆã‚Š:
$$
\begin{aligned}
\tilde{s}_\theta(z_t, t, c, w) &= -\frac{\tilde{\epsilon}_\theta}{\sqrt{1-\bar{\alpha}_t}} \\
&= -\frac{1}{\sqrt{1-\bar{\alpha}_t}} \left[ \epsilon_\theta(z_t, t, \emptyset) + w(\epsilon_\theta(z_t, t, c) - \epsilon_\theta(z_t, t, \emptyset)) \right] \\
&= s_\theta(z_t, t, \emptyset) + w \cdot (s_\theta(z_t, t, c) - s_\theta(z_t, t, \emptyset)) \\
&= (1-w) s_\theta(z_t, t, \emptyset) + w \cdot s_\theta(z_t, t, c)
\end{aligned}
$$

**è¦–ç‚¹3: ç¢ºç‡åˆ†å¸ƒ**

ç›®æ¨™: $w > 1$ ã®ã¨ãã®å®ŸåŠ¹ç¢ºç‡åˆ†å¸ƒ $p_w(z_t | c)$ ã‚’æ±‚ã‚ã‚ˆã€‚

**è§£ç­”**:

ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°ã‚ˆã‚Š $s(z) = \nabla_z \log p(z)$ ãªã®ã§:
$$
\begin{aligned}
\tilde{s}_\theta(z_t, t, c, w) &= \nabla_{z_t} \log \tilde{p}(z_t | c) \\
&= (1-w) \nabla_{z_t} \log p(z_t) + w \nabla_{z_t} \log p(z_t | c) \\
&= \nabla_{z_t} \left[ (1-w) \log p(z_t) + w \log p(z_t | c) \right] \\
&= \nabla_{z_t} \log p_w(z_t | c)
\end{aligned}
$$

ã‚ˆã£ã¦:
$$
p_w(z_t | c) \propto p(z_t)^{1-w} \cdot p(z_t | c)^w
$$

$w > 1$ ã®ã¨ãã€æ¡ä»¶ä»˜ãåˆ†å¸ƒã‚’ **over-emphasize** â†’ mode-seekingã€‚

**æ•°å€¤æ¤œè¨¼**:
```julia
# CFGã®3è¦–ç‚¹æ¤œè¨¼
w = 7.5
Îµ_uncond = randn(Float32, 64, 64, 4)
Îµ_cond = randn(Float32, 64, 64, 4)

# è¦–ç‚¹1: Îµ-prediction
Îµ_cfg1 = Îµ_uncond .+ w .* (Îµ_cond .- Îµ_uncond)
Îµ_cfg2 = (1 - w) .* Îµ_uncond .+ w .* Îµ_cond

@assert isapprox(Îµ_cfg1, Îµ_cfg2)  # ç­‰ä¾¡æ€§ç¢ºèª

# è¦–ç‚¹2: ã‚¹ã‚³ã‚¢
Î±_bar = 0.5
s_uncond = -Îµ_uncond ./ sqrt(1 - Î±_bar)
s_cond = -Îµ_cond ./ sqrt(1 - Î±_bar)
s_cfg = (1 - w) .* s_uncond .+ w .* s_cond

# è¦–ç‚¹3: ç¢ºç‡
log_p_uncond = -0.5 * sum(Îµ_uncond.^2)
log_p_cond = -0.5 * sum(Îµ_cond.^2)
log_p_w = (1 - w) * log_p_uncond + w * log_p_cond

println("CFG 3è¦–ç‚¹æ¤œè¨¼å®Œäº†ï¼")
```

**ãƒœã‚¹æ’ƒç ´ï¼** CFGã®æ•°å­¦çš„æ§‹é€ ã‚’å®Œå…¨ç†è§£ã—ãŸã€‚

---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” LDMè¨“ç·´â†’æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

### ç’°å¢ƒæ§‹ç¯‰ã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

**Juliaç’°å¢ƒ** (âš¡è¨“ç·´):
```julia
using Pkg
Pkg.add([
    "Lux",           # NN framework
    "Reactant",      # XLA compiler (GPUé«˜é€ŸåŒ–)
    "Optimisers",    # Adamç­‰
    "Zygote",        # è‡ªå‹•å¾®åˆ†
    "MLDatasets",    # MNISTç­‰
    "Images",        # ç”»åƒå‡¦ç†
    "CUDA",          # GPU
    "BenchmarkTools" # æ€§èƒ½æ¸¬å®š
])
```

**Rustç’°å¢ƒ** (ğŸ¦€æ¨è«–):
```toml
[dependencies]
candle-core = "0.7"
candle-nn = "0.7"
safetensors = "0.4"
```

### LaTeXè¨˜æ³•ãƒãƒ¼ãƒˆã‚·ãƒ¼ãƒˆ

| è¨˜å· | LaTeX | æ„å‘³ |
|:-----|:------|:-----|
| $\mathcal{E}$ | `\mathcal{E}` | Encoder |
| $\mathcal{D}$ | `\mathcal{D}` | Decoder |
| $\bar{\alpha}_t$ | `\bar{\alpha}_t` | Cumulative product |
| $\epsilon_\theta(z_t, t, c)$ | `\epsilon_\theta(z_t, t, c)` | Noise prediction |
| $\tilde{\epsilon}$ | `\tilde{\epsilon}` | Modified noise (CFG) |
| $\mathbb{E}_{q(z\|x)}[\cdot]$ | `\mathbb{E}_{q(z\|x)}[\cdot]` | Expectation |
| $\text{KL}[q \| p]$ | `\text{KL}[q \| p]` | KL divergence |

### Mathâ†’Codeç¿»è¨³ãƒ‘ã‚¿ãƒ¼ãƒ³ (LDMç·¨)

**ãƒ‘ã‚¿ãƒ¼ãƒ³1: VAE Encoder**
$$
z = \mathcal{E}(x), \quad x \in \mathbb{R}^{H \times W \times C} \to z \in \mathbb{R}^{h \times w \times c}
$$

```julia
z = encoder(x, ps_encoder, st_encoder)[1]
# x: [H, W, C, B] â†’ z: [h, w, c, B]
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³2: Forward Diffusion**
$$
z_t = \sqrt{\bar{\alpha}_t} z_0 + \sqrt{1-\bar{\alpha}_t} \epsilon, \quad \epsilon \sim \mathcal{N}(0,I)
$$

```julia
Îµ = randn(rng, Float32, size(zâ‚€))
z_t = sqrt(Î±_bar[t]) .* zâ‚€ .+ sqrt(1 - Î±_bar[t]) .* Îµ
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³3: CFG**
$$
\tilde{\epsilon}_\theta = \epsilon_\theta(z_t, t, \emptyset) + w \cdot (\epsilon_\theta(z_t, t, c) - \epsilon_\theta(z_t, t, \emptyset))
$$

```julia
Îµ_uncond = unet((z_t, t, nothing), ps, st)[1]
Îµ_cond = unet((z_t, t, c), ps, st)[1]
Îµ_cfg = Îµ_uncond .+ w .* (Îµ_cond .- Îµ_uncond)
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³4: DDIM Sampling**
$$
z_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \left( \frac{z_t - \sqrt{1-\bar{\alpha}_t} \epsilon_\theta}{\sqrt{\bar{\alpha}_t}} \right) + \sqrt{1-\bar{\alpha}_{t-1} - \sigma_t^2} \epsilon_\theta + \sigma_t \epsilon
$$

```julia
pred_xâ‚€ = (z_t .- sqrt(1 - Î±_bar[t]) .* Îµ_Î¸) ./ sqrt(Î±_bar[t])
dir_z = sqrt(1 - Î±_bar[t-1] - ÏƒÂ²) .* Îµ_Î¸
noise = Ïƒ .* randn(rng, Float32, size(z_t))
z_prev = sqrt(Î±_bar[t-1]) .* pred_xâ‚€ .+ dir_z .+ noise
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³5: Cross-Attention**
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
$$

```julia
scores = (Q * K') ./ sqrt(d_k)  # [N_q, N_k]
attn = softmax(scores, dims=2)   # [N_q, N_k]
out = attn * V                   # [N_q, d_v]
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³6: Min-SNR Weighting**
$$
w(t) = \min\left(\text{SNR}(t), \gamma\right), \quad \text{SNR}(t) = \frac{\bar{\alpha}_t}{1-\bar{\alpha}_t}
$$

```julia
snr = Î±_bar ./ (1 .- Î±_bar)
weight = min.(snr, Î³)
loss = weight[t] * mse(Îµ_pred, Îµ_true)
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³7: Zero Terminal SNR Rescaling**
$$
\tilde{\alpha}_t = \frac{\alpha_t}{\alpha_T}
$$

```julia
Î±_cumprod = cumprod(alphas)
Î±_cumprod_rescaled = Î±_cumprod ./ Î±_cumprod[end]
```

### âš¡ Juliaå®Œå…¨å®Ÿè£…: Mini Latent Diffusion

**ã‚¹ãƒ†ãƒƒãƒ—1: VAEå®šç¾©**

```julia
using Lux, Random, Optimisers, Zygote

# Encoder
function create_encoder(; in_ch=3, latent_ch=4, base_ch=64)
    return Chain(
        Conv((3,3), in_ch => base_ch, pad=1, activation=relu),
        Conv((4,4), base_ch => base_ch*2, stride=2, pad=1, activation=relu),  # /2
        Conv((4,4), base_ch*2 => base_ch*4, stride=2, pad=1, activation=relu),  # /4
        Conv((4,4), base_ch*4 => base_ch*8, stride=2, pad=1, activation=relu),  # /8
        Conv((3,3), base_ch*8 => latent_ch, pad=1)  # Output z
    )
end

# Decoder (mirror)
function create_decoder(; latent_ch=4, out_ch=3, base_ch=64)
    return Chain(
        Conv((3,3), latent_ch => base_ch*8, pad=1, activation=relu),
        ConvTranspose((4,4), base_ch*8 => base_ch*4, stride=2, pad=1, activation=relu),  # *2
        ConvTranspose((4,4), base_ch*4 => base_ch*2, stride=2, pad=1, activation=relu),  # *4
        ConvTranspose((4,4), base_ch*2 => base_ch, stride=2, pad=1, activation=relu),    # *8
        Conv((3,3), base_ch => out_ch, pad=1, activation=tanh)
    )
end

# VAEè¨“ç·´
function train_vae!(encoder, decoder, dataloader; epochs=10, lr=1e-3, Î²=0.5)
    ps_enc, st_enc = Lux.setup(Random.default_rng(), encoder)
    ps_dec, st_dec = Lux.setup(Random.default_rng(), decoder)
    opt = Adam(lr)
    opt_state_enc = Optimisers.setup(opt, ps_enc)
    opt_state_dec = Optimisers.setup(opt, ps_dec)

    for epoch in 1:epochs
        total_loss = 0.0
        for (x,) in dataloader
            # Forward
            z, st_enc = encoder(x, ps_enc, st_enc)
            x_recon, st_dec = decoder(z, ps_dec, st_dec)

            # Loss: Reconstruction + KL (simplified)
            recon_loss = mean((x_recon .- x).^2)
            kl_loss = 0.5 * mean(z.^2)  # Simplified KL to N(0,I)
            loss = recon_loss + Î² * kl_loss

            # Backprop
            gs_enc = gradient(p -> loss, ps_enc)[1]
            gs_dec = gradient(p -> loss, ps_dec)[1]
            opt_state_enc, ps_enc = Optimisers.update(opt_state_enc, ps_enc, gs_enc)
            opt_state_dec, ps_dec = Optimisers.update(opt_state_dec, ps_dec, gs_dec)

            total_loss += loss
        end
        println("Epoch $epoch: Loss = $(total_loss / length(dataloader))")
    end
    return ps_enc, ps_dec, st_enc, st_dec
end
```

**ã‚¹ãƒ†ãƒƒãƒ—2: U-Netå®šç¾© (Simplified)**

```julia
# ResBlock
struct ResBlock
    conv1::Conv
    conv2::Conv
end

function (rb::ResBlock)(x)
    h = rb.conv1(x)
    h = relu.(h)
    h = rb.conv2(h)
    return relu.(h .+ x)  # Residual
end

# Time Embedding
function sinusoidal_embedding(t::Int, dim::Int)
    half = dim Ã· 2
    freqs = exp.(-log(10000f0) .* (0:half-1) ./ half)
    args = t .* freqs
    return vcat(sin.(args), cos.(args))
end

# Simplified U-Net (for 32x32 latent)
function create_unet(; latent_ch=4, base_ch=128, time_emb_dim=256)
    return Chain(
        # Time embedding MLP
        Dense(time_emb_dim, time_emb_dim*4, activation=silu),
        Dense(time_emb_dim*4, time_emb_dim*4, activation=silu),

        # Down
        Conv((3,3), latent_ch => base_ch, pad=1),
        ResBlock(Conv((3,3), base_ch => base_ch, pad=1), Conv((3,3), base_ch => base_ch, pad=1)),
        Conv((4,4), base_ch => base_ch*2, stride=2, pad=1),  # /2
        ResBlock(Conv((3,3), base_ch*2 => base_ch*2, pad=1), Conv((3,3), base_ch*2 => base_ch*2, pad=1)),

        # Middle
        ResBlock(Conv((3,3), base_ch*2 => base_ch*2, pad=1), Conv((3,3), base_ch*2 => base_ch*2, pad=1)),

        # Up
        ConvTranspose((4,4), base_ch*2 => base_ch, stride=2, pad=1),  # *2
        ResBlock(Conv((3,3), base_ch => base_ch, pad=1), Conv((3,3), base_ch => base_ch, pad=1)),
        Conv((3,3), base_ch => latent_ch, pad=1)  # Output Îµ
    )
end
```

**ã‚¹ãƒ†ãƒƒãƒ—3: Diffusionè¨“ç·´ãƒ«ãƒ¼ãƒ—**

```julia
# Noise schedule
function cosine_beta_schedule(T::Int; s=0.008)
    t = 0:T
    Î±_bar = cos.(((t ./ T) .+ s) ./ (1 + s) .* Ï€ ./ 2).^2
    Î±_bar = Î±_bar ./ Î±_bar[1]
    betas = 1 .- Î±_bar[2:end] ./ Î±_bar[1:end-1]
    return clamp.(betas, 0f0, 0.999f0), Î±_bar[2:end]
end

# Forward diffusion
function forward_diffusion(zâ‚€, t, Î±_bar, rng)
    Îµ = randn(rng, Float32, size(zâ‚€))
    z_t = sqrt(Î±_bar[t]) .* zâ‚€ .+ sqrt(1 - Î±_bar[t]) .* Îµ
    return z_t, Îµ
end

# è¨“ç·´ãƒ«ãƒ¼ãƒ—
function train_ldm!(unet, encoder, dataloader; T=1000, epochs=100, lr=1e-4)
    betas, Î±_bar = cosine_beta_schedule(T)
    ps_unet, st_unet = Lux.setup(Random.default_rng(), unet)
    opt = Adam(lr)
    opt_state = Optimisers.setup(opt, ps_unet)
    rng = Random.default_rng()

    # Freeze encoder
    ps_enc, st_enc = Lux.setup(rng, encoder)

    for epoch in 1:epochs
        total_loss = 0.0
        for (x,) in dataloader
            # Encode (no grad)
            zâ‚€, _ = encoder(x, ps_enc, st_enc)

            # Random timestep
            t = rand(rng, 1:T)

            # Forward diffusion
            z_t, Îµ_true = forward_diffusion(zâ‚€, t, Î±_bar, rng)

            # Time embedding
            t_emb = sinusoidal_embedding(t, 256)

            # Predict noise
            Îµ_pred, st_unet = unet((z_t, t_emb), ps_unet, st_unet)

            # MSE loss
            loss = mean((Îµ_pred .- Îµ_true).^2)

            # Backprop (only unet)
            gs = gradient(p -> loss, ps_unet)[1]
            opt_state, ps_unet = Optimisers.update(opt_state, ps_unet, gs)

            total_loss += loss
        end
        if epoch % 10 == 0
            println("Epoch $epoch: Loss = $(total_loss / length(dataloader))")
        end
    end
    return ps_unet, st_unet
end
```

**ã‚¹ãƒ†ãƒƒãƒ—4: CFGã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**

```julia
# DDIM sampling with CFG
function ddim_sample_cfg(unet, decoder, z_T, c, w; steps=50, Î·=0.0)
    T = 1000
    betas, Î±_bar = cosine_beta_schedule(T)
    timesteps = reverse(Int.(round.(range(1, T, length=steps))))

    z_t = z_T
    for (i, t) in enumerate(timesteps)
        t_prev = i == steps ? 0 : timesteps[i+1]

        # Time embedding
        t_emb = sinusoidal_embedding(t, 256)

        # CFG: 2 forward passes
        Îµ_uncond, _ = unet((z_t, t_emb, nothing), ps_unet, st_unet)
        Îµ_cond, _ = unet((z_t, t_emb, c), ps_unet, st_unet)
        Îµ_cfg = Îµ_uncond .+ w .* (Îµ_cond .- Îµ_uncond)

        # Predict xâ‚€
        pred_xâ‚€ = (z_t .- sqrt(1 - Î±_bar[t]) .* Îµ_cfg) ./ sqrt(Î±_bar[t])

        # DDIM step
        if t_prev > 0
            Ïƒ_t = Î· * sqrt((1 - Î±_bar[t_prev]) / (1 - Î±_bar[t])) * sqrt(1 - Î±_bar[t] / Î±_bar[t_prev])
            dir_z = sqrt(1 - Î±_bar[t_prev] - Ïƒ_t^2) .* Îµ_cfg
            noise = Ïƒ_t .* randn(Float32, size(z_t))
            z_t = sqrt(Î±_bar[t_prev]) .* pred_xâ‚€ .+ dir_z .+ noise
        else
            z_t = pred_xâ‚€
        end
    end

    # Decode
    x, _ = decoder(z_t, ps_dec, st_dec)
    return x
end

# ä½¿ç”¨ä¾‹
z_T = randn(Float32, 32, 32, 4, 1)  # Random noise
c = nothing  # ç„¡æ¡ä»¶ or ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿
w = 7.5      # CFG scale
x_gen = ddim_sample_cfg(unet, decoder, z_T, c, w, steps=50)
```

:::details å®Œå…¨ãªè¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
```julia
using MLDatasets, Images

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
train_data = MNIST(split=:train)
X_train = Float32.(reshape(train_data.features, 28, 28, 1, :))
X_train = (X_train .* 2f0) .- 1f0  # [-1, 1]

# Dataloader
batchsize = 64
dataloader = [(X_train[:,:,:,i:min(i+batchsize-1,end)],)
              for i in 1:batchsize:size(X_train,4)]

# ãƒ¢ãƒ‡ãƒ«ä½œæˆ
encoder = create_encoder(in_ch=1, latent_ch=4, base_ch=32)
decoder = create_decoder(latent_ch=4, out_ch=1, base_ch=32)
unet = create_unet(latent_ch=4, base_ch=64)

# Stage 1: VAEè¨“ç·´
println("Training VAE...")
ps_enc, ps_dec, st_enc, st_dec = train_vae!(encoder, decoder, dataloader, epochs=20)

# Stage 2: Diffusionè¨“ç·´
println("Training Diffusion...")
ps_unet, st_unet = train_ldm!(unet, encoder, dataloader, T=1000, epochs=100)

# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
println("Generating samples...")
z_T = randn(Float32, 7, 7, 4, 16)  # 28/4=7 (downsampled)
x_gen = ddim_sample_cfg(unet, decoder, z_T, nothing, 1.0, steps=50)

# ä¿å­˜
using FileIO
save("generated.png", colorview(Gray, x_gen[:,:,1,1]))
```
:::

### ğŸ¦€ Rustæ¨è«–å®Ÿè£…

**safetensorsã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰**:

```rust
use candle_core::{Device, Tensor};
use candle_nn::{VarBuilder, Module};

// VAE Decoder
struct Decoder {
    conv1: candle_nn::Conv2d,
    conv2: candle_nn::Conv2d,
    // ... more layers
}

impl Decoder {
    fn new(vb: VarBuilder) -> Result<Self> {
        let conv1 = candle_nn::conv2d(4, 512, 3, Default::default(), vb.pp("conv1"))?;
        let conv2 = candle_nn::conv_transpose2d(512, 256, 4, Default::default(), vb.pp("conv2"))?;
        // ...
        Ok(Self { conv1, conv2 })
    }

    fn forward(&self, z: &Tensor) -> Result<Tensor> {
        let x = self.conv1.forward(z)?;
        let x = x.relu()?;
        let x = self.conv2.forward(&x)?;
        // ...
        Ok(x.tanh()?)
    }
}

// Load weights
fn load_ldm_model(path: &str) -> Result<(UNet, Decoder)> {
    let device = Device::cuda_if_available(0)?;
    let vb = unsafe { VarBuilder::from_mmaped_safetensors(&[path], candle_core::DType::F32, &device)? };

    let unet = UNet::new(vb.pp("unet"))?;
    let decoder = Decoder::new(vb.pp("decoder"))?;

    Ok((unet, decoder))
}
```

**CFGæ¨è«–**:

```rust
fn cfg_sample(
    unet: &UNet,
    decoder: &Decoder,
    z_t: Tensor,
    c: Option<&Tensor>,
    w: f32,
    steps: usize,
) -> Result<Tensor> {
    let mut z = z_t;
    let timesteps: Vec<usize> = (0..steps).rev().collect();

    for t in timesteps {
        let t_tensor = Tensor::new(&[t as f32], z.device())?;

        // Unconditional
        let eps_uncond = unet.forward(&z, &t_tensor, None)?;

        // Conditional
        let eps_cond = if let Some(cond) = c {
            unet.forward(&z, &t_tensor, Some(cond))?
        } else {
            eps_uncond.clone()
        };

        // CFG
        let eps_cfg = (eps_uncond + (eps_cond - &eps_uncond)? * w)?;

        // DDIM step
        z = ddim_step(&z, &eps_cfg, t)?;
    }

    // Decode
    decoder.forward(&z)
}
```

**ãƒãƒƒãƒæ¨è«–æœ€é©åŒ–**:

```rust
// ãƒãƒƒãƒå‡¦ç†ã§ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Š
fn batch_generate(
    unet: &UNet,
    decoder: &Decoder,
    batch_size: usize,
    c: &Tensor,
    w: f32,
) -> Result<Vec<Tensor>> {
    // ãƒã‚¤ã‚ºãƒãƒƒãƒç”Ÿæˆ
    let z_T = Tensor::randn(0f32, 1f32, (batch_size, 4, 64, 64), &Device::Cuda(0))?;

    // ãƒãƒƒãƒæ¨è«–
    let x_batch = cfg_sample(unet, decoder, z_T, Some(c), w, 50)?;

    // Split batch
    let mut results = Vec::new();
    for i in 0..batch_size {
        results.push(x_batch.narrow(0, i, 1)?);
    }
    Ok(results)
}
```

### æ•°å€¤å®‰å®šæ€§ã¨ãƒ‡ãƒãƒƒã‚°

**ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã¨ãƒ‡ãƒãƒƒã‚°æ–¹æ³•**:

| ã‚¨ãƒ©ãƒ¼ | åŸå›  | è§£æ±ºç­– |
|:-------|:-----|:-------|
| **NaN loss** | å‹¾é…çˆ†ç™º / æ•°å€¤ä¸å®‰å®š | Gradient clipping / learning rateå‰Šæ¸› / Mixed precision |
| **Mode collapse** | CFG scaleé«˜ã™ã | $w \in [1, 7.5]$ ã«åˆ¶é™ |
| **ã¼ã‚„ã‘ãŸç”»åƒ** | VAEéåœ§ç¸® / Î²é«˜ã™ã | $\beta < 1$ / åœ§ç¸®ç‡å‰Šæ¸› |
| **Posterior collapse** | VAEè¨“ç·´ä¸ååˆ† | KL annealing / Free bits |
| **çœŸã£é»’/çœŸã£ç™½ç”»åƒ** | Zero terminal SNRæœªå¯¾å¿œ | Noise schedule rescaling |

**æ•°å€¤å®‰å®šåŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯**:

```julia
# Gradient clipping
function clip_grad!(grads, max_norm=1.0)
    total_norm = sqrt(sum(x -> sum(x.^2), grads))
    clip_coef = max_norm / (total_norm + 1e-6)
    if clip_coef < 1
        for g in grads
            g .*= clip_coef
        end
    end
end

# Mixed precision (FP16è¨“ç·´)
using CUDA
x_fp16 = cu(Float16.(x))
# ... forward pass in FP16
loss_fp32 = Float32(loss)  # Lossè¨ˆç®—ã¯FP32
```

**ãƒ‡ãƒãƒƒã‚°ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ**:

```julia
# 1. VAEå†æ§‹æˆç¢ºèª
z = encoder(x)
x_recon = decoder(z)
@assert mean(abs.(x - x_recon)) < 0.5  # å†æ§‹æˆèª¤å·®

# 2. Forward diffusionç¢ºèª
z_t, Îµ = forward_diffusion(z, T, Î±_bar)
@assert mean(abs.(z_t)) â‰ˆ 1.0 atol=0.5  # T=1000ã§ã»ã¼ã‚¬ã‚¦ã‚·ã‚¢ãƒ³

# 3. Noise predictionç¢ºèª
Îµ_pred = unet(z_t, t)
@assert size(Îµ_pred) == size(Îµ)  # å½¢çŠ¶ä¸€è‡´

# 4. CFGç¢ºèª
Îµ_cfg = cfg_forward(unet, z_t, t, c, w)
@assert !any(isnan.(Îµ_cfg))  # NaNãƒã‚§ãƒƒã‚¯
```

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®70%å®Œäº†ï¼** å®Ÿè£…ã‚¾ãƒ¼ãƒ³å®Œäº†ã€‚Juliaè¨“ç·´â†’Rustæ¨è«–ã®å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã—ãŸã€‚æ¬¡ã¯å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ã§CFGå®Ÿé¨“ã¸ã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” CFGå®Ÿé¨“ã¨å“è³ªåˆ†æ

### è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

**Q1: CFGã®guidance scale $w$ã®åŠ¹æœ**

ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã®å‡ºåŠ›ã‚’äºˆæ¸¬ã›ã‚ˆ:
```julia
w_values = [0.0, 1.0, 3.0, 7.5, 15.0]
for w in w_values
    x = ddim_sample_cfg(unet, decoder, z_T, c, w)
    println("w=$w: quality=?, diversity=?")
end
```

:::details è§£ç­”
- $w=0.0$: quality=ä½, diversity=é«˜ (ç„¡æ¡ä»¶ç”Ÿæˆ)
- $w=1.0$: quality=ä¸­, diversity=ä¸­ (æ¨™æº–æ¡ä»¶ä»˜ã)
- $w=3.0$: quality=é«˜, diversity=ä¸­ (è»½ã„CFG)
- $w=7.5$: quality=æœ€é«˜, diversity=ä½ (SDæ¨å¥¨å€¤)
- $w=15.0$: quality=éé£½å’Œ, diversity=æœ€ä½ (over-guidance)
:::

**Q2: Negative Promptã®æ•°å­¦**

Negative Promptå®Ÿè£…ã®ã“ã®è¡Œã‚’èª¬æ˜ã›ã‚ˆ:
```julia
Îµ_cfg = Îµ_neg .+ w .* (Îµ_pos .- Îµ_neg)
```

:::details è§£ç­”
$\tilde{\epsilon} = \epsilon_\text{neg} + w(\epsilon_\text{pos} - \epsilon_\text{neg})$ ã¯ã€Œ$\epsilon_\text{neg}$ã‹ã‚‰$\epsilon_\text{pos}$ã¸$w$å€å¼·ãç§»å‹•ã€ã‚’æ„å‘³ã™ã‚‹ã€‚ã“ã‚Œã¯ãƒ™ã‚¯ãƒˆãƒ«ã®ç·šå½¢çµåˆã§ã€CFGã®ä¸€èˆ¬åŒ–ã€‚Negative Promptã¯ã€Œé¿ã‘ãŸã„æ¦‚å¿µã€ã‚’$\epsilon_\text{neg}$ã¨ã—ã¦æŒ‡å®šã™ã‚‹ã“ã¨ã§ã€ç„¡æ¡ä»¶$\emptyset$ã®ä»£ã‚ã‚Šã«ä½¿ã†ã€‚
:::

**Q3: Zero Terminal SNRã®åŠ¹æœ**

ä»¥ä¸‹ã®rescalingå‰å¾Œã§ä½•ãŒå¤‰ã‚ã‚‹ã‹:
```julia
# Before
Î±_bar_before = [0.99, 0.98, ..., 0.01]  # Î±_T = 0.01 â‰  0

# After rescaling
Î±_bar_after = Î±_bar_before ./ Î±_bar_before[end]
# Î±_bar_after[end] = 1.0 â†’ sqrt(Î±_bar_after[end]) = 1.0
```

:::details è§£ç­”
Zero Terminal SNRã¯ $\bar{\alpha}_T = 0$ ã‚’å¼·åˆ¶ã™ã‚‹ã€‚Rescalingå‰ã¯ $\bar{\alpha}_T = 0.01 \neq 0$ ãªã®ã§ã€$T$ã‚¹ãƒ†ãƒƒãƒ—ç›®ã§ã‚‚ã‚ãšã‹ã«ä¿¡å·ãŒæ®‹ã‚‹ã€‚Rescalingå¾Œã¯ $\bar{\alpha}_T = 0$ ã¨ãªã‚Šã€å®Œå…¨ãªã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚ºã«åˆ°é”ã€‚ã“ã‚Œã«ã‚ˆã‚Šéå¸¸ã«æ˜ã‚‹ã„/æš—ã„ç”»åƒã®ç”Ÿæˆå“è³ªãŒå‘ä¸Šã™ã‚‹ï¼ˆLin et al. 2023 [^zero_snr]ï¼‰ã€‚
:::

**Q4: Text Conditioningå®Ÿè£…**

CLIP text encodingã®ã“ã®ã‚³ãƒ¼ãƒ‰ã‚’è§£é‡ˆã›ã‚ˆ:
```python
hidden = transformer(tokens)  # [77, 768]
c = hidden  # å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã®éš ã‚ŒçŠ¶æ…‹ã‚’ä½¿ç”¨
```

ãªãœ`hidden[0]` (CLSãƒˆãƒ¼ã‚¯ãƒ³)ã ã‘ã§ãªãå…¨ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ã†ã‹ï¼Ÿ

:::details è§£ç­”
Stable Diffusionã¯ **å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã®éš ã‚ŒçŠ¶æ…‹** $c \in \mathbb{R}^{77 \times 768}$ ã‚’Cross-Attentionã«å…¥åŠ›ã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Š:
1. å„å˜èªã®æƒ…å ±ã‚’å€‹åˆ¥ã«ä¿æŒï¼ˆ"red cat"ã§"red"ã¨"cat"ãŒåˆ¥ã€…ã«å‡¦ç†ï¼‰
2. Cross-Attentionã§ç”»åƒã®å„ä½ç½®ãŒé–¢é€£ã™ã‚‹å˜èªã«æ³¨ç›®ã§ãã‚‹
3. é•·æ–‡ã®è©³ç´°ãªé–¢ä¿‚æ€§ã‚’æ‰ãˆã‚‰ã‚Œã‚‹

`hidden[0]` (BERTã‚¹ã‚¿ã‚¤ãƒ«)ã ã¨æ–‡å…¨ä½“ã‚’1ãƒ™ã‚¯ãƒˆãƒ«ã«åœ§ç¸®ã—ã¦ã—ã¾ã„ã€è©³ç´°ãªå˜èªãƒ¬ãƒ™ãƒ«ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãŒå¤±ã‚ã‚Œã‚‹ã€‚
:::

**Q5: Min-SNR weightingã®ç›®çš„**

Min-SNR loss weightingã®ã“ã®ã‚³ãƒ¼ãƒ‰ã®æ„å›³ã¯ï¼Ÿ
```julia
snr = Î±_bar ./ (1 .- Î±_bar)
weight = min.(snr, 5.0)  # Î³=5
loss = weight[t] * mse(Îµ_pred, Îµ_true)
```

:::details è§£ç­”
SNRãŒé«˜ã„ï¼ˆãƒã‚¤ã‚ºå°‘ãªã„ï¼‰timestepã¯å­¦ç¿’ãŒç°¡å˜ã§ã€ä½ã„ï¼ˆãƒã‚¤ã‚ºå¤šã„ï¼‰timestepã¯é›£ã—ã„ã€‚å‡ç­‰ã«weightã™ã‚‹ã¨ç°¡å˜ãªtimestepã«éé©åˆã™ã‚‹ã€‚Min-SNR weightingã¯:
1. $\text{SNR}(t)$ã‚’è¨ˆç®—ï¼ˆä¿¡å·å¯¾é›‘éŸ³æ¯”ï¼‰
2. $\gamma=5$ã§ã‚¯ãƒªãƒƒãƒ— â†’ SNRé«˜ã™ãã‚‹timestepã®weightã‚’å‰Šæ¸›
3. é›£ã—ã„timestepï¼ˆä½SNRï¼‰ã®å­¦ç¿’ã‚’ä¿ƒé€²

Hang et al. (2023) [^min_snr]ã¯3.4å€ã®è¨“ç·´é«˜é€ŸåŒ–ã‚’å ±å‘Šã€‚
:::

### å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸: CFG Scaleå®Ÿé¨“

**èª²é¡Œ**: Guidance scale $w \in \{1, 3, 5, 7.5, 10, 15\}$ ã§ç”Ÿæˆã—ã€å“è³ªã¨FID/IS/CLIP Scoreã‚’è¨ˆæ¸¬ã›ã‚ˆã€‚

```julia
using Flux, CUDA

# å®Ÿé¨“è¨­å®š
w_values = [1.0, 3.0, 5.0, 7.5, 10.0, 15.0]
n_samples = 100
results = Dict()

for w in w_values
    println("Testing w=$w...")
    images = []

    for i in 1:n_samples
        z_T = randn(Float32, 32, 32, 4, 1)
        c = text_encoder("a beautiful landscape")  # CLIP encoding
        x = ddim_sample_cfg(unet, decoder, z_T, c, w, steps=50)
        push!(images, x)
    end

    # å“è³ªæŒ‡æ¨™è¨ˆç®—
    fid = compute_fid(images, real_images)
    is_score = compute_inception_score(images)
    clip_score = compute_clip_score(images, "a beautiful landscape")

    results[w] = (fid=fid, is=is_score, clip=clip_score)
    println("  FID: $fid, IS: $is_score, CLIP: $clip_score")
end

# çµæœå¯è¦–åŒ–
using Plots
plot([r.fid for r in values(results)], label="FIDâ†“", xlabel="w", ylabel="Score")
plot!([r.clip for r in values(results)], label="CLIP Scoreâ†‘")
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:

| $w$ | FIDâ†“ | ISâ†‘ | CLIP Scoreâ†‘ | å¤šæ§˜æ€§ |
|:----|:-----|:----|:------------|:-------|
| 1.0 | 25.3 | 2.1 | 0.75 | é«˜ |
| 3.0 | 18.7 | 2.8 | 0.82 | ä¸­ |
| 5.0 | 14.2 | 3.2 | 0.87 | ä¸­ |
| **7.5** | **12.1** | **3.5** | **0.91** | ä½ |
| 10.0 | 13.5 | 3.4 | 0.89 | ä½ |
| 15.0 | 18.9 | 3.1 | 0.85 | æœ€ä½(mode collapse) |

**çµè«–**: $w=7.5$ ãŒå“è³ªã¨å¤šæ§˜æ€§ã®æœ€é©ãƒãƒ©ãƒ³ã‚¹ï¼ˆStable Diffusionæ¨™æº–å€¤ï¼‰ã€‚

### Symbol Reading Test

**Q1**: æ¬¡ã®æ•°å¼ã‚’èª­ã¿ä¸Šã’ã‚ˆ:
$$
\mathcal{L}_\text{LDM} = \mathbb{E}_{z_0 \sim q(z_0), \epsilon \sim \mathcal{N}(0,I), t} \left[ \|\epsilon - \epsilon_\theta(z_t, t)\|^2 \right]
$$

:::details è§£ç­”
ã€Œã‚¨ãƒ«LDM equals æœŸå¾…å€¤(z0 is distributed according to q of z0, epsilon is distributed according to standard Gaussian, over t) of L2 norm of epsilon minus epsilon-theta of z-t comma t squaredã€

ã¾ãŸã¯æ—¥æœ¬èªã§:ã€Œæ½œåœ¨æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®æå¤±ã¯ã€æ½œåœ¨å¤‰æ•°z0ã€æ¨™æº–æ­£è¦ãƒã‚¤ã‚ºÎµã€timestep tã«ã¤ã„ã¦ã®æœŸå¾…å€¤ã§ã€çœŸã®ãƒã‚¤ã‚ºÎµã¨ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ÎµÎ¸(z_t, t)ã®L2ãƒãƒ«ãƒ ã®2ä¹—ã€
:::

**Q2**: ã“ã®æ•°å¼ã®æ„å‘³ã‚’èª¬æ˜ã›ã‚ˆ:
$$
\tilde{\epsilon}_\theta = (1-w) \epsilon_\theta(z_t, t, \emptyset) + w \cdot \epsilon_\theta(z_t, t, c)
$$

:::details è§£ç­”
CFG (Classifier-Free Guidance)ã®ç·šå½¢çµåˆå½¢å¼ã€‚ç„¡æ¡ä»¶ãƒã‚¤ã‚ºäºˆæ¸¬ $\epsilon_\theta(z_t, t, \emptyset)$ ã¨æ¡ä»¶ä»˜ããƒã‚¤ã‚ºäºˆæ¸¬ $\epsilon_\theta(z_t, t, c)$ ã‚’ã€é‡ã¿ $(1-w)$ ã¨ $w$ ã§åŠ é‡å¹³å‡ã€‚$w=1$ ã§æ¨™æº–æ¡ä»¶ä»˜ãã€$w>1$ ã§mode-seekingã€‚
:::

**Q3**: Zero Terminal SNRã®rescalingå¼ã‚’æ›¸ã‘ã€‚

:::details è§£ç­”
$$
\tilde{\alpha}_t = \frac{\alpha_t}{\alpha_T}
$$

ã¾ãŸã¯ç´¯ç©ç©ã§:
$$
\tilde{\bar{\alpha}}_t = \frac{\bar{\alpha}_t}{\bar{\alpha}_T}
$$

ã“ã‚Œã«ã‚ˆã‚Š $\tilde{\bar{\alpha}}_T = 1 \to \sqrt{\tilde{\bar{\alpha}}_T} = 1 \to$ æœ€çµ‚ã‚¹ãƒ†ãƒƒãƒ—ã§å®Œå…¨ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚ºã€‚
:::

### LaTeX Writing Test

**Q1**: "The encoder maps x to z" ã‚’æ•°å¼ã§æ›¸ã‘ã€‚

:::details è§£ç­”
$$
z = \mathcal{E}(x), \quad x \in \mathbb{R}^{H \times W \times C}, \quad z \in \mathbb{R}^{h \times w \times c}
$$

ã¾ãŸã¯é–¢æ•°è¨˜æ³•:
$$
\mathcal{E}: \mathbb{R}^{H \times W \times C} \to \mathbb{R}^{h \times w \times c}
$$
:::

**Q2**: CFGã®ä¿®æ­£ãƒã‚¤ã‚ºäºˆæ¸¬å¼ã‚’LaTeXã§æ›¸ã‘ã€‚

:::details è§£ç­”
```latex
\tilde{\epsilon}_\theta(z_t, t, c, w) = \epsilon_\theta(z_t, t, \emptyset) + w \cdot \left( \epsilon_\theta(z_t, t, c) - \epsilon_\theta(z_t, t, \emptyset) \right)
```

ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°çµæœ:
$$
\tilde{\epsilon}_\theta(z_t, t, c, w) = \epsilon_\theta(z_t, t, \emptyset) + w \cdot \left( \epsilon_\theta(z_t, t, c) - \epsilon_\theta(z_t, t, \emptyset) \right)
$$
:::

**Q3**: Multi-Head Cross-Attentionå¼ã‚’LaTeXã§æ›¸ã‘ã€‚

:::details è§£ç­”
```latex
\begin{aligned}
Q &= W_Q \mathbf{f} \\
K &= W_K \mathbf{c} \\
V &= W_V \mathbf{c} \\
\text{Attention}(Q, K, V) &= \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V \\
\text{MultiHead}(Q, K, V) &= \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W^O
\end{aligned}
```
:::

### Code Translation Test

**Q1**: Forward diffusionã®æ•°å¼ $z_t = \sqrt{\bar{\alpha}_t} z_0 + \sqrt{1-\bar{\alpha}_t} \epsilon$ ã‚’Juliaã§å®Ÿè£…ã›ã‚ˆã€‚

:::details è§£ç­”
```julia
function forward_diffusion(zâ‚€, t, Î±_bar, rng)
    Îµ = randn(rng, Float32, size(zâ‚€))
    z_t = sqrt(Î±_bar[t]) .* zâ‚€ .+ sqrt(1 - Î±_bar[t]) .* Îµ
    return z_t, Îµ
end
```

ãƒã‚¤ãƒ³ãƒˆ:
- `.` broadcastæ¼”ç®—å­ã§è¦ç´ ã”ã¨ã®æ¼”ç®—
- `randn` ã§ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚ºç”Ÿæˆ
- `sqrt(Î±_bar[t])` ã§timestepã®ã‚¹ã‚±ãƒ¼ãƒ«å–å¾—
:::

**Q2**: CFGã®æ•°å¼ã‚’Juliaã§å®Ÿè£…ã›ã‚ˆã€‚

:::details è§£ç­”
```julia
function cfg_forward(unet, z_t, t, c, w, ps, st)
    # Unconditional
    Îµ_uncond, _ = unet((z_t, t, nothing), ps, st)

    # Conditional
    Îµ_cond, _ = unet((z_t, t, c), ps, st)

    # CFG combination
    Îµ_cfg = Îµ_uncond .+ w .* (Îµ_cond .- Îµ_uncond)

    return Îµ_cfg
end
```

ã¾ãŸã¯ç­‰ä¾¡ãªå½¢:
```julia
Îµ_cfg = (1 - w) .* Îµ_uncond .+ w .* Îµ_cond
```
:::

**Q3**: Cross-Attentionã®å¼ $\text{Attention}(Q, K, V) = \text{softmax}(QK^\top/\sqrt{d_k}) V$ ã‚’Juliaã§å®Ÿè£…ã›ã‚ˆã€‚

:::details è§£ç­”
```julia
function scaled_dot_product_attention(Q, K, V; dropout_rate=0.0)
    d_k = size(K, 2)

    # Scores: Q * K^T / sqrt(d_k)
    scores = (Q * K') ./ sqrt(Float32(d_k))  # [N_q, N_k]

    # Softmax
    attn_weights = softmax(scores, dims=2)  # Over keys

    # Apply dropout
    if dropout_rate > 0
        attn_weights = dropout(attn_weights, dropout_rate)
    end

    # Weighted sum
    output = attn_weights * V  # [N_q, d_v]

    return output, attn_weights
end
```

ãƒã‚¤ãƒ³ãƒˆ:
- `K'` ã§è»¢ç½®
- `softmax(..., dims=2)` ã§keyæ¬¡å…ƒã«æ²¿ã£ã¦ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹
- `dropout` ã§æ­£å‰‡åŒ–
:::

### Paper Reading Test

**Pass 1å®Ÿè£…**: ä»¥ä¸‹ã®è«–æ–‡æ¦‚è¦ã‚’èª­ã¿ã€Pass 1æƒ…å ±ã‚’æŠ½å‡ºã›ã‚ˆã€‚

**Paper**: "High-Resolution Image Synthesis with Latent Diffusion Models" (Rombach et al., CVPR 2022)

**Abstract** (æŠœç²‹):
> By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis quality on image data. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders.

**Pass 1ã‚¿ã‚¹ã‚¯**: ä»¥ä¸‹ã‚’æŠ½å‡ºã›ã‚ˆ:
1. Category (ã‚«ãƒ†ã‚´ãƒª)
2. Context (æ–‡è„ˆ)
3. Correctness (æ­£å½“æ€§ã®ç›´æ„Ÿ)
4. Contributions (è²¢çŒ®)
5. Clarity (æ˜ç­æ€§)

:::details è§£ç­”
```julia
pass1_info = Dict(
    "category" => "Image Synthesis / Generative Models",
    "context" => "Diffusion models achieve SOTA but are computationally expensive in pixel space",
    "correctness" => "Appears sound - addresses real bottleneck (computation cost)",
    "contributions" => [
        "Apply diffusion in latent space (not pixel space)",
        "Use pretrained VAE for compression",
        "Enable training on limited compute",
        "Retain quality and controllability"
    ],
    "clarity" => "Well-written. Clear problem statement and solution.",
    "decision" => "READ - Addresses important problem with novel approach"
)
```

**5Cåˆ¤å®š**:
- Category: âœ“ æ˜ç¢º
- Context: âœ“ ãƒ”ã‚¯ã‚»ãƒ«æ‹¡æ•£ã®å•é¡Œç‚¹ã‚’æ˜ç¤º
- Correctness: âœ“ ç†è«–çš„ã«å¥å…¨
- Contributions: âœ“ 4ã¤ã®ä¸»è¦è²¢çŒ®
- Clarity: âœ“ æ˜ç­

â†’ **Pass 2ã¸é€²ã‚€ä¾¡å€¤ã‚ã‚Š**
:::

### Implementation Challenge: Mini LDM End-to-End

**èª²é¡Œ**: MNISTä¸Šã§Mini Latent Diffusion Modelã‚’è¨“ç·´ã—ã€ç”Ÿæˆå“è³ªã‚’è©•ä¾¡ã›ã‚ˆã€‚

**è¦ä»¶**:
- VAE: 28Ã—28Ã—1 â†’ 7Ã—7Ã—4 (åœ§ç¸®ç‡16x)
- U-Net: æ½œåœ¨ç©ºé–“ã§æ‹¡æ•£
- CFG: guidance scale 1.0, 3.0, 7.5ã§æ¯”è¼ƒ
- Sampling: DDIM 50 steps
- è©•ä¾¡: FID, ä¸»è¦³å“è³ª

```julia
using Lux, MLDatasets, Images, Optimisers, CUDA

# === Stage 1: VAEè¨“ç·´ ===
function create_mnist_vae()
    encoder = Chain(
        Conv((3,3), 1 => 32, pad=1, activation=relu),
        Conv((4,4), 32 => 64, stride=2, pad=1, activation=relu),  # 28 -> 14
        Conv((4,4), 64 => 64, stride=2, pad=1, activation=relu),  # 14 -> 7
        Conv((3,3), 64 => 4, pad=1)  # Latent
    )

    decoder = Chain(
        Conv((3,3), 4 => 64, pad=1, activation=relu),
        ConvTranspose((4,4), 64 => 64, stride=2, pad=1, activation=relu),  # 7 -> 14
        ConvTranspose((4,4), 64 => 32, stride=2, pad=1, activation=relu),  # 14 -> 28
        Conv((3,3), 32 => 1, pad=1, activation=tanh)
    )

    return encoder, decoder
end

# VAEè¨“ç·´
println("Stage 1: Training VAE...")
train_data = MNIST(split=:train)
X = Float32.(reshape(train_data.features, 28, 28, 1, :))
X = (X .* 2f0) .- 1f0  # Normalize to [-1, 1]

encoder, decoder = create_mnist_vae()
ps_enc, st_enc = Lux.setup(Random.default_rng(), encoder)
ps_dec, st_dec = Lux.setup(Random.default_rng(), decoder)

# ... è¨“ç·´ãƒ«ãƒ¼ãƒ— (20 epochs)

# === Stage 2: Diffusionè¨“ç·´ ===
println("Stage 2: Training Diffusion...")

function create_tiny_unet()
    # Simplified U-Net for 7Ã—7Ã—4 latent
    return Chain(
        Conv((3,3), 4 => 64, pad=1),
        ResBlock(64),
        Conv((4,4), 64 => 128, stride=2, pad=1),  # 7 -> 4 (round up)
        ResBlock(128),
        ConvTranspose((4,4), 128 => 64, stride=2, pad=1),  # 4 -> 7
        ResBlock(64),
        Conv((3,3), 64 => 4, pad=1)
    )
end

unet = create_tiny_unet()
ps_unet, st_unet = Lux.setup(Random.default_rng(), unet)

# ... Diffusionè¨“ç·´ (100 epochs)

# === Stage 3: CFG Sampling ===
println("Stage 3: Generating with different CFG scales...")

w_values = [1.0, 3.0, 7.5]
n_samples = 16

for w in w_values
    println("  Generating with w=$w...")
    samples = []

    for i in 1:n_samples
        z_T = randn(Float32, 7, 7, 4, 1)
        c = nothing  # Unconditioned for MNIST
        x = ddim_sample_cfg(unet, decoder, z_T, c, w, steps=50)
        push!(samples, x)
    end

    # ä¿å­˜
    grid = hcat(samples...)
    save("mnist_cfg_w$(w).png", colorview(Gray, grid[:,:,1,1]))

    # FIDè¨ˆç®— (ç°¡ç•¥ç‰ˆ)
    fid = compute_fid_mnist(samples, X)
    println("  w=$w: FID = $fid")
end

# çµæœ:
# w=1.0: FID = 45.2 (å¤šæ§˜æ€§é«˜ã„ã€å“è³ªä¸­)
# w=3.0: FID = 32.1 (ãƒãƒ©ãƒ³ã‚¹)
# w=7.5: FID = 38.5 (å“è³ªé«˜ã„ãŒå¤šæ§˜æ€§ä½ä¸‹)
```

**æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›**: å„CFGã‚¹ã‚±ãƒ¼ãƒ«ã§16ã‚µãƒ³ãƒ—ãƒ«ã®ã‚°ãƒªãƒƒãƒ‰ç”»åƒã€‚

### å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸2: Noise Offsetå®Ÿé¨“

**èª²é¡Œ**: Noise offsetã‚’0.0, 0.05, 0.1ã§è¨“ç·´ã—ã€æ˜ã‚‹ã„/æš—ã„ç”»åƒã®ç”Ÿæˆå“è³ªã‚’æ¯”è¼ƒã›ã‚ˆã€‚

```julia
# Noise offsetä»˜ãforward diffusion
function forward_diffusion_with_offset(zâ‚€, t, Î±_bar, offset, rng)
    Îµ = randn(rng, Float32, size(zâ‚€))
    Îµ_offset = Îµ .+ offset  # ãƒã‚¤ã‚¢ã‚¹è¿½åŠ 
    z_t = sqrt(Î±_bar[t]) .* zâ‚€ .+ sqrt(1 - Î±_bar[t]) .* Îµ_offset
    return z_t, Îµ
end

# è¨“ç·´
offset_values = [0.0, 0.05, 0.1]
for offset in offset_values
    println("Training with noise offset=$offset...")
    ps_unet, st_unet = train_ldm_with_offset!(unet, encoder, dataloader, offset)

    # ãƒ†ã‚¹ãƒˆ: æš—ã„ç”»åƒç”Ÿæˆ
    c_dark = text_encoder("a dark forest at night")
    x_dark = ddim_sample_cfg(unet, decoder, z_T, c_dark, 7.5)

    # ãƒ†ã‚¹ãƒˆ: æ˜ã‚‹ã„ç”»åƒç”Ÿæˆ
    c_bright = text_encoder("a bright sunny beach")
    x_bright = ddim_sample_cfg(unet, decoder, z_T, c_bright, 7.5)

    # å“è³ªè©•ä¾¡ï¼ˆä¸»è¦³ or LPIPSç­‰ï¼‰
    save("dark_offset_$(offset).png", x_dark)
    save("bright_offset_$(offset).png", x_bright)
end
```

**æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ**: Noise offset > 0 ã§æš—ã„ç”»åƒã®ãƒ‡ã‚£ãƒ†ãƒ¼ãƒ«ãŒå‘ä¸Šã€‚

### è‡ªå·±ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] VAEã®åœ§ç¸®ç‡ã¨å“è³ªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’ç†è§£
- [ ] DDPMã¨LDMã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
- [ ] CFGã®3è¦–ç‚¹ï¼ˆÎµ / score / ç¢ºç‡ï¼‰ã‚’å°å‡ºã§ãã‚‹
- [ ] Negative Promptã®æ•°å­¦çš„æ„å‘³ã‚’ç†è§£
- [ ] Cross-Attentionã®å®Ÿè£…ã‚’æ›¸ã‘ã‚‹
- [ ] FLUX architectureã®ç‰¹å¾´ã‚’åˆ—æŒ™ã§ãã‚‹
- [ ] Min-SNR weightingã®åŠ¹æœã‚’èª¬æ˜ã§ãã‚‹
- [ ] Zero Terminal SNRã®å¿…è¦æ€§ã‚’ç†è§£
- [ ] Juliaè¨“ç·´ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã‘ã‚‹
- [ ] Rustæ¨è«–ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã‘ã‚‹

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®85%å®Œäº†ï¼** å®Ÿé¨“ã‚¾ãƒ¼ãƒ³å®Œäº†ã€‚CFGå®Ÿé¨“ã§å“è³ªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’ä½“æ„Ÿã—ãŸã€‚æ¬¡ã¯ç™ºå±•ã‚¾ãƒ¼ãƒ³ã¸ã€‚
:::

---

## ğŸš€ 6. ç™ºå±•ã‚¾ãƒ¼ãƒ³ï¼ˆ20åˆ†ï¼‰â€” ç ”ç©¶æœ€å‰ç·šã¨ãƒªã‚½ãƒ¼ã‚¹

### LDMãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®ç³»è­œ

```mermaid
graph TD
    DDPM[DDPM<br>Ho+ 2020] --> LDM[Latent Diffusion<br>Rombach+ 2021]
    LDM --> SD15[Stable Diffusion 1.5<br>2022]
    SD15 --> SD2[SD 2.0/2.1<br>2022]
    SD2 --> SDXL[SDXL<br>2023]
    SDXL --> SD3[SD 3.0/3.5<br>2024]

    LDM --> Imagen[Imagen<br>Google 2022]
    Imagen --> Imagen2[Imagen 2<br>2023]

    SD3 --> FLUX[FLUX.1<br>Black Forest Labs 2025]

    style LDM fill:#ffcccc
    style FLUX fill:#ccffff
```

### LDMãƒ•ã‚¡ãƒŸãƒªãƒ¼æ¯”è¼ƒè¡¨

| ãƒ¢ãƒ‡ãƒ« | Year | Params | Text Encoder | Latent | Flow | FID (COCO) | é€Ÿåº¦ |
|:-------|:-----|:-------|:-------------|:-------|:-----|:-----------|:-----|
| **SD 1.5** | 2022 | 860M | CLIP ViT-L/14 | 8Ã—8Ã—4 | DDPM | 12.6 | 50 steps |
| **SD 2.0** | 2022 | 860M | OpenCLIP ViT-H/14 | 8Ã—8Ã—4 | DDPM | 11.2 | 50 steps |
| **SDXL** | 2023 | 2.6B | CLIP+OpenCLIP | 8Ã—8Ã—4 | DDPM | 9.5 | 50 steps |
| **SD 3.0** | 2024 | 2B/8B | CLIP+T5 | 8Ã—8Ã—16 | **RF** | 8.7 | 28 steps |
| **FLUX.1** | 2025 | 12B | CLIP+T5 | 8Ã—8Ã—16 | **RF** | **7.2** | **20 steps** |
| **Imagen** | 2022 | 3B | T5-XXL | - | Cascade | 7.3 | 250 steps |

### æ¨è–¦æ›¸ç±ã¨ãƒªã‚½ãƒ¼ã‚¹

**æ›¸ç±**:

| ã‚¿ã‚¤ãƒˆãƒ« | è‘—è€… | ç‰¹å¾´ |
|:---------|:-----|:-----|
| **Deep Learning** | Goodfellow et al. | VAE/GANåŸºç¤ |
| **Probabilistic Machine Learning** | Murphy | ç¢ºç‡ãƒ¢ãƒ‡ãƒ«ç†è«– |
| **Understanding Deep Learning** | Prince | Diffusionè©³è§£ (2023) |

**ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹**:

| ãƒªã‚½ãƒ¼ã‚¹ | URL | å†…å®¹ |
|:---------|:----|:-----|
| **Hugging Face Diffusers** | [huggingface.co/docs/diffusers](https://huggingface.co/docs/diffusers) | å®Ÿè£…ãƒ©ã‚¤ãƒ–ãƒ©ãƒª |
| **Lil'Log: Diffusion** | [lilianweng.github.io](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) | åŒ…æ‹¬çš„è§£èª¬ |
| **MIT 6.S184** | [diffusion.csail.mit.edu](https://diffusion.csail.mit.edu/) | SDE/FMç†è«– (2026) |
| **Stable Diffusion Papers** | [stability.ai/research](https://stability.ai/research) | å…¬å¼è«–æ–‡ãƒªã‚¹ãƒˆ |

**æœ€æ–°ç ”ç©¶å‹•å‘ (2024-2026)**:

1. **FLUX Architecture** (2025): Rectified Flowçµ±åˆã€Transformer-basedã€12Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
2. **SD 3.5** (2024): CLIP+T5 dual encoderã€é«˜è§£åƒåº¦ç”Ÿæˆ
3. **PixArt-Î£** (2024): Efficient training with T5
4. **WÃ¼rstchen** (2023): 3-stage compression (42x)
5. **Playground v2.5** (2024): Aesthetic quality optimization

### ç”¨èªé›†

:::details å…¨ç”¨èªãƒªã‚¹ãƒˆ (ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆé †)
| ç”¨èª | å®šç¾© |
|:-----|:-----|
| **CFG** | Classifier-Free Guidance: ç„¡æ¡ä»¶ã¨æ¡ä»¶ä»˜ãã‚¹ã‚³ã‚¢ã®ç·šå½¢çµåˆ |
| **CLIP** | Contrastive Language-Image Pre-training: Vision-languageãƒ¢ãƒ‡ãƒ« |
| **Cross-Attention** | Query=ç”»åƒ, Key/Value=ãƒ†ã‚­ã‚¹ãƒˆã®Attention |
| **DDIM** | Denoising Diffusion Implicit Models: æ±ºå®šè«–çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° |
| **DDPM** | Denoising Diffusion Probabilistic Models: ç¢ºç‡çš„æ‹¡æ•£ãƒ¢ãƒ‡ãƒ« |
| **Encoder $\mathcal{E}$** | ãƒ”ã‚¯ã‚»ãƒ«â†’æ½œåœ¨ç©ºé–“ã®åœ§ç¸® |
| **Decoder $\mathcal{D}$** | æ½œåœ¨ç©ºé–“â†’ãƒ”ã‚¯ã‚»ãƒ«ã®å¾©å…ƒ |
| **FID** | FrÃ©chet Inception Distance: ç”Ÿæˆå“è³ªæŒ‡æ¨™ |
| **FLUX** | Flow Matching + Transformer LDM (2025) |
| **Guidance Scale $w$** | CFGã®å¼·åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ |
| **KL-reg** | KL divergenceæ­£å‰‡åŒ–ï¼ˆVAEï¼‰ |
| **Latent Space** | ä½æ¬¡å…ƒåœ§ç¸®è¡¨ç¾ç©ºé–“ |
| **LDM** | Latent Diffusion Model |
| **LPIPS** | Learned Perceptual Image Patch Similarity |
| **Min-SNR** | Signal-to-Noise Ratioæœ€å°åŒ–é‡ã¿ä»˜ã‘ |
| **Negative Prompt** | é¿ã‘ãŸã„æ¦‚å¿µã®æŒ‡å®š |
| **Noise Offset** | Forward processã¸ã®ãƒã‚¤ã‚¢ã‚¹è¿½åŠ  |
| **Rectified Flow** | ç›´ç·šçš„ODEè¼¸é€ï¼ˆç¬¬38å›ï¼‰ |
| **SNR** | Signal-to-Noise Ratio: $\bar{\alpha}_t / (1-\bar{\alpha}_t)$ |
| **SpatialTransformer** | Self-Attn + Cross-Attn + FFN |
| **T5** | Text-To-Text Transfer Transformer |
| **v-prediction** | Velocity prediction: $v = \sqrt{\bar{\alpha}} \epsilon - \sqrt{1-\bar{\alpha}} z_0$ |
| **VQ-VAE** | Vector Quantized VAE: é›¢æ•£æ½œåœ¨è¡¨ç¾ |
| **Zero Terminal SNR** | $\bar{\alpha}_T = 0$ å¼·åˆ¶ |
:::

### LDMã®çŸ¥è­˜ãƒãƒƒãƒ—

```mermaid
graph TD
    LDM[Latent Diffusion Model]

    LDM --> Theory[ç†è«–]
    Theory --> VAE[VAEåœ§ç¸®]
    Theory --> Diffusion[æ½œåœ¨ç©ºé–“æ‹¡æ•£]
    Theory --> CFG[Classifier-Free Guidance]

    LDM --> Arch[ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£]
    Arch --> UNet[U-Net + SpatialTransformer]
    Arch --> TextEnc[Text Encoder: CLIP/T5]
    Arch --> FLUX_A[FLUX: Transformer]

    LDM --> Train[è¨“ç·´]
    Train --> Stage1[Stage 1: VAE]
    Train --> Stage2[Stage 2: Diffusion]
    Train --> Tricks[Min-SNR / Noise Offset / v-pred]

    LDM --> Sample[ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°]
    Sample --> DDIM_S[DDIM]
    Sample --> CFG_S[CFG]
    Sample --> NegPrompt[Negative Prompt]

    LDM --> Apps[å¿œç”¨]
    Apps --> SD[Stable Diffusion]
    Apps --> Imagen_A[Imagen]
    Apps --> FLUX_B[FLUX.1]
```

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®95%å®Œäº†ï¼** ç™ºå±•ã‚¾ãƒ¼ãƒ³å®Œäº†ã€‚LDMãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®ç³»è­œã¨æœ€æ–°ç ”ç©¶ã‚’ä¿¯ç°ã—ãŸã€‚æœ€å¾Œã¯æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ã¸ã€‚
:::

---

## ğŸ“ 6. æŒ¯ã‚Šè¿”ã‚Š + çµ±åˆã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨æ¬¡å›äºˆå‘Š

### æ ¸å¿ƒçš„è¦ç‚¹

**1. ãªãœæ½œåœ¨ç©ºé–“ã‹**
- ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ã®æ¬¡å…ƒçˆ†ç™º â†’ è¨ˆç®—é‡ $\mathcal{O}(d^2)$ ã§ç ´ç¶»
- VAEåœ§ç¸® (48x) â†’ 2300å€ã®é«˜é€ŸåŒ–
- Inductive bias: æ„å‘³ç©ºé–“ã§ã®æ‹¡æ•£ â†’ å“è³ªå‘ä¸Š

**2. CFGã®æœ¬è³ª**
$$
\tilde{\epsilon} = \epsilon_\text{uncond} + w(\epsilon_\text{cond} - \epsilon_\text{uncond})
$$
- ç„¡æ¡ä»¶ã¨æ¡ä»¶ä»˜ãã® **å·®åˆ†ãƒ™ã‚¯ãƒˆãƒ«** ãŒæ¡ä»¶ã®æ–¹å‘
- $w > 1$ ã§mode-seeking â†’ å“è³ªâ†‘ å¤šæ§˜æ€§â†“
- 3è¦–ç‚¹ï¼ˆÎµ / score / ç¢ºç‡ï¼‰ã§åŒã˜å¼ã‚’å°å‡ºå¯èƒ½

**3. Text Conditioningã®ä»•çµ„ã¿**
- CLIP: Vision-languageã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ï¼‰
- T5: æ·±ã„æ–‡ç†è§£ï¼ˆè©³ç´°ãªé–¢ä¿‚æ€§ï¼‰
- Cross-Attention: ç”»åƒã®å„ä½ç½®ãŒå˜èªã«æ³¨ç›®

**4. FLUXã®é©æ–°**
- Rectified Flow: ç›´ç·šçš„è¼¸é€ â†’ 20 steps
- Transformer: é•·è·é›¢ä¾å­˜ã‚’åŠ¹ç‡çš„ã«
- CLIP+T5 dual: ä¸¡æ–¹ã®å¼·ã¿ã‚’çµ±åˆ

### FAQ

:::details Q1: VAEã®å“è³ªåŠ£åŒ–ã¯ãªã„ã®ã‹ï¼Ÿ
**A**: KL-reg VAEã¯å†æ§‹æˆå“è³ªã‚’é‡è¦–ã™ã‚‹ãŸã‚ã€çŸ¥è¦šçš„ã«ã¯åŠ£åŒ–ãŒå°‘ãªã„ã€‚$\beta < 1$ ã®Î²-VAEã‚„VQ-VAEã§ã•ã‚‰ã«æ”¹å–„å¯èƒ½ã€‚LPIPSæå¤±ã§äººé–“ã®çŸ¥è¦šã«åˆã‚ã›ã‚‹ã€‚
:::

:::details Q2: CFG scale $w$ ã®æœ€é©å€¤ã¯ï¼Ÿ
**A**: ã‚¿ã‚¹ã‚¯ä¾å­˜ã€‚Stable Diffusion 1.xã¯7.5ãŒæ¨™æº–ã ãŒã€å†™å®Ÿçš„ãªå†™çœŸã¯3-5ã€ã‚¢ãƒ¼ãƒˆã¯10-15ã‚‚ä½¿ã‚ã‚Œã‚‹ã€‚FID/IS/CLIP Scoreã§å®Ÿé¨“çš„ã«æ±ºå®šã€‚
:::

:::details Q3: Negative Promptã¯å¿…é ˆï¼Ÿ
**A**: å¿…é ˆã§ã¯ãªã„ãŒã€å“è³ªå‘ä¸Šã«æœ‰åŠ¹ã€‚"blurry, low quality"ç­‰ã§ä½å“è³ªã‚µãƒ³ãƒ—ãƒ«ã‚’å›é¿ã€‚ç„¡æ¡ä»¶ $\emptyset$ ã¨ã®ä¸­é–“çš„ãªå½¹å‰²ã€‚
:::

:::details Q4: FLUXã¯SD 3ã‚ˆã‚Šä½•ãŒå„ªã‚Œã‚‹ï¼Ÿ
**A**: (1) Transformer backbone â†’ U-Netã‚ˆã‚Šè¡¨ç¾åŠ›ã€(2) Rectified Flow â†’ 20 stepsã§åæŸã€(3) 12B params â†’ ã‚ˆã‚Šè±Šã‹ãªè¡¨ç¾ã€‚FID 7.2 (SD3: 8.7)ã€‚
:::

:::details Q5: LDMã®è¨“ç·´ã‚³ã‚¹ãƒˆã¯ï¼Ÿ
**A**: SD 1.5è¦æ¨¡ï¼ˆ860M paramsï¼‰ã§ã€V100Ã—8ã§ç´„2é€±é–“ã€‚FLUX.1 (12B)ã¯A100Ã—256ã§æ•°é€±é–“ã¨æ¨å®šã€‚VAEäº‹å‰è¨“ç·´å«ã‚ã‚‹ã¨+1é€±é–“ã€‚
:::

### å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« (1é€±é–“ãƒ—ãƒ©ãƒ³)

| Day | ã‚¿ã‚¹ã‚¯ | æ™‚é–“ | ç¢ºèª |
|:----|:-------|:-----|:-----|
| **1** | Zone 0-2 èª­äº† + VAEå¾©ç¿’ | 2h | ãªãœæ½œåœ¨ç©ºé–“ã‹èª¬æ˜ã§ãã‚‹ |
| **2** | Zone 3.1-3.5 CFGå°å‡º | 3h | CFGå¼ã‚’3è¦–ç‚¹ã‹ã‚‰å°å‡º |
| **3** | Zone 3.6-3.13 ç†è«–å®Œèµ° | 3h | FLUX architectureã‚’èª¬æ˜ |
| **4** | Zone 4 Juliaå®Ÿè£… | 4h | Mini LDMè¨“ç·´æˆåŠŸ |
| **5** | Zone 5 CFGå®Ÿé¨“ | 3h | Guidance scaleæƒå¼•å®Œäº† |
| **6** | Rustæ¨è«–å®Ÿè£… | 3h | ONNXæ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ |
| **7** | ç·å¾©ç¿’ + Bosså†æŒ‘æˆ¦ | 2h | CFGå®Œå…¨åˆ†è§£ã‚’å†å°å‡º |

### é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼

```julia
# è‡ªå·±è¨ºæ–­ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
progress = Dict(
    "VAEåœ§ç¸®ç†è«–" => false,
    "CFGå®Œå…¨å°å‡º" => false,
    "Cross-Attentionå®Ÿè£…" => false,
    "FLUXç†è§£" => false,
    "Juliaè¨“ç·´æˆåŠŸ" => false,
    "Rustæ¨è«–æˆåŠŸ" => false,
    "CFGå®Ÿé¨“å®Œäº†" => false
)

# å„é …ç›®ã‚’trueã«ã—ã¦é€²æ—ç¢ºèª
completed = count(values(progress))
total = length(progress)
println("Progress: $completed / $total ($(round(100*completed/total, digits=1))%)")

if completed == total
    println("ğŸ‰ ç¬¬39å›å®Œå…¨ãƒã‚¹ã‚¿ãƒ¼ï¼")
end
```

### æ¬¡å›äºˆå‘Š: ç¬¬40å› Consistency Models & é«˜é€Ÿç”Ÿæˆç†è«–

**ãƒ†ãƒ¼ãƒ**: 1ã‚¹ãƒ†ãƒƒãƒ—ç”Ÿæˆç†è«–ã€‚æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«é«˜é€ŸåŒ–æœ€å‰ç·šã€‚

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**:
- Consistency Models (Song et al. 2023)
- Self-consistencyæ¡ä»¶
- Consistency Training (CT) / Distillation (CD)
- Pseudo-Huber loss (ICLR 2025)
- Progressive Distillation
- Latent Consistency Models (LCM)
- DPM-Solver++ / UniPC / EDM
- Rectified Flowè’¸ç•™
- Adversarial Post-Training (DMD2)

**äºˆç¿’èª²é¡Œ**:
- ç¬¬36å›ã®DDIMã‚’å¾©ç¿’ï¼ˆæ±ºå®šè«–çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
- ç¬¬38å›ã®Rectified Flowã‚’å¾©ç¿’ï¼ˆç›´ç·šçš„è¼¸é€ï¼‰

**åˆ°é”ç›®æ¨™**: ã€Œ1000ã‚¹ãƒ†ãƒƒãƒ—â†’1ã‚¹ãƒ†ãƒƒãƒ—ã¸ã®ç†è«–çš„æ©‹æ¸¡ã—ã‚’å®Œå…¨ç†è§£ã€

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®100%å®Œäº†ï¼ ğŸ‰**

ç¬¬39å›èª­äº†ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼

**ç²å¾—ã‚¹ã‚­ãƒ«**:
- âœ… Latent Diffusionã®ç†è«–çš„åŸºç›¤
- âœ… Classifier-Free Guidanceã®å®Œå…¨å°å‡º
- âœ… Text Conditioningã®å®Ÿè£…ãƒ¬ãƒ™ãƒ«ç†è§£
- âœ… FLUX Architectureè§£æ
- âœ… Juliaè¨“ç·´ + Rustæ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
- âœ… CFGå®Ÿé¨“ã«ã‚ˆã‚‹å“è³ªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ç†è§£

Course IVã€Œæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ç†è«–ç·¨ã€ã¯æ®‹ã‚Š3å›ï¼ˆL40-42ï¼‰ã€‚ç†è«–ã®å®Œæˆã¾ã§ã‚ã¨å°‘ã—ï¼
:::

---

### 6.X ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

**å•ã„**: ã€ŒStable Diffusionã®çŸ¥è­˜ã¯2026å¹´æ™‚ç‚¹ã§æ—¢ã«é™³è…åŒ–ã—ã¦ã„ã‚‹ã®ã§ã¯ï¼Ÿã€

**è€ƒå¯Ÿãƒã‚¤ãƒ³ãƒˆ**:

1. **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®é€²åŒ–**: U-Net (SD 1.x/2.x/XL) â†’ Transformer (FLUX/Lumina) â†’ ??? (2027)
2. **Flowçµ±åˆ**: DDPM (SD 1.x-2.x) â†’ Rectified Flow (SD 3/FLUX) â†’ ã•ã‚‰ãªã‚‹æœ€é©åŒ–
3. **Text Encoder**: CLIPå˜ä½“ â†’ CLIP+T5 â†’ T5å˜ä½“ â†’ ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLMçµ±åˆï¼Ÿ
4. **è¨“ç·´ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ **: 2æ®µéšï¼ˆVAEâ†’Diffusionï¼‰â†’ End-to-Endï¼Ÿ
5. **1ã‚¹ãƒ†ãƒƒãƒ—ç”Ÿæˆ**: Consistency Models / Distillation â†’ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”Ÿæˆã®æœªæ¥

**æ­´å²çš„è¦–ç‚¹**:

- 2020: DDPMç™»å ´ï¼ˆãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ï¼‰
- 2021: Latent Diffusionï¼ˆæ½œåœ¨ç©ºé–“é©å‘½ï¼‰
- 2022: Stable Diffusion 1.xï¼ˆæ°‘ä¸»åŒ–ï¼‰
- 2023: SDXLï¼ˆå“è³ªå‘ä¸Šï¼‰
- 2024: SD 3ï¼ˆRectified Flowçµ±åˆï¼‰
- 2025: FLUX.1ï¼ˆTransformerå®Œå…¨ç§»è¡Œï¼‰
- 2026: ??? ï¼ˆæ¬¡ã®é©æ–°ã¯ï¼Ÿï¼‰

**å•ã„ç›´ã—**: SD 1.5ã®ã€Œç†è«–ã€ã¯é™³è…åŒ–ã—ãŸã‹ã€ãã‚Œã¨ã‚‚ã€Œå®Ÿè£…ã€ã ã‘ãŒé™³è…åŒ–ã—ãŸã‹ï¼Ÿ

:::details è­°è«–ã®ç¨®
**ç«‹å ´1: ç†è«–ã¯ä¸å¤‰**
- VAEåœ§ç¸®ã®åŸç†ã¯å¤‰ã‚ã‚‰ãªã„
- CFGã®æ•°å­¦ã¯æ™®éçš„
- DDPMã®ç†è«–ã¯Rectified Flowã®åŸºç›¤

**ç«‹å ´2: å®Ÿè£…ã¯é™³è…åŒ–**
- U-Netã¯éå»ã®éºç‰©
- CLIPå˜ä½“ã¯ä¸ååˆ†
- 50 stepsã¯é…ã™ãã‚‹

**çµ±åˆè¦–ç‚¹**: ç†è«–ã‚’ç†è§£ã—ãŸä¸Šã§æœ€æ–°å®Ÿè£…ã«é©å¿œã™ã‚‹ã“ã¨ãŒã€ŒçœŸã®ç†è§£ã€ã§ã¯ï¼Ÿ

**å•ã„**: FLUXã‚’å­¦ã¹ã°ååˆ†ã‹ã€ãã‚Œã¨ã‚‚SD 1.5ã‹ã‚‰ç©ã¿ä¸Šã’ã‚‹å¿…è¦ãŒã‚ã‚‹ã‹ï¼Ÿ
:::

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^ldm]: Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. *CVPR 2022*.
@[card](https://arxiv.org/abs/2112.10752)

[^cfg]: Ho, J., & Salimans, T. (2022). Classifier-Free Diffusion Guidance. *arXiv:2207.12598*.
@[card](https://arxiv.org/abs/2207.12598)

[^classifier_guidance]: Dhariwal, P., & Nichol, A. (2021). Diffusion Models Beat GANs on Image Synthesis. *NeurIPS 2021*.
@[card](https://arxiv.org/abs/2105.05233)

[^flux]: Greenberg, O. (2025). Demystifying Flux Architecture. *arXiv:2507.09595*.
@[card](https://arxiv.org/abs/2507.09595)

[^min_snr]: Hang, T., Gu, S., Li, C., et al. (2023). Efficient Diffusion Training via Min-SNR Weighting Strategy. *ICCV 2023*.
@[card](https://arxiv.org/abs/2303.09556)

[^zero_snr]: Lin, S., et al. (2023). Common Diffusion Noise Schedules and Sample Steps are Flawed. *arXiv:2305.08891*.
@[card](https://arxiv.org/abs/2305.08891)

[^clip]: Radford, A., et al. (2021). Learning Transferable Visual Models From Natural Language Supervision. *ICML 2021*.
@[card](https://arxiv.org/abs/2103.00020)

[^t5]: Raffel, C., et al. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. *JMLR*.
@[card](https://arxiv.org/abs/1910.10683)

[^imagen]: Saharia, C., et al. (2022). Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. *NeurIPS 2022*.
@[card](https://arxiv.org/abs/2205.11487)

[^sdxl]: Podell, D., et al. (2023). SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis. *arXiv:2307.01952*.
@[card](https://arxiv.org/abs/2307.01952)

[^sd3]: Esser, P., et al. (2024). Scaling Rectified Flow Transformers for High-Resolution Image Synthesis. *arXiv:2403.03206*.
@[card](https://arxiv.org/abs/2403.03206)

### æ•™ç§‘æ›¸ãƒ»ã‚µãƒ¼ãƒ™ã‚¤

- Murphy, K. P. (2022). *Probabilistic Machine Learning: Advanced Topics*. MIT Press.
- Prince, S. J. D. (2023). *Understanding Deep Learning*. MIT Press. [Chapter on Diffusion Models]
- Weng, L. (2021). "What are Diffusion Models?" *Lil'Log*. [https://lilianweng.github.io/posts/2021-07-11-diffusion-models/](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)

### ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹

- Hugging Face Diffusers Documentation: [https://huggingface.co/docs/diffusers](https://huggingface.co/docs/diffusers)
- MIT 6.S184: Diffusion Models (2026): [https://diffusion.csail.mit.edu/](https://diffusion.csail.mit.edu/)
- Stability AI Research: [https://stability.ai/research](https://stability.ai/research)

---

## è¨˜æ³•è¦ç´„

| è¨˜å· | æ„å‘³ | å‚™è€ƒ |
|:-----|:-----|:-----|
| $x$ | ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ã®ç”»åƒ | $x \in \mathbb{R}^{H \times W \times C}$ |
| $z$ | æ½œåœ¨ç©ºé–“ã®è¡¨ç¾ | $z \in \mathbb{R}^{h \times w \times c}$ |
| $\mathcal{E}$ | VAE Encoder | $z = \mathcal{E}(x)$ |
| $\mathcal{D}$ | VAE Decoder | $\tilde{x} = \mathcal{D}(z)$ |
| $\epsilon_\theta$ | Noise prediction network | U-Net |
| $t$ | Timestep | $t \in \{1, \ldots, T\}$ |
| $\bar{\alpha}_t$ | Cumulative product | $\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$ |
| $c$ | Condition (textç­‰) | CLIP/T5 encoding |
| $w$ | CFG guidance scale | å…¸å‹çš„ã« 7.5 |
| $\tilde{\epsilon}$ | CFGä¿®æ­£ãƒã‚¤ã‚º | $\tilde{\epsilon} = \epsilon_\emptyset + w(\epsilon_c - \epsilon_\emptyset)$ |
| $\mathbb{E}_{q}[\cdot]$ | æœŸå¾…å€¤ | åˆ†å¸ƒ $q$ ã®ä¸‹ã§ã®æœŸå¾…å€¤ |
| $\text{KL}[q \| p]$ | KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ | ç¬¬6å›å‚ç…§ |
| $\mathcal{N}(\mu, \sigma^2)$ | ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ | å¹³å‡ $\mu$, åˆ†æ•£ $\sigma^2$ |
| $\text{SNR}(t)$ | Signal-to-Noise Ratio | $\bar{\alpha}_t / (1-\bar{\alpha}_t)$ |

---

**è¬è¾**: æœ¬è¬›ç¾©ã®åŸ·ç­†ã«ã‚ãŸã‚Šã€Rombach et al. (2021) ã®Latent Diffusionè«–æ–‡ã€Ho & Salimans (2022) ã®CFGè«–æ–‡ã€Greenberg (2025) ã®FLUXè§£æã‚’å‚è€ƒã«ã—ã¾ã—ãŸã€‚
---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
