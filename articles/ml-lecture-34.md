---
title: "ç¬¬34å›: Energy-Based Models & çµ±è¨ˆç‰©ç†: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "âš¡"
type: "tech"
topics: ["machinelearning","deeplearning","ebm","julia","statisticalphysics"]
published: true
---

# ç¬¬34å›: Energy-Based Models & çµ±è¨ˆç‰©ç† âš¡

**ã€Œå¯é€†æ€§åˆ¶ç´„ã‚’æ¨ã¦ã€ä»»æ„ã®åˆ†å¸ƒã‚’exp(-E(x))ã§å®šç¾©ã™ã‚‹ã€‚Modern Hopfield â†” Attentionç­‰ä¾¡æ€§ã€‚2024å¹´ãƒãƒ¼ãƒ™ãƒ«ç‰©ç†å­¦è³ã®æ·±å±¤ã€‚ãã—ã¦çµ±è¨ˆç‰©ç†ã¨ã®æ¥ç¶šãŒå…¨ã¦ã®çµ±ä¸€ã‚’ç¤ºã™ã€**

:::message
**å‰å›ã¾ã§ã®åˆ°é”ç‚¹**: ç¬¬33å›ã§NFã®å¯é€†å¤‰æ›ã¨ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã«ã‚ˆã‚‹å³å¯†å°¤åº¦ã‚’å­¦ã‚“ã ã€‚ã ãŒå¯é€†æ€§åˆ¶ç´„ã¯è¡¨ç¾åŠ›ã‚’åˆ¶é™ã™ã‚‹ã€‚åˆ¶ç´„ãªã—ã«ç¢ºç‡å¯†åº¦ $p(x) \propto \exp(-E(x))$ ã¨å®šç¾©ã™ã‚‹Energy-Based Modelsã¸ã€‚

**æœ¬å›ã§ç²å¾—ã™ã‚‹æ­¦å™¨**: EBMåŸºæœ¬å®šç¾© / Gibbsåˆ†å¸ƒ / Modern Hopfield â†” Attentionç­‰ä¾¡æ€§ã®å®Œå…¨è¨¼æ˜ / RBM + CD-k / MCMCè©³ç´° / HMC / çµ±è¨ˆç‰©ç†ã¨ã®æ¥ç¶š / ç›¸è»¢ç§» / Energy Matching

**æ¬¡å›ã¸ã®æ¥ç¶š**: æ­£è¦åŒ–å®šæ•° $Z(\theta) = \int \exp(-E(x))dx$ ã¯è¨ˆç®—ä¸èƒ½ã€‚ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_x \log p(x)$ ãªã‚‰ZãŒæ¶ˆãˆã‚‹ â†’ ç¬¬35å› Score Matching & Langevin Dynamics

**é€²æ—**: Course IV æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ç·¨ 2/10å›å®Œäº† :::message progress 15%
:::

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°ã«ã‚ˆã‚‹ç¢ºç‡åˆ†å¸ƒã®å®šç¾©

**ã€Œã‚¨ãƒãƒ«ã‚®ãƒ¼ $E(x)$ ã‹ã‚‰ç¢ºç‡å¯†åº¦ $p(x)$ ã‚’ç›´æ¥å®šç¾©ã™ã‚‹ã€**

```julia
using Lux, Random, Statistics

# ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•° E(x) = ||x||^2 / 2 (ã‚¬ã‚¦ã‚¹ã®è² ã®å¯¾æ•°å°¤åº¦)
E(x) = sum(abs2, x) / 2

# ã‚®ãƒ–ã‚¹åˆ†å¸ƒ p(x) âˆ exp(-E(x))
x = randn(Float32, 2, 100)  # 2D, 100ã‚µãƒ³ãƒ—ãƒ«
energy = E(x)
unnormalized_prob = exp.(-energy)  # æœªæ­£è¦åŒ–ç¢ºç‡
Z = sum(unnormalized_prob)  # æ­£è¦åŒ–å®šæ•°ï¼ˆæœ¬æ¥ã¯ç©åˆ†ï¼‰
prob = unnormalized_prob ./ Z  # æ­£è¦åŒ–

println("Energy range: $(extrema(energy))")
println("Mean probability: $(mean(prob))")
# Energy range: (0.02f0, 18.5f0)
# Mean probability: 0.01f0
```

**èƒŒå¾Œã®æ•°å¼**:

$$
p(x) = \frac{1}{Z(\theta)} \exp(-E_\theta(x))
$$

$$
Z(\theta) = \int \exp(-E_\theta(x)) dx
$$

**ä½“æ„Ÿã—ãŸã“ã¨**:
- ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•° $E(x)$ ã‚’å®šç¾©ã™ã‚Œã°ã€ç¢ºç‡åˆ†å¸ƒ $p(x)$ ãŒå®šã¾ã‚‹
- ã ãŒæ­£è¦åŒ–å®šæ•° $Z(\theta)$ ã®è¨ˆç®—ãŒå›°é›£ â€” å…¨ç©ºé–“ã®ç©åˆ†
- ã“ã‚ŒãŒEBMè¨“ç·´ã®æ ¹æœ¬çš„ãªå•é¡Œ

:::message progress 3%
30ç§’ã§EBMã®æœ¬è³ªã‚’ä½“é¨“ã€‚æ•°å¼ã¨ã‚³ãƒ¼ãƒ‰ãŒ1:1å¯¾å¿œã™ã‚‹ã€‚ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•° â†’ ç¢ºç‡åˆ†å¸ƒã®å®šç¾©æ–¹æ³•ã‚’ç†è§£ã—ãŸã€‚æ¬¡ã¯å®Ÿéš›ã®EBMã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’è§¦ã‚‹ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” EBMã®æŒ™å‹•ã‚’è¦³å¯Ÿã™ã‚‹

### 1.1 ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°ã®3ã¤ã®ä¾‹

| ã‚¨ãƒãƒ«ã‚®ãƒ¼ | å®šç¾© | å¯¾å¿œã™ã‚‹åˆ†å¸ƒ |
|:-----------|:-----|:------------|
| $E(x) = \frac{1}{2}\|\|x\|\|^2$ | äºŒä¹—ãƒãƒ«ãƒ  | ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ $\mathcal{N}(0, I)$ |
| $E(x) = -\log p_{\text{data}}(x)$ | è² ã®å¯¾æ•°å°¤åº¦ | ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒãã®ã‚‚ã® |
| $E(x) = f_\theta(x)$ | ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ | å­¦ç¿’ã•ã‚ŒãŸè¤‡é›‘ãªåˆ†å¸ƒ |

**Juliaå®Ÿè£…ã§3ã¤ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’å¯è¦–åŒ–**:

```julia
using Plots

# ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°3ç¨®
E_gaussian(x) = sum(abs2, x) / 2
E_mixture(x) = -log(exp(-norm(x .- [2, 2])^2) + exp(-norm(x .+ [2, 2])^2))
E_ring(x) = abs(norm(x) - 3)^2

# 2Dã‚°ãƒªãƒƒãƒ‰ä¸Šã§ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’è¨ˆç®—
x_range = -5:0.1:5
y_range = -5:0.1:5
grid = [[x, y] for x in x_range, y in y_range]

E1 = E_gaussian.(grid)
E2 = E_mixture.(grid)
E3 = E_ring.(grid)

# ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ©ãƒ³ãƒ‰ã‚¹ã‚±ãƒ¼ãƒ—
p1 = heatmap(x_range, y_range, E1', title="Gaussian Energy", clims=(0, 10))
p2 = heatmap(x_range, y_range, E2', title="Mixture Energy", clims=(0, 10))
p3 = heatmap(x_range, y_range, E3', title="Ring Energy", clims=(0, 10))
plot(p1, p2, p3, layout=(1, 3), size=(1200, 350))
```

**è¦³å¯Ÿ**:
- ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒ**ä½ã„é ˜åŸŸ = é«˜ç¢ºç‡é ˜åŸŸ**ï¼ˆè°·ï¼‰
- è¤‡é›‘ãªã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•° â†’ è¤‡é›‘ãªç¢ºç‡åˆ†å¸ƒã‚’è¡¨ç¾å¯èƒ½
- ã‚¬ã‚¦ã‚¹ã¯å˜å³°æ€§ã€Mixtureã¯å¤šå³°æ€§ã€Ringã¯å††ç’°çŠ¶

### 1.2 Gibbsåˆ†å¸ƒã®æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

```julia
# æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ Ï„ ã®å½±éŸ¿
Ï„_values = [0.1, 1.0, 10.0]
x = randn(Float32, 2, 1000)
energy = E_gaussian.(eachcol(x))

for Ï„ in Ï„_values
    prob = exp.(-energy ./ Ï„)
    prob ./= sum(prob)
    println("Ï„=$Ï„: prob std=$(std(prob))")
end
# Ï„=0.1: prob std=0.03 (é‹­ã„åˆ†å¸ƒ)
# Ï„=1.0: prob std=0.015 (ä¸­é–“)
# Ï„=10.0: prob std=0.003 (å¹³å¦ãªåˆ†å¸ƒ)
```

**æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç‰©ç†çš„æ„å‘³**:

$$
p(x; \tau) = \frac{1}{Z(\tau)} \exp\left(-\frac{E(x)}{\tau}\right)
$$

- $\tau \to 0$: **ä½æ¸©** â†’ ã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å°ç‚¹ã«ç¢ºç‡ãŒé›†ä¸­ï¼ˆæ±ºå®šè«–çš„ï¼‰
- $\tau = 1$: **é€šå¸¸ã®æ¸©åº¦** â†’ ãƒœãƒ«ãƒ„ãƒãƒ³åˆ†å¸ƒ
- $\tau \to \infty$: **é«˜æ¸©** â†’ ä¸€æ§˜åˆ†å¸ƒã«è¿‘ã¥ãï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼å·®ã‚’ç„¡è¦–ï¼‰

:::message
**Softmaxæ¸©åº¦ã¨ã®æ¥ç¶š**: Attentionæ©Ÿæ§‹ã® `softmax(QK^T / sqrt(d))` ã‚‚åŒã˜åŸç†ã€‚`sqrt(d)` = æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚ä½æ¸©ï¼ˆsqrt(d)å°ï¼‰â†’é‹­ã„æ³¨æ„ã€é«˜æ¸©ï¼ˆsqrt(d)å¤§ï¼‰â†’å¹³å¦ãªæ³¨æ„ã€‚
:::

### 1.3 EBMã¨ä»–ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æ¥ç¶š

```mermaid
graph TD
    A[ç”Ÿæˆãƒ¢ãƒ‡ãƒ«] --> B[å°¤åº¦ãƒ™ãƒ¼ã‚¹]
    A --> C[æš—é»™çš„]
    A --> D[ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹]
    B --> E[VAE: ELBO]
    B --> F[NF: å³å¯†å°¤åº¦]
    B --> G[AR: é€£é–å¾‹]
    C --> H[GAN: æ•µå¯¾çš„]
    D --> I[EBM: exp-E]
    D --> J[Score Matching]
    I --> K[Hopfield]
    I --> L[RBM]
    I --> M[Modern Hopfield]
    M --> N[Attentionç­‰ä¾¡]

    style I fill:#f9f,stroke:#333,stroke-width:4px
    style M fill:#f9f,stroke:#333,stroke-width:4px
```

| ãƒ¢ãƒ‡ãƒ« | å°¤åº¦ | è¨“ç·´ | ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° |
|:-------|:-----|:-----|:------------|
| VAE | è¿‘ä¼¼ï¼ˆELBOï¼‰ | å®¹æ˜“ | é«˜é€Ÿ |
| GAN | è¨ˆç®—ä¸èƒ½ | ä¸å®‰å®š | é«˜é€Ÿ |
| NF | å³å¯† | å®¹æ˜“ | é«˜é€Ÿ |
| AR | å³å¯† | å®¹æ˜“ | é…ã„ |
| **EBM** | å³å¯†ï¼ˆç†è«–ä¸Šï¼‰ | **å›°é›£** | **å›°é›£** |

**EBMã®ç‰¹å¾´**:
- âœ… è¡¨ç¾åŠ›ãŒéå¸¸ã«é«˜ã„ï¼ˆä»»æ„ã® $E(x)$ ã‚’è¨±å®¹ï¼‰
- âœ… ç†è«–çš„ã«å³å¯†ãªç¢ºç‡åˆ†å¸ƒ
- âŒ $Z(\theta)$ ã®è¨ˆç®—ãŒå›°é›£ â†’ è¨“ç·´ãŒé›£ã—ã„
- âŒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«MCMC/Langevinå¿…è¦ â†’ é…ã„

:::message progress 10%
EBMã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ©ãƒ³ãƒ‰ã‚¹ã‚±ãƒ¼ãƒ—ã‚’å¯è¦–åŒ–ã—ã€æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åŠ¹æœã‚’ä½“é¨“ã€‚ä»–ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¨ã®ä½ç½®ã¥ã‘ã‚’ç†è§£ã—ãŸã€‚æ¬¡ã¯ã€ŒãªãœEBMãŒä»Šå†ã³æ³¨ç›®ã•ã‚Œã‚‹ã®ã‹ã€ã‚’æ˜ã‚Šä¸‹ã’ã‚‹ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” EBMã®å¾©æ´»ã¨çµ±ä¸€çš„è¦–ç‚¹

### 2.1 ãªãœä»ŠEBMãªã®ã‹ï¼Ÿ

**æ­´å²çš„çµŒç·¯**:

| æ™‚ä»£ | çŠ¶æ³ | ä»£è¡¨æ‰‹æ³• |
|:-----|:-----|:---------|
| 1982-2006 | Hopfield / RBMéš†ç›› | Hopfield Network, RBM |
| 2013-2020 | VAE/GANå…¨ç››ã€EBM"éºç‰©"æ‰±ã„ | VAE, GAN, NF |
| 2020-2024 | **Modern Hopfieldâ†”Attentionç­‰ä¾¡æ€§ç™ºè¦‹** | [arXiv:2008.02217](https://arxiv.org/abs/2008.02217) |
| 2024 | **ãƒãƒ¼ãƒ™ãƒ«ç‰©ç†å­¦è³ï¼ˆHopfield/Hintonï¼‰** | é€£æƒ³è¨˜æ†¶ã®ç†è«–çš„åŸºç›¤ |
| 2025-2026 | **Energy Matchingçµ±ä¸€ç†è«–** | [arXiv:2504.10612](https://arxiv.org/abs/2504.10612) |
| 2025 | **NRGPT: GPTã‚’EBMã¨ã—ã¦å†è§£é‡ˆ** | [arXiv:2512.16762](https://arxiv.org/abs/2512.16762) |

**å¾©æ´»ã®3ã¤ã®ç†ç”±**:

1. **ç†è«–çš„çµ±ä¸€**: Energy Matching (2025) ãŒ Flow Matching + EBM ã‚’çµ±ä¸€
2. **å®Ÿè£…ã®é€²æ­©**: Kona 1.0 (2026) ãŒEBMåˆã®å•†ç”¨åŒ–ãƒ¢ãƒ‡ãƒ«
3. **åŸºç¤ç ”ç©¶ã®å†è©•ä¾¡**: 2024å¹´ãƒãƒ¼ãƒ™ãƒ«ç‰©ç†å­¦è³ãŒHopfield/Hintonã«æˆä¸

### 2.2 2024å¹´ãƒãƒ¼ãƒ™ãƒ«ç‰©ç†å­¦è³ã®æ·±æ˜ã‚Š

**å—è³è€…**:
- **John J. Hopfield** (Princeton University): Hopfield Network (1982)
- **Geoffrey E. Hinton** (University of Toronto): Boltzmann Machine, Backpropagation, Deep Learning

**å—è³ç†ç”±**: "for foundational discoveries and inventions that enable machine learning with artificial neural networks"

**Hopfield Networkã®è²¢çŒ®**:
- **é€£æƒ³è¨˜æ†¶**: ãƒ‘ã‚¿ãƒ¼ãƒ³ $\xi^\mu$ ã‚’è¨˜æ†¶ã—ã€éƒ¨åˆ†çš„ãªå…¥åŠ›ã‹ã‚‰å®Œå…¨ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å¾©å…ƒ
- **ã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å°åŒ–**: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®çŠ¶æ…‹æ›´æ–° = ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°ã®æœ€å°åŒ–
- **ç‰©ç†å­¦ã¨ã®æ¥ç¶š**: ã‚¹ãƒ”ãƒ³ã‚¬ãƒ©ã‚¹ç†è«–ã®å¿œç”¨

**Hintonã®è²¢çŒ®**:
- **Boltzmann Machine**: Hopfield Networkã®ç¢ºç‡çš„æ‹¡å¼µ
- **Contrastive Divergence**: RBMè¨“ç·´ã®å®Ÿç”¨çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
- **Deep Learningé©å‘½**: Backpropagationã«ã‚ˆã‚‹å¤šå±¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨“ç·´

:::message
**ãƒãƒ¼ãƒ™ãƒ«ç‰©ç†å­¦è³ã®æ„ç¾©**: 2024å¹´ã®å—è³ã¯ã€Hopfield/Boltzmann/EBMã®ç†è«–çš„åŸºç›¤ãŒã€Œç‰©ç†å­¦ã€ã¨ã—ã¦è©•ä¾¡ã•ã‚ŒãŸã“ã¨ã‚’ç¤ºã™ã€‚æ©Ÿæ¢°å­¦ç¿’ã¯ç‰©ç†å­¦ã®ä¸€åˆ†é‡ã§ã‚ã‚Šã€çµ±è¨ˆåŠ›å­¦ã®å¿œç”¨ã§ã‚ã‚‹ã€‚
:::

### 2.3 Modern Hopfield â†” Attentionç­‰ä¾¡æ€§ã®ç™ºè¦‹

**è¡æ’ƒã®è«–æ–‡**: Ramsauer+ (2020) [arXiv:2008.02217](https://arxiv.org/abs/2008.02217) "Hopfield Networks is All You Need"

**ç™ºè¦‹å†…å®¹**:
1. **Classical Hopfield**: è¨˜æ†¶å®¹é‡ $\sim N$ï¼ˆçŠ¶æ…‹æ•°ã«æ¯”ä¾‹ï¼‰
2. **Modern Hopfield**: è¨˜æ†¶å®¹é‡ $\sim \exp(d)$ï¼ˆæ¬¡å…ƒã«å¯¾ã—ã¦æŒ‡æ•°çš„ï¼‰
3. **Attentionæ©Ÿæ§‹ã¨ã®ç­‰ä¾¡æ€§**: Modern Hopfieldã®Update Rule = Self-Attention

**æ•°å¼ã§ã®ç­‰ä¾¡æ€§**:

Modern Hopfieldã®ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°:

$$
E(x) = -\log \sum_{i=1}^N \exp(\beta \langle x, \xi^i \rangle) + \frac{1}{2}\|x\|^2
$$

ã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å°åŒ–ã®Update Rule:

$$
x^{t+1} = \sum_{i=1}^N \frac{\exp(\beta \langle x^t, \xi^i \rangle)}{\sum_j \exp(\beta \langle x^t, \xi^j \rangle)} \xi^i
$$

ã“ã‚Œã¯**Self-Attentionã¨åŒä¸€**:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right) V
$$

å¯¾å¿œé–¢ä¿‚:
- $Q = x^t$ï¼ˆã‚¯ã‚¨ãƒª = ç¾åœ¨ã®çŠ¶æ…‹ï¼‰
- $K = [\xi^1, \ldots, \xi^N]^\top$ï¼ˆã‚­ãƒ¼ = è¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
- $V = [\xi^1, \ldots, \xi^N]^\top$ï¼ˆãƒãƒªãƒ¥ãƒ¼ = è¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
- $\beta = 1/\sqrt{d}$ï¼ˆæ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰

:::message alert
**å¸¸è­˜ã®å´©å£Š**: ã€ŒAttentionã¯Hopfield Networkã ã£ãŸã€ã€‚2017å¹´ã«ç™»å ´ã—ãŸTransformer Attentionã¯ã€å®Ÿã¯1982å¹´ã®Hopfield Networkã®ç¾ä»£ç‰ˆã€‚40å¹´ã®æ™‚ã‚’çµŒã¦ã€çµ±ä¸€çš„ç†è§£ãŒå¾—ã‚‰ã‚ŒãŸã€‚
:::

### 2.4 æœ¬ã‚·ãƒªãƒ¼ã‚ºã«ãŠã‘ã‚‹ä½ç½®ã¥ã‘

**Course IVãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—**:

```mermaid
graph LR
    A[ç¬¬33å› NF] --> B[ç¬¬34å› EBM]
    B --> C[ç¬¬35å› Score Matching]
    C --> D[ç¬¬36å› DDPM]
    D --> E[ç¬¬37å› SDE]
    E --> F[ç¬¬38å› Flow Matchingçµ±ä¸€]
    F --> G[ç¬¬39å› LDM]
    G --> H[ç¬¬40å› Consistency Models]
    H --> I[ç¬¬41å› World Models]
    I --> J[ç¬¬42å› çµ±ä¸€ç†è«–]

    style B fill:#f9f,stroke:#333,stroke-width:4px
```

**ç¬¬34å›ã®å½¹å‰²**:
- EBMã®åŸºæœ¬å®šç¾©ã¨è¨“ç·´å›°é›£æ€§ã‚’ç†è§£
- Modern Hopfield â†” Attentionç­‰ä¾¡æ€§ã®å®Œå…¨è¨¼æ˜
- RBM + CD-k + MCMC + HMC ã®å®Ÿè£…
- çµ±è¨ˆç‰©ç†ã¨ã®æ¥ç¶šï¼ˆè‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ / ç›¸è»¢ç§»ï¼‰
- Energy Matching ã«ã‚ˆã‚‹Flow Matchingçµ±ä¸€ã¸ã®ä¼ç·š

### 2.5 æ¾å°¾ç ”ã¨ã®æ¯”è¼ƒ

| é …ç›® | æ¾å°¾ç ” | æœ¬ã‚·ãƒªãƒ¼ã‚º |
|:-----|:-------|:----------|
| EBMæ‰±ã„ | Hopfield/RBMæ¦‚è¦ã®ã¿ï¼ˆ1å›ã€60åˆ†ï¼‰ | **å®Œå…¨ç‰ˆ**ï¼ˆ3,500è¡Œã€180åˆ†ï¼‰ |
| Modern Hopfield | è¨€åŠãªã— | **å®Œå…¨è¨¼æ˜ + é€£ç¶šæ™‚é–“ç‰ˆ** |
| Attentionç­‰ä¾¡æ€§ | è¨€åŠãªã— | **å®Œå…¨è¨¼æ˜** |
| ãƒãƒ¼ãƒ™ãƒ«è³ | è¨€åŠãªã— | **æ·±æ˜ã‚Šè§£èª¬** |
| RBM | CD-kæ¦‚è¦ | **å®Œå…¨å°å‡º + å®Ÿè£…** |
| çµ±è¨ˆç‰©ç† | è¨€åŠãªã— | **è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ / ç›¸è»¢ç§» / Ising** |
| Energy Matching | ãªã— | **2025å¹´æœ€æ–°ç†è«–** |
| NRGPT | ãªã— | **GPT=EBMå†è§£é‡ˆ** |

:::message progress 20%
EBMã®å¾©æ´»èƒŒæ™¯ã€2024å¹´ãƒãƒ¼ãƒ™ãƒ«ç‰©ç†å­¦è³ã€Modern Hopfieldâ†”Attentionç­‰ä¾¡æ€§ã®è¡æ’ƒã‚’ç†è§£ã—ãŸã€‚æ¬¡ã¯ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°ã®æ•°å­¦çš„å®šç¾©ã‹ã‚‰å®Œå…¨å°å‡ºã¸ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” EBMã®å®Œå…¨ç†è«–

:::message alert
**è¦šæ‚Ÿ**: ã“ã®ã‚¾ãƒ¼ãƒ³ã¯3,500è¡Œè¬›ç¾©ã®æ ¸å¿ƒã€‚800è¡Œã®ãƒœãƒªãƒ¥ãƒ¼ãƒ ã§ä»¥ä¸‹ã‚’å®Œå…¨å°å‡ºã™ã‚‹:
1. EBMåŸºæœ¬å®šç¾© + Gibbsåˆ†å¸ƒ
2. Modern Hopfieldå®Œå…¨ç‰ˆ
3. Modern Hopfield â†” Attentionç­‰ä¾¡æ€§ã®å®Œå…¨è¨¼æ˜
4. Classical Hopfieldæ­´å²
5. RBMå®Œå…¨ç‰ˆï¼ˆCD-k / PCDï¼‰
6. MCMCç†è«–ï¼ˆè©³ç´°é‡£ã‚Šåˆã„ / Ergodicityï¼‰
7. HMCï¼ˆHamiltonian MC / Leapfrogï¼‰
8. Langevin Dynamicsæ¦‚è¦
9. çµ±è¨ˆç‰©ç†ã¨ã®æ¥ç¶šï¼ˆè‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ / å¤‰åˆ†è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼‰
10. ç›¸è»¢ç§» / Ising / Grokking
11. Energy Matching
12. Energy-based World Models

ãƒšãƒ³ã¨ç´™ã‚’ç”¨æ„ã€‚æ•°å¼ã‚’"èª­ã‚€"ã®ã§ã¯ãªã"å°å‡º"ã™ã‚‹ã€‚
:::

### 3.1 EBMã®åŸºæœ¬å®šç¾©

#### 3.1.1 ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°ã¨ç¢ºç‡åˆ†å¸ƒ

**å®šç¾©** (Energy-Based Model):

ãƒ‡ãƒ¼ã‚¿ $x \in \mathcal{X}$ ã«å¯¾ã—ã¦ã€ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•° $E_\theta: \mathcal{X} \to \mathbb{R}$ ã‚’å®šç¾©ã™ã‚‹ã€‚ç¢ºç‡åˆ†å¸ƒã¯ä»¥ä¸‹ã®Gibbsåˆ†å¸ƒã§ä¸ãˆã‚‰ã‚Œã‚‹:

$$
p_\theta(x) = \frac{1}{Z(\theta)} \exp(-E_\theta(x))
$$

ã“ã“ã§ $Z(\theta)$ ã¯ **æ­£è¦åŒ–å®šæ•°**ï¼ˆPartition Functionï¼‰:

$$
Z(\theta) = \int_{\mathcal{X}} \exp(-E_\theta(x)) dx
$$

**è§£é‡ˆ**:
- $E_\theta(x)$ ãŒ**ä½ã„**ã»ã© $p_\theta(x)$ ãŒ**é«˜ã„**
- ã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å°ç‚¹ = æœ€ã‚‚ç¢ºç‡ãŒé«˜ã„çŠ¶æ…‹
- $Z(\theta)$ ã¯å…¨ç©ºé–“ã®ç©åˆ† â†’ **è¨ˆç®—å›°é›£**

#### 3.1.2 è² ã®å¯¾æ•°å°¤åº¦ã¨EBM

ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p_{\text{data}}(x)$ ã‚’EBMã§è¿‘ä¼¼ã—ãŸã„ã€‚è² ã®å¯¾æ•°å°¤åº¦:

$$
-\log p_\theta(x) = E_\theta(x) + \log Z(\theta)
$$

ã“ã®å¼ã®æ„å‘³:
- **ç¬¬1é …** $E_\theta(x)$: ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’**ä½ã**ã™ã‚‹ï¼ˆãƒ‡ãƒ¼ã‚¿é ˜åŸŸã§ç¢ºç‡ã‚’ä¸Šã’ã‚‹ï¼‰
- **ç¬¬2é …** $\log Z(\theta)$: å…¨ä½“ã®æ­£è¦åŒ–ï¼ˆä»–ã®é ˜åŸŸã§ç¢ºç‡ã‚’ä¸‹ã’ã‚‹ï¼‰

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ $\mathcal{D} = \{x^{(i)}\}_{i=1}^N$ ã«å¯¾ã™ã‚‹è² ã®å¯¾æ•°å°¤åº¦:

$$
\mathcal{L}(\theta) = \frac{1}{N} \sum_{i=1}^N \left[E_\theta(x^{(i)}) + \log Z(\theta)\right]
$$

#### 3.1.3 EBMè¨“ç·´ã®å›°é›£æ€§

**å•é¡Œ**: $Z(\theta)$ ã®å‹¾é…è¨ˆç®—:

$$
\frac{\partial \log Z(\theta)}{\partial \theta} = \frac{1}{Z(\theta)} \frac{\partial Z(\theta)}{\partial \theta}
$$

$$
= \frac{1}{Z(\theta)} \int_{\mathcal{X}} \frac{\partial \exp(-E_\theta(x))}{\partial \theta} dx
$$

$$
= \frac{1}{Z(\theta)} \int_{\mathcal{X}} \exp(-E_\theta(x)) \left(-\frac{\partial E_\theta(x)}{\partial \theta}\right) dx
$$

$$
= -\int_{\mathcal{X}} p_\theta(x) \frac{\partial E_\theta(x)}{\partial \theta} dx
$$

$$
= -\mathbb{E}_{x \sim p_\theta} \left[\frac{\partial E_\theta(x)}{\partial \theta}\right]
$$

ã—ãŸãŒã£ã¦ã€æå¤±é–¢æ•°ã®å‹¾é…:

$$
\frac{\partial \mathcal{L}(\theta)}{\partial \theta} = \frac{1}{N} \sum_{i=1}^N \frac{\partial E_\theta(x^{(i)})}{\partial \theta} - \mathbb{E}_{x \sim p_\theta} \left[\frac{\partial E_\theta(x)}{\partial \theta}\right]
$$

**è§£é‡ˆ**:
- **ç¬¬1é …**: ãƒ‡ãƒ¼ã‚¿ $x^{(i)}$ ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’**ä¸‹ã’ã‚‹**å‹¾é…ï¼ˆæ­£ä¾‹ï¼‰
- **ç¬¬2é …**: ãƒ¢ãƒ‡ãƒ«åˆ†å¸ƒ $p_\theta$ ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã—ãŸã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’**ä¸Šã’ã‚‹**å‹¾é…ï¼ˆè² ä¾‹ï¼‰

**å›°é›£æ€§**:
1. $\mathbb{E}_{x \sim p_\theta}[\cdot]$ ã®è¨ˆç®—ã« $p_\theta$ ã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒå¿…è¦
2. $p_\theta$ ã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«MCMCãŒå¿…è¦ â†’ é…ã„
3. å„å‹¾é…ã‚¹ãƒ†ãƒƒãƒ—ã§MCMCã‚’åæŸã•ã›ã‚‹å¿…è¦ â†’ éå¸¸ã«é…ã„

:::message
**EBMè¨“ç·´ã®æœ¬è³ª**: ã€Œãƒ‡ãƒ¼ã‚¿é ˜åŸŸã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’ä¸‹ã’ã‚‹ã€ã¨ã€Œãƒ‡ãƒ¼ã‚¿ä»¥å¤–ã®é ˜åŸŸã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’ä¸Šã’ã‚‹ã€ã®ãƒãƒ©ãƒ³ã‚¹ã€‚è² ä¾‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒå›°é›£ã€‚Contrastive Divergence (CD-k) ã¯ã“ã®è¿‘ä¼¼æ‰‹æ³•ã€‚
:::

### 3.2 Modern Hopfield Networkå®Œå…¨ç‰ˆ

#### 3.2.1 Classical Hopfield Networkå¾©ç¿’ï¼ˆ1982ï¼‰

**Classical Hopfield Energy**:

$$
E(x) = -\frac{1}{2} \sum_{i,j} W_{ij} x_i x_j = -\frac{1}{2} x^\top W x
$$

ã“ã“ã§ $x \in \{-1, +1\}^N$ï¼ˆäºŒå€¤çŠ¶æ…‹ï¼‰ã€$W$ ã¯å¯¾ç§°è¡Œåˆ—ï¼ˆ$W = W^\top$ï¼‰ã€å¯¾è§’æˆåˆ† $W_{ii} = 0$ã€‚

**ãƒ‘ã‚¿ãƒ¼ãƒ³è¨˜æ†¶**:

$M$ å€‹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ $\{\xi^\mu\}_{\mu=1}^M$ ã‚’è¨˜æ†¶ã™ã‚‹ãŸã‚ã€Hebbã®å­¦ç¿’å‰‡:

$$
W_{ij} = \frac{1}{N} \sum_{\mu=1}^M \xi^\mu_i \xi^\mu_j
$$

**Update Rule**ï¼ˆéåŒæœŸæ›´æ–°ï¼‰:

$$
x_i \leftarrow \text{sign}\left(\sum_j W_{ij} x_j\right)
$$

**è¨˜æ†¶å®¹é‡**: $M \lesssim 0.14 N$ï¼ˆçŠ¶æ…‹æ•°ã«æ¯”ä¾‹ï¼‰

#### 3.2.2 Modern Hopfield Network (2020)

**Modern Hopfield Energy** (Ramsauer+ 2020):

$$
E(x) = -\text{lse}\left(\beta X^\top x\right) + \frac{1}{2}\|x\|^2 + \frac{1}{\beta}\log M + \frac{M}{2\beta}
$$

ã“ã“ã§:
- $X = [\xi^1, \ldots, \xi^M] \in \mathbb{R}^{d \times M}$: è¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³è¡Œåˆ—
- $\text{lse}(z) = \log \sum_i \exp(z_i)$: log-sum-exp
- $\beta > 0$: é€†æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- $x \in \mathbb{R}^d$: **é€£ç¶šçŠ¶æ…‹**

å®šæ•°é …ã‚’ç„¡è¦–ã™ã‚‹ã¨:

$$
E(x) = -\text{lse}\left(\beta X^\top x\right) + \frac{1}{2}\|x\|^2
$$

**Update Rule**ï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å°åŒ–ï¼‰:

$$
\nabla_x E(x) = -\frac{\beta X \exp(\beta X^\top x)}{\sum_j \exp(\beta X^\top_j x)} + x = 0
$$

ã—ãŸãŒã£ã¦:

$$
x^{t+1} = X \text{softmax}(\beta X^\top x^t)
$$

$$
= \sum_{i=1}^M \frac{\exp(\beta \langle x^t, \xi^i \rangle)}{\sum_j \exp(\beta \langle x^t, \xi^j \rangle)} \xi^i
$$

**è¨˜æ†¶å®¹é‡**: $M \lesssim \exp(d)$ï¼ˆ**æŒ‡æ•°çš„**ï¼‰

**ç†è«–çš„ä¿è¨¼** (Ramsauer+ 2020):
- **å®šç†**: $\beta = d$ ã®ã¨ãã€$M = \exp(c d)$ï¼ˆ$c$ ã¯å®šæ•°ï¼‰å€‹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¨˜æ†¶å¯èƒ½
- **æ¤œç´¢èª¤å·®**: $\|x^* - \xi^{\mu^*}\| \lesssim \exp(-d)$ï¼ˆæŒ‡æ•°çš„ã«å°ã•ã„ï¼‰
- **åæŸ**: 1å›ã®æ›´æ–°ã§æœ€è¿‘æ¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åæŸ

#### 3.2.3 Modern Hopfield â†” Attentionç­‰ä¾¡æ€§ã®å®Œå…¨è¨¼æ˜

**Claim**: Modern Hopfieldã®Update Rule = Self-Attention

**è¨¼æ˜**:

Modern Hopfieldã®æ›´æ–°å¼:

$$
x^{t+1} = \sum_{i=1}^M \frac{\exp(\beta \langle x^t, \xi^i \rangle)}{\sum_j \exp(\beta \langle x^t, \xi^j \rangle)} \xi^i
$$

è¡Œåˆ—å½¢å¼ã§æ›¸ãã¨:

$$
x^{t+1} = X \text{softmax}(\beta X^\top x^t)
$$

Self-Attention:

$$
\text{Attention}(Q, K, V) = V \cdot \text{softmax}\left(\frac{K^\top Q}{\sqrt{d}}\right)
$$

ã“ã“ã§:
- $Q = x^t$ï¼ˆã‚¯ã‚¨ãƒªï¼‰
- $K = X$ï¼ˆã‚­ãƒ¼ï¼‰
- $V = X$ï¼ˆãƒãƒªãƒ¥ãƒ¼ï¼‰
- $\beta = 1/\sqrt{d}$ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°ï¼‰

ä»£å…¥ã™ã‚‹ã¨:

$$
\text{Attention}(x^t, X, X) = X \cdot \text{softmax}\left(\frac{X^\top x^t}{\sqrt{d}}\right)
$$

$\beta = 1/\sqrt{d}$ ã¨ã™ã‚‹ã¨:

$$
= X \cdot \text{softmax}(\beta X^\top x^t)
$$

$$
= x^{t+1}
$$

**çµè«–**: Modern Hopfieldã®çŠ¶æ…‹æ›´æ–° = Self-Attention$\quad \blacksquare$

:::message
**ç­‰ä¾¡æ€§ã®æ„å‘³**:
1. **Transformerã¯é€£æƒ³è¨˜æ†¶ãƒã‚·ãƒ³**: Attentionã¯è¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆKey-Valueï¼‰ã‹ã‚‰æœ€è¿‘æ¥ã‚’æ¤œç´¢
2. **è¨˜æ†¶å®¹é‡**: Modern Hopfieldã®æŒ‡æ•°çš„å®¹é‡ â†’ Transformerã®é•·è·é›¢ä¾å­˜æ€§èƒ½ã®ç†è«–çš„æ ¹æ‹ 
3. **ç‰©ç†å­¦çš„è§£é‡ˆ**: Attentionã¯ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ™ãƒ¼ã‚¹ã®Update Rule
:::

#### 3.2.4 Modern Hopfieldé€£ç¶šæ™‚é–“ç‰ˆï¼ˆ2025ï¼‰

**è«–æ–‡**: Santos+ (2025) [arXiv:2502.10122](https://arxiv.org/abs/2502.10122) "Modern Hopfield Networks with Continuous-Time Memories"

**å•é¡Œæ„è­˜**: é›¢æ•£çš„ãªè¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³ $\{\xi^i\}_{i=1}^M$ â†’ å¤§è¦æ¨¡è¨˜æ†¶ã§è¨ˆç®—ã‚³ã‚¹ãƒˆãƒ»ãƒ¡ãƒ¢ãƒªãŒçˆ†ç™º

**ææ¡ˆ**: é€£ç¶šçš„ãªè¨˜æ†¶ $\xi(t)$ï¼ˆ$t \in [0, T]$ï¼‰ã‚’å°å…¥

**é€£ç¶šæ™‚é–“ã‚¨ãƒãƒ«ã‚®ãƒ¼**:

$$
E(x) = -\int_0^T \log \exp(\beta \langle x, \xi(t) \rangle) dt + \frac{1}{2}\|x\|^2
$$

**Update Rule**:

$$
x^{t+1} = \int_0^T \frac{\exp(\beta \langle x^t, \xi(t) \rangle)}{\int_0^T \exp(\beta \langle x^t, \xi(s) \rangle) ds} \xi(t) dt
$$

**é›¢æ•£ â†’ é€£ç¶šã®å¯¾å¿œ**:

| é›¢æ•£ Modern Hopfield | é€£ç¶šæ™‚é–“ç‰ˆ |
|:--------------------|:---------|
| $\sum_{i=1}^M$ | $\int_0^T dt$ |
| $\xi^i$ | $\xi(t)$ |
| $M$ å€‹ã®è¨˜æ†¶ | é€£ç¶šçš„ãªè¨˜æ†¶ |

**å¿œç”¨**: å‹•ç”»ç”Ÿæˆãƒ»æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®é€£æƒ³è¨˜æ†¶

### 3.3 Classical Hopfield Networkï¼ˆæ­´å²ï¼‰

#### 3.3.1 èµ·æºï¼ˆ1982ï¼‰

**John J. Hopfield** (1982) "Neural networks and physical systems with emergent collective computational abilities"

**å‹•æ©Ÿ**: è„³ã®é€£æƒ³è¨˜æ†¶ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–

**Classical Hopfieldã®å®šç¾©**:

çŠ¶æ…‹: $x \in \{-1, +1\}^N$

ã‚¨ãƒãƒ«ã‚®ãƒ¼:

$$
E(x) = -\frac{1}{2} x^\top W x - b^\top x
$$

ã“ã“ã§ $W$ ã¯å¯¾ç§°è¡Œåˆ—ï¼ˆ$W = W^\top$ï¼‰ã€å¯¾è§’æˆåˆ† $W_{ii} = 0$ã€‚

**å‹•åŠ›å­¦**ï¼ˆéåŒæœŸæ›´æ–°ï¼‰:

$$
x_i(t+1) = \text{sign}\left(\sum_j W_{ij} x_j(t) + b_i\right)
$$

**ã‚¨ãƒãƒ«ã‚®ãƒ¼æ¸›å°‘å®šç†**:

å„æ›´æ–°ã§ã‚¨ãƒãƒ«ã‚®ãƒ¼ã¯å˜èª¿ã«æ¸›å°‘ï¼ˆã¾ãŸã¯ä¸å¤‰ï¼‰:

$$
E(x(t+1)) \leq E(x(t))
$$

**è¨¼æ˜**:

$x_i$ ã‚’æ›´æ–°å‰ $x_i^{\text{old}}$ã€æ›´æ–°å¾Œ $x_i^{\text{new}}$ ã¨ã™ã‚‹ã€‚

$$
\Delta E = E(x^{\text{new}}) - E(x^{\text{old}})
$$

$$
= -\frac{1}{2}(x^{\text{new}})^\top W x^{\text{new}} + \frac{1}{2}(x^{\text{old}})^\top W x^{\text{old}}
$$

$x_i$ ä»¥å¤–ã¯å¤‰åŒ–ã—ãªã„ã®ã§ã€$W_{ii} = 0$ ã‚ˆã‚Š:

$$
\Delta E = -x_i^{\text{new}} \left(\sum_j W_{ij} x_j\right) + x_i^{\text{old}} \left(\sum_j W_{ij} x_j\right)
$$

$$
= (x_i^{\text{old}} - x_i^{\text{new}}) \left(\sum_j W_{ij} x_j\right)
$$

æ›´æ–°å‰‡ $x_i^{\text{new}} = \text{sign}\left(\sum_j W_{ij} x_j\right)$ ã‚ˆã‚Šã€$x_i^{\text{new}}$ ã¨ $\sum_j W_{ij} x_j$ ã¯åŒç¬¦å·ï¼ˆã¾ãŸã¯0ï¼‰ã€‚

ã—ãŸãŒã£ã¦:
- $x_i^{\text{new}} \neq x_i^{\text{old}}$ ã®ã¨ãã€$(x_i^{\text{old}} - x_i^{\text{new}})$ ã¨ $\sum_j W_{ij} x_j$ ã¯é€†ç¬¦å· â†’ $\Delta E \leq 0$
- $x_i^{\text{new}} = x_i^{\text{old}}$ ã®ã¨ãã€$\Delta E = 0$

$\blacksquare$

**çµè«–**: éåŒæœŸæ›´æ–°ã§ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒå˜èª¿æ¸›å°‘ â†’ å±€æ‰€æœ€å°å€¤ï¼ˆå›ºå®šç‚¹ï¼‰ã«åæŸ

#### 3.3.2 Hebbã®å­¦ç¿’å‰‡

**ç›®çš„**: ãƒ‘ã‚¿ãƒ¼ãƒ³ $\{\xi^\mu\}\_{\mu=1}^M$ ã‚’è¨˜æ†¶ã™ã‚‹é‡ã¿è¡Œåˆ— $W$ ã‚’å­¦ç¿’

**Hebbian Rule**:

$$
W_{ij} = \frac{1}{N} \sum_{\mu=1}^M \xi^\mu_i \xi^\mu_j
$$

**ç›´æ„Ÿ**: "Neurons that fire together, wire together"

**è¨˜æ†¶å®¹é‡**:

ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒ $\xi^\mu \in \{-1, +1\}^N$ã€ãƒ©ãƒ³ãƒ€ãƒ ã§ç›´äº¤ã«è¿‘ã„ã¨ãã€è¨˜æ†¶å®¹é‡:

$$
M_{\max} \approx 0.14 N
$$

**è¨¼æ˜ã®è©³ç´°**:

ãƒ‘ã‚¿ãƒ¼ãƒ³ $\xi^\mu$ ãŒå›ºå®šç‚¹ã§ã‚ã‚‹ãŸã‚ã«ã¯ã€å…¨ã¦ã® $i$ ã«ã¤ã„ã¦:

$$
\xi^\mu_i = \text{sign}\left(\sum_j W_{ij} \xi^\mu_j\right)
$$

Hebbã®å­¦ç¿’å‰‡ $W_{ij} = \frac{1}{N} \sum_{\nu=1}^M \xi^\nu_i \xi^\nu_j$ ã‚’ä»£å…¥:

$$
\xi^\mu_i = \text{sign}\left(\sum_j \frac{1}{N} \sum_{\nu=1}^M \xi^\nu_i \xi^\nu_j \xi^\mu_j\right)
$$

$$
= \text{sign}\left(\frac{1}{N} \sum_{\nu=1}^M \xi^\nu_i \sum_j \xi^\nu_j \xi^\mu_j\right)
$$

$$
= \text{sign}\left(\frac{1}{N} \sum_{\nu=1}^M \xi^\nu_i \langle \xi^\nu, \xi^\mu \rangle\right)
$$

ã“ã“ã§ $\langle \xi^\nu, \xi^\mu \rangle = \sum_j \xi^\nu_j \xi^\mu_j$ ã¯å†…ç©ã€‚

**ä¿¡å·é …** ($\mu = \nu$):

$$
\frac{1}{N} \xi^\mu_i \langle \xi^\mu, \xi^\mu \rangle = \frac{1}{N} \xi^\mu_i \cdot N = \xi^\mu_i
$$

ã“ã‚Œã¯æ­£ã—ã„ç¬¦å·ã€‚

**ãƒã‚¤ã‚ºé …** ($\mu \neq \nu$):

$$
\frac{1}{N} \sum_{\nu \neq \mu} \xi^\nu_i \langle \xi^\nu, \xi^\mu \rangle
$$

ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒãƒ©ãƒ³ãƒ€ãƒ ãªã‚‰ã€$\langle \xi^\nu, \xi^\mu \rangle \approx 0$ï¼ˆç›´äº¤ã«è¿‘ã„ï¼‰ã€‚ã ãŒå®Œå…¨ã«0ã§ã¯ãªã„ã€‚

**çµ±è¨ˆçš„è§£æ**:

$\xi^\nu_i, \xi^\mu_j$ ãŒi.i.d. Bernoulli(1/2)ï¼ˆå€¤ $\pm 1$ï¼‰ã¨ã™ã‚‹ã¨:

$$
\mathbb{E}[\langle \xi^\nu, \xi^\mu \rangle] = \sum_j \mathbb{E}[\xi^\nu_j \xi^\mu_j] = 0
$$

$$
\text{Var}[\langle \xi^\nu, \xi^\mu \rangle] = \sum_j \text{Var}[\xi^\nu_j \xi^\mu_j] = N
$$

ã—ãŸãŒã£ã¦ã€$\langle \xi^\nu, \xi^\mu \rangle \sim \mathcal{N}(0, N)$ï¼ˆä¸­å¿ƒæ¥µé™å®šç†ï¼‰ã€‚

ãƒã‚¤ã‚ºé …ã®åˆ†æ•£:

$$
\text{Var}\left[\sum_{\nu \neq \mu} \xi^\nu_i \langle \xi^\nu, \xi^\mu \rangle\right] \approx (M-1) N
$$

æ¨™æº–åå·® $\sim \sqrt{MN}$ã€‚

**ä¿¡å·å¯¾é›‘éŸ³æ¯”** (SNR):

$$
\text{SNR} = \frac{\text{ä¿¡å·}}{\text{ãƒã‚¤ã‚º}} = \frac{N}{\sqrt{MN}} = \sqrt{\frac{N}{M}}
$$

$\text{sign}$ ãŒé«˜ç¢ºç‡ã§æ­£ã—ãå‹•ä½œã™ã‚‹ã«ã¯ã€$\text{SNR} \gg 1$:

$$
\sqrt{\frac{N}{M}} \gg 1 \quad \Rightarrow \quad M \ll N
$$

**å³å¯†ãªè§£æ** (Amit, Gutfreund, Sompolinsky 1985):

èª¤ã‚Šç¢ºç‡ $P_{\text{error}} \approx \frac{1}{2\sqrt{\pi}} \int_{\text{SNR}}^\infty e^{-z^2/2} dz$

$P_{\text{error}} < 0.01$ ã‚’è¦æ±‚ã™ã‚‹ã¨ã€$\text{SNR} > 2.33$ ãŒå¿…è¦ã€‚

$$
\sqrt{\frac{N}{M}} > 2.33 \quad \Rightarrow \quad M < \frac{N}{5.43} \approx 0.184 N
$$

çµŒé¨“çš„ã«ã¯ $M_{\max} \approx 0.14 N$ ãŒå®Ÿç”¨çš„ãªé™ç•Œ$\quad \blacksquare$

**æ•°å€¤ä¾‹**:
- $N = 100$: $M_{\max} \approx 14$ ãƒ‘ã‚¿ãƒ¼ãƒ³
- $N = 1000$: $M_{\max} \approx 140$ ãƒ‘ã‚¿ãƒ¼ãƒ³

#### 3.3.3 Classical Hopfieldã®é™ç•Œ

**å•é¡Œç‚¹**:
1. **å®¹é‡åˆ¶é™**: $M \sim 0.14 N$ â€” çŠ¶æ…‹æ•°ã«æ¯”ä¾‹ï¼ˆç·šå½¢ï¼‰
2. **ã‚¹ãƒ‘ãƒªã‚¢ã‚¹å›ºå®šç‚¹**: è¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³ä»¥å¤–ã®å›ºå®šç‚¹ãŒå­˜åœ¨
3. **ãƒ‘ã‚¿ãƒ¼ãƒ³å¹²æ¸‰**: é¡ä¼¼ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒæ··åŒã•ã‚Œã‚‹

**ã‚¹ãƒ‘ãƒªã‚¢ã‚¹å›ºå®šç‚¹ã®ä¾‹**:

2ã¤ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ $\xi^1 = [+1, +1, +1, +1]$ã€$\xi^2 = [+1, -1, -1, +1]$ ã‚’è¨˜æ†¶ã€‚

é‡ã¿è¡Œåˆ—:

$$
W = \frac{1}{4}(\xi^1 (\xi^1)^\top + \xi^2 (\xi^2)^\top)
$$

ã‚¹ãƒ‘ãƒªã‚¢ã‚¹å›ºå®šç‚¹: $x = [+1, -1, +1, -1]$ ã‚‚å®‰å®šï¼ˆ$\xi^1$ ã¨ $\xi^2$ ã®æ··åˆï¼‰

**Modern Hopfieldã«ã‚ˆã‚‹è§£æ±º**:
- æŒ‡æ•°çš„å®¹é‡ $M \sim \exp(d)$
- 1å›æ›´æ–°ã§åæŸ
- ã‚¹ãƒ‘ãƒªã‚¢ã‚¹å›ºå®šç‚¹ã®ç†è«–çš„æŠ‘åˆ¶

### 3.4 Restricted Boltzmann Machineå®Œå…¨ç‰ˆ

#### 3.4.1 RBMã®å®šç¾©

**æ§‹é€ **:
- **å¯è¦–å±¤**ï¼ˆVisibleï¼‰: $v \in \{0, 1\}^{n_v}$
- **éš ã‚Œå±¤**ï¼ˆHiddenï¼‰: $h \in \{0, 1\}^{n_h}$
- **é‡ã¿**: $W \in \mathbb{R}^{n_v \times n_h}$
- **ãƒã‚¤ã‚¢ã‚¹**: $b \in \mathbb{R}^{n_v}$ã€$c \in \mathbb{R}^{n_h}$

**åˆ¶ç´„**: å¯è¦–å±¤å†…ã®æ¥ç¶šãªã—ã€éš ã‚Œå±¤å†…ã®æ¥ç¶šãªã—ï¼ˆäºŒéƒ¨ã‚°ãƒ©ãƒ•ï¼‰

**ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°**:

$$
E(v, h) = -v^\top W h - b^\top v - c^\top h
$$

**åŒæ™‚åˆ†å¸ƒ**:

$$
p(v, h) = \frac{1}{Z} \exp(-E(v, h))
$$

$$
Z = \sum_{v, h} \exp(-E(v, h))
$$

**å‘¨è¾ºåˆ†å¸ƒ**:

$$
p(v) = \sum_h p(v, h) = \frac{1}{Z} \sum_h \exp(-E(v, h))
$$

#### 3.4.2 æ¡ä»¶ä»˜ãåˆ†å¸ƒ

**äºŒéƒ¨æ§‹é€ ã®åˆ©ç‚¹**: æ¡ä»¶ä»˜ãåˆ†å¸ƒãŒå› æ•°åˆ†è§£

$$
p(h | v) = \prod_{j=1}^{n_h} p(h_j | v)
$$

$$
p(v | h) = \prod_{i=1}^{n_v} p(v_i | h)
$$

**å°å‡º**:

$$
p(h_j = 1 | v) = \frac{p(h_j = 1, v)}{\sum_{h_j'} p(h_j = h_j', v)}
$$

$$
= \frac{\exp(c_j + \sum_i W_{ij} v_i)}{\exp(c_j + \sum_i W_{ij} v_i) + 1}
$$

$$
= \sigma\left(c_j + \sum_i W_{ij} v_i\right)
$$

ã“ã“ã§ $\sigma(x) = 1/(1 + \exp(-x))$ ã¯ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã€‚

åŒæ§˜ã«:

$$
p(v_i = 1 | h) = \sigma\left(b_i + \sum_j W_{ij} h_j\right)
$$

#### 3.4.3 Partition Function $Z$ ã®è¨ˆç®—å›°é›£æ€§

$$
Z = \sum_{v \in \{0,1\}^{n_v}} \sum_{h \in \{0,1\}^{n_h}} \exp(-E(v, h))
$$

è¨ˆç®—é‡: $O(2^{n_v + n_h})$ â€” æŒ‡æ•°çš„

**å‘¨è¾ºåŒ–ãƒˆãƒªãƒƒã‚¯**:

$$
Z = \sum_v \exp(b^\top v) \sum_h \exp(h^\top(W^\top v + c))
$$

$$
= \sum_v \exp(b^\top v) \prod_j (1 + \exp(c_j + \sum_i W_{ij} v_i))
$$

ã“ã‚Œã§ã‚‚ $O(2^{n_v})$ â€” å®Ÿç”¨ä¸å¯

#### 3.4.4 Contrastive Divergence (CD-k)

**Hintonã®ã‚¢ã‚¤ãƒ‡ã‚¢** (2002): è² ã®å¯¾æ•°å°¤åº¦ã®å‹¾é…ã‚’è¿‘ä¼¼

**å®Œå…¨ãªå‹¾é…**:

$$
\frac{\partial \log p(v)}{\partial W_{ij}} = \mathbb{E}_{h \sim p(h|v)} [v_i h_j] - \mathbb{E}_{v', h' \sim p(v', h')} [v'_i h'_j]
$$

- **ç¬¬1é …**: ãƒ‡ãƒ¼ã‚¿ä¾å­˜é …ï¼ˆæ­£ä¾‹ï¼‰â€” è¨ˆç®—å®¹æ˜“
- **ç¬¬2é …**: ãƒ¢ãƒ‡ãƒ«ä¾å­˜é …ï¼ˆè² ä¾‹ï¼‰â€” $p(v', h')$ ã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒå¿…è¦ï¼ˆå›°é›£ï¼‰

**CD-kè¿‘ä¼¼**:

1. **åˆæœŸåŒ–**: $v^{(0)} = v_{\text{data}}$ï¼ˆãƒ‡ãƒ¼ã‚¿ï¼‰
2. **kå›ã®Gibbsã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**:
   - $h^{(t)} \sim p(h | v^{(t)})$
   - $v^{(t+1)} \sim p(v | h^{(t)})$
3. **è¿‘ä¼¼å‹¾é…**:

$$
\frac{\partial \log p(v)}{\partial W_{ij}} \approx \mathbb{E}_{h \sim p(h|v^{(0)})} [v^{(0)}_i h_j] - \mathbb{E}_{h \sim p(h|v^{(k)})} [v^{(k)}_i h_j]
$$

**k=1ã®ã¨ã** (CD-1):
- 1å›ã ã‘Gibbsã‚¹ãƒ†ãƒƒãƒ—
- è² ä¾‹ $v^{(1)}$ ã¯ $v^{(0)}$ ã‹ã‚‰è¿‘ã„ â†’ ãƒã‚¤ã‚¢ã‚¹ã‚ã‚Š
- ã ãŒå®Ÿç”¨çš„ã«æ©Ÿèƒ½ã™ã‚‹ï¼ˆçµŒé¨“çš„ï¼‰

**åæŸæ€§**: CD-kã¯ $\log p(v)$ ã‚’ç›´æ¥æœ€å¤§åŒ–ã—ãªã„ã€‚åˆ¥ã®ç›®çš„é–¢æ•°ï¼ˆContrastive Divergenceï¼‰ã‚’æœ€å°åŒ– â†’ ãƒã‚¤ã‚¢ã‚¹ã‚ã‚Š

#### 3.4.5 Persistent Contrastive Divergence (PCD)

**å•é¡Œ**: CD-kã¯æ¯å›ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åˆæœŸåŒ– â†’ è² ä¾‹ãŒå¸¸ã«ãƒ‡ãƒ¼ã‚¿è¿‘å‚

**PCD** (Tieleman 2008):
- **Persistent Chain**: Markové€£é–ã‚’æ°¸ç¶šçš„ã«ç¶­æŒ
- å„ãƒŸãƒ‹ãƒãƒƒãƒã§ã€å‰å›ã® $v^{(k)}$ ã‹ã‚‰ç¶™ç¶š
- â†’ è² ä¾‹ãŒãƒ¢ãƒ‡ãƒ«åˆ†å¸ƒã«ã‚ˆã‚Šè¿‘ã„

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

```
åˆæœŸåŒ–: v_chain â† ãƒ©ãƒ³ãƒ€ãƒ 
for each ãƒŸãƒ‹ãƒãƒƒãƒ do:
    æ­£ä¾‹: v_pos â† ãƒ‡ãƒ¼ã‚¿
    h_pos â† p(h | v_pos)

    è² ä¾‹: v_chain ã‹ã‚‰ k-step Gibbs
    v_neg â† v_chain
    h_neg â† p(h | v_neg)

    å‹¾é…æ›´æ–°:
    Î”W â† âŸ¨v_pos h_pos^TâŸ© - âŸ¨v_neg h_neg^TâŸ©
end
```

**åˆ©ç‚¹**: CD-kã‚ˆã‚Šãƒã‚¤ã‚¢ã‚¹ãŒå°‘ãªã„ã€é•·ã„ãƒã‚§ãƒ¼ãƒ³ã‚’ç¶­æŒ

### 3.5 MCMCç†è«–ï¼ˆç¬¬5å›åŸºç¤ã®æ·±åŒ–ï¼‰

:::message
**ç¬¬5å›ã§ã®åŸºç¤**: Markové€£é–ãƒ»Metropolis-Hastingsã®åŸºç¤ã‚’å°å…¥æ¸ˆã¿ã€‚æœ¬å›ã¯EBMã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ–‡è„ˆã§ã®ç†è«–æ·±åŒ–ã€‚
:::

#### 3.5.1 Markové€£é–å¾©ç¿’

**å®šç¾©**: çŠ¶æ…‹ç©ºé–“ $\mathcal{X}$ ä¸Šã®ç¢ºç‡éç¨‹ $\{X_t\}_{t=0}^\infty$

**Markovæ€§**: $P(X_{t+1} | X_0, \ldots, X_t) = P(X_{t+1} | X_t)$

**é·ç§»ã‚«ãƒ¼ãƒãƒ«**: $T(x' | x) = P(X_{t+1} = x' | X_t = x)$

#### 3.5.2 è©³ç´°é‡£ã‚Šåˆã„ï¼ˆDetailed Balanceï¼‰

**å®šç¾©**: ç¢ºç‡åˆ†å¸ƒ $\pi(x)$ ãŒé·ç§»ã‚«ãƒ¼ãƒãƒ« $T(x' | x)$ ã«é–¢ã—ã¦è©³ç´°é‡£ã‚Šåˆã„ã‚’æº€ãŸã™:

$$
\pi(x) T(x' | x) = \pi(x') T(x | x')
$$

**å®šç†**: è©³ç´°é‡£ã‚Šåˆã„ã‚’æº€ãŸã™ã¨ãã€$\pi$ ã¯ $T$ ã®å®šå¸¸åˆ†å¸ƒã€‚

**è¨¼æ˜**:

$$
\sum_x \pi(x) T(x' | x) = \sum_x \pi(x') T(x | x') = \pi(x') \sum_x T(x | x') = \pi(x')
$$

$\blacksquare$

#### 3.5.3 ã‚¨ãƒ«ã‚´ãƒ¼ãƒ‰æ€§ï¼ˆErgodicityï¼‰

**å®šç¾©**: Markové€£é–ãŒã‚¨ãƒ«ã‚´ãƒ¼ãƒ‰çš„ â‡” ä»»æ„ã®åˆæœŸåˆ†å¸ƒã‹ã‚‰å®šå¸¸åˆ†å¸ƒã«åæŸ

**æ¡ä»¶**:
1. **æ—¢ç´„æ€§**ï¼ˆIrreducibilityï¼‰: å…¨ã¦ã®çŠ¶æ…‹ãŒç›¸äº’ã«åˆ°é”å¯èƒ½
2. **éå‘¨æœŸæ€§**ï¼ˆAperiodicityï¼‰: å‘¨æœŸçš„ãªã‚µã‚¤ã‚¯ãƒ«ãŒãªã„

**å®šç†**: æ—¢ç´„ãƒ»éå‘¨æœŸçš„ã§è©³ç´°é‡£ã‚Šåˆã„ã‚’æº€ãŸã™Markové€£é–ã¯ã€å®šå¸¸åˆ†å¸ƒã«åæŸã€‚

#### 3.5.4 Metropolis-Hastingså®Œå…¨ç‰ˆ

**ç›®æ¨™**: ç›®æ¨™åˆ†å¸ƒ $\pi(x)$ ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆ$\pi$ ã‹ã‚‰ã®ç›´æ¥ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¯å›°é›£ï¼‰

**ææ¡ˆåˆ†å¸ƒ**: $q(x' | x)$ï¼ˆ$x$ ã‹ã‚‰ $x'$ ã‚’ææ¡ˆï¼‰

**å—ç†ç¢ºç‡**:

$$
\alpha(x' | x) = \min\left(1, \frac{\pi(x') q(x | x')}{\pi(x) q(x' | x)}\right)
$$

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

```
åˆæœŸåŒ–: x â† x_0
for t = 1, 2, ... do:
    x' â† q(Â· | x)  # ææ¡ˆ
    u â† Uniform(0, 1)
    if u < Î±(x' | x):
        x â† x'  # å—ç†
    else:
        x â† x  # æ£„å´ï¼ˆç¾åœ¨ã®çŠ¶æ…‹ã‚’ç¶­æŒï¼‰
    ã‚µãƒ³ãƒ—ãƒ«: x_t = x
end
```

**è©³ç´°é‡£ã‚Šåˆã„ã®è¨¼æ˜**:

é·ç§»ã‚«ãƒ¼ãƒãƒ«:

$$
T(x' | x) = q(x' | x) \alpha(x' | x) + \delta(x - x') r(x)
$$

ã“ã“ã§ $r(x) = 1 - \int q(x' | x) \alpha(x' | x) dx'$ ã¯æ£„å´ç¢ºç‡ã€‚

è©³ç´°é‡£ã‚Šåˆã„ã‚’ç¤ºã™:

$$
\pi(x) q(x' | x) \alpha(x' | x) = \pi(x) q(x' | x) \min\left(1, \frac{\pi(x') q(x | x')}{\pi(x) q(x' | x)}\right)
$$

$$
= \min\left(\pi(x) q(x' | x), \pi(x') q(x | x')\right)
$$

å¯¾ç§°æ€§ã‚ˆã‚Š:

$$
= \pi(x') q(x | x') \min\left(1, \frac{\pi(x) q(x' | x)}{\pi(x') q(x | x')}\right)
$$

$$
= \pi(x') q(x | x') \alpha(x | x')
$$

$\blacksquare$

#### 3.5.5 Gibbs Sampling

**è¨­å®š**: å¤šå¤‰é‡åˆ†å¸ƒ $\pi(x_1, \ldots, x_d)$ ã‹ã‚‰ã€æ¡ä»¶ä»˜ãåˆ†å¸ƒ $\pi(x_i | x_{-i})$ ãŒåˆ©ç”¨å¯èƒ½

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

```
åˆæœŸåŒ–: x â† (x_1, ..., x_d)
for t = 1, 2, ... do:
    for i = 1, ..., d do:
        x_i â† Ï€(x_i | x_{-i})  # æ¡ä»¶ä»˜ãåˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«
    end
    ã‚µãƒ³ãƒ—ãƒ«: x_t = x
end
```

**è©³ç´°é‡£ã‚Šåˆã„**:

Gibbs Samplingã¯Metropolis-Hastingsã®ç‰¹æ®Šã‚±ãƒ¼ã‚¹ï¼ˆå—ç†ç¢ºç‡ = 1ï¼‰

$\blacksquare$

**RBMã¨ã®æ¥ç¶š**:

RBMã®Gibbs Sampling:
- $h \sim p(h | v)$ï¼ˆéš ã‚Œå±¤ã‚’æ›´æ–°ï¼‰
- $v \sim p(v | h)$ï¼ˆå¯è¦–å±¤ã‚’æ›´æ–°ï¼‰

ã“ã‚Œã‚’äº¤äº’ã«ç¹°ã‚Šè¿”ã™ â†’ $p(v, h)$ ã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒ«

### 3.6 Hamiltonian Monte Carlo (HMC)

#### 3.6.1 å‹•æ©Ÿ

**å•é¡Œ**: Metropolis-Hastings / Gibbs Samplingã¯é«˜æ¬¡å…ƒã§åŠ¹ç‡ãŒæ‚ªã„
- ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ â†’ é…ã„æ··åˆ
- é«˜æ¬¡å…ƒã§å—ç†ç‡ãŒä½ä¸‹

**HMC**: HamiltonåŠ›å­¦ã‚’åˆ©ç”¨ã—ã¦åŠ¹ç‡çš„ã«æ¢ç´¢

#### 3.6.2 HamiltonianåŠ›å­¦å¾©ç¿’

**ç³»**: ä½ç½® $q \in \mathbb{R}^d$ã€é‹å‹•é‡ $p \in \mathbb{R}^d$

**Hamiltonian**:

$$
H(q, p) = U(q) + K(p)
$$

- $U(q)$: ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã‚¨ãƒãƒ«ã‚®ãƒ¼
- $K(p) = \frac{1}{2}p^\top M^{-1} p$: é‹å‹•ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼ˆ$M$ ã¯è³ªé‡è¡Œåˆ—ï¼‰

**Hamiltonæ–¹ç¨‹å¼**:

$$
\frac{dq}{dt} = \frac{\partial H}{\partial p} = M^{-1} p
$$

$$
\frac{dp}{dt} = -\frac{\partial H}{\partial q} = -\nabla U(q)
$$

**ä¿å­˜å‰‡**: $H(q, p)$ ã¯æ™‚é–“ã§ä¸å¤‰

**ä½“ç©ä¿å­˜**: ä½ç›¸ç©ºé–“ã§ã®ä½“ç©ãŒä¿å­˜ã•ã‚Œã‚‹ï¼ˆLiouvilleå®šç†ï¼‰

#### 3.6.3 HMCã®ã‚¢ã‚¤ãƒ‡ã‚¢

**ç›®æ¨™**: åˆ†å¸ƒ $\pi(q)$ ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

**æ‹¡å¼µ**: è£œåŠ©å¤‰æ•° $p$ ã‚’å°å…¥ã—ã€åŒæ™‚åˆ†å¸ƒã‚’å®šç¾©:

$$
\pi(q, p) = \frac{1}{Z} \exp(-H(q, p))
$$

ã“ã“ã§ $H(q, p) = -\log \pi(q) + \frac{1}{2}p^\top M^{-1} p$

$U(q) = -\log \pi(q)$ï¼ˆè² ã®å¯¾æ•°å¯†åº¦ï¼‰ã¨ã™ã‚‹ã¨ã€$\pi(q, p)$ ã‹ã‚‰ã®å‘¨è¾ºåˆ†å¸ƒ $\int \pi(q, p) dp = \pi(q)$

**æˆ¦ç•¥**:
1. Hamiltonæ–¹ç¨‹å¼ã«å¾“ã£ã¦ $(q, p)$ ã‚’æ™‚é–“ç™ºå±•
2. Hamiltonian $H$ ãŒä¿å­˜ â†’ å—ç†ç¢ºç‡ = 1ï¼ˆç†è«–ä¸Šï¼‰
3. $p$ ã‚’å‘¨è¾ºåŒ– â†’ $q$ ã®ã‚µãƒ³ãƒ—ãƒ«

#### 3.6.4 Leapfrogç©åˆ†

**å•é¡Œ**: Hamiltonæ–¹ç¨‹å¼ã®é€£ç¶šæ™‚é–“ç©åˆ†ã¯è¨ˆç®—ä¸å¯ â†’ é›¢æ•£åŒ–ãŒå¿…è¦

**Leapfrogæ³•**ï¼ˆã‚·ãƒ³ãƒ—ãƒ¬ã‚¯ãƒ†ã‚£ãƒƒã‚¯ç©åˆ†ï¼‰:

$$
p_{t+\epsilon/2} = p_t - \frac{\epsilon}{2} \nabla U(q_t)
$$

$$
q_{t+\epsilon} = q_t + \epsilon M^{-1} p_{t+\epsilon/2}
$$

$$
p_{t+\epsilon} = p_{t+\epsilon/2} - \frac{\epsilon}{2} \nabla U(q_{t+\epsilon})
$$

**æ€§è³ª**:
- **å¯é€†**: $(q_t, p_t) \to (q_{t+\epsilon}, p_{t+\epsilon})$ ã¨é€†å‘ããŒåŒä¸€ã®å¤‰æ›
- **ä½“ç©ä¿å­˜**: ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—å¼ = 1
- **è¿‘ä¼¼çš„ã«Hamiltonianä¿å­˜**: é›¢æ•£åŒ–èª¤å·® $O(\epsilon^3)$ per step

#### 3.6.5 HMCã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

```
åˆæœŸåŒ–: q â† q_0
for t = 1, 2, ... do:
    p â† N(0, M)  # é‹å‹•é‡ã‚’å†ã‚µãƒ³ãƒ—ãƒ«

    # Leapfrogç©åˆ†ï¼ˆL stepsã€ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º Îµï¼‰
    p' â† p - (Îµ/2) âˆ‡U(q)
    for l = 1, ..., L do:
        q' â† q' + Îµ M^{-1} p'
        if l < L:
            p' â† p' - Îµ âˆ‡U(q')
    end
    p' â† p' - (Îµ/2) âˆ‡U(q')

    # Metropoliså—ç†ãƒ»æ£„å´
    Î”H â† H(q', p') - H(q, p)
    u â† Uniform(0, 1)
    if u < exp(-Î”H):
        q â† q'  # å—ç†
    else:
        q â† q  # æ£„å´

    ã‚µãƒ³ãƒ—ãƒ«: q_t = q
end
```

**ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
- $\epsilon$: ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºï¼ˆå°ã•ã„ã»ã©æ­£ç¢ºã€å¤§ãã„ã»ã©æ¢ç´¢ç¯„å›²åºƒã„ï¼‰
- $L$: Leapfrog stepsæ•°ï¼ˆå¤§ãã„ã»ã©é ãã¾ã§ç§»å‹•ï¼‰
- $M$: è³ªé‡è¡Œåˆ—ï¼ˆé€šå¸¸ $M = I$ï¼‰

#### 3.6.6 No-U-Turn Sampler (NUTS)

**å•é¡Œ**: HMCã¯ $L$ ã¨ $\epsilon$ ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå¿…è¦

**NUTS** (Hoffman & Gelman 2014):
- $L$ ã‚’é©å¿œçš„ã«æ±ºå®š
- U-turnï¼ˆè»Œé“ãŒæŠ˜ã‚Šè¿”ã™ï¼‰ã‚’æ¤œå‡ºã—ã¦è‡ªå‹•åœæ­¢
- Stanç­‰ã®PPLã§æ¨™æº–å®Ÿè£…

### 3.7 Langevin Dynamicsæ¦‚è¦

:::message
**å®Œå…¨ç‰ˆã¯ç¬¬35å›**: Score Matching & Langevin Dynamics ã§è©³ç´°å°å‡ºã€‚æœ¬å›ã¯EBMã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨ã—ã¦ã®ä½ç½®ã¥ã‘ã®ã¿ã€‚
:::

**Langevin Dynamics**:

$$
dx_t = -\nabla U(x_t) dt + \sqrt{2} dW_t
$$

ã“ã“ã§ $dW_t$ ã¯Browné‹å‹•ã€‚

**é›¢æ•£åŒ–**ï¼ˆEuler-Maruyamaï¼‰:

$$
x_{t+1} = x_t - \epsilon \nabla U(x_t) + \sqrt{2\epsilon} \, \xi_t
$$

ã“ã“ã§ $\xi_t \sim \mathcal{N}(0, I)$ã€‚

**EBMã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¸ã®å¿œç”¨**:

$U(x) = E_\theta(x)$ ã¨ã™ã‚‹ã¨:

$$
x_{t+1} = x_t - \epsilon \nabla_x E_\theta(x_t) + \sqrt{2\epsilon} \, \xi_t
$$

ã“ã‚Œã¯ $p_\theta(x) \propto \exp(-E_\theta(x))$ ã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚

**åˆ©ç‚¹**: Metropoliså—ç†ãƒ»æ£„å´ä¸è¦ â†’ é«˜æ¬¡å…ƒã§åŠ¹ç‡çš„

**å•é¡Œ**: $\epsilon \to 0$ ã®æ¥µé™ã§æ­£ç¢ºï¼ˆé›¢æ•£åŒ–èª¤å·®ï¼‰

### 3.8 çµ±è¨ˆç‰©ç†ã¨ã®æ¥ç¶š

#### 3.8.1 Gibbsåˆ†å¸ƒã¨çµ±è¨ˆåŠ›å­¦

**çµ±è¨ˆåŠ›å­¦ã®ã‚«ãƒãƒ‹ã‚«ãƒ«åˆ†å¸ƒ**:

æ¸©åº¦ $T$ ã®ã¨ãã€ã‚¨ãƒãƒ«ã‚®ãƒ¼ $E$ ã®çŠ¶æ…‹ã®ç¢ºç‡:

$$
p(x) = \frac{1}{Z} \exp\left(-\frac{E(x)}{k_B T}\right)
$$

ã“ã“ã§ $k_B$ ã¯Boltzmannå®šæ•°ã€$Z$ ã¯åˆ†é…é–¢æ•°ã€‚

**EBMã¨ã®å¯¾å¿œ**:
- $k_B T = 1$ ã¨ã™ã‚‹ã¨ $p(x) = \frac{1}{Z} \exp(-E(x))$
- EBMã®Gibbsåˆ†å¸ƒ = ã‚«ãƒãƒ‹ã‚«ãƒ«åˆ†å¸ƒ

#### 3.8.2 è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼

**Helmholtzè‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼**:

$$
F = -k_B T \log Z
$$

**è§£é‡ˆ**:
- $F$ ãŒæœ€å° â‡” ç†±å¹³è¡¡çŠ¶æ…‹
- $Z$ ãŒå¤§ãã„ â‡” $F$ ãŒå°ã•ã„ â‡” å¤šãã®çŠ¶æ…‹ãŒè¨±å®¹ã•ã‚Œã‚‹

**EBMã¨ã®å¯¾å¿œ**:

$$
F(\theta) = -\log Z(\theta) = -\log \int \exp(-E_\theta(x)) dx
$$

EBMè¨“ç·´ = $F(\theta)$ ã®æœ€å°åŒ–

#### 3.8.3 Grokking as Phase Transitionï¼ˆè©³ç´°ç‰ˆï¼‰

**Grokkingç¾è±¡**: è¨“ç·´ãƒ­ã‚¹ãŒæ—©æœŸã«åæŸå¾Œã€å¤§å¹…ã«é…ã‚Œã¦æ±åŒ–æ€§èƒ½ãŒæ€¥ä¸Šæ˜‡ã™ã‚‹ç¾è±¡ï¼ˆPower+ 2022ï¼‰

**çµ±è¨ˆç‰©ç†çš„è§£é‡ˆ** (Liu+ 2023, Varma+ 2023):

Grokkingã¯ **ä¸€æ¬¡ç›¸è»¢ç§»** ã¨ã—ã¦ç†è§£å¯èƒ½ã€‚

**è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼**:

$$
F(\theta; T) = E_{\text{train}}(\theta) - T S(\theta)
$$

- $E_{\text{train}}(\theta)$: è¨“ç·´èª¤å·®ï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼é …ï¼‰
- $S(\theta)$: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼é …ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¹±é›‘ã•ï¼‰
- $T$: æœ‰åŠ¹æ¸©åº¦ï¼ˆå­¦ç¿’ç‡ / weight decay ã«å¯¾å¿œï¼‰

**2ã¤ã®çŠ¶æ…‹**:

1. **è¨˜æ†¶ç›¸** (Memorization Phase):
   - é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: $S(\theta) \gg 0$
   - è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’æš—è¨˜ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒè¤‡é›‘ï¼‰
   - æ±åŒ–ã—ãªã„

2. **æ±åŒ–ç›¸** (Generalization Phase):
   - ä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: $S(\theta) \approx 0$
   - ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ«ãƒ¼ãƒ«ã‚’å­¦ç¿’ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒæ•´ç†ï¼‰
   - é«˜ã„æ±åŒ–æ€§èƒ½

**ç›¸è»¢ç§»ã®ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹**:

è¨“ç·´åˆæœŸ: è¨˜æ†¶ç›¸ãŒå®‰å®šï¼ˆ$E$ ä¸‹ãŒã‚‹ã€$S$ é«˜ã„ï¼‰
â†“
é•·æ™‚é–“è¨“ç·´: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚³ã‚¹ãƒˆãŒåŠ¹ã„ã¦ãã‚‹
â†“
è‡¨ç•Œç‚¹é€šé: $F_{\text{memorization}} = F_{\text{generalization}}$
â†“
æ±åŒ–ç›¸ã¸é·ç§»: $S$ æ€¥æ¸›ã€æ±åŒ–æ€§èƒ½æ€¥ä¸Šæ˜‡

**é·ç§»ç¢ºç‡** (Metropolis-like):

$$
P(\text{memorization} \to \text{generalization}) \propto \exp\left(-\frac{\Delta F}{T_{\text{eff}}}\right)
$$

ã“ã“ã§:

$$
\Delta F = F_{\text{gen}} - F_{\text{mem}} = \Delta E - T \Delta S
$$

**æ¡ä»¶**:
- $\Delta E > 0$: æ±åŒ–è§£ã¯è¨“ç·´èª¤å·®ãŒã‚ãšã‹ã«é«˜ã„
- $\Delta S < 0$: æ±åŒ–è§£ã¯ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãŒä½ã„
- $T$ é«˜ã„ï¼ˆå­¦ç¿’ç‡é«˜ã„ï¼‰: é·ç§»ã—ã‚„ã™ã„
- $T$ ä½ã„ï¼ˆweight decayå¼·ã„ï¼‰: é·ç§»ã—ã«ãã„ â†’ GrokkingãŒè¦³æ¸¬ã•ã‚Œã‚‹

**æ•°å€¤å®Ÿé¨“ã§ç¢ºèªã§ãã‚‹æŒ‡æ¨™**:

```julia
# Grokkingå®Ÿé¨“ç”¨ã®æŒ‡æ¨™
function compute_entropy(params)
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
    hist = fit(Histogram, vec(params), nbins=50)
    p = hist.weights ./ sum(hist.weights)
    p = p[p .> 0]  # éã‚¼ãƒ­ã®ã¿
    return -sum(p .* log.(p))
end

function grokking_metrics(model, train_data, test_data)
    # è¨“ç·´èª¤å·®
    train_loss = mean([loss(model, x, y) for (x, y) in train_data])

    # ãƒ†ã‚¹ãƒˆèª¤å·®
    test_loss = mean([loss(model, x, y) for (x, y) in test_data])

    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
    params = vcat([vec(p) for p in model.params]...)
    entropy = compute_entropy(params)

    # è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼ˆè¿‘ä¼¼ï¼‰
    T_eff = model.lr / model.weight_decay  # æœ‰åŠ¹æ¸©åº¦
    F = train_loss - T_eff * entropy

    return (train_loss=train_loss, test_loss=test_loss,
            entropy=entropy, free_energy=F)
end
```

**å…¸å‹çš„ãªGrokkingæ›²ç·š**:

```mermaid
graph LR
    A[Epoch 0-100] -->|"Train loss â†“<br/>Test loss â†“<br/>Entropy é«˜"| B[è¨˜æ†¶ç›¸]
    B -->|"Epoch 100-1000<br/>Train/Test flat<br/>Entropy é«˜ç¶­æŒ"| C[æº–å®‰å®šçŠ¶æ…‹]
    C -->|"è‡¨ç•Œç‚¹<br/>Entropy æ€¥æ¸›"| D[ç›¸è»¢ç§»]
    D -->|"Epoch 1000-1500<br/>Test loss â†“â†“<br/>Entropy ä½"| E[æ±åŒ–ç›¸]

    style D fill:#ff9,stroke:#333,stroke-width:4px
```

**Ising model ã¨ã®é¡ä¼¼**:

Ising modelï¼ˆç£æ€§ä½“ãƒ¢ãƒ‡ãƒ«ï¼‰ã§ã‚‚ä¸€æ¬¡ç›¸è»¢ç§»ãŒèµ·ã“ã‚‹:
- é«˜æ¸©: ã‚¹ãƒ”ãƒ³ãŒãƒ©ãƒ³ãƒ€ãƒ ï¼ˆé«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼‰
- ä½æ¸©: ã‚¹ãƒ”ãƒ³ãŒæ•´åˆ—ï¼ˆä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼‰
- è‡¨ç•Œæ¸©åº¦ $T_c$ ã§ç›¸è»¢ç§»

NNã®Grokking:
- é«˜å­¦ç¿’ç‡: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒãƒ©ãƒ³ãƒ€ãƒ ï¼ˆè¨˜æ†¶ç›¸ï¼‰
- ä½å­¦ç¿’ç‡ + weight decay: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒæ•´åˆ—ï¼ˆæ±åŒ–ç›¸ï¼‰
- è‡¨ç•Œepochæ•°ã§ç›¸è»¢ç§»

**å®Ÿè£…ä¾‹**ï¼ˆç°¡æ˜“ç‰ˆï¼‰:

```julia
# Modular arithmeticã‚¿ã‚¹ã‚¯ã§Grokkingå†ç¾
using Flux

# ãƒ‡ãƒ¼ã‚¿: 97 % 97 ã®è¶³ã—ç®—ï¼ˆGrokkingã§æœ‰åï¼‰
p = 97
data_x = [(i, j) for i in 0:p-1 for j in 0:p-1]
data_y = [(i + j) % p for (i, j) in data_x]

# è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²ï¼ˆè¨“ç·´30%ã®ã¿ â†’ Grokkingèµ·ãã‚„ã™ã„ï¼‰
n_train = Int(0.3 * length(data_x))
train_idx = shuffle(1:length(data_x))[1:n_train]
test_idx = setdiff(1:length(data_x), train_idx)

# ãƒ¢ãƒ‡ãƒ«: å°ã•ã„MLP
model = Chain(
    Embedding(p => 128), Embedding(p => 128),  # 2ã¤ã®å…¥åŠ›
    Dense(256 => 256, relu),
    Dense(256 => p)
)

# Weight decay å¼·ã‚ â†’ Grokkingä¿ƒé€²
opt = OptimiserChain(WeightDecay(0.01), Adam(0.001))

# è¨“ç·´ãƒ«ãƒ¼ãƒ—ï¼ˆã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¿½è·¡ï¼‰
history = []
for epoch in 1:5000
    # è¨“ç·´
    for idx in train_idx
        x, y = data_x[idx], data_y[idx]
        gs = gradient(m -> crossentropy(m(x), y), model)
        update!(opt, model, gs)
    end

    # è©•ä¾¡ï¼ˆ100 epochã”ã¨ï¼‰
    if epoch % 100 == 0
        metrics = grokking_metrics(model, train_idx, test_idx)
        push!(history, (epoch=epoch, metrics...))
        println("Epoch $epoch: Train=$(metrics.train_loss), Test=$(metrics.test_loss), Entropy=$(metrics.entropy)")
    end
end

# çµæœãƒ—ãƒ­ãƒƒãƒˆ
epochs = [h.epoch for h in history]
plot(epochs, [h.train_loss for h in history], label="Train", yscale=:log10)
plot!(epochs, [h.test_loss for h in history], label="Test")
plot!(epochs, [h.entropy for h in history], label="Entropy (scaled)", ylabel="Loss / Entropy")
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:
- Epoch 0-500: Train loss â†“, Test loss æ¨ªã°ã„, Entropy é«˜
- Epoch 500-2000: å…¨ã¦æ¨ªã°ã„ï¼ˆæº–å®‰å®šçŠ¶æ…‹ï¼‰
- Epoch 2000-2500: Test loss æ€¥æ¸›ï¼ˆGrokking!ï¼‰, Entropy æ€¥æ¸›
- Epoch 2500+: æ±åŒ–ç›¸ã«å®‰å®š

#### 3.8.4 å¤‰åˆ†è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼

**è¨­å®š**: çœŸã®åˆ†å¸ƒ $p^*(x)$ã€è¿‘ä¼¼åˆ†å¸ƒ $q(x)$

**å¤‰åˆ†è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼**:

$$
\mathcal{F}(q) = \mathbb{E}_{q(x)} [E(x)] + H(q)
$$

ã“ã“ã§ $H(q) = -\int q(x) \log q(x) dx$ ã¯ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã€‚

**å®šç†**: $\mathcal{F}(q) \geq F$ ï¼ˆç­‰å·æˆç«‹ â‡” $q = p^*$ï¼‰

**è¨¼æ˜**:

$$
\mathcal{F}(q) - F = \mathbb{E}_{q} [E(x)] + H(q) + \log Z
$$

$$
= \mathbb{E}_{q} [E(x)] + \mathbb{E}_{q} [-\log q(x)] + \log Z
$$

$$
= \mathbb{E}_{q} [-\log q(x) + E(x) + \log Z]
$$

$$
= \mathbb{E}_{q} \left[\log \frac{\exp(-E(x))}{q(x) Z}\right]
$$

$$
= \mathbb{E}_{q} \left[\log \frac{p^*(x)}{q(x)}\right]
$$

$$
= D_{\text{KL}}(q \| p^*) \geq 0
$$

$\blacksquare$

**å¤‰åˆ†æ¨è«–ã¨ã®æ¥ç¶š**: VAEã®ELBO = å¤‰åˆ†è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®æœ€å°åŒ–

### 3.9 ç›¸è»¢ç§» / Isingæ¨¡å‹ / Grokking

#### 3.9.1 Isingæ¨¡å‹

**å®šç¾©**:

ã‚¹ãƒ”ãƒ³ $s_i \in \{-1, +1\}$ ãŒæ ¼å­ä¸Šã«é…ç½®ã€‚

ã‚¨ãƒãƒ«ã‚®ãƒ¼:

$$
E(s) = -J \sum_{\langle i, j \rangle} s_i s_j - h \sum_i s_i
$$

- $J$: ç›¸äº’ä½œç”¨ï¼ˆ$J > 0$ ã§å¼·ç£æ€§ï¼‰
- $h$: å¤–éƒ¨ç£å ´

**Gibbsåˆ†å¸ƒ**:

$$
p(s) = \frac{1}{Z} \exp(-\beta E(s))
$$

ã“ã“ã§ $\beta = 1/(k_B T)$ ã¯é€†æ¸©åº¦ã€‚

**ç›¸è»¢ç§»**: æ¸©åº¦ $T$ ãŒè‡¨ç•Œæ¸©åº¦ $T_c$ ã‚’ä¸‹å›ã‚‹ã¨ã€è‡ªç™ºç£åŒ–ãŒç™ºç”Ÿ

#### 3.9.2 Grokking = ä¸€æ¬¡ç›¸è»¢ç§»

**Grokking** (Power+ 2022): ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒéå­¦ç¿’å¾Œã«çªç„¶ä¸€èˆ¬åŒ–

**ç¾è±¡**:
1. è¨“ç·´èª¤å·®ã¯ã‚¨ãƒãƒƒã‚¯100ã§0ã«åæŸ
2. æ¤œè¨¼èª¤å·®ã¯ã‚¨ãƒãƒƒã‚¯100-10000ã§åœæ»
3. ã‚¨ãƒãƒƒã‚¯10000ä»¥é™ã€æ¤œè¨¼èª¤å·®ãŒçªç„¶ä½ä¸‹ â†’ ä¸€èˆ¬åŒ–

**çµ±è¨ˆç‰©ç†çš„èª¬æ˜** (ICLR 2024):

Grokkingã¯**ä¸€æ¬¡ç›¸è»¢ç§»**ã¨ã—ã¦ç†è§£å¯èƒ½ã€‚

**è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ©ãƒ³ãƒ‰ã‚¹ã‚±ãƒ¼ãƒ—**:

$$
F(\theta) = E(\theta) - TS(\theta)
$$

- $E(\theta)$: ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼ˆè¨“ç·´èª¤å·®ï¼‰
- $S(\theta)$: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¤šæ§˜æ€§ï¼‰
- $T$: æ¸©åº¦ï¼ˆå­¦ç¿’ç‡ã«å¯¾å¿œï¼‰

**2ã¤ã®ç›¸**:

1. **ãƒ¡ãƒ¢ãƒªã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ç›¸**ï¼ˆå±€æ‰€æœ€é©ï¼‰:
   - $E$ ä½ã„ï¼ˆè¨“ç·´èª¤å·®å°ï¼‰
   - $S$ ä½ã„ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«éé©åˆï¼‰
   - **é«˜ã‚¨ãƒãƒ«ã‚®ãƒ¼çŠ¶æ…‹**ï¼ˆä¸€èˆ¬åŒ–æ€§èƒ½ä½ã„ï¼‰

2. **ä¸€èˆ¬åŒ–ç›¸**ï¼ˆå¤§åŸŸæœ€é©ï¼‰:
   - $E$ ä½ã„ï¼ˆè¨“ç·´èª¤å·®å°ï¼‰
   - $S$ é«˜ã„ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒä¸€èˆ¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ï¼‰
   - **ä½ã‚¨ãƒãƒ«ã‚®ãƒ¼çŠ¶æ…‹**ï¼ˆä¸€èˆ¬åŒ–æ€§èƒ½é«˜ã„ï¼‰

**ç›¸è»¢ç§»ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ **:

è¨“ç·´åˆæœŸ:
- SGDãŒãƒ¡ãƒ¢ãƒªã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ç›¸ã«æ•ç²
- ã‚¨ãƒãƒ«ã‚®ãƒ¼éšœå£ãŒé«˜ãã€ä¸€èˆ¬åŒ–ç›¸ã«ç§»è¡Œã§ããªã„

é•·æ™‚é–“è¨“ç·´:
- ç¢ºç‡çš„ãƒã‚¤ã‚ºï¼ˆSGDã®ãƒ©ãƒ³ãƒ€ãƒ æ€§ï¼‰ãŒã‚¨ãƒãƒ«ã‚®ãƒ¼éšœå£ã‚’ä¹—ã‚Šè¶Šãˆã‚‹
- **ç›¸è»¢ç§»**: ãƒ¡ãƒ¢ãƒªã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ç›¸ â†’ ä¸€èˆ¬åŒ–ç›¸
- æ¤œè¨¼èª¤å·®ãŒçªç„¶ä½ä¸‹

**æ•°å¼**:

ã‚¨ãƒãƒ«ã‚®ãƒ¼éšœå£ã®é«˜ã• $\Delta F$:

$$
\Delta F = F_{\text{barrier}} - F_{\text{mem}}
$$

ç›¸è»¢ç§»ç¢ºç‡:

$$
P_{\text{transition}} \propto \exp(-\Delta F / T)
$$

$T$ ãŒé«˜ã„ï¼ˆå­¦ç¿’ç‡å¤§ï¼‰ã¾ãŸã¯ $\Delta F$ ãŒä½ã„ã»ã©ã€ç›¸è»¢ç§»ãŒæ—©ãèµ·ã“ã‚‹ã€‚

**å®Ÿé¨“çš„æ¤œè¨¼**:
- å­¦ç¿’ç‡ã‚’ä¸Šã’ã‚‹ â†’ GrokkingãŒæ—©ãèµ·ã“ã‚‹
- Weight Decayã‚’åŠ ãˆã‚‹ â†’ $\Delta F$ ã‚’ä¸‹ã’ã‚‹ â†’ GrokkingãŒæ—©ãèµ·ã“ã‚‹

#### 3.9.3 Isingæ¨¡å‹ã¨ã®æ¥ç¶š

**Isingæ¨¡å‹** â†” **ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ** å¯¾å¿œ:

| Isingæ¨¡å‹ | ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ |
|:----------|:---------------|
| ã‚¹ãƒ”ãƒ³ $s_i \in \{-1, +1\}$ | ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ´»æ€§ $h_i$ |
| ç›¸äº’ä½œç”¨ $J_{ij}$ | é‡ã¿ $W_{ij}$ |
| å¤–éƒ¨ç£å ´ $h_i$ | ãƒã‚¤ã‚¢ã‚¹ $b_i$ |
| æ¸©åº¦ $T$ | ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ« / å­¦ç¿’ç‡ |
| ç›¸è»¢ç§» | Grokking / å­¦ç¿’ã®ç›¸è»¢ç§» |

**Curieæ¸©åº¦** $T_c$:

$$
k_B T_c = J \cdot z
$$

ã“ã“ã§ $z$ ã¯é…ä½æ•°ï¼ˆæœ€è¿‘æ¥ã‚¹ãƒ”ãƒ³æ•°ï¼‰ã€‚

$T < T_c$: å¼·ç£æ€§ç›¸ï¼ˆå…¨ã‚¹ãƒ”ãƒ³ãŒæƒã†ï¼‰
$T > T_c$: å¸¸ç£æ€§ç›¸ï¼ˆã‚¹ãƒ”ãƒ³ãŒãƒ©ãƒ³ãƒ€ãƒ ï¼‰

**ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®ç›¸è»¢ç§»**:

$T_c$ ã«ç›¸å½“ã™ã‚‹ã€Œè‡¨ç•Œå­¦ç¿’ç‡ã€ãŒå­˜åœ¨:
- å­¦ç¿’ç‡ < $T_c$: éå­¦ç¿’ç›¸ï¼ˆãƒ¡ãƒ¢ãƒªã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
- å­¦ç¿’ç‡ > $T_c$: ä¸€èˆ¬åŒ–ç›¸

#### 3.9.4 è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®å¤‰åˆ†åŸç†

**Helmholtzè‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å°åŒ–åŸç†**:

ç†±å¹³è¡¡çŠ¶æ…‹ã§ã¯ã€è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ $F$ ãŒæœ€å°:

$$
F = \langle E \rangle - TS
$$

**å°å‡º**:

ã‚«ãƒãƒ‹ã‚«ãƒ«åˆ†å¸ƒ:

$$
p(x) = \frac{1}{Z} \exp(-\beta E(x))
$$

$$
Z = \sum_x \exp(-\beta E(x))
$$

å¹³å‡ã‚¨ãƒãƒ«ã‚®ãƒ¼:

$$
\langle E \rangle = \sum_x p(x) E(x)
$$

ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼:

$$
S = -\sum_x p(x) \log p(x)
$$

è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼:

$$
F = -k_B T \log Z
$$

$$
= \langle E \rangle - TS
$$

**å¤‰åˆ†åŸç†**:

ä»»æ„ã®åˆ†å¸ƒ $q(x)$ ã«å¯¾ã—ã€å¤‰åˆ†è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼:

$$
\mathcal{F}(q) = \sum_x q(x) E(x) + k_B T \sum_x q(x) \log q(x)
$$

**å®šç†**: $\mathcal{F}(q) \geq F$ï¼ˆç­‰å·æˆç«‹ â‡” $q = p$ï¼‰

**è¨¼æ˜**ï¼ˆæ—¢å‡ºã®å†æ²ï¼‰:

$$
\mathcal{F}(q) - F = D_{\text{KL}}(q \| p) \geq 0
$$

$\blacksquare$

**EBMè¨“ç·´ã¨ã®æ¥ç¶š**:

EBMè¨“ç·´ = $F(\theta)$ ã®æœ€å°åŒ– = å¤‰åˆ†è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®æœ€å°åŒ– = VAEã®ELBOæœ€å¤§åŒ–ã¨åŒä¸€åŸç†

### 3.10 Energy Matchingï¼ˆ2025ï¼‰â€” Flow Matching + EBMçµ±ä¸€ç†è«–

**è«–æ–‡**: [arXiv:2504.10612](https://arxiv.org/abs/2504.10612) "Energy Matching: Unifying Flow Matching and Energy-Based Models for Generative Modeling"

#### 3.10.1 å‹•æ©Ÿ

**Flow Matchingã®å•é¡Œ**:
- é«˜é€Ÿè¨“ç·´ãƒ»ç”Ÿæˆï¼ˆæ±ºå®šè«–çš„OTç›´ç·šè¼¸é€ï¼‰
- ã ãŒå¤šå³°çš„åˆ†å¸ƒã®è¡¨ç¾ãŒå›°é›£ï¼ˆç›´ç·šè¼¸é€ã¯å˜å³°çš„ã«åæŸã—ã‚„ã™ã„ï¼‰

**EBMã®å•é¡Œ**:
- è¡¨ç¾åŠ›ãŒæ¥µã‚ã¦é«˜ã„ï¼ˆä»»æ„ã®åˆ†å¸ƒï¼‰
- ã ãŒè¨“ç·´ãƒ»ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒå›°é›£ï¼ˆMCMCå¿…è¦ï¼‰

**Energy Matchingã®ææ¡ˆ**:
- ä¸¡è€…ã®åˆ©ç‚¹ã‚’çµ±åˆ
- é æ–¹ã§ã¯Flow Matchingï¼ˆé«˜é€Ÿè¼¸é€ï¼‰
- ãƒ‡ãƒ¼ã‚¿å¤šæ§˜ä½“è¿‘å‚ã§ã¯EBMï¼ˆå¤šå³°çš„è¡¨ç¾ï¼‰

#### 3.10.2 å®šå¼åŒ–

**æ™‚é–“ä¾å­˜ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°**:

$$
E(x, t) = E_{\text{transport}}(x, t) + \tau(t) \cdot E_{\text{entropic}}(x)
$$

ã“ã“ã§:
- $E_{\text{transport}}(x, t) = \frac{1}{2}\|x - \mu_t\|^2$: OTè¼¸é€é …ï¼ˆ$\mu_t = t x_{\text{data}} + (1-t)x_{\text{noise}}$ï¼‰
- $E_{\text{entropic}}(x) = -\log \rho(x)$: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒƒã‚¯é …ï¼ˆå­¦ç¿’ã•ã‚Œã‚‹ã‚¹ã‚«ãƒ©ãƒ¼å ´ï¼‰
- $\tau(t)$: æ¸©åº¦ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ$\tau(0) = 0$ã€$\tau(1) = 1$ï¼‰

**æ™‚é–“ä¾å­˜Gibbsåˆ†å¸ƒ**:

$$
p_t(x) = \frac{1}{Z_t} \exp(-E(x, t))
$$

#### 3.10.3 æ™‚é–“ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å½¹å‰²

**$t = 0$**ï¼ˆãƒã‚¤ã‚ºåˆ†å¸ƒï¼‰:

$$
E(x, 0) = \frac{1}{2}\|x - x_{\text{noise}}\|^2 + 0 \cdot E_{\text{entropic}}(x)
$$

â†’ ã‚¬ã‚¦ã‚¹ãƒã‚¤ã‚ºåˆ†å¸ƒï¼ˆå˜å³°çš„ï¼‰

**$t \in (0, 1)$**ï¼ˆè¼¸é€ä¸­ï¼‰:

$$
E(x, t) = \frac{1}{2}\|x - \mu_t\|^2 + \tau(t) \cdot E_{\text{entropic}}(x)
$$

- $\tau(t)$ å°: OTç›´ç·šè¼¸é€ãŒæ”¯é…çš„ï¼ˆé«˜é€Ÿç§»å‹•ï¼‰
- $\tau(t)$ å¤§: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒƒã‚¯é …ãŒå¯„ä¸ï¼ˆå¤šå³°æ€§å‡ºç¾ï¼‰

**$t = 1$**ï¼ˆãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒï¼‰:

$$
E(x, 1) = 0 + 1 \cdot E_{\text{entropic}}(x)
$$

â†’ ç´”ç²‹ãªEBMï¼ˆå¤šå³°çš„åˆ†å¸ƒï¼‰

#### 3.10.4 è¨“ç·´ç›®çš„é–¢æ•°

**Flow Matchingã¨ã®æ¥ç¶š**:

ãƒ™ã‚¯ãƒˆãƒ«å ´ $v_t(x)$:

$$
v_t(x) = -\nabla_x E(x, t)
$$

$$
= -(x - \mu_t) - \tau(t) \nabla_x E_{\text{entropic}}(x)
$$

**è¨“ç·´æå¤±** (Conditional Flow Matching):

$$
\mathcal{L}(\theta) = \mathbb{E}_{t, x_{\text{data}}, x_t} \left[\|v_\theta(x_t, t) - v_t^*(x_t)\|^2\right]
$$

ã“ã“ã§ $x_t = \mu_t + \epsilon$ã€$\epsilon \sim \mathcal{N}(0, \sigma_t^2 I)$ã€‚

**é‡è¦**: $E_{\text{entropic}}(x)$ ã¯**æ™‚é–“ç‹¬ç«‹**ã®ã‚¹ã‚«ãƒ©ãƒ¼å ´ã¨ã—ã¦å­¦ç¿’ã•ã‚Œã‚‹ã€‚

#### 3.10.5 ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

**Probability Flow ODE**:

$$
\frac{dx}{dt} = v_t(x)
$$

$$
= -(x - \mu_t) - \tau(t) \nabla_x E_{\text{entropic}}(x)
$$

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

```
åˆæœŸåŒ–: x â† N(0, I)  # ãƒã‚¤ã‚º
for t in [0, Îµ, 2Îµ, ..., 1]:
    # ãƒ™ã‚¯ãƒˆãƒ«å ´è¨ˆç®—
    v â† -(x - Î¼_t) - Ï„(t) âˆ‡E_entropic(x)
    # Euleræ³•
    x â† x + Îµ Â· v
end
return x
```

**ç‰¹å¾´**:
- $t = 0 \to 0.5$: é«˜é€ŸOTè¼¸é€ï¼ˆå¤§åŸŸç§»å‹•ï¼‰
- $t = 0.5 \to 1$: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒƒã‚¯é …ãŒåŠ¹ãï¼ˆå±€æ‰€èª¿æ•´ãƒ»å¤šå³°æ€§ï¼‰

#### 3.10.6 ç†è«–çš„ä¿è¨¼

**å®šç†** (Energy Matching, 2025):

æ¸©åº¦ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« $\tau(t)$ ãŒé©åˆ‡ãªã‚‰ã€$p_1(x) \approx p_{\text{data}}(x)$ ã¨ãªã‚‹ã€‚

**è¨¼æ˜ã®ã‚¹ã‚±ãƒƒãƒ**:

$t = 0$: $p_0(x) = \mathcal{N}(0, I)$ï¼ˆæ—¢çŸ¥ï¼‰

$t = 1$: $p_1(x) = \frac{1}{Z} \exp(-E_{\text{entropic}}(x))$ï¼ˆEBMï¼‰

Probability Flow ODEã¯ $p_t$ ã‚’ä¿å­˜ã™ã‚‹ã‚ˆã†ã«è¨­è¨ˆã•ã‚Œã¦ã„ã‚‹ï¼ˆé€£ç¶šæ€§æ–¹ç¨‹å¼ï¼‰:

$$
\frac{\partial p_t}{\partial t} + \nabla_x \cdot (p_t v_t) = 0
$$

ã—ãŸãŒã£ã¦ã€$p_1$ ã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã«è¿‘ã¥ã$\quad \blacksquare$

#### 3.10.7 å®Ÿé¨“çµæœï¼ˆ2025è«–æ–‡ã‚ˆã‚Šï¼‰

| ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ | ãƒ¢ãƒ‡ãƒ« | FID | ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚é–“ |
|:------------|:-------|:----|:----------------|
| CIFAR-10 | DDPM | 3.17 | 1000 steps (50s) |
| CIFAR-10 | Flow Matching | 3.92 | 100 steps (5s) |
| CIFAR-10 | **Energy Matching** | **2.84** | 100 steps (5s) |
| ImageNet 64x64 | EDM | 2.44 | 79 steps (4s) |
| ImageNet 64x64 | **Energy Matching** | **2.21** | 50 steps (2.5s) |

**çµè«–**: Energy Matching = Flow Matchingã®é€Ÿåº¦ + EBMã®è¡¨ç¾åŠ›

#### 3.10.8 å®Ÿè£…ã®ãƒã‚¤ãƒ³ãƒˆ

**$E_{\text{entropic}}(x)$ ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­è¨ˆ**:

U-Netãƒ™ãƒ¼ã‚¹ã®ã‚¹ã‚«ãƒ©ãƒ¼é–¢æ•°:

```python
class EnergyNet(nn.Module):
    def forward(self, x):
        # U-Netå‡¦ç†
        features = self.unet(x)
        # ã‚¹ã‚«ãƒ©ãƒ¼å‡ºåŠ›ï¼ˆglobal average poolingï¼‰
        energy = features.mean(dim=[2, 3])  # (B, C) â†’ (B,)
        return energy.mean()  # Batchå¹³å‡
```

**æ¸©åº¦ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«**:

$$
\tau(t) = \begin{cases}
0 & t < t_0 \\
\frac{t - t_0}{1 - t_0} & t \geq t_0
\end{cases}
$$

è«–æ–‡ã§ã¯ $t_0 = 0.5$ ãŒæ¨å¥¨ï¼ˆå‰åŠã¯OTã€å¾ŒåŠã¯EBMï¼‰ã€‚

### 3.11 Energy-based World Models â€” ç’°å¢ƒã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã¨ã—ã¦ã®EBM

**è«–æ–‡**: ç¬¬41å›ã§å®Œå…¨ç‰ˆã€‚æœ¬å›ã¯æ¦‚å¿µã®ã¿ã€‚

#### 3.11.1 World Modelsã®å®šç¾©

**World Model**: ç’°å¢ƒã® dynamics $p(s_{t+1} | s_t, a_t)$ ã‚’å­¦ç¿’ã™ã‚‹ãƒ¢ãƒ‡ãƒ«

- $s_t$: çŠ¶æ…‹ï¼ˆç”»åƒãƒ»ã‚»ãƒ³ã‚µãƒ¼å€¤ãªã©ï¼‰
- $a_t$: è¡Œå‹•
- $s_{t+1}$: æ¬¡çŠ¶æ…‹

#### 3.11.2 Energy-basedå®šå¼åŒ–

**ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°**:

$$
E(s_{t+1} | s_t, a_t) = -\log p(s_{t+1} | s_t, a_t)
$$

**æ¡ä»¶ä»˜ãGibbsåˆ†å¸ƒ**:

$$
p(s_{t+1} | s_t, a_t) = \frac{1}{Z(s_t, a_t)} \exp(-E(s_{t+1} | s_t, a_t))
$$

#### 3.11.3 ç‰©ç†æ³•å‰‡ã®çµ„ã¿è¾¼ã¿

**ç‰©ç†åˆ¶ç´„**:

ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°ã«ç‰©ç†æ³•å‰‡ã‚’ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰:

$$
E(s_{t+1} | s_t, a_t) = E_{\text{data}}(s_{t+1} | s_t, a_t) + \lambda E_{\text{physics}}(s_{t+1})
$$

ä¾‹: ä¿å­˜å‰‡

$$
E_{\text{physics}}(s_{t+1}) = \|E_{\text{kinetic}}(s_{t+1}) + E_{\text{potential}}(s_{t+1}) - E_{\text{total}}\|^2
$$

#### 3.11.4 å¤šå³°çš„æœªæ¥äºˆæ¸¬

**å¾“æ¥ã®æ±ºå®šè«–çš„World Model**:

$$
s_{t+1} = f_\theta(s_t, a_t)
$$

â†’ å˜ä¸€ã®æœªæ¥ã®ã¿äºˆæ¸¬

**Energy-based World Model**:

$$
p(s_{t+1} | s_t, a_t) = \frac{1}{Z} \exp(-E(s_{t+1} | s_t, a_t))
$$

â†’ è¤‡æ•°ã®æœªæ¥ã‚’ç¢ºç‡åˆ†å¸ƒã¨ã—ã¦è¡¨ç¾

**ä¾‹**: ãƒ­ãƒœãƒƒãƒˆã‚¢ãƒ¼ãƒ ã§ç‰©ä½“ã‚’æ´ã‚€
- æˆåŠŸãƒ‘ã‚¹ï¼ˆç¢ºç‡80%ï¼‰
- å¤±æ•—ãƒ‘ã‚¹1ï¼ˆç¢ºç‡10%ï¼‰: ç‰©ä½“ãŒæ»‘ã‚‹
- å¤±æ•—ãƒ‘ã‚¹2ï¼ˆç¢ºç‡10%ï¼‰: ç‰©ä½“ãŒè»¢ãŒã‚‹

EBMã¯3ã¤ã®ãƒ¢ãƒ¼ãƒ‰ã‚’å…¨ã¦è¡¨ç¾å¯èƒ½ã€‚

#### 3.11.5 JEPA / V-JPAã¨ã®æ¥ç¶š

**JEPA** (LeCun 2023): Joint-Embedding Predictive Architecture

**ã‚¨ãƒãƒ«ã‚®ãƒ¼è¦–ç‚¹ã§ã®å†è§£é‡ˆ**:

$$
E(z_{t+1}, z_t, a_t) = \|z_{t+1} - f_\theta(z_t, a_t)\|^2
$$

ã“ã“ã§ $z_t = \text{Encoder}(s_t)$ ã¯æ½œåœ¨è¡¨ç¾ã€‚

**V-JEPA**: Videoã§ã®JEPAå®Ÿè£… â†’ ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ™ãƒ¼ã‚¹é€£æƒ³è¨˜æ†¶ã¨ã—ã¦ç†è§£å¯èƒ½

#### 3.11.6 Transfusionã¨ã®æ¥ç¶š

**Transfusion** (2024): ARï¼ˆé›¢æ•£ï¼‰ + Diffusionï¼ˆé€£ç¶šï¼‰ã‚’çµ±ä¸€

**ã‚¨ãƒãƒ«ã‚®ãƒ¼è¦–ç‚¹**:

$$
E(x_{\text{image}}, x_{\text{text}}) = E_{\text{discrete}}(x_{\text{text}}) + E_{\text{continuous}}(x_{\text{image}})
$$

- $E_{\text{discrete}}$: è‡ªå·±å›å¸°ã®è² ã®å¯¾æ•°å°¤åº¦
- $E_{\text{continuous}}$: Diffusionã®ã‚¨ãƒãƒ«ã‚®ãƒ¼

â†’ ä¸¡è€…ã‚’å˜ä¸€ã®EBMã¨ã—ã¦çµ±ä¸€çš„ã«è¨“ç·´

---

:::message progress 50%
**ãƒœã‚¹æˆ¦ã‚¯ãƒªã‚¢ï¼** 800è¡Œã®EBMç†è«–ã‚’å®Œå…¨å°å‡ºã—ãŸã€‚Modern Hopfield â†” Attentionç­‰ä¾¡æ€§ã®è¨¼æ˜ã€RBM + CD-kã€MCMC/HMCã€çµ±è¨ˆç‰©ç†ã¨ã®æ¥ç¶šã‚’ç†è§£ã€‚æ¬¡ã¯å®Ÿè£…ã¸ã€‚
:::

---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” Juliaå®Ÿè£…ã§RBM + Modern Hopfield + MCMC

### 4.1 ç’°å¢ƒæ§‹ç¯‰

```julia
using Pkg
Pkg.add(["Lux", "Random", "Statistics", "Plots", "Distributions", "LinearAlgebra"])

using Lux, Random, Statistics, Plots, Distributions, LinearAlgebra
```

### 4.2 RBMå®Ÿè£…

#### 4.2.1 RBMãƒ‡ãƒ¼ã‚¿æ§‹é€ 

```julia
# RBMãƒ¢ãƒ‡ãƒ«å®šç¾©
# T: å‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆFloat32 or Float64ï¼‰
struct RBM{T}
    W::Matrix{T}  # é‡ã¿è¡Œåˆ— (n_visible Ã— n_hidden)
                   # æ•°å¼: W_{ij} â€” å¯è¦–å±¤ i ã¨éš ã‚Œå±¤ j ã®æ¥ç¶šå¼·åº¦
    b::Vector{T}  # å¯è¦–å±¤ãƒã‚¤ã‚¢ã‚¹ (n_visible,)
                   # æ•°å¼: b_i â€” å¯è¦–å±¤ãƒãƒ¼ãƒ‰ i ã®ãƒã‚¤ã‚¢ã‚¹
    c::Vector{T}  # éš ã‚Œå±¤ãƒã‚¤ã‚¢ã‚¹ (n_hidden,)
                   # æ•°å¼: c_j â€” éš ã‚Œå±¤ãƒãƒ¼ãƒ‰ j ã®ãƒã‚¤ã‚¢ã‚¹
end

# RBMåˆæœŸåŒ–é–¢æ•°
function RBM(n_visible::Int, n_hidden::Int; T=Float32)
    rng = Random.default_rng()
    # é‡ã¿ã‚’å°ã•ãªãƒ©ãƒ³ãƒ€ãƒ å€¤ã§åˆæœŸåŒ–
    # ç†ç”±: å¤§ããªåˆæœŸå€¤ã¯å­¦ç¿’ã‚’ä¸å®‰å®šã«ã™ã‚‹
    W = randn(rng, T, n_visible, n_hidden) .* T(0.01)
    # ãƒã‚¤ã‚¢ã‚¹ã¯0åˆæœŸåŒ–ï¼ˆæ¨™æº–çš„ãªæ…£ç¿’ï¼‰
    b = zeros(T, n_visible)
    c = zeros(T, n_hidden)
    RBM(W, b, c)
end
```

**æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œ**:
- `W[i, j]` â†” $W_{ij}$
- `b[i]` â†” $b_i$
- `c[j]` â†” $c_j$

#### 4.2.2 ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°

```julia
# ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•° E(v, h) = -v'Wh - b'v - c'h
function energy(rbm::RBM, v, h)
    # æ•°å¼: E(v, h) = -v^T W h - b^T v - c^T h
    # v: å¯è¦–å±¤ã®çŠ¶æ…‹ (n_visible,) or (n_visible, batch)
    # h: éš ã‚Œå±¤ã®çŠ¶æ…‹ (n_hidden,) or (n_hidden, batch)

    # ç¬¬1é …: -v^T W h
    term1 = v' * rbm.W * h
    # ç¬¬2é …: -b^T v
    term2 = rbm.b' * v
    # ç¬¬3é …: -c^T h
    term3 = rbm.c' * h

    # å…¨ã¦ã‚’åˆè¨ˆã—ã¦ç¬¦å·åè»¢
    return -(term1 + term2 + term3)
end
```

**æ•°å¼ç¢ºèª**:

$$
E(v, h) = -\sum_{i,j} W_{ij} v_i h_j - \sum_i b_i v_i - \sum_j c_j h_j
$$

$$
= -v^\top W h - b^\top v - c^\top h
$$

#### 4.2.3 æ¡ä»¶ä»˜ãã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

```julia
# æ¡ä»¶ä»˜ãç¢ºç‡ p(h_j = 1 | v) = Ïƒ(c_j + Î£_i W_ij v_i)
function sample_h_given_v(rbm::RBM, v)
    # æ•°å¼: p(h_j = 1 | v) = Ïƒ(c_j + Î£_i W_ij v_i)
    #                      = Ïƒ(c_j + (W^T v)_j)

    # ãƒ­ã‚¸ãƒƒãƒˆè¨ˆç®—: c + W' * v
    # W' ã¯ W ã®è»¢ç½® (n_hidden Ã— n_visible)
    # v ã¯ (n_visible, batch)
    # çµæœã¯ (n_hidden, batch)
    logits = rbm.c .+ rbm.W' * v

    # ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°é©ç”¨ â†’ ç¢ºç‡
    h_prob = sigmoid.(logits)

    # Bernoulliåˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    # å„ h_j ã‚’ç¢ºç‡ h_prob[j] ã§ 1ã€ç¢ºç‡ 1-h_prob[j] ã§ 0
    h_sample = rand.(Bernoulli.(h_prob))

    return h_sample, h_prob
end

# æ¡ä»¶ä»˜ãç¢ºç‡ p(v_i = 1 | h) = Ïƒ(b_i + Î£_j W_ij h_j)
function sample_v_given_h(rbm::RBM, h)
    # æ•°å¼: p(v_i = 1 | h) = Ïƒ(b_i + Î£_j W_ij h_j)
    #                      = Ïƒ(b_i + (W h)_i)

    # ãƒ­ã‚¸ãƒƒãƒˆè¨ˆç®—: b + W * h
    logits = rbm.b .+ rbm.W * h

    # ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°é©ç”¨
    v_prob = sigmoid.(logits)

    # Bernoulliåˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    v_sample = rand.(Bernoulli.(v_prob))

    return v_sample, v_prob
end
```

**æ•°å¼â†”ã‚³ãƒ¼ãƒ‰ç¢ºèª**:

| æ•°å¼ | Juliaå®Ÿè£… |
|:-----|:----------|
| $p(h_j=1\|v) = \sigma(c_j + \sum_i W_{ij} v_i)$ | `sigmoid.(rbm.c .+ rbm.W' * v)` |
| $p(v_i=1\|h) = \sigma(b_i + \sum_j W_{ij} h_j)$ | `sigmoid.(rbm.b .+ rbm.W * h)` |

**Broadcastæ¼”ç®—ã®å¨åŠ›**:

Juliaã® `.` (broadcast) ã«ã‚ˆã‚Šã€ãƒ™ã‚¯ãƒˆãƒ«æ¼”ç®—ãŒè‡ªå‹•ã§ãƒãƒƒãƒå‡¦ç†ã«æ‹¡å¼µã•ã‚Œã‚‹ã€‚

```julia
# å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«: v ã¯ (n_visible,)
h_prob = sigmoid.(rbm.c .+ rbm.W' * v)  # (n_hidden,)

# ãƒãƒƒãƒ: v ã¯ (n_visible, batch_size)
h_prob = sigmoid.(rbm.c .+ rbm.W' * v)  # (n_hidden, batch_size)
# rbm.c ã¯è‡ªå‹•ã§ (n_hidden, 1) â†’ (n_hidden, batch_size) ã«ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ
```

#### 4.2.4 Gibbs Sampling

```julia
# Gibbs Sampling (1 step)
function gibbs_step(rbm::RBM, v)
    # 1. h ã‚’ã‚µãƒ³ãƒ—ãƒ«: h ~ p(h | v)
    h, h_prob = sample_h_given_v(rbm, v)

    # 2. v ã‚’ã‚µãƒ³ãƒ—ãƒ«: v_new ~ p(v | h)
    v_new, v_prob = sample_v_given_h(rbm, h)

    # æˆ»ã‚Šå€¤:
    # v_new: æ–°ã—ã„å¯è¦–å±¤ã®çŠ¶æ…‹
    # h: ã‚µãƒ³ãƒ—ãƒ«ã•ã‚ŒãŸéš ã‚Œå±¤
    # v_prob: p(v_new | h) ã®ç¢ºç‡
    # h_prob: p(h | v) ã®ç¢ºç‡
    return v_new, h, v_prob, h_prob
end
```

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ç¢ºèª**:

Gibbs Samplingã¯ä»¥ä¸‹ã‚’äº¤äº’ã«å®Ÿè¡Œ:
1. $h^{(t)} \sim p(h | v^{(t)})$
2. $v^{(t+1)} \sim p(v | h^{(t)})$

ã“ã‚Œã‚’ç¹°ã‚Šè¿”ã™ã¨ã€$p(v, h)$ ã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒ«ãŒå¾—ã‚‰ã‚Œã‚‹ï¼ˆã‚¨ãƒ«ã‚´ãƒ¼ãƒ‰æ€§ï¼‰ã€‚

#### 4.2.5 Contrastive Divergence (CD-k)

```julia
# Contrastive Divergence (CD-k)
function cd_k(rbm::RBM, v_data; k=1, lr=0.01f0)
    # v_data: ãƒ‡ãƒ¼ã‚¿ã®ãƒŸãƒ‹ãƒãƒƒãƒ (n_visible, batch_size)
    batch_size = size(v_data, 2)

    # ========== æ­£ä¾‹ï¼ˆãƒ‡ãƒ¼ã‚¿ï¼‰ã®çµ±è¨ˆé‡ ==========
    # æ•°å¼: âŸ¨v_i h_jâŸ©_data = (1/N) Î£_n v_i^(n) p(h_j=1 | v^(n))
    h_pos, h_pos_prob = sample_h_given_v(rbm, v_data)

    # æ­£ä¾‹ã®å‹¾é…: v_data * h_pos_prob^T / batch_size
    # v_data: (n_visible, batch)
    # h_pos_prob^T: (batch, n_hidden)
    # çµæœ: (n_visible, n_hidden)
    pos_grad = v_data * h_pos_prob' ./ batch_size

    # ========== è² ä¾‹ï¼ˆãƒ¢ãƒ‡ãƒ«ï¼‰ã®çµ±è¨ˆé‡ ==========
    # k-step Gibbs Sampling
    v_neg = copy(v_data)  # ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åˆæœŸåŒ–ï¼ˆCD-kã®ç‰¹å¾´ï¼‰
    for _ in 1:k
        v_neg, h_neg, _, _ = gibbs_step(rbm, v_neg)
    end

    # è² ä¾‹ã®éš ã‚Œå±¤ç¢ºç‡
    h_neg, h_neg_prob = sample_h_given_v(rbm, v_neg)

    # è² ä¾‹ã®å‹¾é…
    neg_grad = v_neg * h_neg_prob' ./ batch_size

    # ========== å‹¾é…æ›´æ–° ==========
    # æ•°å¼: Î”W_ij = Î· (âŸ¨v_i h_jâŸ©_data - âŸ¨v_i h_jâŸ©_model)
    Î”W = lr .* (pos_grad .- neg_grad)

    # ãƒã‚¤ã‚¢ã‚¹ã®å‹¾é…
    # æ•°å¼: Î”b_i = Î· (âŸ¨v_iâŸ©_data - âŸ¨v_iâŸ©_model)
    Î”b = lr .* mean(v_data .- v_neg, dims=2)[:]

    # æ•°å¼: Î”c_j = Î· (âŸ¨h_jâŸ©_data - âŸ¨h_jâŸ©_model)
    Î”c = lr .* mean(h_pos_prob .- h_neg_prob, dims=2)[:]

    # æ–°ã—ã„RBMã‚’è¿”ã™ï¼ˆé–¢æ•°å‹ã‚¹ã‚¿ã‚¤ãƒ«ï¼‰
    return RBM(rbm.W .+ Î”W, rbm.b .+ Î”b, rbm.c .+ Î”c)
end
```

**CD-kã®ç†è«–**:

å®Œå…¨ãªå‹¾é…:

$$
\frac{\partial \log p(v)}{\partial W_{ij}} = \mathbb{E}_{p(h|v_{\text{data}})} [v_i h_j] - \mathbb{E}_{p(v, h)} [v_i h_j]
$$

- **ç¬¬1é …**: ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¨ˆç®—å¯èƒ½ï¼ˆé«˜é€Ÿï¼‰
- **ç¬¬2é …**: $p(v, h)$ ã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒå¿…è¦ï¼ˆå›°é›£ï¼‰

CD-kè¿‘ä¼¼:

$$
\mathbb{E}_{p(v, h)} [v_i h_j] \approx \mathbb{E}_{p(v^{(k)}, h^{(k)})} [v_i h_j]
$$

ã“ã“ã§ $v^{(k)}$ ã¯ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ $k$ stepã®Gibbs Samplingã€‚

**k=1ã®æ„å‘³**:
- 1å›ã ã‘Gibbs â†’ è² ä¾‹ã¯ãƒ‡ãƒ¼ã‚¿è¿‘å‚
- ç†è«–çš„ã«ã¯ãƒã‚¤ã‚¢ã‚¹ã‚ã‚Š
- å®Ÿç”¨ä¸Šã¯ååˆ†æ©Ÿèƒ½ï¼ˆHinton 2002ï¼‰

#### 4.2.6 RBMè¨“ç·´ãƒ«ãƒ¼ãƒ—

```julia
# RBMè¨“ç·´ãƒ«ãƒ¼ãƒ—
function train_rbm(rbm, data; epochs=10, k=1, lr=0.01f0, batch_size=32)
    # data: å…¨è¨“ç·´ãƒ‡ãƒ¼ã‚¿ (n_visible, n_samples)
    n_samples = size(data, 2)

    for epoch in 1:epochs
        # ãƒŸãƒ‹ãƒãƒƒãƒã‚·ãƒ£ãƒƒãƒ•ãƒ«
        indices = shuffle(1:n_samples)

        # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’1å›èµ°æŸ»ï¼ˆ1 epochï¼‰
        for i in 1:batch_size:n_samples
            # ãƒŸãƒ‹ãƒãƒƒãƒæŠ½å‡º
            batch_idx = indices[i:min(i+batch_size-1, n_samples)]
            batch = data[:, batch_idx]

            # CD-kæ›´æ–°
            rbm = cd_k(rbm, batch; k=k, lr=lr)
        end

        # ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚ã®è©•ä¾¡
        # ãƒ©ãƒ³ãƒ€ãƒ ãªã‚µãƒ³ãƒ—ãƒ«ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’è¨ˆç®—
        v_sample = data[:, rand(1:n_samples)]
        h_sample, _ = sample_h_given_v(rbm, v_sample)
        E = energy(rbm, v_sample, h_sample)

        println("Epoch $epoch: Energy = $E")
        # ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒä¸‹ãŒã‚‹ â†’ å­¦ç¿’ãŒé€²ã‚“ã§ã„ã‚‹
    end

    return rbm
end
```

**è¨“ç·´ãƒ«ãƒ¼ãƒ—ã®è¨­è¨ˆãƒã‚¤ãƒ³ãƒˆ**:

1. **Epoch**: å…¨ãƒ‡ãƒ¼ã‚¿ã‚’1å›èµ°æŸ»
2. **Shuffle**: æ¯epochã§ãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ« â†’ SGDã®ãƒ©ãƒ³ãƒ€ãƒ æ€§
3. **Minibatch**: ãƒŸãƒ‹ãƒãƒƒãƒå˜ä½ã§æ›´æ–° â†’ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ + ä¸¦åˆ—åŒ–
4. **è©•ä¾¡**: ã‚¨ãƒãƒ«ã‚®ãƒ¼ç›£è¦– â†’ å­¦ç¿’ã®åæŸç¢ºèª

**ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®è§£é‡ˆ**:

- ã‚¨ãƒãƒ«ã‚®ãƒ¼ä½ã„ â†’ ãã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç¢ºç‡ãŒé«˜ã„
- è¨“ç·´ãŒé€²ã‚€ã¨ã€ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒä¸‹ãŒã‚‹ â†’ ãƒ¢ãƒ‡ãƒ«ãŒãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã«é©åˆ
```

### 4.3 Modern Hopfieldå®Ÿè£…

#### 4.3.1 Modern Hopfieldãƒ‡ãƒ¼ã‚¿æ§‹é€ 

```julia
# Modern Hopfield Network
# T: å‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆFloat32 or Float64ï¼‰
struct ModernHopfield{T}
    X::Matrix{T}  # è¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³è¡Œåˆ— (d Ã— M)
                   # X = [Î¾Â¹, Î¾Â², ..., Î¾á´¹]
                   # d: ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¬¡å…ƒ
                   # M: è¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°
    Î²::T  # é€†æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆÎ² > 0ï¼‰
          # Î²å¤§ â†’ é‹­ã„æ¤œç´¢ï¼ˆæœ€è¿‘æ¥ã®ã¿ï¼‰
          # Î²å° â†’ å¹³æ»‘ãªæ¤œç´¢ï¼ˆè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ··åˆï¼‰
end

# ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿
function ModernHopfield(patterns::Matrix{T}; Î²=1.0f0) where T
    # patterns: è¨˜æ†¶ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¡Œåˆ— (d Ã— M)
    ModernHopfield(patterns, T(Î²))
end
```

**æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œ**:
- `X[:, i]` â†” $\xi^i$ ï¼ˆç¬¬ $i$ ç•ªç›®ã®è¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
- `Î²` â†” $\beta$ ï¼ˆé€†æ¸©åº¦ï¼‰

#### 4.3.2 ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°

```julia
# ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•° E(x) = -lse(Î² X'x) + 0.5||x||^2
function energy(hopfield::ModernHopfield, x)
    # æ•°å¼: E(x) = -log Î£_i exp(Î² âŸ¨x, Î¾^iâŸ©) + (1/2)||x||^2

    # ã‚¹ãƒ†ãƒƒãƒ—1: å†…ç©è¨ˆç®— X' * x
    # X: (d Ã— M)
    # x: (d,) ã¾ãŸã¯ (d, batch)
    # X' * x: (M,) ã¾ãŸã¯ (M, batch)
    # ã“ã‚Œã¯ âŸ¨x, Î¾^iâŸ© ã‚’å…¨ã¦ã® i ã«ã¤ã„ã¦è¨ˆç®—
    inner_products = hopfield.X' * x

    # ã‚¹ãƒ†ãƒƒãƒ—2: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° Î² âŸ¨x, Î¾^iâŸ©
    logits = hopfield.Î² .* inner_products

    # ã‚¹ãƒ†ãƒƒãƒ—3: log-sum-exp(logits)
    # lse(z) = log Î£_i exp(z_i)
    # æ•°å€¤å®‰å®šç‰ˆã®å®Ÿè£…ï¼ˆmax-trickä½¿ç”¨ï¼‰
    lse_term = logsumexp(logits)

    # ã‚¹ãƒ†ãƒƒãƒ—4: æ­£å‰‡åŒ–é … (1/2)||x||^2
    reg_term = 0.5f0 * sum(abs2, x)

    # å…¨ä½“ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼
    return -lse_term + reg_term
end
```

**log-sum-expã®æ•°å€¤å®‰å®šæ€§**:

$$
\text{lse}(z) = \log \sum_i \exp(z_i)
$$

Naiveå®Ÿè£…: $\exp(z_i)$ ãŒå¤§ãã„ã¨ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼

å®‰å®šç‰ˆï¼ˆmax-trickï¼‰:

$$
\text{lse}(z) = \max(z) + \log \sum_i \exp(z_i - \max(z))
$$

Juliaã® `logsumexp` ã¯è‡ªå‹•ã§å®‰å®šç‰ˆã‚’ä½¿ç”¨ã€‚

**ã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å°åŒ– = ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢**:

$E(x)$ ã‚’æœ€å°åŒ–ã™ã‚‹ $x$ ã¯ã€è¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³ $\{\xi^i\}$ ã®ä¸­ã§æœ€ã‚‚è¿‘ã„ã‚‚ã®ã«å¯¾å¿œã€‚

#### 4.3.3 Update Rule

```julia
# Update Rule: x^{t+1} = X softmax(Î² X'x^t)
function update(hopfield::ModernHopfield, x)
    # æ•°å¼: x^{t+1} = Î£_i softmax_i(Î² X'x^t) Î¾^i
    #              = X softmax(Î² X'x^t)

    # ã‚¹ãƒ†ãƒƒãƒ—1: å†…ç©è¨ˆç®—
    inner_products = hopfield.X' * x  # (M,) or (M, batch)

    # ã‚¹ãƒ†ãƒƒãƒ—2: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° + softmax
    logits = hopfield.Î² .* inner_products
    weights = softmax(logits)  # (M,) or (M, batch)

    # ã‚¹ãƒ†ãƒƒãƒ—3: é‡ã¿ä»˜ãå’Œ
    # X: (d Ã— M)
    # weights: (M,) or (M, batch)
    # X * weights: (d,) or (d, batch)
    return hopfield.X * weights
end
```

**æ•°å¼ç¢ºèª**:

$$
x^{t+1} = \sum_{i=1}^M \frac{\exp(\beta \langle x^t, \xi^i \rangle)}{\sum_j \exp(\beta \langle x^t, \xi^j \rangle)} \xi^i
$$

$$
= \sum_{i=1}^M \text{softmax}_i(\beta X^\top x^t) \xi^i
$$

$$
= X \cdot \text{softmax}(\beta X^\top x^t)
$$

**Softmaxã®å½¹å‰²**:

- $\beta$ å¤§ â†’ softmaxé‹­ã„ â†’ æœ€è¿‘æ¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿é¸æŠ
- $\beta$ å° â†’ softmaxå¹³å¦ â†’ è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ··åˆ

#### 4.3.4 åæŸåˆ¤å®šä»˜ãRetrieve

```julia
# åæŸã¾ã§update
function retrieve(hopfield::ModernHopfield, x_init; max_iters=10, tol=1e-6)
    # x_init: åˆæœŸã‚¯ã‚¨ãƒªï¼ˆãƒã‚¤ã‚ºä»˜ããƒ‘ã‚¿ãƒ¼ãƒ³ãªã©ï¼‰
    # max_iters: æœ€å¤§åå¾©æ•°
    # tol: åæŸåˆ¤å®šã®é–¾å€¤

    x = copy(x_init)

    for t in 1:max_iters
        # 1ã‚¹ãƒ†ãƒƒãƒ—æ›´æ–°
        x_new = update(hopfield, x)

        # åæŸåˆ¤å®š: ||x_new - x|| < tol
        if norm(x_new - x) < tol
            println("Converged at iteration $t")
            break
        end

        # æ¬¡ã®åå¾©ã¸
        x = x_new
    end

    return x
end
```

**åæŸæ€§ã®ç†è«–**:

Modern Hopfieldã®å®šç†ï¼ˆRamsauer+ 2020ï¼‰:
- **1å›æ›´æ–°ã§åæŸ**: $\beta = d$ ã®ã¨ãã€1å›ã®æ›´æ–°ã§æœ€è¿‘æ¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åæŸ
- **æŒ‡æ•°çš„ç²¾åº¦**: æ¤œç´¢èª¤å·® $\|x^* - \xi^{\mu^*}\| \lesssim \exp(-d)$

å®Ÿè£…ã§ã¯å®‰å…¨ã®ãŸã‚ `max_iters=10` è¨­å®šã€ã ãŒé€šå¸¸1-2å›ã§åæŸã€‚

#### 4.3.5 Attentionç­‰ä¾¡æ€§ã®å®Ÿè¨¼

```julia
# Modern Hopfield â†” Attentionç­‰ä¾¡æ€§ã®å®Ÿè¨¼
function attention_equivalent(hopfield::ModernHopfield, x_query)
    # Self-Attention: Attention(Q, K, V) = V softmax(K^T Q / âˆšd)
    # Modern Hopfield: x^{t+1} = X softmax(Î² X^T x^t)

    # å¯¾å¿œé–¢ä¿‚:
    # Q = x_query ï¼ˆã‚¯ã‚¨ãƒªï¼‰
    # K = X ï¼ˆã‚­ãƒ¼ = è¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    # V = X ï¼ˆãƒãƒªãƒ¥ãƒ¼ = è¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    # Î² = 1/âˆšd ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°ï¼‰

    d = size(hopfield.X, 1)  # æ¬¡å…ƒ

    # Attentionè¨ˆç®—
    # logits = K^T Q / âˆšd = X^T x_query / âˆšd
    logits = (hopfield.X' * x_query) ./ sqrt(d)

    # Softmax
    weights = softmax(logits)

    # é‡ã¿ä»˜ãå’Œ: V * weights = X * weights
    return hopfield.X * weights
end
```

**ç­‰ä¾¡æ€§ã®ç¢ºèª**:

Modern Hopfieldã§ $\beta = 1/\sqrt{d}$ ã¨ã™ã‚‹ã¨:

$$
x^{t+1} = X \cdot \text{softmax}\left(\frac{X^\top x^t}{\sqrt{d}}\right)
$$

ã“ã‚Œã¯ Self-Attention:

$$
\text{Attention}(Q, K, V) = V \cdot \text{softmax}\left(\frac{K^\top Q}{\sqrt{d}}\right)
$$

ã¨å®Œå…¨ã«ä¸€è‡´ï¼ˆ$Q = x^t$ã€$K = V = X$ï¼‰ã€‚

**ã‚³ãƒ¼ãƒ‰å®Ÿé¨“**:

```julia
# å®Ÿé¨“: Modern Hopfield vs Attention
d, M = 20, 10
patterns = randn(Float32, d, M)
x_query = randn(Float32, d)

hopfield = ModernHopfield(patterns; Î²=1.0f0/sqrt(d))

# Modern Hopfieldæ›´æ–°
x_hopfield = update(hopfield, x_query)

# Attentionç­‰ä¾¡è¨ˆç®—
x_attention = attention_equivalent(hopfield, x_query)

# å·®ã®ç¢ºèª
println("Difference: $(norm(x_hopfield - x_attention))")
# Difference: 0.0f0 ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰
```

### 4.4 MCMCã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè£…

MCMCï¼ˆMarkov Chain Monte Carloï¼‰ã¯ã€EBMã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ãŸã‚ã®åŸºç¤ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€‚

**ç†è«–èƒŒæ™¯**:
- **ç›®æ¨™**: ç¢ºç‡åˆ†å¸ƒ $p(x)$ ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆ
- **å•é¡Œ**: $p(x) = \frac{1}{Z} \exp(-E(x))$ ã ãŒ $Z$ ãŒè¨ˆç®—å›°é›£
- **è§£æ±º**: è©³ç´°é‡£ã‚Šåˆã„æ¡ä»¶ã‚’æº€ãŸã™ãƒãƒ«ã‚³ãƒ•é€£é–ã‚’æ§‹ç¯‰ â†’ å®šå¸¸åˆ†å¸ƒãŒ $p(x)$ ã«ãªã‚‹

#### 4.4.1 Metropolis-Hastings

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:
1. ææ¡ˆåˆ†å¸ƒ $q(x' | x)$ ã‹ã‚‰å€™è£œ $x'$ ã‚’ç”Ÿæˆ
2. å—ç†ç¢ºç‡ $\alpha = \min(1, \frac{p(x') q(x|x')}{p(x) q(x'|x)})$ ã§å—ç†ãƒ»æ£„å´
3. $x_{t+1} = x'$ ï¼ˆå—ç†ï¼‰ã¾ãŸã¯ $x_{t+1} = x_t$ ï¼ˆæ£„å´ï¼‰

```julia
# Metropolis-Hastings Algorithm
# target_log_prob: log p(x) ã‚’è¿”ã™é–¢æ•°ï¼ˆZã¯ä¸è¦ï¼ï¼‰
# x_init: åˆæœŸçŠ¶æ…‹
# proposal_std: ææ¡ˆåˆ†å¸ƒã®æ¨™æº–åå·®ï¼ˆãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
function metropolis_hastings(target_log_prob, x_init; n_samples=1000, proposal_std=0.1f0)
    d = length(x_init)

    # ã‚µãƒ³ãƒ—ãƒ«ä¿å­˜ç”¨ã®ãƒãƒƒãƒ•ã‚¡
    samples = zeros(Float32, d, n_samples)

    # ç¾åœ¨ã®çŠ¶æ…‹
    x = copy(x_init)
    log_p_x = target_log_prob(x)  # log p(x) ã‚’è¨ˆç®—ï¼ˆZã¯ç›¸æ®ºã•ã‚Œã‚‹ï¼‰

    n_accept = 0  # å—ç†å›æ•°ã‚«ã‚¦ãƒ³ã‚¿

    for i in 1:n_samples
        # ========== ã‚¹ãƒ†ãƒƒãƒ—1: ææ¡ˆ ==========
        # ææ¡ˆåˆ†å¸ƒ: q(x' | x) = N(x, proposal_std^2 I)
        # ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ææ¡ˆï¼ˆå¯¾ç§°çš„: q(x'|x) = q(x|x')ï¼‰
        x_prop = x .+ proposal_std .* randn(Float32, d)
        log_p_prop = target_log_prob(x_prop)

        # ========== ã‚¹ãƒ†ãƒƒãƒ—2: å—ç†ãƒ»æ£„å´ ==========
        # å—ç†ç¢ºç‡: Î± = min(1, p(x')/p(x))
        # logç©ºé–“ã§è¨ˆç®—: log Î± = log p(x') - log p(x)
        # å¯¾ç§°çš„ææ¡ˆãªã®ã§ q(x'|x) = q(x|x') â†’ ç›¸æ®º
        log_Î± = log_p_prop - log_p_x

        # å—ç†åˆ¤å®š: u ~ Uniform(0, 1) ã¨ã—ã¦ log(u) < log Î± ãªã‚‰ã°å—ç†
        if log(rand()) < log_Î±
            # å—ç†: æ–°ã—ã„çŠ¶æ…‹ã«é·ç§»
            x = x_prop
            log_p_x = log_p_prop
            n_accept += 1
        # æ£„å´ã®å ´åˆ: x ã¯å¤‰ã‚ã‚‰ãšï¼ˆç¾åœ¨ã®çŠ¶æ…‹ã‚’å†åº¦ã‚µãƒ³ãƒ—ãƒ«ï¼‰
        end

        # ========== ã‚¹ãƒ†ãƒƒãƒ—3: ã‚µãƒ³ãƒ—ãƒ«ä¿å­˜ ==========
        # ãƒãƒ¼ãƒ³ã‚¤ãƒ³å¾Œã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ä¿å­˜
        samples[:, i] = x
    end

    # å—ç†ç‡: ç†æƒ³ã¯ 0.2-0.5ï¼ˆé«˜æ¬¡å…ƒã§ã¯ä½ä¸‹ï¼‰
    acceptance_rate = n_accept / n_samples
    println("Acceptance rate: $acceptance_rate")
    # proposal_std ãŒå¤§ãã™ãã‚‹ã¨å—ç†ç‡ä½ä¸‹
    # proposal_std ãŒå°ã•ã™ãã‚‹ã¨æ¢ç´¢ãŒé…ã„

    return samples
end
```

**æ•°å¼â†”ã‚³ãƒ¼ãƒ‰ç¢ºèª**:

| æ•°å¼ | Juliaå®Ÿè£… |
|:-----|:----------|
| $\alpha = \min(1, \frac{p(x')}{p(x)})$ | `log_Î± = log_p_prop - log_p_x` |
| $u \sim \text{Uniform}(0, 1)$ | `rand()` |
| $\log u < \log \alpha$ ãªã‚‰ã°å—ç† | `if log(rand()) < log_Î±` |

**è©³ç´°é‡£ã‚Šåˆã„æ¡ä»¶ã®æº€è¶³**:

$$
p(x) q(x' | x) \alpha(x \to x') = p(x') q(x | x') \alpha(x' \to x)
$$

ã“ã‚ŒãŒæˆã‚Šç«‹ã¤ â†’ å®šå¸¸åˆ†å¸ƒãŒ $p(x)$ ã«ãªã‚‹ï¼ˆãƒãƒ«ã‚³ãƒ•é€£é–ã®ç†è«–ï¼‰ã€‚

#### 4.4.2 Hamiltonian Monte Carlo (HMC)

**ç‰©ç†çš„ç›´è¦³**:
- ä½ç½® $x$ ã¨é‹å‹•é‡ $p$ ã‚’å°å…¥
- ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³: $H(x, p) = U(x) + K(p)$
  - $U(x) = -\log p(x)$: ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã‚¨ãƒãƒ«ã‚®ãƒ¼
  - $K(p) = \frac{1}{2}p^\top p$: é‹å‹•ã‚¨ãƒãƒ«ã‚®ãƒ¼
- ãƒãƒŸãƒ«ãƒˆãƒ³æ–¹ç¨‹å¼ã§æ™‚é–“ç™ºå±• â†’ ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜

**åˆ©ç‚¹**:
- å‹¾é… $\nabla U(x)$ ã‚’ä½¿ã† â†’ åŠ¹ç‡çš„æ¢ç´¢
- ææ¡ˆãŒé ãã¾ã§é£›ã¶ â†’ å—ç†ç‡é«˜ã„ï¼ˆtypical: 0.65-0.95ï¼‰

```julia
# Hamiltonian Monte Carlo Algorithm
# U: ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã‚¨ãƒãƒ«ã‚®ãƒ¼ U(x) = -log p(x) + const
# âˆ‡U: ãã®å‹¾é… âˆ‡U(x)
# L: Leapfrogç©åˆ†ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°
# Îµ: Leapfrogç©åˆ†ã®æ™‚é–“åˆ»ã¿å¹…
function hmc(U, âˆ‡U, x_init; n_samples=1000, L=10, Îµ=0.01f0)
    d = length(x_init)
    samples = zeros(Float32, d, n_samples)
    x = copy(x_init)

    n_accept = 0

    for i in 1:n_samples
        # ========== ã‚¹ãƒ†ãƒƒãƒ—1: é‹å‹•é‡ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° ==========
        # p ~ N(0, I) ï¼ˆã‚¬ã‚¦ã‚¹åˆ†å¸ƒï¼‰
        # é‹å‹•ã‚¨ãƒãƒ«ã‚®ãƒ¼: K(p) = (1/2) p^T p
        p = randn(Float32, d)

        # ç¾åœ¨ã®ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³
        # H(x, p) = U(x) + (1/2)||p||^2
        H_current = U(x) + 0.5f0 * sum(abs2, p)

        # ========== ã‚¹ãƒ†ãƒƒãƒ—2: Leapfrogç©åˆ† ==========
        # ãƒãƒŸãƒ«ãƒˆãƒ³æ–¹ç¨‹å¼:
        #   dx/dt = âˆ‚H/âˆ‚p = p
        #   dp/dt = -âˆ‚H/âˆ‚x = -âˆ‡U(x)
        # Symplecticç©åˆ†å™¨ï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜ãŒè‰¯ã„ï¼‰

        x_new, p_new = x, p

        # Half-step for momentum (åˆæœŸ)
        # p_{1/2} = p_0 - (Îµ/2) âˆ‡U(x_0)
        p_new = p_new .- (Îµ/2) .* âˆ‡U(x_new)

        # Full-steps: Lå›ç¹°ã‚Šè¿”ã—
        for step in 1:L
            # Full-step for position
            # x_{t+1} = x_t + Îµ p_{t+1/2}
            x_new = x_new .+ Îµ .* p_new

            # Full-step for momentum (æœ€å¾Œä»¥å¤–)
            # p_{t+3/2} = p_{t+1/2} - Îµ âˆ‡U(x_{t+1})
            if step < L  # æœ€å¾Œã®ã‚¹ãƒ†ãƒƒãƒ—ã¯ä¸‹ã§å‡¦ç†
                p_new = p_new .- Îµ .* âˆ‡U(x_new)
            end
        end

        # Half-step for momentum (æœ€çµ‚)
        # p_L = p_{L-1/2} - (Îµ/2) âˆ‡U(x_L)
        p_new = p_new .- (Îµ/2) .* âˆ‡U(x_new)

        # ========== ã‚¹ãƒ†ãƒƒãƒ—3: Metropoliså—ç†ãƒ»æ£„å´ ==========
        # æ–°ã—ã„ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³
        H_new = U(x_new) + 0.5f0 * sum(abs2, p_new)

        # å—ç†ç¢ºç‡: Î± = min(1, exp(H_current - H_new))
        # Leapfrogç©åˆ†ãŒå®Œå…¨ãªã‚‰ H_new â‰ˆ H_current â†’ Î± â‰ˆ 1
        # æ•°å€¤èª¤å·®ã«ã‚ˆã‚Š H ãŒå¤‰å‹• â†’ Metropolisè£œæ­£ã§èª¿æ•´
        if log(rand()) < H_current - H_new
            # å—ç†: æ–°ã—ã„ä½ç½®ã«ç§»å‹•
            x = x_new
            n_accept += 1
        # æ£„å´: å…ƒã®ä½ç½®ã‚’ä¿æŒï¼ˆé‹å‹•é‡ã¯æ¨ã¦ã‚‹ï¼‰
        end

        # ========== ã‚¹ãƒ†ãƒƒãƒ—4: ã‚µãƒ³ãƒ—ãƒ«ä¿å­˜ ==========
        samples[:, i] = x
    end

    # å—ç†ç‡: HMCã¯é«˜ã„ï¼ˆ0.65-0.95ãŒå…¸å‹ï¼‰
    acceptance_rate = n_accept / n_samples
    println("Acceptance rate: $acceptance_rate")
    # Îµ, L ã®èª¿æ•´ãŒé‡è¦:
    # - Îµ å¤§ â†’ æ•°å€¤èª¤å·®å¤§ â†’ å—ç†ç‡ä½ä¸‹
    # - Îµ å° â†’ L å¤§å¿…è¦ â†’ è¨ˆç®—ã‚³ã‚¹ãƒˆå¢—
    # - L å¤§ â†’ é ãã¾ã§æ¢ç´¢ â†’ åŠ¹ç‡çš„

    return samples
end
```

**Leapfrogç©åˆ†ã®è©³ç´°**:

1. **Half-step**: $p_{1/2} = p_0 - \frac{\varepsilon}{2} \nabla U(x_0)$
2. **Full-steps** ($L$ å›):
   - $x_{t+1} = x_t + \varepsilon p_{t+1/2}$
   - $p_{t+3/2} = p_{t+1/2} - \varepsilon \nabla U(x_{t+1})$
3. **Final half-step**: $p_L = p_{L-1/2} - \frac{\varepsilon}{2} \nabla U(x_L)$

**Symplecticæ€§**:
- Leapfrogã¯ symplecticç©åˆ† â†’ ä½ç›¸ç©ºé–“ã®ä½“ç©ä¿å­˜
- ã‚¨ãƒãƒ«ã‚®ãƒ¼èª¤å·®ãŒæœ‰ç•Œ â†’ é•·æ™‚é–“ç©åˆ†ã§ã‚‚å®‰å®š

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é¸æŠ**:
- **$\varepsilon$ (step size)**: å°ã•ã„ â†’ ç²¾åº¦é«˜ã„ã€é…ã„
- **$L$ (num steps)**: å¤§ãã„ â†’ é è·é›¢æ¢ç´¢ã€å‹¾é…è¨ˆç®—ã‚³ã‚¹ãƒˆå¢—
- **è‡ªå‹•èª¿æ•´**: NUTS (No-U-Turn Sampler) ãŒè‡ªå‹•ã§ $L$ ã‚’é©å¿œèª¿æ•´

**HMC vs Metropolis-Hastings**:

| æ‰‹æ³• | å‹¾é…ä½¿ç”¨ | å—ç†ç‡ | åŠ¹ç‡ | é©ç”¨ç¯„å›² |
|:-----|:---------|:-------|:-----|:---------|
| MH | âŒ | ä½ï¼ˆé«˜æ¬¡å…ƒã§0.01ä»¥ä¸‹ã‚‚ï¼‰ | ä½ | æ±ç”¨ |
| HMC | âœ… | é«˜ï¼ˆ0.65-0.95ï¼‰ | é«˜ | å¾®åˆ†å¯èƒ½åˆ†å¸ƒ |

**å®Ÿç”¨ä¸Šã®æ³¨æ„**:
- HMCã¯ $\nabla U(x)$ ã®è¨ˆç®—ã‚³ã‚¹ãƒˆæ¬¡ç¬¬
- è‡ªå‹•å¾®åˆ†ï¼ˆZygote.jlï¼‰ã§å‹¾é…å–å¾—ãŒå®¹æ˜“ â†’ HMCæ¨å¥¨
- è¤‡é›‘ãªåˆ†å¸ƒï¼ˆå¤šå³°æ€§ï¼‰ã§ã¯ warmup/tuning ãŒé‡è¦

### 4.5 æ¼”ç¿’: RBM + Modern Hopfield + MCMCå¯è¦–åŒ–

```julia
# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆ2D Gaussian Mixtureï¼‰
n_samples = 1000
data = vcat(
    randn(Float32, 2, n_samplesÃ·2) .+ [2.0f0; 2.0f0],
    randn(Float32, 2, n_samplesÃ·2) .- [2.0f0; 2.0f0]
)

# RBMè¨“ç·´
rbm = RBM(2, 10)
rbm = train_rbm(rbm, data; epochs=20, k=1, lr=0.01f0, batch_size=32)

# Modern Hopfieldè¨“ç·´
patterns = data[:, 1:10:100]  # 10ãƒ‘ã‚¿ãƒ¼ãƒ³è¨˜æ†¶
hopfield = ModernHopfield(patterns; Î²=1.0f0)

# é€£æƒ³è¨˜æ†¶ãƒ†ã‚¹ãƒˆ
x_init = patterns[:, 1] .+ 0.5f0 .* randn(Float32, 2)
x_retrieved = retrieve(hopfield, x_init)
println("Initial: $x_init")
println("Retrieved: $x_retrieved")
println("Target: $(patterns[:, 1])")

# MCMCå¯è¦–åŒ–
target_log_prob(x) = -0.5f0 * norm(x)^2  # ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ
samples_mh = metropolis_hastings(target_log_prob, [0.0f0, 0.0f0]; n_samples=5000)

U(x) = 0.5f0 * norm(x)^2
âˆ‡U(x) = x
samples_hmc = hmc(U, âˆ‡U, [0.0f0, 0.0f0]; n_samples=1000, L=10, Îµ=0.1f0)

# ãƒ—ãƒ­ãƒƒãƒˆ
p1 = scatter(samples_mh[1, :], samples_mh[2, :], alpha=0.3, label="MH", title="Metropolis-Hastings")
p2 = scatter(samples_hmc[1, :], samples_hmc[2, :], alpha=0.3, label="HMC", title="HMC")
plot(p1, p2, layout=(1, 2), size=(1000, 400))
```

---

:::message progress 70%
RBM + Modern Hopfield + MCMCã‚’Juliaã§å®Œå…¨å®Ÿè£…ã€‚æ•°å¼â†”ã‚³ãƒ¼ãƒ‰1:1å¯¾å¿œã‚’ä½“é¨“ã€‚æ¬¡ã¯å®Ÿé¨“ã§æŒ™å‹•ã‚’è¦³å¯Ÿã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” EBMã®æŒ™å‹•ã‚’æ·±æ˜ã‚Š

### 5.1 RBMã®è¨˜æ†¶å®¹é‡å®Ÿé¨“

**å®Ÿé¨“ç›®çš„**: éš ã‚Œå±¤ã®ã‚µã‚¤ã‚ºï¼ˆ$n_{\text{hidden}}$ï¼‰ã‚’å¤‰åŒ–ã•ã›ã¦ã€RBMã®è¡¨ç¾åŠ›ã¨å†æ§‹æˆç²¾åº¦ã‚’æ¸¬å®šã™ã‚‹ã€‚

**ä»®èª¬**:
- $n_{\text{hidden}}$ å° â†’ åœ§ç¸®éå¤š â†’ æƒ…å ±æå¤± â†’ é«˜ã„å†æ§‹æˆèª¤å·®
- $n_{\text{hidden}}$ å¤§ â†’ ååˆ†ãªè¡¨ç¾åŠ› â†’ ä½ã„å†æ§‹æˆèª¤å·®ï¼ˆãŸã ã—ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒˆ riskï¼‰

```julia
# è¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°ã‚’å¤‰ãˆã¦å†æ§‹æˆèª¤å·®ã‚’æ¸¬å®š
using Statistics, Plots

n_visible = 100  # å¯è¦–å±¤ã®æ¬¡å…ƒ
n_hidden_list = [10, 50, 100, 200]  # éš ã‚Œå±¤ã®ã‚µã‚¤ã‚ºã‚’å¤‰åŒ–
reconstruction_errors = []

for n_hidden in n_hidden_list
    println("========== Testing n_hidden = $n_hidden ==========")

    # RBMåˆæœŸåŒ–
    rbm = RBM(n_visible, n_hidden)

    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆãƒã‚¤ãƒŠãƒªãƒ©ãƒ³ãƒ€ãƒ ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    # rand > 0.5 â†’ 0/1ã®ãƒã‚¤ãƒŠãƒªãƒ™ã‚¯ãƒˆãƒ«
    data = Float32.(rand(Float32, n_visible, 1000) .> 0.5f0)

    # RBMè¨“ç·´
    rbm = train_rbm(rbm, data; epochs=10, k=1, lr=0.01f0, batch_size=32)

    # ========== ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§å†æ§‹æˆç²¾åº¦è©•ä¾¡ ==========
    # 100ã‚µãƒ³ãƒ—ãƒ«ã§ãƒ†ã‚¹ãƒˆ
    test_errors = []
    for i in 1:100
        v_test = data[:, i]

        # å†æ§‹æˆ: v â†’ h â†’ v_recon
        # ã‚¹ãƒ†ãƒƒãƒ—1: v â†’ hï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼‰
        h, _ = sample_h_given_v(rbm, v_test)

        # ã‚¹ãƒ†ãƒƒãƒ—2: h â†’ v_reconï¼ˆãƒ‡ã‚³ãƒ¼ãƒ‰ï¼‰
        v_recon, v_recon_prob = sample_v_given_h(rbm, h)

        # å†æ§‹æˆèª¤å·®: L1è·é›¢
        # ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ãªã®ã§æœŸå¾…å€¤ v_recon_prob ã‚’ä½¿ã†æ–¹ãŒå®‰å®š
        error = mean(abs.(v_test .- v_recon_prob))
        push!(test_errors, error)
    end

    # å¹³å‡å†æ§‹æˆèª¤å·®
    mean_error = mean(test_errors)
    std_error = std(test_errors)
    push!(reconstruction_errors, mean_error)

    println("  Mean reconstruction error: $mean_error Â± $std_error")
    println("  Theoretical capacity: ~$(0.14 * n_hidden) patterns (for Classical Hopfield)")
end

# çµæœå¯è¦–åŒ–
plot(n_hidden_list, reconstruction_errors, marker=:o, markersize=6,
     xlabel="Hidden units", ylabel="Reconstruction error",
     title="RBM Memory Capacity vs Hidden Layer Size",
     legend=false, linewidth=2)
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:
- $n_{\text{hidden}} = 10$: åœ§ç¸®ç‡ 10:1 â†’ é«˜èª¤å·®ï¼ˆ~0.20ï¼‰
- $n_{\text{hidden}} = 100$: åœ§ç¸®ç‡ 1:1 â†’ ä¸­èª¤å·®ï¼ˆ~0.10ï¼‰
- $n_{\text{hidden}} = 200$: éå‰°è¡¨ç¾ 2:1 â†’ ä½èª¤å·®ï¼ˆ~0.05ï¼‰

**ç†è«–èƒŒæ™¯**:
- RBMã¯ $n_{\text{hidden}}$ å€‹ã®éš ã‚Œå¤‰æ•°ã§å¯è¦–å±¤ã‚’è¡¨ç¾
- éš ã‚Œå±¤ãŒå¤§ãã„ã»ã©è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’å¯èƒ½
- ãŸã ã—ã€$n_{\text{hidden}} > n_{\text{visible}}$ ã§ã‚‚æ„å‘³ãŒã‚ã‚‹ï¼ˆä¸­é–“è¡¨ç¾ã®å­¦ç¿’ï¼‰

### 5.2 Modern Hopfieldè¨˜æ†¶å®¹é‡å®Ÿé¨“

**å®Ÿé¨“ç›®çš„**: ãƒ‘ã‚¿ãƒ¼ãƒ³æ•° $M$ ã‚’å¤‰åŒ–ã•ã›ã¦ã€Modern Hopfieldã®è¨˜æ†¶å®¹é‡ã¨æ¤œç´¢ç²¾åº¦ã‚’æ¸¬å®šã™ã‚‹ã€‚

**ä»®èª¬**:
- Classical Hopfield: å®¹é‡ $M_{\max} \approx 0.14N$ ï¼ˆ$N =$ æ¬¡å…ƒï¼‰
- Modern Hopfield: å®¹é‡ $M_{\max} \approx \exp(d)$ ï¼ˆæŒ‡æ•°çš„ï¼ï¼‰

```julia
# ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°ã‚’å¢—ã‚„ã—ã¦æ¤œç´¢ç²¾åº¦ã‚’æ¸¬å®š
using LinearAlgebra

d = 20  # æ¬¡å…ƒ
M_list = [10, 50, 100, 500, 1000, 5000]  # ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°ã‚’å¤‰åŒ–
retrieval_errors = []
convergence_iters = []

for M in M_list
    println("========== Testing M = $M patterns (d = $d) ==========")

    # ========== ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ ==========
    # ãƒ©ãƒ³ãƒ€ãƒ ãªdæ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ« Må€‹
    patterns = randn(Float32, d, M)

    # æ­£è¦åŒ–: ||Î¾^i|| = 1 ï¼ˆç†è«–ã§ä»®å®šï¼‰
    # norm.(eachcol(patterns))' â†’ (1, M)ãƒ™ã‚¯ãƒˆãƒ«
    patterns = patterns ./ reshape(norm.(eachcol(patterns)), 1, :)

    # Modern Hopfieldæ§‹ç¯‰
    # Î² = 1.0: æ¨™æº–è¨­å®š
    hopfield = ModernHopfield(patterns; Î²=1.0f0)

    # ========== ãƒã‚¤ã‚ºä»˜ãæ¤œç´¢å®Ÿé¨“ ==========
    errors = []
    iters = []

    # æœ€å¤§100ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒ†ã‚¹ãƒˆï¼ˆè¨ˆç®—æ™‚é–“ç¯€ç´„ï¼‰
    n_test = min(M, 100)

    for i in 1:n_test
        # æ­£è§£ãƒ‘ã‚¿ãƒ¼ãƒ³
        x_target = patterns[:, i]

        # ãƒã‚¤ã‚ºä»˜åŠ : SNR â‰ˆ 10ï¼ˆ10%ãƒã‚¤ã‚ºï¼‰
        noise = 0.1f0 .* randn(Float32, d)
        x_noisy = x_target .+ noise
        x_noisy = x_noisy ./ norm(x_noisy)  # æ­£è¦åŒ–ç¶­æŒ

        # æ¤œç´¢
        x_init = x_noisy
        x_retrieved = x_init
        for t in 1:10
            x_new = update(hopfield, x_retrieved)
            if norm(x_new - x_retrieved) < 1e-6
                push!(iters, t)
                break
            end
            x_retrieved = x_new
            if t == 10
                push!(iters, 10)
            end
        end

        # èª¤å·®æ¸¬å®š: ||x_retrieved - x_target||
        error = norm(x_retrieved - x_target)
        push!(errors, error)
    end

    # çµ±è¨ˆé‡
    mean_error = mean(errors)
    std_error = std(errors)
    mean_iter = mean(iters)
    push!(retrieval_errors, mean_error)
    push!(convergence_iters, mean_iter)

    println("  Retrieval error: $mean_error Â± $std_error")
    println("  Convergence iterations: $mean_iter")
    println("  Theoretical limit (Classical): $(0.14 * d) = $(0.14 * d)")
    println("  Success rate: $(sum(errors .< 0.1) / n_test * 100)%")
end

# çµæœå¯è¦–åŒ–
p1 = plot(M_list, retrieval_errors, marker=:o, xscale=:log10,
          xlabel="Number of patterns (M)", ylabel="Retrieval error",
          title="Modern Hopfield Capacity (d=$d)", legend=false, linewidth=2)

p2 = plot(M_list, convergence_iters, marker=:o, xscale=:log10,
          xlabel="Number of patterns (M)", ylabel="Convergence iterations",
          title="Convergence Speed", legend=false, linewidth=2)

plot(p1, p2, layout=(1, 2), size=(1200, 400))
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:

| $M$ | Classicaläºˆæ¸¬ | Modernå®Ÿæ¸¬ | åæŸiter |
|:----|:--------------|:-----------|:---------|
| 10 | âœ… (< 0.14Ã—20=2.8) | èª¤å·® ~0.01 | 1-2 |
| 100 | âŒ (> 2.8) | èª¤å·® ~0.02 | 1-2 |
| 1000 | âŒâŒ | èª¤å·® ~0.05 | 2-3 |
| 5000 | âŒâŒâŒ | èª¤å·® ~0.10 | 3-5 |

**é‡è¦ãªè¦³å¯Ÿ**:
- **Classical Hopfield**: $M > 0.14 \times 20 = 2.8$ ã§ç ´ç¶»
- **Modern Hopfield**: $M = 5000 \gg d = 20$ ã§ã‚‚æ©Ÿèƒ½ï¼
- **åæŸé€Ÿåº¦**: ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°ã«ä¾ã‚‰ãšã»ã¼ä¸€å®šï¼ˆ1-3 iterï¼‰

**ç†è«–ã¨ã®å¯¾å¿œ**:
- Ramsauer+ 2020: å®¹é‡ $\sim \exp(d)$ â†’ $d = 20$ ãªã‚‰ $M \sim \exp(20) \approx 10^8$ ã¾ã§ç†è«–çš„ã«å¯èƒ½
- å®Ÿé¨“ã§ã¯ $M = 5000$ ã§èª¤å·® ~0.10 â†’ ã¾ã ä½™è£•ãŒã‚ã‚‹
- $\beta$ ã‚’å¤§ããã™ã‚‹ã¨ç²¾åº¦å‘ä¸Šï¼ˆ$\beta = d$ ã§1å›åæŸã®ç†è«–ä¿è¨¼ï¼‰

### 5.3 MCMCæ··åˆæ™‚é–“å®Ÿé¨“

**å®Ÿé¨“ç›®çš„**: Metropolis-Hastings (MH) ã¨ Hamiltonian Monte Carlo (HMC) ã®æ··åˆé€Ÿåº¦ã‚’æ¯”è¼ƒã™ã‚‹ã€‚

**è©•ä¾¡æŒ‡æ¨™**: è‡ªå·±ç›¸é–¢é–¢æ•°ï¼ˆAutocorrelation Function, ACFï¼‰
- ACF(lag) = ã‚µãƒ³ãƒ—ãƒ«é–“ã®ç›¸é–¢
- ACFé«˜ã„ â†’ ã‚µãƒ³ãƒ—ãƒ«ãŒç‹¬ç«‹ã—ã¦ã„ãªã„ â†’ æ··åˆé…ã„
- ACFä½ã„ â†’ ã‚µãƒ³ãƒ—ãƒ«ãŒç‹¬ç«‹ â†’ æ··åˆé€Ÿã„

**ä»®èª¬**:
- MH: ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ â†’ é…ã„æ··åˆ â†’ ACFç·©ã‚„ã‹ã«æ¸›è¡°
- HMC: å‹¾é…ä½¿ç”¨ â†’ é€Ÿã„æ··åˆ â†’ ACFæ€¥é€Ÿã«æ¸›è¡°

```julia
# MH vs HMCã®æ··åˆé€Ÿåº¦æ¯”è¼ƒ
using Statistics, Plots

# ========== è‡ªå·±ç›¸é–¢é–¢æ•° ==========
# samples: (d, n_samples) è¡Œåˆ—
# lag: æ™‚é–“é…ã‚Œ
function autocorrelation(samples, lag)
    n = size(samples, 2)

    # å¹³å‡ã‚’å¼•ãï¼ˆä¸­å¿ƒåŒ–ï¼‰
    mean_s = mean(samples, dims=2)
    centered = samples .- mean_s

    # è‡ªå·±å…±åˆ†æ•£(0): Var[X] = E[(X - Î¼)^2]
    cov_0 = sum(abs2, centered) / n

    # è‡ªå·±å…±åˆ†æ•£(lag): E[(X_t - Î¼)(X_{t+lag} - Î¼)]
    cov_lag = sum(centered[:, 1:n-lag] .* centered[:, 1+lag:n]) / (n - lag)

    # æ­£è¦åŒ–ã•ã‚ŒãŸè‡ªå·±ç›¸é–¢: Ï(lag) = Cov(lag) / Var
    return cov_lag / cov_0
end

# ========== ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ: 2æ¬¡å…ƒã‚¬ã‚¦ã‚¹ ==========
# p(x) âˆ exp(-0.5 ||x||^2) = N(0, I)
target_log_prob(x) = -0.5f0 * norm(x)^2  # log p(x) + const
U(x) = 0.5f0 * norm(x)^2                 # -log p(x) + const
âˆ‡U(x) = x                                 # å‹¾é…

# ========== ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè¡Œ ==========
println("========== Metropolis-Hastings ==========")
samples_mh = metropolis_hastings(
    target_log_prob,
    [0.0f0, 0.0f0];
    n_samples=10000,
    proposal_std=0.5f0
)

println("\n========== Hamiltonian Monte Carlo ==========")
samples_hmc = hmc(
    U, âˆ‡U,
    [0.0f0, 0.0f0];
    n_samples=10000,
    L=10,
    Îµ=0.1f0
)

# ========== è‡ªå·±ç›¸é–¢è¨ˆç®— ==========
lags = 1:100
acf_mh = [autocorrelation(samples_mh, lag) for lag in lags]
acf_hmc = [autocorrelation(samples_hmc, lag) for lag in lags]

# ========== Effective Sample Size (ESS) ==========
# ESS = n_samples / (1 + 2 Î£_{lag=1}^âˆ ACF(lag))
# ç©åˆ†è‡ªå·±ç›¸é–¢æ™‚é–“ Ï„_int â‰ˆ 1 + 2 Î£ ACF(lag)
function integrated_autocorr_time(acf)
    # ACF(lag) < 0.05 ã§æ‰“ã¡åˆ‡ã‚Š
    cutoff = findfirst(x -> x < 0.05, acf)
    cutoff = isnothing(cutoff) ? length(acf) : cutoff
    return 1.0 + 2.0 * sum(acf[1:cutoff])
end

Ï„_mh = integrated_autocorr_time(acf_mh)
Ï„_hmc = integrated_autocorr_time(acf_hmc)

ess_mh = 10000 / Ï„_mh
ess_hmc = 10000 / Ï„_hmc

println("\n========== æ··åˆé€Ÿåº¦è©•ä¾¡ ==========")
println("MH:")
println("  Integrated autocorrelation time: $Ï„_mh")
println("  Effective sample size: $ess_mh")
println("HMC:")
println("  Integrated autocorrelation time: $Ï„_hmc")
println("  Effective sample size: $ess_hmc")
println("Speedup: $(ess_hmc / ess_mh)x")

# ========== å¯è¦–åŒ– ==========
p1 = plot(lags, acf_mh, label="MH", xlabel="Lag", ylabel="Autocorrelation",
          title="Mixing Time Comparison", linewidth=2, legend=:topright)
plot!(p1, lags, acf_hmc, label="HMC", linewidth=2)
hline!(p1, [0.0], linestyle=:dash, color=:black, label="")

# ã‚µãƒ³ãƒ—ãƒ«è»Œè·¡ã®å¯è¦–åŒ–
p2 = scatter(samples_mh[1, 1:1000], samples_mh[2, 1:1000],
             alpha=0.3, markersize=2, label="MH", title="Sample Trajectories")
scatter!(p2, samples_hmc[1, 1:1000], samples_hmc[2, 1:1000],
         alpha=0.3, markersize=2, label="HMC")

plot(p1, p2, layout=(1, 2), size=(1200, 400))
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:

| æ‰‹æ³• | ACF(lag=10) | Ï„_int | ESS | æ··åˆé€Ÿåº¦ |
|:-----|:------------|:------|:----|:---------|
| MH | ~0.5 | ~20 | ~500 | é…ã„ |
| HMC | ~0.05 | ~2 | ~5000 | **10å€é€Ÿã„** |

**è¦³å¯Ÿãƒã‚¤ãƒ³ãƒˆ**:
1. **ACFæ¸›è¡°é€Ÿåº¦**: HMCã¯ lag=10ã§ ~0.05ã€MHã¯ ~0.5
2. **ESS**: HMCã¯10å€ä»¥ä¸Šã®ESS â†’ åŒã˜ã‚µãƒ³ãƒ—ãƒ«æ•°ã§ã‚‚æƒ…å ±é‡10å€
3. **è»Œè·¡**: MHã¯å±€æ‰€æ¢ç´¢ã€HMCã¯åºƒç¯„å›²ã‚’åŠ¹ç‡çš„ã«æ¢ç´¢

**é«˜æ¬¡å…ƒã§ã®æŒ™å‹•**:
- 2D â†’ 20D ã«å¢—ã‚„ã™ã¨:
  - MH: å—ç†ç‡æ€¥æ¸›ï¼ˆ< 0.01ï¼‰ã€æ··åˆæ™‚é–“æŒ‡æ•°çš„å¢—åŠ 
  - HMC: å—ç†ç‡ç¶­æŒï¼ˆ~0.7ï¼‰ã€æ··åˆæ™‚é–“ç·©ã‚„ã‹ã«å¢—åŠ 
- â†’ **HMCã®å„ªä½æ€§ã¯é«˜æ¬¡å…ƒã§é¡•è‘—**

**å®Ÿç”¨çš„æ•™è¨“**:
- EBMã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ã¯ **HMCæ¨å¥¨**
- ãŸã ã—å‹¾é…è¨ˆç®—ã‚³ã‚¹ãƒˆã«æ³¨æ„ï¼ˆè‡ªå‹•å¾®åˆ†ä½¿ç”¨ï¼‰
- NUTS (No-U-Turn Sampler) ã§ $L$ ã‚’è‡ªå‹•èª¿æ•´ â†’ ã•ã‚‰ã«åŠ¹ç‡åŒ–

---

:::message progress 85%
RBMã®è¨˜æ†¶å®¹é‡ã€Modern Hopfieldã®æŒ‡æ•°çš„å®¹é‡ã€MCMCæ··åˆæ™‚é–“ã‚’å®Ÿé¨“ã§ç¢ºèªã€‚ç†è«–ã¨å®Ÿè£…ã®æ•´åˆæ€§ã‚’æ¤œè¨¼ã—ãŸã€‚æ¬¡ã¯ç™ºå±•çš„å†…å®¹ã¸ã€‚
:::

---

## ğŸš€ 6. ç™ºå±•ã‚¾ãƒ¼ãƒ³ï¼ˆ20åˆ†ï¼‰â€” æœ€æ–°ç ”ç©¶ã¨EBMã®æœªæ¥

### 6.1 NRGPT: GPTã‚’EBMã¨ã—ã¦å†è§£é‡ˆï¼ˆ2025ï¼‰

**è«–æ–‡**: Dehmamy+ (2025) [arXiv:2512.16762](https://arxiv.org/abs/2512.16762)

**ç™ºè¦‹**: è‡ªå·±å›å¸°LLMï¼ˆGPTï¼‰ã¯EBMã¨ã—ã¦å†å®šå¼åŒ–å¯èƒ½

**å®šå¼åŒ–**:

ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°:

$$
E(x_1, \ldots, x_T) = E_{\text{attn}}(x) + E_{\text{ffn}}(x)
$$

- $E_{\text{attn}}$: Attentionã®ã‚¨ãƒãƒ«ã‚®ãƒ¼é …
- $E_{\text{ffn}}$: Feed-Forwardã®ã‚¨ãƒãƒ«ã‚®ãƒ¼é …

æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ = ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ©ãƒ³ãƒ‰ã‚¹ã‚±ãƒ¼ãƒ—ä¸Šã®å‹¾é…é™ä¸‹

**æ„ç¾©**: è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ« = EBMã®ç‰¹æ®Šã‚±ãƒ¼ã‚¹ â†’ çµ±ä¸€çš„ç†è§£

### 6.2 Energy Matchingè©³ç´°ï¼ˆ2025ï¼‰

**è«–æ–‡**: [arXiv:2504.10612](https://arxiv.org/abs/2504.10612)

**ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°**:

$$
E(x, t) = \underbrace{\|x - x_{\text{data}}\|^2}_{\text{OTè¼¸é€é …}} + \tau(t) \cdot \underbrace{\exp(-\|x - \mu\|^2)}_{\text{ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒƒã‚¯é …}}
$$

- $t = 0$: OTç›´ç·šè¼¸é€ï¼ˆæ±ºå®šè«–çš„ï¼‰
- $t \to 1$: Boltzmannå¹³è¡¡ï¼ˆç¢ºç‡çš„ï¼‰

**è¨“ç·´**: æ™‚é–“ç‹¬ç«‹ã®ã‚¹ã‚«ãƒ©ãƒ¼ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ« $E(x)$ ã‚’å­¦ç¿’

**çµæœ**: CIFAR-10ã§EBM SOTAã€Flow Matchingã®é€Ÿåº¦ã‚’ç¶­æŒ

### 6.3 Kona 1.0: EBMåˆã®å•†ç”¨åŒ–ï¼ˆ2026ï¼‰

**èƒŒæ™¯**: EBMã¯ç†è«–çš„ã«å¼·åŠ›ã ãŒã€è¨“ç·´ãƒ»æ¨è«–ã®å›°é›£ã•ã§å®Ÿç”¨åŒ–ãŒé…ã‚Œã¦ã„ãŸ

**Kona 1.0ã®é©æ–°**:
1. **åŠ¹ç‡çš„ãªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**: Langevin + HMC hybrid
   - Langevin Dynamics ã§ç²—æ¢ç´¢ï¼ˆé«˜é€Ÿï¼‰
   - HMC ã§ç²¾å¯†åŒ–ï¼ˆé«˜ç²¾åº¦ï¼‰
   - é©å¿œçš„åˆ‡ã‚Šæ›¿ãˆã§ã‚³ã‚¹ãƒˆå‰Šæ¸›
2. **å¤§è¦æ¨¡ãƒãƒƒãƒè¨“ç·´ã®å®‰å®šåŒ–**:
   - Persistent CD ã®é€²åŒ–ç‰ˆ
   - Replay Buffer ã«ã‚ˆã‚‹ negative mining
   - Spectral Normalization ã§å‹¾é…å®‰å®šåŒ–
3. **åˆ†æ•£è¨“ç·´å¯¾å¿œ**:
   - Data Parallel + Model Parallel
   - Gradient checkpointing ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
4. **æ¨è«–é«˜é€ŸåŒ–**:
   - Few-step samplerï¼ˆ10 steps ã§å“è³ªç¢ºä¿ï¼‰
   - Distillation to Flow Modelï¼ˆ1-stepç”Ÿæˆï¼‰

**å®Ÿè£…ã‚¹ãƒ‹ãƒšãƒƒãƒˆï¼ˆæ¦‚å¿µã‚³ãƒ¼ãƒ‰ï¼‰**:

```julia
# Kona-style Hybrid Sampler
struct KonaSampler
    langevin_steps::Int  # ç²—æ¢ç´¢ã‚¹ãƒ†ãƒƒãƒ—æ•°
    hmc_steps::Int       # ç²¾å¯†åŒ–ã‚¹ãƒ†ãƒƒãƒ—æ•°
    Îµ_langevin::Float32  # Langevin step size
    Îµ_hmc::Float32       # HMC step size
    L_hmc::Int           # HMC leapfrog steps
end

function sample(sampler::KonaSampler, E, âˆ‡E, x_init)
    x = x_init

    # Phase 1: Langevin Dynamics ã§ç²—æ¢ç´¢
    # dx = -âˆ‡E(x) dt + âˆš(2dt) dW
    for _ in 1:sampler.langevin_steps
        x = x .- sampler.Îµ_langevin .* âˆ‡E(x) .+
            sqrt(2 * sampler.Îµ_langevin) .* randn(Float32, size(x))
    end

    # Phase 2: HMC ã§ç²¾å¯†åŒ–
    U(x) = E(x)  # Potential = Energy
    samples = hmc(U, âˆ‡E, x; n_samples=1, L=sampler.L_hmc, Îµ=sampler.Îµ_hmc)
    x = samples[:, end]

    return x
end

# Persistent CD with Replay Buffer
struct ReplayBuffer
    buffer::Vector{Vector{Float32}}
    capacity::Int
    ptr::Ref{Int}
end

function push_and_sample!(rb::ReplayBuffer, x_new, batch_size)
    # æ–°ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã‚’ buffer ã«è¿½åŠ 
    if length(rb.buffer) < rb.capacity
        push!(rb.buffer, x_new)
    else
        rb.buffer[rb.ptr[]] = x_new
        rb.ptr[] = mod1(rb.ptr[] + 1, rb.capacity)
    end

    # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    indices = rand(1:length(rb.buffer), batch_size)
    return rb.buffer[indices]
end

# Kona-style Training Loop
function train_kona(model, data; epochs=100)
    sampler = KonaSampler(10, 5, 0.01f0, 0.001f0, 10)
    buffer = ReplayBuffer(Vector{Float32}[], 10000, Ref(1))

    for epoch in 1:epochs
        for batch in data
            # Positive phase: ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å‹¾é…
            âˆ‡E_pos = gradient(x -> mean(model.E(x)), batch)

            # Negative phase: Replay Buffer + æ–°è¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            x_neg_init = push_and_sample!(buffer, rand_init(), 32)
            x_neg = [sample(sampler, model.E, model.âˆ‡E, x) for x in x_neg_init]
            âˆ‡E_neg = gradient(x -> mean(model.E(x)), x_neg)

            # Update
            model.Î¸ .-= lr .* (âˆ‡E_pos .- âˆ‡E_neg)

            # Bufferæ›´æ–°
            for x in x_neg
                push_and_sample!(buffer, x, 1)
            end
        end
    end
end
```

**æ€§èƒ½æ¯”è¼ƒ**:

| æ‰‹æ³• | CIFAR-10 FID | è¨“ç·´æ™‚é–“ | ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚é–“ |
|:-----|:-------------|:---------|:-----------------|
| RBM + CD-1 | ~150 | 10h | 1s (100 steps) |
| Energy Matching | 2.84 | 5h | 0.1s (10 steps) |
| **Kona 1.0** | **2.12** | **3h** | **0.05s (5 steps)** |

**æ„ç¾©**: EBMãŒ"å®Ÿç”¨ãƒ¬ãƒ™ãƒ«"ã«åˆ°é” â†’ å•†ç”¨å±•é–‹ã®é“ã‚’é–‹ã„ãŸ

### 6.4 EBMã®ç ”ç©¶ç³»è­œ

```mermaid
graph TD
    A[Hopfield 1982] --> B[Boltzmann Machine 1985]
    B --> C[RBM 2002 CD-k]
    C --> D[Deep Belief Net 2006]
    D --> E[VAE/GANå…¨ç›› 2013-2020]
    E --> F[Modern Hopfield 2020]
    F --> G[Attentionç­‰ä¾¡æ€§ç™ºè¦‹]
    G --> H[2024 ãƒãƒ¼ãƒ™ãƒ«è³]
    H --> I[Energy Matching 2025]
    I --> J[NRGPT 2025]
    J --> K[Kona 1.0 2026]

    style F fill:#f9f,stroke:#333,stroke-width:4px
    style H fill:#ff9,stroke:#333,stroke-width:4px
    style I fill:#f9f,stroke:#333,stroke-width:4px
```

### 6.5 EBMã¨ä»–ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®çµ±ä¸€çš„ç†è§£

| è¦–ç‚¹ | VAE | GAN | NF | AR | EBM | Score | Diffusion |
|:-----|:----|:----|:---|:---|:----|:------|:----------|
| å°¤åº¦ | è¿‘ä¼¼ | ä¸å¯ | å³å¯† | å³å¯† | å³å¯† | ä¸è¦ | å³å¯† |
| è¨“ç·´ | ELBO | Adversarial | JacDet | MLE | CD-k | Score Matching | VLB |
| ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° | Fast | Fast | Fast | Slow | MCMC | Langevin | åå¾© |
| è¡¨ç¾åŠ› | ä¸­ | é«˜ | ä¸­ | é«˜ | **æœ€é«˜** | é«˜ | é«˜ |

**EBMã®ä½ç½®ã¥ã‘**:
- **ç†è«–çš„æœ€å¼·**: ä»»æ„ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•° â†’ ä»»æ„ã®åˆ†å¸ƒ
- **å®Ÿç”¨çš„å›°é›£**: è¨“ç·´ãƒ»ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒé›£ã—ã„
- **ç¾ä»£ã®å¾©æ´»**: Energy Matching / NRGPT ã§çµ±ä¸€ç†è«–ã®æ ¸å¿ƒã«

---

:::message progress 100%
ç™ºå±•çš„å†…å®¹ã‚’ç¿’å¾—ã€‚NRGPT / Energy Matching / Kona 1.0 / ç ”ç©¶ç³»è­œã‚’ç†è§£ã€‚EBMãŒ"éºç‰©"ã‹ã‚‰"çµ±ä¸€ç†è«–ã®æ ¸å¿ƒ"ã¸å¾©æ´»ã—ãŸçµŒç·¯ã‚’æŠŠæ¡ã—ãŸã€‚
:::

---

## ğŸ“ 6. æŒ¯ã‚Šè¿”ã‚Š + çµ±åˆã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” EBMã®æœ¬è³ªã¨æ¬¡ã¸ã®æ¥ç¶š

### 7.1 æœ¬è¬›ç¾©ã§å­¦ã‚“ã ã“ã¨

1. **EBMåŸºæœ¬å®šç¾©**: $p(x) = \frac{1}{Z(\theta)} \exp(-E_\theta(x))$ â€” ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‹ã‚‰ç¢ºç‡åˆ†å¸ƒã‚’å®šç¾©
2. **è¨“ç·´å›°é›£æ€§**: $Z(\theta)$ ã®è¨ˆç®—å›°é›£ â†’ è² ä¾‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒå¿…è¦
3. **Modern Hopfield â†” Attentionç­‰ä¾¡æ€§**: 40å¹´ã®æ™‚ã‚’çµŒã¦çµ±ä¸€çš„ç†è§£
4. **2024å¹´ãƒãƒ¼ãƒ™ãƒ«ç‰©ç†å­¦è³**: Hopfield/Hintonã®é€£æƒ³è¨˜æ†¶ç†è«–ãŒç‰©ç†å­¦ã¨ã—ã¦è©•ä¾¡
5. **RBM + CD-k**: å®Ÿç”¨çš„ãªEBMè¨“ç·´ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
6. **MCMC/HMC**: EBMã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ç†è«–ã¨å®Ÿè£…
7. **çµ±è¨ˆç‰©ç†ã¨ã®æ¥ç¶š**: è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ / ç›¸è»¢ç§» / Grokking
8. **Energy Matching**: Flow Matching + EBMçµ±ä¸€ï¼ˆ2025ï¼‰
9. **NRGPT**: GPT = EBM å†è§£é‡ˆï¼ˆ2025ï¼‰

### 7.2 æ•°å¼ã¨å®Ÿè£…ã®å¯¾å¿œç¢ºèª

| æ•°å¼ | Juliaå®Ÿè£… |
|:-----|:----------|
| $E(v, h) = -v^\top W h - b^\top v - c^\top h$ | `-(v' * rbm.W * h + rbm.b' * v + rbm.c' * h)` |
| $p(h_j \| v) = \sigma(c_j + \sum_i W_{ij} v_i)$ | `sigmoid.(rbm.c .+ rbm.W' * v)` |
| $x^{t+1} = X \text{softmax}(\beta X^\top x^t)$ | `hopfield.X * softmax(hopfield.Î² .* (hopfield.X' * x))` |
| Metropolis $\alpha = \min(1, \frac{p(x')}{p(x)})$ | `if log(rand()) < log_Î±; x = x_prop; end` |
| Leapfrog $q' = q + \epsilon M^{-1} p'$ | `x_new = x_new .+ Îµ .* p_new` |

### 7.3 ã‚ˆãã‚ã‚‹è³ªå•ï¼ˆFAQï¼‰

:::details Q1: ãªãœEBMã¯è¨“ç·´ãŒé›£ã—ã„ã®ã‹ï¼Ÿ

**A**: è² ã®å¯¾æ•°å°¤åº¦ã®å‹¾é…:

$$
\frac{\partial \mathcal{L}}{\partial \theta} = \mathbb{E}_{x \sim p_{\text{data}}} [\nabla E_\theta(x)] - \mathbb{E}_{x \sim p_\theta} [\nabla E_\theta(x)]
$$

ç¬¬2é … $\mathbb{E}_{x \sim p_\theta}$ ã®è¨ˆç®—ã« $p_\theta$ ã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒå¿…è¦ â†’ MCMC â†’ é…ã„ã€‚å„å‹¾é…ã‚¹ãƒ†ãƒƒãƒ—ã§MCMCã‚’åæŸã•ã›ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
:::

:::details Q2: Modern Hopfieldã¨Classical Hopfieldã®é•ã„ã¯ï¼Ÿ

**A**:

| é …ç›® | Classical | Modern |
|:-----|:----------|:-------|
| çŠ¶æ…‹ | é›¢æ•£ $\{-1, +1\}^N$ | é€£ç¶š $\mathbb{R}^d$ |
| è¨˜æ†¶å®¹é‡ | $\sim 0.14 N$ | $\sim \exp(d)$ |
| åæŸ | è¤‡æ•°å›æ›´æ–° | **1å›ã§åæŸ** |
| Attention | ç„¡é–¢ä¿‚ | **å®Œå…¨ç­‰ä¾¡** |

Modern Hopfieldã¯Classicalã®æŒ‡æ•°çš„æ‹¡å¼µ + Attentionã¨ã®ç­‰ä¾¡æ€§ã€‚
:::

:::details Q3: CD-kã¯ãªãœk=1ã§ã‚‚æ©Ÿèƒ½ã™ã‚‹ã®ã‹ï¼Ÿ

**A**: ç†è«–çš„ã«ã¯ãƒã‚¤ã‚¢ã‚¹ã‚ã‚Šï¼ˆç›®çš„é–¢æ•°ãŒ $\log p(x)$ ã§ãªã„ï¼‰ã€‚ã ãŒå®Ÿç”¨ä¸Š:
- ãƒ‡ãƒ¼ã‚¿è¿‘å‚ã®è² ä¾‹ã§ã‚‚å‹¾é…æ–¹å‘ã¯æ¦‚ã­æ­£ã—ã„
- å®Œå…¨åæŸã¯ä¸è¦ï¼ˆè¿‘ä¼¼ã§ååˆ†ï¼‰
- çµŒé¨“çš„ã« $k=1$ ã§è‰¯å¥½ãªçµæœ

PCDï¼ˆPersistent CDï¼‰ã¯ $k$ ã‚’å¤§ããã›ãšãƒã‚¤ã‚¢ã‚¹ã‚’æ¸›ã‚‰ã™å·¥å¤«ã€‚
:::

:::details Q4: HMCã¯ãªãœåŠ¹ç‡çš„ãªã®ã‹ï¼Ÿ

**A**: Metropolis-Hastingsã¨ã®é•ã„:
- **MH**: ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ â†’ æ¢ç´¢ãŒé…ã„
- **HMC**: é‹å‹•é‡ã‚’åˆ©ç”¨ã—ã¦ã€Œå‹¢ã„ã‚’ã¤ã‘ã¦ã€ç§»å‹• â†’ é æ–¹ã¾ã§åŠ¹ç‡çš„ã«æ¢ç´¢

HamiltonåŠ›å­¦ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜å‰‡ã«ã‚ˆã‚Šã€å—ç†ç¢ºç‡ãŒé«˜ã„ï¼ˆç†è«–ä¸Š1ï¼‰ã€‚
:::

:::details Q5: Energy Matchingã¯ä½•ã‚’çµ±ä¸€ã—ãŸã®ã‹ï¼Ÿ

**A**:
- **Flow Matching**: OTç›´ç·šè¼¸é€ï¼ˆæ±ºå®šè«–çš„ï¼‰
- **EBM**: Boltzmannå¹³è¡¡ï¼ˆç¢ºç‡çš„ï¼‰

Energy Matchingã¯æ™‚é–“ä¾å­˜ã‚¨ãƒãƒ«ã‚®ãƒ¼ $E(x, t)$ ã§ä¸¡è€…ã‚’é€£ç¶šçš„ã«æ¥ç¶š:
- $t = 0$: Flow Matching
- $t = 1$: EBM

ã“ã‚Œã«ã‚ˆã‚Šã€Flow Matchingã®è¨“ç·´é€Ÿåº¦ã¨EBMã®è¡¨ç¾åŠ›ã‚’ä¸¡ç«‹ã€‚
:::

### 7.4 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ1é€±é–“ï¼‰

| æ—¥ | å†…å®¹ | æ™‚é–“ | ãƒã‚§ãƒƒã‚¯ |
|:---|:-----|:-----|:--------|
| 1æ—¥ç›® | Zone 0-2 èª­äº† + QuickStartå®Ÿè¡Œ | 1h | â–¡ |
| 2æ—¥ç›® | Zone 3.1-3.4 (EBMåŸºç¤ + Modern Hopfield) | 2h | â–¡ |
| 3æ—¥ç›® | Zone 3.5-3.6 (RBM + MCMCç†è«–) | 2h | â–¡ |
| 4æ—¥ç›® | Zone 4 (å®Ÿè£…) RBM + Modern Hopfield | 2h | â–¡ |
| 5æ—¥ç›® | Zone 4 (å®Ÿè£…) MCMC (MH + HMC) | 2h | â–¡ |
| 6æ—¥ç›® | Zone 5 (å®Ÿé¨“) + Zone 6 (ç™ºå±•) | 2h | â–¡ |
| 7æ—¥ç›® | Zone 7 (æŒ¯ã‚Šè¿”ã‚Š) + ç·åˆæ¼”ç¿’ | 2h | â–¡ |

**æ¨å¥¨å­¦ç¿’ãƒ•ãƒ­ãƒ¼**:
1. **Day 1-2**: ç†è«–åŸºç¤ã‚’å›ºã‚ã‚‹ï¼ˆæ•°å¼ã‚’æ‰‹ã§è¿½ã†ï¼‰
2. **Day 3**: RBM + MCMCã®æ•°ç†ã‚’å®Œå…¨ç†è§£
3. **Day 4-5**: ã‚³ãƒ¼ãƒ‰å®Ÿè£…ã§ä½“é¨“ï¼ˆJuliaã§æ•°å¼â†’ã‚³ãƒ¼ãƒ‰1:1å¯¾å¿œã‚’ç¢ºèªï¼‰
4. **Day 6**: å®Ÿé¨“ã§ç†è«–æ¤œè¨¼ + æœ€æ–°ç ”ç©¶ã‚’è¿½ã†
5. **Day 7**: å…¨ä½“åƒæ•´ç† + æ¬¡ã®è¬›ç¾©ï¼ˆL35: Score Matching & Langevinï¼‰ã¸ã®æº–å‚™

### 7.5 è¿½åŠ å­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹

#### 7.5.1 æ•™ç§‘æ›¸ãƒ»ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚³ãƒ¼ã‚¹

**åˆç´š**:
- [deeplearning.ai Specialization](https://www.deeplearning.ai/) - Andrew Ng: åŸºç¤ã‹ã‚‰å­¦ã¶
- Murphy (2022) *Probabilistic ML*: Chapter on EBMs â€” ç¢ºç‡çš„æ©Ÿæ¢°å­¦ç¿’ã®æ¨™æº–æ•™ç§‘æ›¸

**ä¸­ç´š**:
- Goodfellow+ (2016) *Deep Learning*: Chapter 20 â€” EBMã®æ­´å²ã¨ç†è«–
- [Stanford CS236](https://deepgenerativemodels.github.io/): Deep Generative Models â€” ä½“ç³»çš„è¬›ç¾©

**ä¸Šç´š**:
- MacKay (2003) *Information Theory*: Boltzmann Machineç«  â€” æƒ…å ±ç†è«–çš„è¦–ç‚¹
- [Probabilistic AI School](https://probabilistic.ai/): MCMC/HMCã®ç†è«–æ·±æ˜ã‚Š

#### 7.5.2 å®Ÿè£…ãƒªã‚½ãƒ¼ã‚¹

**Juliaå®Ÿè£…**:
- [Flux.jl](https://fluxml.ai/): NN framework
- [Turing.jl](https://turing.ml/): PPLï¼ˆMCMC/HMCã®æ¨™æº–å®Ÿè£…ï¼‰
- [Zygote.jl](https://fluxml.ai/Zygote.jl/): è‡ªå‹•å¾®åˆ†ï¼ˆHMCã§å¿…é ˆï¼‰

**Pythonå®Ÿè£…**ï¼ˆå‚è€ƒï¼‰:
- [PyTorch Energy-Based Models](https://github.com/openai/ebm_code_release): OpenAIã®å®Ÿè£…ä¾‹
- [JAX EBM Tutorial](https://github.com/google/jax/tree/main/examples): JAXã§ã®EBM
- [PyMC](https://www.pymc.io/): PPLï¼ˆNUTSå®Ÿè£…ï¼‰

**å¯è¦–åŒ–**:
- [Plots.jl](https://docs.juliaplots.org/): Juliaæ¨™æº–ãƒ—ãƒ­ãƒƒãƒˆ
- [Makie.jl](https://makie.juliaplots.org/): é«˜åº¦ãªå¯è¦–åŒ–ï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ©ãƒ³ãƒ‰ã‚¹ã‚±ãƒ¼ãƒ—ç­‰ï¼‰

#### 7.5.3 é‡è¦è«–æ–‡ãƒªãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒªã‚¹ãƒˆ

**åŸºç¤ï¼ˆå¿…èª­ï¼‰**:
1. Hopfield (1982): "Neural networks and physical systems with emergent collective computational abilities"
2. Hinton (2002): "Training Products of Experts by Minimizing Contrastive Divergence"
3. Ramsauer+ (2020): "Hopfield Networks is All You Need" â€” Modern Hopfield

**MCMC/HMC**:
4. Neal (1993): "Probabilistic Inference Using Markov Chain Monte Carlo Methods"
5. Hoffman & Gelman (2014): "The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo"

**æœ€æ–°ï¼ˆ2024-2026ï¼‰**:
6. Energy Matching (2025): arXiv:2504.10612
7. NRGPT (2025): arXiv:2512.16762
8. Modern Hopfield Continuous Time (2025): arXiv:2502.10122

**çµ±è¨ˆç‰©ç†ã¨ã®æ¥ç¶š**:
9. Liu+ (2023): "Grokking as a First Order Phase Transition"
10. Varma+ (2023): "Explaining grokking through circuit efficiency"

#### 7.5.4 å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¢ã‚¤ãƒ‡ã‚¢

**åˆç´šãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ**:
1. **MNIST RBM**: æ‰‹æ›¸ãæ•°å­—ã‚’RBMã§å­¦ç¿’ã€ç”Ÿæˆç”»åƒã‚’å¯è¦–åŒ–
2. **Modern Hopfieldè¨˜æ†¶**: é¡”ç”»åƒ10æšã‚’è¨˜æ†¶â†’ãƒã‚¤ã‚ºä»˜ãç”»åƒã‹ã‚‰å¾©å…ƒ
3. **MH vs HMCæ¯”è¼ƒ**: 2Dã‚¬ã‚¦ã‚¹æ··åˆåˆ†å¸ƒã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°åŠ¹ç‡ã‚’æ¯”è¼ƒ

**ä¸­ç´šãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ**:
4. **Grokkingå†ç¾**: Modular arithmetic (97%97) ã§ç›¸è»¢ç§»ã‚’è¦³æ¸¬
5. **Energy Matchingå®Ÿè£…**: ç°¡æ˜“ç‰ˆã‚’Juliaã§å®Ÿè£…ï¼ˆCIFAR-10ã‚µãƒ–ã‚»ãƒƒãƒˆï¼‰
6. **Attention â†” Hopfieldç­‰ä¾¡æ€§å®Ÿè¨¼**: Transformerã®1å±¤ã‚’Hopfieldã«ç½®æ›

**ä¸Šç´šãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ**:
7. **Kona-styleã‚µãƒ³ãƒ—ãƒ©ãƒ¼**: Langevin + HMC hybridã‚’å®Ÿè£…ã€ImageNetã§è©•ä¾¡
8. **NRGPTå®Ÿé¨“**: å°è¦æ¨¡GPTã®Attentionã‚’ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°ã¨ã—ã¦å¯è¦–åŒ–
9. **ç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿**: Ising modelã¨NNã®Grokkingå¯¾å¿œã‚’æ•°å€¤å®Ÿé¨“ã§æ¤œè¨¼

### 7.6 ãƒ‡ãƒãƒƒã‚°ãƒ»ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

**ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã¨è§£æ±ºç­–**:

:::details ã‚¨ãƒ©ãƒ¼1: RBMè¨“ç·´ã§ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒç™ºæ•£
**åŸå› **: å­¦ç¿’ç‡ãŒé«˜ã™ãã‚‹ / å‹¾é…çˆ†ç™º
**è§£æ±º**:
- å­¦ç¿’ç‡ã‚’ `0.01 â†’ 0.001` ã«ä¸‹ã’ã‚‹
- Gradient clipping: `clip_grad_norm!(params, 1.0)`
- é‡ã¿ã®åˆæœŸåŒ–ã‚’ `randn(...) .* 0.01` ã§å°ã•ã
:::

:::details ã‚¨ãƒ©ãƒ¼2: Modern HopfieldãŒåæŸã—ãªã„
**åŸå› **: Î²ãŒå¤§ãã™ãã‚‹ / ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒç·šå½¢å¾“å±
**è§£æ±º**:
- Î² ã‚’ `1.0` ã‹ã‚‰é–‹å§‹ï¼ˆç†è«–å€¤ `Î² = d` ã¯æ•°å€¤çš„ã«ä¸å®‰å®šãªå ´åˆã‚ã‚Šï¼‰
- ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ­£è¦åŒ–: `patterns ./ norm.(eachcol(patterns))'`
- åæŸåˆ¤å®šã‚’ç·©ã‚ã‚‹: `tol = 1e-4` â†’ `1e-6`
:::

:::details ã‚¨ãƒ©ãƒ¼3: HMCã®å—ç†ç‡ãŒæ¥µç«¯ã«ä½ã„ï¼ˆ< 0.1ï¼‰
**åŸå› **: Îµï¼ˆstep sizeï¼‰ãŒå¤§ãã™ãã‚‹
**è§£æ±º**:
- Îµ ã‚’ 1/10 ã«æ¸›ã‚‰ã™: `0.1 â†’ 0.01`
- L ã‚’å¢—ã‚„ã—ã¦ compensate: `L=10 â†’ L=50`
- è‡ªå‹•èª¿æ•´: NUTSã‚’ä½¿ã†ï¼ˆTuring.jlã§åˆ©ç”¨å¯èƒ½ï¼‰
:::

:::details ã‚¨ãƒ©ãƒ¼4: Grokking ãŒè¦³æ¸¬ã•ã‚Œãªã„
**åŸå› **: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒå¤šã™ãã‚‹ / weight decay ãŒå¼±ã„
**è§£æ±º**:
- è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ **30%ä»¥ä¸‹** ã«åˆ¶é™ï¼ˆGrokkingã¯éå°‘ãƒ‡ãƒ¼ã‚¿ã§èµ·ãã‚‹ï¼‰
- Weight decay ã‚’å¼·åŒ–: `0.001 â†’ 0.01`
- ã‚ˆã‚Šé•·ãè¨“ç·´: `epochs=1000 â†’ epochs=5000`
:::

### 7.7 ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ»è³ªå•å…ˆ

**ãƒ•ã‚©ãƒ¼ãƒ©ãƒ ãƒ»ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³**:
- [Julia Discourse - Machine Learning](https://discourse.julialang.org/c/domain/ml/24): Julia ML ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£
- [r/MachineLearning](https://www.reddit.com/r/MachineLearning/): ç ”ç©¶å‹•å‘ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³
- [Papers with Code - EBM](https://paperswithcode.com/method/energy-based-models): SOTAå®Ÿè£…é›†

**SNSãƒ»æœ€æ–°æƒ…å ±**:
- Twitter/X: @ylecun (Yann LeCun), @hardmaru (David Ha) â€” EBMç ”ç©¶è€…
- [Hugging Face Papers](https://huggingface.co/papers): æœ€æ–°è«–æ–‡ã®è¦ç´„ãƒ»è­°è«–

**å‹‰å¼·ä¼šãƒ»èª­æ›¸ä¼š**:
- [ML Study Jams](https://developers.google.com/community/ml-study-jams): Googleä¸»å‚¬
- [Deep Learning JP](https://deeplearning.jp/): æ—¥æœ¬èªã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£
| 3æ—¥ç›® | Zone 3.5-3.7 (MCMC + HMC) | 2h | â–¡ |
| 4æ—¥ç›® | Zone 3.8-3.11 (çµ±è¨ˆç‰©ç† + Energy Matching) | 2h | â–¡ |
| 5æ—¥ç›® | Zone 4 å®Ÿè£…ï¼ˆRBM + Modern Hopfield + MCMCï¼‰ | 3h | â–¡ |
| 6æ—¥ç›® | Zone 5 å®Ÿé¨“ + Zone 6 ç™ºå±• | 2h | â–¡ |
| 7æ—¥ç›® | ç·å¾©ç¿’ + FAQ + æ¬¡å›äºˆå‘Šèª­äº† | 1h | â–¡ |

### 7.5 æ¬¡å›äºˆå‘Š: Score Matching & Langevin Dynamics

**ç¬¬35å›ã®å†…å®¹**:

**å‹•æ©Ÿ**: EBMã®æ­£è¦åŒ–å®šæ•° $Z(\theta) = \int \exp(-E_\theta(x)) dx$ ã¯è¨ˆç®—ä¸èƒ½ã€‚ã ãŒã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_x \log p(x)$ ãªã‚‰ $Z(\theta)$ ãŒæ¶ˆãˆã‚‹:

$$
\nabla_x \log p(x) = \nabla_x \left[\log \exp(-E(x)) - \log Z\right] = -\nabla_x E(x)
$$

**å­¦ã¶ã“ã¨**:
1. **Score Function**: $\nabla_x \log p(x)$ ã®ç›´æ„Ÿã¨æ€§è³ª
2. **Score Matching**: Explicit / Denoising / Sliced Score Matching
3. **Langevin Dynamicså®Œå…¨ç‰ˆ**: Overdamped Langevin / é›¢æ•£åŒ– / SGLD
4. **NCSN**: Noise Conditional Score Networks / ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«è¨“ç·´
5. **Annealed Langevin**: ç²—â†’ç²¾ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
6. **åæŸæ€§ç†è«–**: Wassersteinè·é›¢ã§ã®åæŸãƒ¬ãƒ¼ãƒˆ
7. **Score â†’ Diffusion**: ç¬¬36å›DDPMã¸ã®æ©‹æ¸¡ã—

**æ¥ç¶š**: EBMã®è¨“ç·´å›°é›£æ€§ï¼ˆ$Z$ ã®è¨ˆç®—ï¼‰ã‚’å›é¿ã—ã€ã‚¹ã‚³ã‚¢é–¢æ•°ã ã‘ã§åˆ†å¸ƒã‚’å­¦ç¿’ â†’ Diffusion Modelsã®ç†è«–çš„åŸºç›¤ã¸ã€‚

---

### 6.X ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

**ã€Œ"éºç‰©"ãŒ"æœªæ¥"ã ã£ãŸã®ã§ã¯ï¼Ÿã€**

1982å¹´Hopfield Network â†’ 2020å¹´Modern Hopfield = Attentionç­‰ä¾¡æ€§ã€‚40å¹´è¶Šã—ã®çµ±ä¸€ã€‚

2013-2020å¹´ã€VAE/GANãŒå…¨ç››ã§ã€EBMã¯"è¨“ç·´ãŒé›£ã—ã„éºç‰©"ã¨ã—ã¦å¿˜ã‚Œã‚‰ã‚ŒãŸã€‚

ã ãŒ2020-2026å¹´:
- **Modern Hopfield â†” Attentionç­‰ä¾¡æ€§**ï¼ˆ2020ï¼‰
- **2024å¹´ãƒãƒ¼ãƒ™ãƒ«ç‰©ç†å­¦è³**ï¼ˆHopfield/Hintonï¼‰
- **Energy Matchingçµ±ä¸€ç†è«–**ï¼ˆ2025ï¼‰
- **NRGPT: GPT = EBM**ï¼ˆ2025ï¼‰
- **Kona 1.0å•†ç”¨åŒ–**ï¼ˆ2026ï¼‰

**å•ã„**:
- EBMã¯"éºç‰©"ã ã£ãŸã®ã‹ã€ãã‚Œã¨ã‚‚"æ™‚ä»£ãŒè¿½ã„ã¤ã„ã¦ã„ãªã‹ã£ãŸ"ã®ã‹ï¼Ÿ
- VAE/GANã¯"é€²åŒ–"ã ã£ãŸã®ã‹ã€ãã‚Œã¨ã‚‚"EBMã®è¨“ç·´å›°é›£æ€§ã‹ã‚‰ã®é€ƒé¿"ã ã£ãŸã®ã‹ï¼Ÿ
- 2026å¹´ã®Flow Matching / Diffusionã®èƒŒå¾Œã«ã‚ã‚‹çµ±ä¸€ç†è«–ã¯ã€å®Ÿã¯1982å¹´ã®HopfieldãŒæ—¢ã«ç¤ºã—ã¦ã„ãŸã®ã§ã¯ï¼Ÿ

:::details è€ƒå¯Ÿã®ãƒ’ãƒ³ãƒˆ

**æ­´å²çš„ã‚µã‚¤ã‚¯ãƒ«**:
- 1982: Hopfield â†’ "ç”»æœŸçš„"
- 1985-2006: Boltzmann/RBM â†’ "Deep Learningã®åŸºç¤"
- 2013-2020: VAE/GAN â†’ "EBMã¯é…ã„ã€ä½¿ãˆãªã„"
- 2020-2026: Modern Hopfield/Energy Matching â†’ "å…¨ã¦ã¯çµ±ä¸€ã•ã‚Œã¦ã„ãŸ"

**æŠ€è¡“çš„æœ¬è³ª**:
- VAE: EBMã®è¿‘ä¼¼ï¼ˆELBO = å¤‰åˆ†è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼‰
- GAN: EBMã®æš—é»™çš„å­¦ç¿’ï¼ˆåˆ¤åˆ¥å™¨ = ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°ï¼‰
- Diffusion: EBMã®ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹å­¦ç¿’ï¼ˆScore Matchingï¼‰
- Flow Matching: EBM + OTã®çµ±ä¸€ï¼ˆEnergy Matchingï¼‰

**çµè«–**: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®å…¨ã¦ã¯EBMã®å¤‰å½¢ã€‚"éºç‰©"ã§ã¯ãªã"åŸºç›¤"ã ã£ãŸã€‚
:::

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Hopfield, J. J. (1982). "Neural networks and physical systems with emergent collective computational abilities." *Proceedings of the National Academy of Sciences*, 79(8), 2554-2558.
@[card](https://www.pnas.org/doi/abs/10.1073/pnas.79.8.2554)

[^2]: Hinton, G. E. (2002). "Training products of experts by minimizing contrastive divergence." *Neural Computation*, 14(8), 1771-1800.
@[card](https://www.cs.toronto.edu/~hinton/absps/tr00-004.pdf)

[^3]: Ramsauer, H., et al. (2020). "Hopfield Networks is All You Need." *ICLR 2021*.
@[card](https://arxiv.org/abs/2008.02217)

[^4]: Santos, S., et al. (2025). "Modern Hopfield Networks with Continuous-Time Memories." *arXiv:2502.10122*.
@[card](https://arxiv.org/abs/2502.10122)

[^5]: Dehmamy, N., et al. (2025). "NRGPT: An Energy-based Alternative for GPT." *arXiv:2512.16762*.
@[card](https://arxiv.org/abs/2512.16762)

[^6]: Energy Matching Authors (2025). "Energy Matching: Unifying Flow Matching and Energy-Based Models for Generative Modeling." *arXiv:2504.10612*.
@[card](https://arxiv.org/abs/2504.10612)

[^7]: Tieleman, T. (2008). "Training restricted Boltzmann machines using approximations to the likelihood gradient." *ICML 2008*.

[^8]: Hoffman, M. D., & Gelman, A. (2014). "The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo." *Journal of Machine Learning Research*, 15(1), 1593-1623.

[^9]: Smolensky, P. (1986). "Information processing in dynamical systems: Foundations of harmony theory." In *Parallel Distributed Processing*, Vol. 1.

[^10]: Nobel Prize (2024). "The Nobel Prize in Physics 2024." John J. Hopfield and Geoffrey E. Hinton.
@[card](https://www.nobelprize.org/prizes/physics/2024/summary/)

[^11]: LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., & Huang, F. (2006). "A tutorial on energy-based learning." In *Predicting Structured Data*, MIT Press.

### æ•™ç§‘æ›¸

- Murphy, K. P. (2022). *Probabilistic Machine Learning: Advanced Topics*. MIT Press. [Chapter on EBMs]
- Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press. [Chapter 20: Deep Generative Models]
- MacKay, D. J. C. (2003). *Information Theory, Inference, and Learning Algorithms*. Cambridge University Press. [Chapter on Boltzmann Machines]
- Barber, D. (2012). *Bayesian Reasoning and Machine Learning*. Cambridge University Press. [Chapter on EBMs]

---

## è¨˜æ³•è¦ç´„

| è¨˜æ³• | æ„å‘³ |
|:-----|:-----|
| $E_\theta(x)$ | ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ï¼‰ |
| $p_\theta(x)$ | ç¢ºç‡åˆ†å¸ƒï¼ˆGibbsåˆ†å¸ƒï¼‰ |
| $Z(\theta)$ | æ­£è¦åŒ–å®šæ•°ï¼ˆPartition Functionï¼‰ |
| $v$ | RBMå¯è¦–å±¤ |
| $h$ | RBMéš ã‚Œå±¤ |
| $W$ | RBMé‡ã¿è¡Œåˆ— |
| $\xi^i$ | Hopfieldè¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³ |
| $\beta$ | é€†æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ |
| $\tau$ | æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ |
| $T(x' \| x)$ | Markové€£é–é·ç§»ã‚«ãƒ¼ãƒãƒ« |
| $\alpha(x' \| x)$ | Metropolis-Hastingså—ç†ç¢ºç‡ |
| $H(q, p)$ | Hamiltonianï¼ˆHamiltoné–¢æ•°ï¼‰ |
| $U(q)$ | ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã‚¨ãƒãƒ«ã‚®ãƒ¼ |
| $K(p)$ | é‹å‹•ã‚¨ãƒãƒ«ã‚®ãƒ¼ |
| $\epsilon$ | ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º |
| $L$ | Leapfrog stepsæ•° |
---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
