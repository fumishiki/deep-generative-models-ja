---
title: "ç¬¬4å›: ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
emoji: "ğŸ²"
type: "tech"
topics: ["æ©Ÿæ¢°å­¦ç¿’", "ç¢ºç‡è«–", "çµ±è¨ˆå­¦", "æ•°å­¦", "Python"]
published: true
slug: "ml-lecture-04-part2"
difficulty: "intermediate"
time_estimate: "90 minutes"
languages: ["Python"]
keywords: ["ç¢ºç‡åˆ†å¸ƒå®Ÿè£…", "MLEå®Ÿè£…", "ãƒ™ã‚¤ã‚ºæ¨è«–", "SciPy", "çµ±è¨ˆçš„æ¨å®š"]
---

# ç¬¬4å›: ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦ã€å¾Œç·¨ã€‘

> ç†è«–ç·¨ã¯ [ã€å‰ç·¨ã€‘ç¬¬4å›: ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦](/articles/ml-lecture-04-part1) ã‚’ã”è¦§ãã ã•ã„ã€‚

## Learning Objectives

ã“ã®å®Ÿè£…ç·¨ã‚’ä¿®äº†ã™ã‚‹ã¨ã€ä»¥ä¸‹ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™:

- [ ] PyTorch `torch.distributions` ã§ `log_prob`ãƒ»`entropy`ãƒ»`kl_divergence` ã‚’ä½¿ã„ã“ãªã›ã‚‹
- [ ] Gaussian MLE ã‚’ `D.Normal(mu, sigma).log_prob(x).sum()` ã§å®Ÿè£…ãƒ»æ¤œè¨¼ã§ãã‚‹
- [ ] è‡ªå·±å›å¸°å°¤åº¦ $\log p(\mathbf{x}) = \sum_t \log p(x_t \mid x_{<t})$ ã‚’ PyTorch ã§æ•°å€¤å®‰å®šã«è¨ˆç®—ã§ãã‚‹
- [ ] `torch.logsumexp` ã®æ•°å€¤å®‰å®šæ€§ã®æ ¹æ‹ ã‚’å¼ã‹ã‚‰èª¬æ˜ã§ãã‚‹
- [ ] Triton ã‚«ãƒ¼ãƒãƒ«ã§å„ãƒ‡ãƒ¼ã‚¿ç‚¹ä¸¦åˆ—ã®å¯¾æ•°å°¤åº¦è¨ˆç®—ã‚’å®Ÿè£…ã§ãã‚‹
- [ ] å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã®æ¡ä»¶ä»˜ãåˆ†å¸ƒï¼ˆSchur è£œè¡Œåˆ—ï¼‰ã®å°å‡ºã¨ Cholesky å®‰å®šåŒ–ã®å¿…è¦æ€§ã‚’èª¬æ˜ã§ãã‚‹
- [ ] KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ãƒ»Fisher æƒ…å ±é‡ãƒ»CramÃ©r-Rao ä¸‹ç•Œã®é–¢ä¿‚ã‚’å°å‡ºã‹ã‚‰ç¤ºã›ã‚‹

---

## ğŸ’» Z5. è©¦ç·´ï¼ˆ75åˆ†ï¼‰â€” 5ãƒˆãƒ”ãƒƒã‚¯å®Œå…¨å®Ÿè£…+æ¤œè¨¼

### 5.1 ç¢ºç‡åˆ†å¸ƒã®å®Œå…¨å®Ÿè£… â€” PDFãƒ»CDFãƒ»ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ»MLE

ç¢ºç‡åˆ†å¸ƒã‚’ã€Œä½¿ãˆã‚‹ã€ã¨ã¯ã©ã†ã„ã†ã“ã¨ã‹ã€‚PDF ã‚’è©•ä¾¡ã—ã€ç´¯ç©ç¢ºç‡ã‚’è¨ˆç®—ã—ã€ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã—ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¨å®šã™ã‚‹â€”â€”ã“ã®4ã¤ãŒã‚»ãƒƒãƒˆã ã€‚

**Gaussian: æœ€ã‚‚é‡è¦ãªåˆ†å¸ƒ**

$X \sim \mathcal{N}(\mu, \sigma^2)$ ã®ã¨ã:

$$
f(x; \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\!\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
$$

- shape: `data: (N,)` â†’ `log_prob(data): (N,)` â†’ `.sum()` ã§ã‚¹ã‚«ãƒ©ãƒ¼å¯¾æ•°å°¤åº¦
- `D.Normal(mu, sigma)` ã®ç¬¬2å¼•æ•°ã¯**æ¨™æº–åå·®** $\sigma$ï¼ˆåˆ†æ•£ $\sigma^2$ ã§ã¯ãªã„ï¼‰ã€‚æ··åŒã™ã‚‹ã¨å°¤åº¦ãŒå…¨ã¦é–“é•ã†
- æ•°å€¤å®‰å®šåŒ–: `.log_prob()` ã¯å†…éƒ¨ã§ $\log$ ç©ºé–“è¨ˆç®—ã‚’è¡Œã„ `exp(-...)` ã®ã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ãƒ­ãƒ¼ã‚’å›é¿ã™ã‚‹

è¨˜å· â†” å¤‰æ•°å¯¾å¿œ:
- $\mu, \sigma$ â†” `mu_mle`, `sigma_mle`ï¼ˆã‚¹ã‚«ãƒ©ãƒ¼ãƒ†ãƒ³ã‚½ãƒ«ï¼‰
- $\ell(\mu,\sigma) = \sum_i \log \mathcal{N}(x^{(i)}\mid\mu,\sigma)$ â†” `D.Normal(mu_mle, sigma_mle).log_prob(data).sum()`
- $\mathcal{H}(\boldsymbol{\pi}) = -\sum_k \pi_k \log \pi_k$ â†” `D.Categorical(probs=pi).entropy()`
- $D_{\mathrm{KL}}(p \| q)$ â†” `D.kl_divergence(p, q)`ï¼ˆç™»éŒ²æ¸ˆã¿ãƒšã‚¢ã«å¯¾ã—ã¦é–‰å½¢å¼ï¼‰

æ¤œç®—: (i) MLE ãŒå¯¾æ•°å°¤åº¦ã‚’æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ï¼ˆæ‘‚å‹•å¾Œã®å°¤åº¦ãŒä¸‹ãŒã‚‹ï¼‰ã€(ii) ä¸€æ§˜åˆ†å¸ƒã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ $= \log K$ã€(iii) $D_{\mathrm{KL}}(p\|p) = 0$ã€ã®3ç‚¹ã§ãã‚Œãã‚Œ assert ã™ã‚‹ã€‚

$$
\ell(\mu, \sigma) = \sum_{i=1}^{N} \log \mathcal{N}(x^{(i)} \mid \mu, \sigma)
= -\frac{N}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^{N}(x^{(i)}-\mu)^2
$$

```python
import torch
import torch.distributions as D

torch.manual_seed(42)

# --- Block 1 / 3: torch.distributions â€” Normal, Categorical, MultivariateNormal ---

# Gaussian MLE: mu_hat = x.mean(),  sigma_hat = x.std(unbiased=False)
mu_true, sigma_true = torch.tensor(2.0), torch.tensor(1.5)
data = D.Normal(mu_true, sigma_true).sample((500,))      # (500,)

mu_mle    = data.mean()                                  # mu
sigma_mle = data.std(unbiased=False)                     # sigma (biased MLE, ddof=0)

# log p(D) = sum_i log N(x_i | mu, sigma)
ll_mle       = D.Normal(mu_mle,       sigma_mle).log_prob(data).sum()
ll_perturbed = D.Normal(mu_mle + 0.1, sigma_mle).log_prob(data).sum()
assert ll_mle > ll_perturbed                             # MLE is the argmax

# Categorical: H[Uniform(K)] = log K
pi_uniform = torch.full((5,), 1.0 / 5)
H_cat = D.Categorical(probs=pi_uniform).entropy()        # scalar
assert abs(H_cat - torch.log(torch.tensor(5.0))) < 1e-5  # H = log K

# MultivariateNormal: already shown â€” torch handles Cholesky internally
mu_mv  = torch.zeros(2)
cov_mv = torch.tensor([[2.0, 0.8], [0.8, 1.0]])
dist_mv = D.MultivariateNormal(loc=mu_mv, covariance_matrix=cov_mv)
x0 = torch.tensor([1.0, -1.0])
print(f"log N(x0|mu,Sigma) = {dist_mv.log_prob(x0):.6f}")

# KL divergence (closed form for registered pairs)
p = D.Normal(0.0, 1.0)
q = D.Normal(1.0, 2.0)
kl_pq = D.kl_divergence(p, q)                           # KL[N(0,1) || N(1,2)]
assert kl_pq > 0 and D.kl_divergence(p, p) < 1e-6       # KL >= 0, KL(p||p)=0

print(f"mu_mle={mu_mle:.4f}, sigma_mle={sigma_mle:.4f}")
print(f"H[Uniform(5)]={H_cat:.4f}, log(5)={torch.log(torch.tensor(5.0)):.4f}")
print(f"KL[N(0,1)||N(1,2)]={kl_pq:.4f}")
```

**Bernoulli â†’ Categorical: é›¢æ•£åˆ†å¸ƒã®ç³»è­œ**

$$
P(X=k \mid \mathbf{p}) = p_k, \quad k \in \{1,\ldots,K\},\quad \sum_k p_k = 1
$$

Bernoulli ã¯ $K=2$ ã®ç‰¹æ®Šã‚±ãƒ¼ã‚¹ã€‚Softmax ãŒ Categorical ã®å‡ºåŠ›å±¤ã«ãªã‚‹ç†ç”±: $\mathbf{p} = \text{softmax}(\mathbf{z})$ ã¨ã™ã‚Œã° $\sum_k p_k = 1$ ãŒè‡ªå‹•çš„ã«æº€ãŸã•ã‚Œã‚‹ã€‚

MLE: $N$ å€‹ã®è¦³æ¸¬ $x^{(1)},\ldots,x^{(N)}$ ã‹ã‚‰:

$$
\hat{p}_k = \frac{\#\{i : x^{(i)} = k\}}{N}
$$

ã‚«ã‚¦ãƒ³ãƒˆã‚’ $N$ ã§å‰²ã‚‹ã ã‘ã€‚äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤± $-\sum_k y_k \log p_k$ ã®æœ€å°åŒ– = Categorical MLE ã ã€‚

**å¤§æ•°ã®æ³•å‰‡ (LLN) ã¨ä¸­å¿ƒæ¥µé™å®šç† (CLT) â€” æ•°å€¤æ¤œè¨¼**

ç†è«–çš„ã«ä¿è¨¼ã•ã‚Œã¦ã„ã‚‹ãŒã€å…·ä½“çš„ã«ã©ã†åæŸã™ã‚‹ã‹æ•°å€¤ã§ç¢ºèªã™ã‚‹ã€‚

LLN: $\bar{X}_N \xrightarrow{P} \mu$ï¼ˆç¢ºç‡åæŸï¼‰

$$
P(|\bar{X}_N - \mu| > \epsilon) \leq \frac{\sigma^2}{N \epsilon^2}
$$

CLT: $\sqrt{N}(\bar{X}_N - \mu) \xrightarrow{d} \mathcal{N}(0, \sigma^2)$ï¼ˆåˆ†å¸ƒåæŸï¼‰

$$
Z_N = \frac{\bar{X}_N - \mu}{\sigma/\sqrt{N}} \xrightarrow{d} \mathcal{N}(0, 1)
$$

è¨˜å· â†” å¤‰æ•°å¯¾å¿œ:
- $\bar{X}_N = \frac{1}{N}\sum_{i=1}^N X_i$ â†” `X.mean(axis=1)` shape `(n_trials,)`
- $Z_N$ï¼ˆæ¨™æº–åŒ–æ¨™æœ¬å¹³å‡ï¼‰â†” `Z_N: (n_trials,)` â†’ `N(0,1)` ã«åæŸ
- $\text{KS}$ï¼ˆKolmogorov-Smirnovæ¤œå®šé‡ï¼‰â†” CLTåæŸã®å®šé‡çš„è©•ä¾¡

**åæŸã®é€Ÿã• â€” Berry-Esseen å®šç†**:

CLT ã¯ $Z_N \xrightarrow{d} \mathcal{N}(0,1)$ ã‚’ä¿è¨¼ã™ã‚‹ãŒã€Œã„ã¤åæŸã™ã‚‹ã‹ã€ã¯è¿°ã¹ãªã„ã€‚Berry-Esseen å®šç†ãŒå®šé‡åŒ–ã™ã‚‹:

$$
\sup_x \left| P(Z_N \leq x) - \Phi(x) \right| \leq \frac{C \rho}{\sigma^3 \sqrt{N}}, \quad C \leq 0.4748
$$

ã“ã“ã§ $\rho = \mathbb{E}[|X - \mu|^3]$ï¼ˆä¸‰æ¬¡çµ¶å¯¾ä¸­å¿ƒãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆï¼‰ã€‚Exponential$(1)$ ã§ã¯ $\mu=1$, $\sigma^2=1$, $\rho=\mathbb{E}[|X-1|^3]=2$ ãªã®ã§:

$$
\text{èª¤å·®ä¸Šç•Œ} \leq \frac{0.4748 \times 2}{\sqrt{N}} = \frac{0.9496}{\sqrt{N}}
$$

$N=5$: èª¤å·® $\leq 0.424$ï¼ˆExponential ã®æ­ªåº¦ = 2 ãŒå¤§ãã„ãŸã‚åæŸãŒé…ã„ï¼‰ã€‚  
$N=500$: èª¤å·® $\leq 0.042$ï¼ˆKS æ¤œå®šã§æœ‰æ„å·®ãŒæ¤œå‡ºã•ã‚Œã«ãã„æ°´æº–ï¼‰ã€‚

**æ­ªåº¦ã¨åæŸé€Ÿåº¦**: $\rho/\sigma^3$ ã¯åˆ†å¸ƒã®ã€Œæ­ªã¿ã€ã‚’æ‰ãˆã‚‹ã€‚æ­£è¦åˆ†å¸ƒè‡ªä½“ã®æ­ªåº¦ã¯ 0 ã ãŒã€é‡‘èåç›Šç‡ã‚„è‡ªç„¶è¨€èªã®å˜èªé »åº¦ã¯ Power-lawï¼ˆZipf ã®æ³•å‰‡ï¼‰ã«å¾“ã„ã€ä¸‰æ¬¡ä»¥ä¸Šã®ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆãŒç„¡é™å¤§ã«ãªã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚ãã®ã‚ˆã†ãªåˆ†å¸ƒã§ã¯ CLT ã®åæŸãŒä¿è¨¼ã•ã‚Œãšã€æ­£è¦è¿‘ä¼¼ã¯å±é™ºã ã€‚

LLN ã®åæŸé€Ÿåº¦ã¯ Chebyshev ä¸ç­‰å¼ã‹ã‚‰ç›´æ¥å°ã‘ã‚‹:

$$
P(|\bar{X}_N - \mu| > \epsilon) \leq \frac{\sigma^2}{N\epsilon^2}
$$

ã“ã‚Œã¯ $O(1/N)$ ã®ç¢ºç‡ä¿è¨¼ã ãŒã€æ¨™æœ¬å¹³å‡ã®æ¨™æº–åå·® $\sigma/\sqrt{N}$ ã‚’è¦‹ã‚‹ã¨å®Ÿè³ªçš„ãªç²¾åº¦ã¯ $O(1/\sqrt{N})$ã€‚**ãƒ‡ãƒ¼ã‚¿ã‚’ 100 å€ã«ã—ã¦ã‚‚ç²¾åº¦ã¯ 10 å€ã«ã—ã‹ãªã‚‰ãªã„** â€” ã“ã‚ŒãŒå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿åé›†ã®ã€Œé™ç•Œåç›Šé€“æ¸›ã€ã®æ•°å­¦çš„æ ¹æ‹ ã ã€‚

**è§£é‡ˆ**: Exponential åˆ†å¸ƒã¯å³è£¾ãŒé‡ã„ï¼ˆæ­ªåº¦ = 2ï¼‰ãŒã€$N=500$ ã§æ¨™æœ¬å¹³å‡ã®åˆ†å¸ƒã¯ã»ã¼æ­£è¦åˆ†å¸ƒã«åæŸã™ã‚‹ã€‚LLN èª¤å·®ã¯ $N$ ãŒå¢—ãˆã‚‹ã«ã¤ã‚Œ $O(1/\sqrt{N})$ ã§æ¸›å°‘ã™ã‚‹ã€‚

**è§£é‡ˆ**: Exponentialåˆ†å¸ƒã¯å³è£¾ãŒé‡ã„ãŒã€N=500ã§æ¨™æœ¬å¹³å‡ã®åˆ†å¸ƒã¯ã»ã¼æ­£è¦åˆ†å¸ƒã«åæŸã™ã‚‹ã€‚LLNèª¤å·®ã¯NãŒå¢—ãˆã‚‹ã«ã¤ã‚Œ $O(1/\sqrt{N})$ ã§æ¸›å°‘ â€” Chebyshevä¸ç­‰å¼ã® $O(1/N)$ ã‚ˆã‚Šé€Ÿã„ï¼ˆæœŸå¾…å€¤ã®åæŸé€Ÿåº¦ï¼‰ã€‚

**Softmax ã¨ Categorical ã®å®Œå…¨å®Ÿè£…**:

$p_k = \frac{\exp(z_k)}{\sum_j \exp(z_j)}$ï¼ˆSoftmax = Categorical ã®è‡ªç„¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\boldsymbol{\eta}$ ã‹ã‚‰æœŸå¾…å€¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\boldsymbol{\pi}$ ã¸ã®å¤‰æ›ï¼‰

è¨˜å· â†” å¤‰æ•°å¯¾å¿œ:
- $\mathbf{z}$ï¼ˆlogitï¼‰â†” `z: (K,)`
- $\boldsymbol{\pi} = \text{softmax}(\mathbf{z})$ â†” `pi: (K,)`, `sum=1`
- $\mathcal{H}(\boldsymbol{\pi}) = -\sum_k \pi_k \log \pi_k$ï¼ˆã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼‰â†” `H: float`

**ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æœ€å¤§åŒ–ã®æ•°å­¦ â€” Lagrange ä¹—æ•°æ³•**:

ã€Œåˆ¶ç´„ã®ã‚‚ã¨ã§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’æœ€å¤§åŒ–ã™ã‚‹ã¨ä¸€æ§˜åˆ†å¸ƒãŒå¾—ã‚‰ã‚Œã‚‹ã€ã‚’ç¤ºã™ã€‚

å•é¡Œ: $\max_{\boldsymbol{\pi}} \mathcal{H}(\boldsymbol{\pi}) = -\sum_{k=1}^{K} \pi_k \log \pi_k$ subject to $\sum_k \pi_k = 1$, $\pi_k \geq 0$

Lagrangian ã‚’æ§‹æˆã—åœç•™æ¡ä»¶ã‚’å–ã‚‹:

$$
\mathcal{L} = -\sum_k \pi_k \log \pi_k + \lambda\!\left(\sum_k \pi_k - 1\right)
$$

$$
\frac{\partial \mathcal{L}}{\partial \pi_k} = -\log \pi_k - 1 + \lambda = 0 \implies \pi_k = e^{\lambda - 1}
$$

å…¨ $k$ ã§åŒã˜å€¤ â†’ æ­£è¦åŒ–æ¡ä»¶ $\sum_k \pi_k = 1$ ã‚ˆã‚Š $\pi_k = 1/K$ã€‚ã“ã®ã¨ã $\mathcal{H} = \log K$ã€‚ã‚ˆã£ã¦:

$$
\mathcal{H}(\boldsymbol{\pi}) \leq \log K, \quad \text{ç­‰å·ã¯ } \boldsymbol{\pi} = (1/K, \ldots, 1/K) \text{ ã®ã¨ã}
$$

**ä¸€ç‚¹ã¸ã®é›†ä¸­ã§ $\mathcal{H} \to 0$**: $\pi_1 \to 1$ï¼ˆone-hotï¼‰ã¨ã™ã‚‹ã¨ $-1 \cdot \log 1 - \sum_{k \geq 2} 0 \cdot \log 0 = 0$ï¼ˆ$0 \log 0 = 0$ ã¨å®šç¾©ï¼‰ã€‚ã“ã‚ŒãŒã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æœ€å°ã€‚

**æ•°å€¤å®‰å®š softmax ã®æ ¸å¿ƒ**:

$$
\text{softmax}(\mathbf{z})_k = \frac{e^{z_k}}{\sum_j e^{z_j}} = \frac{e^{z_k - c}}{\sum_j e^{z_j - c}}, \quad c = \max_k z_k
$$

$c$ ã‚’å¼•ã„ã¦ã‚‚æ¯”ã¯å¤‰ã‚ã‚‰ãªã„ï¼ˆåˆ†å­ãƒ»åˆ†æ¯ã« $e^{-c}$ ãŒå…±é€šå› å­ï¼‰ã€‚$c = \max_k z_k$ ã¨ã™ã‚‹ã¨ $e^{z_k - c} \leq 1$ ãŒä¿è¨¼ã•ã‚Œ `exp` ãŒã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ã—ãªã„ã€‚$\log \text{softmax}(\mathbf{z})_k = z_k - c - \log \sum_j e^{z_j - c}$ ãŒ `F.log_softmax` ã®è¨ˆç®—å¼ã ã€‚

ã“ã® $\log \sum_j e^{z_j}$ ãŒ `torch.logsumexp(z, dim=-1)` ã§ã‚ã‚Šã€æ•°å€¤å®‰å®šã« $\log Z$ ã‚’è¨ˆç®—ã™ã‚‹åŸºæœ¬ãƒ„ãƒ¼ãƒ«ã ã€‚æ¬¡ã® identity ã¯å¸¸ã«æˆç«‹ã™ã‚‹:

$$
\log \sum_j e^{z_j} = c + \log \sum_j e^{z_j - c}, \quad c = \max_k z_k
$$

**å¤§æ•°ã®æ³•å‰‡ã®ç›´æ„Ÿ**: Bernoulli$(p)$ ã®æ¨™æº–åå·®ã¯ $\sqrt{p(1-p)}$ã€‚$p=0.3$ ã§ $\sigma \approx 0.458$ã€‚æ¨™æœ¬å¹³å‡ã®æ¨™æº–èª¤å·®ã¯ $0.458/\sqrt{N}$ã€‚$N=10^4$ ã§ $\approx 0.0046$ â€” A/B ãƒ†ã‚¹ãƒˆã§ã€Œæ•°åƒã‚µãƒ³ãƒ—ãƒ«å¿…è¦ã€ã¨è¨€ã‚ã‚Œã‚‹æ ¹æ‹ ã ã€‚ç²¾åº¦ã‚’2å€ã«ã™ã‚‹ã«ã¯ãƒ‡ãƒ¼ã‚¿ãŒ4å€å¿…è¦ã¨ã„ã† $O(1/\sqrt{N})$ ã®å£ã¯ LLN ã®æœ¬è³ªçš„ãªé™ç•Œã ã€‚

### 5.2 å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒ â€” å®Œå…¨å®Ÿè£…ã¨ç›´æ„Ÿ

1æ¬¡å…ƒGaussianã®è‡ªç„¶ãªæ‹¡å¼µã¯ã€ã€Œå¤‰æ•°é–“ã®ç›¸é–¢ã€ã‚’æ‰ãˆã‚‹ã€‚

**å®šç¾©**:

$$
\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}) =
\frac{1}{(2\pi)^{d/2} |\boldsymbol{\Sigma}|^{1/2}}
\exp\!\left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{x}-\boldsymbol{\mu})\right)
$$

- shape: `x` ã¯ `(d,)`, `mu` ã¯ `(d,)`, `Sigma` ã¯ `(d,d)` æ­£å®šå€¤å¯¾ç§°è¡Œåˆ—
- Mahalanobisè·é›¢ $D_M^2 = (\mathbf{x}-\boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{x}-\boldsymbol{\mu})$ ã¯ã€Œæ¥•å††ä½“ã®è·é›¢ã€
- $\boldsymbol{\Sigma}^{-1}$ ã®ç›´æ¥è¨ˆç®—ã¯é¿ã‘ã‚‹: `torch.linalg.solve(Sigma, x - mu)` ã‹ `D.MultivariateNormal` ã‚’ä½¿ã†

**æ¡ä»¶ä»˜ãåˆ†å¸ƒ** (Schur complement å…¬å¼):

å¤‰æ•°ã‚’ $[\mathbf{x}_1, \mathbf{x}_2]$ ã«åˆ†å‰²ã™ã‚‹ã¨:

$$
p(\mathbf{x}_1 \mid \mathbf{x}_2) = \mathcal{N}(\boldsymbol{\mu}_{1|2},\, \boldsymbol{\Sigma}_{1|2})
$$

$$
\boldsymbol{\mu}_{1|2} = \boldsymbol{\mu}_1 + \boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}(\mathbf{x}_2 - \boldsymbol{\mu}_2)
$$

$$
\boldsymbol{\Sigma}_{1|2} = \boldsymbol{\Sigma}_{11} - \boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}\boldsymbol{\Sigma}_{21}
$$

$\boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}$ ã¯ã€ŒKalman gainã€ã®å½¢ã€‚$\mathbf{x}_2$ ã‚’è¦³æ¸¬ã™ã‚‹ã“ã¨ã§ã€$\mathbf{x}_1$ ã®ä¸ç¢ºå®Ÿæ€§ $\boldsymbol{\Sigma}_{1|2}$ ã¯å…ƒã® $\boldsymbol{\Sigma}_{11}$ ã‚ˆã‚Šå¿…ãšå°ã•ããªã‚‹ï¼ˆåŠæ­£å®šå€¤ã®æ„å‘³ã§ï¼‰ã€‚

**MLE**: å…¨å¾®åˆ†ã—ã¦ã‚¼ãƒ­ç‚¹ã‚’è§£ãã¨:

$$
\hat{\boldsymbol{\mu}} = \frac{1}{N}\sum_{i=1}^N \mathbf{x}^{(i)}, \quad
\hat{\boldsymbol{\Sigma}} = \frac{1}{N}\sum_{i=1}^N (\mathbf{x}^{(i)} - \hat{\boldsymbol{\mu}})(\mathbf{x}^{(i)} - \hat{\boldsymbol{\mu}})^\top
$$

ã‚µãƒ³ãƒ—ãƒ«å¹³å‡ã¨ã‚µãƒ³ãƒ—ãƒ«å…±åˆ†æ•£è¡Œåˆ—ãŒãã®ã¾ã¾MLEè§£ã ï¼ˆ1æ¬¡å…ƒã¨åŒã˜æ§‹é€ ï¼‰ã€‚


**Choleskyåˆ†è§£ã«ã‚ˆã‚‹å®‰å®šå®Ÿè£…**:

$\boldsymbol{\Sigma}$ ãŒæ­£å®šå€¤ â†’ $\boldsymbol{\Sigma} = LL^\top$ ã® Cholesky åˆ†è§£ãŒå­˜åœ¨ã™ã‚‹ã€‚

$$
\log \mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}) =
-\frac{d}{2}\log 2\pi - \frac{1}{2}\log|\boldsymbol{\Sigma}|
- \frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})
$$

è¨˜å· â†” å¤‰æ•°å¯¾å¿œï¼ˆPyTorch ã§ã¯ Block 1 ã® `D.MultivariateNormal` ãŒæ‹…ã†ï¼‰:
- $\boldsymbol{\mu}$ â†” `mu_mv: (d,)` ãƒ†ãƒ³ã‚½ãƒ«
- $\boldsymbol{\Sigma}$ â†” `cov_mv: (d,d)` æ­£å®šå€¤å¯¾ç§°ãƒ†ãƒ³ã‚½ãƒ«
- $\log |\boldsymbol{\Sigma}| = 2\sum_i \log L_{ii}$ï¼ˆCholesky å› å­ã®å¯¾è§’ç©ï¼‰â†” `dist_mv.log_prob(x)` ã«å†…åŒ…
- Mahalanobis è·é›¢ $D_M^2 = (\mathbf{x}-\boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})$ â†” `v @ v`ï¼ˆ`v = L^{-1}(x-mu)`ï¼‰

shape: `x: (d,)`, `mu: (d,)`, `Sigma: (d,d)`, å‡ºåŠ› `log_prob: scalar`

**Cholesky å®‰å®šåŒ–ã®ç†ç”±**: $\boldsymbol{\Sigma}^{-1}$ ã‚’ç›´æ¥è¨ˆç®—ã™ã‚‹ã¨æ•°å€¤èª¤å·®ãŒ $O(\kappa^2(\boldsymbol{\Sigma}))$ ã§å¢—å¹…ã•ã‚Œã‚‹ï¼ˆ$\kappa$ = æ¡ä»¶æ•°ï¼‰ã€‚Cholesky åˆ†è§£ $\boldsymbol{\Sigma} = LL^\top$ ã‚’çµŒç”±ã™ã‚‹ã¨:
- $\log|\boldsymbol{\Sigma}| = 2\sum_i \log L_{ii}$ï¼ˆå¯¾è§’æˆåˆ†ã®å¯¾æ•°å’Œï¼‰
- $\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu}) = L^{-\top}L^{-1}(\mathbf{x}-\boldsymbol{\mu})$ï¼ˆå‰é€²ä»£å…¥ + å¾Œé€€ä»£å…¥ï¼‰

ç›´æ¥é€†è¡Œåˆ—ã‚’æ±‚ã‚ã‚‹ã‚ˆã‚Šæ•°å€¤èª¤å·®ãŒ $O(\kappa)$ ã§æŠ‘ãˆã‚‰ã‚Œã‚‹ã€‚PyTorch ã® `D.MultivariateNormal` ã¯å†…éƒ¨ã§ Cholesky å› å­ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã€åŒã˜åˆ†å¸ƒã§è¤‡æ•°ã® `log_prob` è©•ä¾¡ã‚’è¡Œã†å ´åˆã«åŠ¹ç‡çš„ã ã€‚

**MLE ã¨æ­£å®šå€¤åˆ¶ç´„**: $N$ å€‹ã® $d$ æ¬¡å…ƒã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰ $\hat{\boldsymbol{\Sigma}} = \frac{1}{N}\sum_i (\mathbf{x}^{(i)}-\hat{\boldsymbol{\mu}})(\mathbf{x}^{(i)}-\hat{\boldsymbol{\mu}})^\top$ ã‚’è¨ˆç®—ã™ã‚‹å ´åˆã€$N < d$ ã§ã¯è¡Œåˆ—ã®ãƒ©ãƒ³ã‚¯ãŒ $N$ ã«ãªã‚ŠåŠæ­£å®šå€¤ï¼ˆ$\hat{\boldsymbol{\Sigma}} \succeq 0$ ã ãŒ $\hat{\boldsymbol{\Sigma}} \not\succ 0$ï¼‰ã€‚Cholesky åˆ†è§£ãŒå¤±æ•—ã™ã‚‹ã€‚

å›é¿ç­–: $\hat{\boldsymbol{\Sigma}}_\text{reg} = \hat{\boldsymbol{\Sigma}} + \epsilon \mathbf{I}$ï¼ˆ$\epsilon \sim 10^{-6}$ï¼‰ã§æ­£å‰‡åŒ–ã€‚VAE ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å‡ºåŠ› $\boldsymbol{\Sigma}_\phi = \text{diag}(\boldsymbol{\sigma}_\phi^2)$ ãŒå¯¾è§’è¡Œåˆ—ã«é™å®šã•ã‚Œã‚‹ã®ã‚‚ã€ãƒ•ãƒ«ãƒ©ãƒ³ã‚¯å…±åˆ†æ•£ã®æ¨å®šå›°é›£ã‚’å›é¿ã™ã‚‹ãŸã‚ã ã€‚

**Block 1 ã§ã®ç¢ºèª**: ä¸Šã® PyTorch ãƒ–ãƒ­ãƒƒã‚¯ã§ `D.MultivariateNormal(mu_mv, cov_mv).log_prob(x0)` ãŒ Cholesky çµŒç”±ã§å®‰å®šã«è¨ˆç®—ã•ã‚Œã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚`covariance_matrix` ã«ä»£ãˆã¦ `scale_tril=L`ï¼ˆCholesky å› å­ç›´æ¥æ¸¡ã—ï¼‰ã‚‚ä½¿ãˆã‚‹ â€” æ—¢ã« Cholesky åˆ†è§£æ¸ˆã¿ã®å ´åˆã¯å¾Œè€…ãŒåŠ¹ç‡çš„ã ã€‚

**æ¡ä»¶ä»˜ãåˆ†å¸ƒ**:

$$
\boldsymbol{\mu}_{1|2} = \boldsymbol{\mu}_1 + \boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}(\mathbf{x}_2 - \boldsymbol{\mu}_2)
$$

$$
\boldsymbol{\Sigma}_{1|2} = \boldsymbol{\Sigma}_{11} - \boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}\boldsymbol{\Sigma}_{21}
$$

$\boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}$ ã¯ Kalman gain ã¨åŒå‹ã€‚$\mathbf{x}_2$ ã‚’è¦³æ¸¬ã™ã‚‹ã¨åˆ†æ•£ã¯å¿…ãšç¸®ã‚€: $\boldsymbol{\Sigma}_{1|2} \preceq \boldsymbol{\Sigma}_{11}$ï¼ˆåŠæ­£å®šå€¤é †åºï¼‰ã€‚

**Schur è£œè¡Œåˆ—å…¬å¼ã®å°å‡º**:

ãƒ–ãƒ­ãƒƒã‚¯è¡Œåˆ—ã®é€†è¡Œåˆ—ã‚’ä½¿ã†ã€‚$\boldsymbol{\Sigma} = \begin{pmatrix}\boldsymbol{\Sigma}_{11}&\boldsymbol{\Sigma}_{12}\\\boldsymbol{\Sigma}_{21}&\boldsymbol{\Sigma}_{22}\end{pmatrix}$ ã®é€†è¡Œåˆ—ã®ãƒ–ãƒ­ãƒƒã‚¯ $(1,1)$ æˆåˆ†ãŒ $(\boldsymbol{\Sigma}_{11} - \boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}\boldsymbol{\Sigma}_{21})^{-1}$ã€‚ã“ã‚ŒãŒ Schur è£œè¡Œåˆ— $\boldsymbol{\Sigma}_{1|2}^{-1}$ ã ã€‚

çµåˆæ­£è¦åˆ†å¸ƒã®å®šç¾©ã‹ã‚‰æ¡ä»¶ä»˜ãåˆ†å¸ƒã‚’å°ã:

$$
\begin{aligned}
\log p(\mathbf{x}_1, \mathbf{x}_2) &= -\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu}) + \text{const} \\
p(\mathbf{x}_1 \mid \mathbf{x}_2) &\propto p(\mathbf{x}_1, \mathbf{x}_2) \quad (\mathbf{x}_2 \text{ å›ºå®š})
\end{aligned}
$$

$\mathbf{x}_2 = \mathbf{a}$ ã‚’å›ºå®šã—ã¦ $\mathbf{x}_1$ ã«ã¤ã„ã¦ã®äºŒæ¬¡å½¢å¼ã‚’æ•´ç†ã™ã‚‹ã¨:

$$
-\frac{1}{2}\bigl(\mathbf{x}_1 - \boldsymbol{\mu}_{1|2}\bigr)^\top \boldsymbol{\Sigma}_{1|2}^{-1}\bigl(\mathbf{x}_1 - \boldsymbol{\mu}_{1|2}\bigr) + \text{const}
$$

ã“ã‚Œã¯ $\mathcal{N}(\boldsymbol{\mu}_{1|2}, \boldsymbol{\Sigma}_{1|2})$ ã®å¯¾æ•°å¯†åº¦ã ã€‚

**åˆ†æ•£ãŒç¸®ã‚€ç†ç”±**: $\boldsymbol{\Sigma}_{1|2} = \boldsymbol{\Sigma}_{11} - \boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}\boldsymbol{\Sigma}_{21}$ ã«ãŠã„ã¦ã€å¼•ã‹ã‚Œã‚‹é … $\boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}\boldsymbol{\Sigma}_{21}$ ã¯åŠæ­£å®šå€¤ï¼ˆ$\mathbf{v}^\top \boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}\boldsymbol{\Sigma}_{21}\mathbf{v} = \|\boldsymbol{\Sigma}_{22}^{-1/2}\boldsymbol{\Sigma}_{21}\mathbf{v}\|^2 \geq 0$ï¼‰ã€‚ã‚ˆã£ã¦ $\boldsymbol{\Sigma}_{1|2} \preceq \boldsymbol{\Sigma}_{11}$ï¼ˆåŠæ­£å®šå€¤é †åºï¼‰â€” **è¦³æ¸¬ã™ã‚‹ã»ã©ä¸ç¢ºå®Ÿæ€§ã¯å¿…ãšæ¸›å°‘ã™ã‚‹**ã€‚

å…·ä½“ä¾‹: $\boldsymbol{\Sigma} = \begin{pmatrix}2 & 0.8 \\ 0.8 & 1\end{pmatrix}$, $\mathbf{x}_2 = -1$ ã‚’è¦³æ¸¬ã™ã‚‹ã¨:

$$
\boldsymbol{\mu}_{1|2} = 1 + 0.8 \cdot 1^{-1} \cdot (-1 - (-2)) = 1 + 0.8 = 1.8
$$

$$
\boldsymbol{\Sigma}_{1|2} = 2 - 0.8^2/1 = 2 - 0.64 = 1.36 < 2 = \boldsymbol{\Sigma}_{11}
$$

æ­£ã®ç›¸é–¢ $\rho = 0.8/\sqrt{2 \cdot 1} \approx 0.566$ ãŒã‚ã‚‹ãŸã‚ã€$x_2$ ã®è¦³æ¸¬ãŒ $x_1$ ã®äºˆæ¸¬ã‚’ä¸Šæ–¹ä¿®æ­£ã—ã€ä¸ç¢ºå®Ÿæ€§ã‚’ $2 \to 1.36$ï¼ˆ32\%å‰Šæ¸›ï¼‰ã™ã‚‹ã€‚ã“ã®å…¬å¼ã¯ Kalman ãƒ•ã‚£ãƒ«ã‚¿ã®æ›´æ–°å¼ã¨åŒå‹ã§ã‚ã‚Šã€GPGPU ä¸Šã®çŠ¶æ…‹æ¨å®šã‹ã‚‰ VAE ã®äº‹å¾Œåˆ†å¸ƒè¨ˆç®—ã¾ã§åºƒãä½¿ã‚ã‚Œã‚‹ã€‚

### 5.3 æŒ‡æ•°å‹åˆ†å¸ƒæ— â€” çµ±ä¸€çš„è¨˜è¿°

Gaussian, Bernoulli, Poisson, Gamma... ä¸€è¦‹ãƒãƒ©ãƒãƒ©ã«è¦‹ãˆã‚‹åˆ†å¸ƒãŒã€ŒåŒã˜æ–‡æ³•ã€ã§æ›¸ã‘ã‚‹ã€‚

**æ¨™æº–å½¢**:

$$
p(x \mid \boldsymbol{\eta}) = h(x) \exp\!\left(\boldsymbol{\eta}^\top T(x) - A(\boldsymbol{\eta})\right)
$$

- $\boldsymbol{\eta}$: è‡ªç„¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆnatural parameterï¼‰
- $T(x)$: ååˆ†çµ±è¨ˆé‡ï¼ˆsufficient statisticï¼‰â€” ãƒ‡ãƒ¼ã‚¿ã®ã€Œè¦ç´„ã€
- $A(\boldsymbol{\eta})$: å¯¾æ•°åˆ†é…é–¢æ•°ï¼ˆlog partition functionï¼‰â€” æ­£è¦åŒ–å®šæ•°

**Gaussian ã®å ´åˆ** ($d=1$):

$$
\boldsymbol{\eta} = \begin{pmatrix}\mu/\sigma^2 \\ -1/(2\sigma^2)\end{pmatrix},\quad
T(x) = \begin{pmatrix}x \\ x^2\end{pmatrix},\quad
A(\boldsymbol{\eta}) = -\frac{\eta_1^2}{4\eta_2} + \frac{1}{2}\log\frac{\pi}{-\eta_2}
$$

**MLEã®ç¾ã—ã•**: æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®MLEã¯ã€Œç†è«–çš„æœŸå¾…å€¤ = çµŒé¨“çš„æœŸå¾…å€¤ã€ã¨ã„ã†æ¡ä»¶:

$$
\mathbb{E}_{p(x|\hat{\boldsymbol{\eta}})}[T(x)] = \frac{1}{N}\sum_{i=1}^N T(x^{(i)})
$$

Gaussianãªã‚‰ $T(x) = (x, x^2)$ ãªã®ã§ã€å¹³å‡ã¨äºŒä¹—å¹³å‡ãŒä¸€è‡´ã™ã‚‹æ¡ä»¶ = ã‚µãƒ³ãƒ—ãƒ«å¹³å‡ãƒ»åˆ†æ•£ãŒMLEã€‚

**å…±å½¹äº‹å‰åˆ†å¸ƒ**: äº‹å‰åˆ†å¸ƒã‚’ $p(\boldsymbol{\eta}) = h(\boldsymbol{\eta})\exp(\boldsymbol{\chi}^\top \boldsymbol{\eta} - \nu A(\boldsymbol{\eta}))$ ã¨æ›¸ãã¨ã€äº‹å¾Œåˆ†å¸ƒãŒåŒã˜æ—ã«å±ã™ã‚‹ï¼ˆå…±å½¹æ€§ï¼‰ã€‚Gaussian-Gaussian å…±å½¹ã€Beta-Bernoulli å…±å½¹ ã¯ã“ã®ç‰¹æ®Šã‚±ãƒ¼ã‚¹ã€‚


**æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®çµ±ä¸€å®Ÿè£…**:

æŠ½è±¡çš„ã«è¦‹ãˆã‚‹ãŒã€Gaussian/Bernoulli/PoissonãŒåŒã˜ã‚¯ãƒ©ã‚¹ã§æ›¸ã‘ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹ã€‚

è¨˜å· â†” å¤‰æ•°å¯¾å¿œ:
- $\boldsymbol{\eta}$ï¼ˆè‡ªç„¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰â†” `eta: (k,)` where $k$ = ååˆ†çµ±è¨ˆé‡ã®æ¬¡å…ƒ
- $T(x)$ï¼ˆååˆ†çµ±è¨ˆé‡ï¼‰â†” `suff_stat(x)`: å……åˆ†ãªæƒ…å ±ã‚’æŒã¤ã€Œãƒ‡ãƒ¼ã‚¿ã®åœ§ç¸®è¡¨ç¾ã€
- $A(\boldsymbol{\eta})$ï¼ˆå¯¾æ•°åˆ†é…é–¢æ•°ï¼‰â†” `log_partition(eta)`: æ­£è¦åŒ–å®šæ•°ã®å¯¾æ•°
- MLE æ¡ä»¶ $\mathbb{E}[T(x)] = \overline{T}$ â†” çµŒé¨“çš„ååˆ†çµ±è¨ˆé‡ã¨ç†è«–çš„æœŸå¾…å€¤ã®ä¸€è‡´

shape: Gaussian ã®å ´åˆ `eta: (2,)`, `T(x): (2,)` = $(x, x^2)$

**å¯¾æ•°åˆ†é…é–¢æ•° $A(\boldsymbol{\eta})$ ã®3ã¤ã®å½¹å‰²**:

1. **æ­£è¦åŒ–**: $A(\boldsymbol{\eta})$ ã¯ $\int p(x \mid \boldsymbol{\eta}) dx = 1$ ã‚’ä¿è¨¼ã™ã‚‹ã€‚

2. **æœŸå¾…å€¤ç”Ÿæˆ**: $\nabla_{\boldsymbol{\eta}} A(\boldsymbol{\eta}) = \mathbb{E}_{p}[T(x)]$ã€‚Gaussian ã§ã¯ $\partial_{\eta_1} A = \mu$ï¼ˆæœŸå¾…å€¤ï¼‰ã€$\partial_{\eta_2} A = \mu^2 + \sigma^2$ï¼ˆäºŒæ¬¡ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆï¼‰ã€‚

3. **åˆ†æ•£ãƒ»å…±åˆ†æ•£ç”Ÿæˆ**: $\nabla^2_{\boldsymbol{\eta}} A(\boldsymbol{\eta}) = \text{Cov}[T(x)] \succeq 0$ã€‚$A$ ãŒå‡¸ã§ã‚ã‚‹ã“ã¨ã®ç›´æ¥ã®è¨¼æ˜ã ã€‚

**$A$ ã®å‡¸æ€§ã¨ MLE ã®å¤§åŸŸæœ€é©æ€§**:

$A$ ãŒå‡¸ $\implies$ $-\sum_i \log p(x^{(i)} \mid \boldsymbol{\eta}) = \sum_i A(\boldsymbol{\eta}) - \boldsymbol{\eta}^\top T(x^{(i)}) + \text{const}$ ã‚‚å‡¸ï¼ˆ$\boldsymbol{\eta}$ ã®ç·šå½¢é …ã‚’å¼•ã„ãŸå‡¸é–¢æ•°ï¼‰ã€‚ã‚ˆã£ã¦å±€æ‰€è§£ = å¤§åŸŸè§£ã€‚ã“ã‚ŒãŒæŒ‡æ•°å‹åˆ†å¸ƒæ—ã®ã€Œå­¦ç¿’ã—ã‚„ã™ã•ã€ã®æœ¬è³ªã ã€‚

**Gaussian ã®å…·ä½“è¨ˆç®—**:

$\boldsymbol{\eta} = (\eta_1, \eta_2) = (\mu/\sigma^2,\; -1/(2\sigma^2))$ ã‹ã‚‰é€†å¤‰æ›:

$$
\sigma^2 = -\frac{1}{2\eta_2}, \quad \mu = \eta_1 \cdot \sigma^2 = -\frac{\eta_1}{2\eta_2}
$$

å¯¾æ•°åˆ†é…é–¢æ•°:

$$
A(\boldsymbol{\eta}) = -\frac{\eta_1^2}{4\eta_2} + \frac{1}{2}\log\frac{\pi}{-\eta_2}
$$

ã“ã‚Œã‚’ $\eta_1$ ã§å¾®åˆ†ã™ã‚‹ã¨ $-\eta_1/(2\eta_2) = \mu = \mathbb{E}[X]$ã€$\eta_2$ ã§å¾®åˆ†ã™ã‚‹ã¨ $\eta_1^2/(4\eta_2^2) + 1/(2\eta_2) = \mu^2 + \sigma^2 = \mathbb{E}[X^2]$ã€‚

MLE æ¡ä»¶ã€Œ$\mathbb{E}[T(x)] = \frac{1}{N}\sum T(x^{(i)})$ã€ã¯ Gaussian ã§ã¯ $(\mathbb{E}[X], \mathbb{E}[X^2]) = (\bar{x}, \overline{x^2})$ã€ã¤ã¾ã‚Šã‚µãƒ³ãƒ—ãƒ«å¹³å‡ã¨äºŒä¹—å¹³å‡ãŒä¸€è‡´ã™ã‚‹ â€” ã“ã‚ŒãŒ MLE è§£ $\hat{\mu} = \bar{x}$, $\hat{\sigma}^2 = \overline{x^2} - \bar{x}^2$ ã¨ç­‰ä¾¡ã ã€‚

**è‡ªç„¶å‹¾é…æ³• (Natural Gradient) ã¸ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼**:

æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã¯ Fisher æƒ…å ±è¡Œåˆ— $\mathbf{I}(\boldsymbol{\eta})$ ãŒè¨ˆé‡ã‚’ä¸ãˆã‚‹ã€ŒRiemannian å¤šæ§˜ä½“ã€ã ã€‚é€šå¸¸ã®å‹¾é…é™ä¸‹ã¨è‡ªç„¶å‹¾é…é™ä¸‹ã®é•ã„:

$$
\text{é€šå¸¸}: \boldsymbol{\eta}_{t+1} = \boldsymbol{\eta}_t - \alpha \nabla_{\boldsymbol{\eta}} \mathcal{L}, \quad
\text{è‡ªç„¶å‹¾é…}: \boldsymbol{\eta}_{t+1} = \boldsymbol{\eta}_t - \alpha \mathbf{I}^{-1}(\boldsymbol{\eta}_t) \nabla_{\boldsymbol{\eta}} \mathcal{L}
$$

æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®ç‰¹åˆ¥ãªæ€§è³ª: è‡ªç„¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã§ã®è‡ªç„¶å‹¾é… = æœŸå¾…å€¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ $\boldsymbol{\mu} = \mathbb{E}[T(x)]$ ã§ã®é€šå¸¸å‹¾é…ã€‚ã¤ã¾ã‚Š $\mathbf{I}^{-1} \nabla_{\boldsymbol{\eta}} = \nabla_{\boldsymbol{\mu}}$ã€‚ã“ã‚ŒãŒ Adam ãªã©ã®é©å¿œçš„æœ€é©åŒ–ã®ç†è«–çš„åŸºç›¤ã ï¼ˆç¬¬12å›ã§è©³èª¬ï¼‰ã€‚

**ãªãœå¯¾æ•°åˆ†é…é–¢æ•° $A(\boldsymbol{\eta})$ ãŒé‡è¦ã‹**: $A$ ã®ä¸€æ¬¡å¾®åˆ†ãŒæœŸå¾…å€¤ã€äºŒæ¬¡å¾®åˆ†ãŒå…±åˆ†æ•£ã‚’ä¸ãˆã‚‹ã€‚

$$
\nabla_{\boldsymbol{\eta}} A(\boldsymbol{\eta}) = \mathbb{E}_{p(x|\boldsymbol{\eta})}[T(x)]
$$

$$
\nabla^2_{\boldsymbol{\eta}} A(\boldsymbol{\eta}) = \text{Cov}_{p}[T(x), T(x)] \succeq 0
$$

$A$ ãŒå‡¸ â†’ è² ã®å¯¾æ•°å°¤åº¦ã‚‚å‡¸ â†’ MLEã¯å¤§åŸŸçš„æœ€é©è§£ã€‚ã“ã‚ŒãŒæŒ‡æ•°å‹åˆ†å¸ƒæ—ã®ã€Œå­¦ç¿’ã—ã‚„ã™ã•ã€ã®æœ¬è³ªã ã€‚

**è‡ªç„¶å‹¾é…æ³• (Natural Gradient) ã¸ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼**:

æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã¯ã€ŒRiemannianå¤šæ§˜ä½“ã€ã ã€‚Fisheræƒ…å ±è¡Œåˆ— $\mathbf{I}(\boldsymbol{\eta})$ ãŒãã®ç©ºé–“ã®è¨ˆé‡ã‚’ä¸ãˆã‚‹ã€‚

é€šå¸¸ã®å‹¾é…é™ä¸‹: $\boldsymbol{\eta}_{t+1} = \boldsymbol{\eta}_t - \alpha \nabla_{\boldsymbol{\eta}} \mathcal{L}$

è‡ªç„¶å‹¾é…é™ä¸‹: $\boldsymbol{\eta}_{t+1} = \boldsymbol{\eta}_t - \alpha \mathbf{I}^{-1}(\boldsymbol{\eta}_t) \nabla_{\boldsymbol{\eta}} \mathcal{L}$

è‡ªç„¶å‹¾é…ã¯ã€Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã®è·é›¢ã€ã§ã¯ãªãã€Œåˆ†å¸ƒç©ºé–“ã®KLè·é›¢ã€ã§ã‚¹ãƒ†ãƒƒãƒ—ã‚’åˆ¶å¾¡ã™ã‚‹ã€‚åŒã˜åˆ†å¸ƒã®å¤‰åŒ–é‡ã«å¯¾å¿œã™ã‚‹ã‚¹ãƒ†ãƒƒãƒ—ãŒã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å€¤ã«ä¾å­˜ã—ãªã„ â€” ã“ã‚ŒãŒAdamãªã©ã®é©å¿œçš„æœ€é©åŒ–ã®ç†è«–çš„åŸºç›¤ã ï¼ˆç¬¬12å›ã§è©³èª¬ï¼‰ã€‚

æŒ‡æ•°å‹åˆ†å¸ƒæ—ã§ã¯è‡ªç„¶å‹¾é…ã«é–‰å½¢å¼ãŒã‚ã‚‹: $\mathbf{I}^{-1}(\boldsymbol{\eta}) \nabla_{\boldsymbol{\eta}} \mathcal{L} = \nabla_{\boldsymbol{\mu}} \mathcal{L}$ï¼ˆæœŸå¾…å€¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã®é€šå¸¸å‹¾é…ã¨ç­‰ä¾¡ï¼‰ã€‚

### 5.4 å®Ÿè£…æ¼”ç¿’: ã‚¬ã‚¦ã‚¹æ··åˆãƒ¢ãƒ‡ãƒ«ï¼ˆGMMï¼‰ã®MLE

ç¬¬8å›ï¼ˆEMç®—æ³•ï¼‰ã¸ã®æ©‹æ¸¡ã—ã¨ã—ã¦ã€2æˆåˆ†GMMã®ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã‚’å®Ÿè£…ã™ã‚‹ã€‚ã“ã“ã§ã¯EMç®—æ³•ã®å‰æ®µéšã¨ã—ã¦ã€å˜ä¸€ã‚¬ã‚¦ã‚¹ã®MLEã‚’æ‹¡å¼µã™ã‚‹å½¢ã§å•é¡Œã®å›°é›£ã•ã‚’ä½“æ„Ÿã™ã‚‹ã€‚

$$
\begin{aligned}
p(x \mid \theta) &= \pi\,\mathcal{N}(x \mid \mu_1, \sigma_1^2) + (1-\pi)\,\mathcal{N}(x \mid \mu_2, \sigma_2^2) \\[6pt]
\ell(\theta) &= \sum_{i=1}^{N} \log p(x_i \mid \theta) \\[6pt]
\mathcal{N}(x \mid \mu, \sigma^2) &= \frac{1}{\sqrt{2\pi}\,\sigma}\exp\!\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
\end{aligned}
$$

**ãªãœ GMM ã® MLE ã¯é–‰ã˜ãŸå½¢ã§è§£ã‘ãªã„ã®ã‹**: å¯¾æ•°å°¤åº¦ã«**å’Œã®å¯¾æ•°** $\log[\pi \mathcal{N}_1 + (1-\pi)\mathcal{N}_2]$ ãŒç¾ã‚Œã€å¯¾æ•°ã¨å’Œã®é †åºã‚’å…¥ã‚Œæ›¿ãˆã‚‰ã‚Œãªã„ã€‚å¾®åˆ†ã—ã¦ã‚‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒäº’ã„ã«çµ¡ã¿åˆã†:

$$
\frac{\partial \ell}{\partial \mu_1} = \sum_i \frac{\pi \mathcal{N}(x_i|\mu_1,\sigma_1^2)}{\pi \mathcal{N}(x_i|\mu_1,\sigma_1^2) + (1-\pi)\mathcal{N}(x_i|\mu_2,\sigma_2^2)} \cdot \frac{x_i - \mu_1}{\sigma_1^2} = 0
$$

å³è¾ºã®åˆ†æ•° $r_i = P(\text{æˆåˆ†1} \mid x_i, \theta)$ ã¯ **è²¬ä»»åº¦ï¼ˆresponsibilityï¼‰** ã¨å‘¼ã°ã‚Œã‚‹ã€‚$\mu_1$ ã®å¼ãŒ $r_i$ ã«ä¾å­˜ã—ã€$r_i$ ãŒ $\mu_2, \sigma_2, \pi$ ã«ä¾å­˜ã™ã‚‹ â†’ å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒé€£ç«‹ã™ã‚‹ã€‚

ã“ã‚Œã‚’è§£ãåå¾©ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒ EM ç®—æ³•ï¼ˆç¬¬8å›ï¼‰ã :

1. **E ã‚¹ãƒ†ãƒƒãƒ—**: ç¾åœ¨ã® $\theta$ ã§ $r_i$ ã‚’è¨ˆç®—  
2. **M ã‚¹ãƒ†ãƒƒãƒ—**: $r_i$ ã‚’å›ºå®šã—ã¦å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å€‹åˆ¥ã«æ›´æ–°  
   - $\hat{\mu}_1 = \sum_i r_i x_i / \sum_i r_i$ï¼ˆè²¬ä»»åº¦ã§é‡ã¿ä»˜ã‘ã—ãŸæ¨™æœ¬å¹³å‡ï¼‰

å˜ä¸€ Gaussian ã¨ã®å¯¾æ•°å°¤åº¦ã®å·®ï¼ˆgapï¼‰ãŒ GMM ã®ã€Œãƒ¢ãƒ‡ãƒ«è¡¨ç¾åŠ›ã®åˆ©å¾—ã€ã‚’æ•°å€¤åŒ–ã™ã‚‹ã€‚$\text{gap} = \ell_\text{GMM} - \ell_\text{single} > 0$ ã¯æ··åˆãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦ãªè¨¼æ‹ ã ã€‚å¤§ãã„ã»ã©å˜å³°åˆ†å¸ƒã®ä»®å®šãŒèª¤ã‚Šã ã£ãŸåº¦åˆã„ã‚’ç¤ºã™ã€‚

**ãªãœGMMã®MLEã¯é–‰ã˜ãŸå½¢ã§è§£ã‘ãªã„ã®ã‹**: å¯¾æ•°å°¤åº¦ã®ä¸­ã«**å’Œã®å¯¾æ•°** $\log[\pi \mathcal{N}(x \mid \mu_1, \sigma_1^2) + (1-\pi)\mathcal{N}(x \mid \mu_2, \sigma_2^2)]$ ãŒç¾ã‚Œã‚‹ã€‚å¯¾æ•°ã¨å’Œã®é †åºã‚’å…¥ã‚Œæ›¿ãˆã‚‰ã‚Œãªã„ãŸã‚ã€å¾®åˆ†ã—ã¦ã‚‚å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒäº’ã„ã«çµ¡ã¿åˆã†ã€‚ã“ã®å›°é›£ãŒç¬¬8å›ã®EMç®—æ³•ã®å‹•æ©Ÿã ã€‚

### 5.5a å®Ÿè£…æ¼”ç¿’: ãƒ™ã‚¤ã‚ºæ¨è«–ã®ã‚°ãƒªãƒƒãƒ‰è¿‘ä¼¼

$$
\begin{aligned}
\theta &\sim \mathrm{Beta}(a,b), \quad x_i \sim \mathrm{Bernoulli}(\theta) \\[4pt]
p(\theta \mid \mathbf{x}) &\propto \theta^{a+h-1}(1-\theta)^{b+t-1} \\[4pt]
\theta \mid \mathbf{x} &\sim \mathrm{Beta}(a+h,\; b+t)
\end{aligned}
$$

ã“ã“ã§ $h = \sum_i x_i$ï¼ˆè¡¨ã®å›æ•°ï¼‰ã€$t = N - h$ï¼ˆè£ã®å›æ•°ï¼‰ã€‚äº‹å‰ Beta$(1,1)$ï¼ˆä¸€æ§˜ï¼‰ã‹ã‚‰å§‹ã‚ã€ãƒ‡ãƒ¼ã‚¿ã‚’è¦³æ¸¬ã™ã‚‹ã”ã¨ã«æŒ‡æ•° $(a,b)$ ãŒ $(a+h, b+t)$ ã«æ›´æ–°ã•ã‚Œã‚‹ã€‚å°¤åº¦é–¢æ•° $L(\theta) = \theta^h (1-\theta)^t$ ãŒ Beta åˆ†å¸ƒã¨ã€ŒåŒã˜å½¢ã€ã«ãªã£ã¦ã„ã‚‹ã®ãŒå…±å½¹æ€§ã®æ ¸å¿ƒã ã€‚

**ã‚°ãƒªãƒƒãƒ‰è¿‘ä¼¼ãŒå®Ÿç”¨çš„ã§ãªã„ç†ç”± â€” æ¬¡å…ƒã®å‘ªã„**:

$d$ æ¬¡å…ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å„è»¸ $M$ ç‚¹ã®ã‚°ãƒªãƒƒãƒ‰ã‚’å¼µã‚‹ã¨ $M^d$ ç‚¹ãŒå¿…è¦ã :

| $d$ | $M=10$ | $M=100$ | ãƒ¡ãƒ¢ãƒªï¼ˆfloat64ï¼‰|
|:---:|:-------:|:--------:|:----------------:|
| 2 | $10^2$ | $10^4$ | 80 KB |
| 5 | $10^5$ | $10^{10}$ | 80 GB |
| 10 | $10^{10}$ | $10^{20}$ | â‰« å®‡å®™ã®åŸå­æ•° |

10æ¬¡å…ƒã§å„è»¸100ç‚¹ã¯ $10^{20}$ ç‚¹ â€” ç‰©ç†çš„ã«ä¸å¯èƒ½ã€‚ã“ã‚ŒãŒ**æ¬¡å…ƒã®å‘ªã„**ã ã€‚

**ä»£æ›¿æ‰‹æ³•ã®ä¸‰æœ¬æŸ±**:

1. **MCMCï¼ˆMarkov Chain Monte Carloï¼‰**: äº‹å¾Œåˆ†å¸ƒã‹ã‚‰ç›´æ¥ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚Metropolis-Hastings ã‚„ Hamiltonian Monte Carlo (HMC) ãŒä»£è¡¨ä¾‹ã€‚æ¬¡å…ƒãŒå¢—ãˆã¦ã‚‚ï¼ˆã‚ã‚‹æ„å‘³ï¼‰ã‚¹ã‚±ãƒ¼ãƒ«ã™ã‚‹ â€” ç¬¬5å›ã§è©³èª¬ã€‚

2. **å¤‰åˆ†æ¨è«–ï¼ˆVariational Inferenceï¼‰**: äº‹å¾Œåˆ†å¸ƒ $p(\theta|\mathbf{x})$ ã‚’ç°¡å˜ãªæ— $q_\phi(\theta)$ ã§è¿‘ä¼¼ã—ã€$D_{\mathrm{KL}}(q_\phi \| p)$ ã‚’æœ€å°åŒ–ï¼ˆ= ELBO æœ€å¤§åŒ–ï¼‰ã€‚VAE ã®æ ¸å¿ƒã€‚

3. **Laplace è¿‘ä¼¼**: äº‹å¾Œåˆ†å¸ƒã‚’æœ€é »å€¤ï¼ˆMAPï¼‰å‘¨ã‚Šã§äºŒæ¬¡è¿‘ä¼¼ã™ã‚‹ã€‚MAP æ¨å®š + Hessian ã§ã‚¬ã‚¦ã‚¹è¿‘ä¼¼ã‚’å¾—ã‚‹ã€‚å¤§æ¬¡å…ƒã§ã‚‚è¨ˆç®—å¯èƒ½ã ãŒã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªäº‹å¾Œåˆ†å¸ƒã«å¼±ã„ã€‚

**äº‹å¾Œä¸€è‡´æ€§ï¼ˆposterior consistencyï¼‰**: æ­£å‰‡æ¡ä»¶ã®ã‚‚ã¨ã§ $N \to \infty$ ã®ã¨ãäº‹å¾Œåˆ†å¸ƒã¯çœŸã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta^*$ ã«é›†ä¸­ã™ã‚‹ã€‚äº‹å‰åˆ†å¸ƒã®å½±éŸ¿ãŒè–„ã‚Œã€ãƒ™ã‚¤ã‚ºæ¨å®šã¯ MLE ã«åæŸã™ã‚‹ â€” ã€Œäº‹å‰åˆ†å¸ƒã¯æ­£å‰‡åŒ–ã®ä¸€å½¢æ…‹ã€ã¨å‰²ã‚Šåˆ‡ã‚Œã‚‹ç†ç”±ã ã€‚Beta-Bernoulli ã§ã¯ $\hat{\theta}_\text{Bayes} = (a+h)/(a+b+N) \to h/N = \hat{\theta}_\text{MLE}$ as $N \to \infty$ã€‚

> **Note:** **å®Ÿè£…ã®æ•™è¨“**: ãƒ‡ãƒ¼ã‚¿ãŒå¢—ãˆã‚‹ã»ã©ã€äº‹å‰åˆ†å¸ƒã®å½±éŸ¿ã¯è–„ã‚Œã€ãƒ™ã‚¤ã‚ºæ¨å®šã¯MLEã«è¿‘ã¥ãã€‚ã“ã‚Œã¯äº‹å¾Œåˆ†å¸ƒãŒã€Œå°¤åº¦ã«æ”¯é…ã•ã‚Œã‚‹ã€ãŸã‚ã€‚é€†ã«ã€ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„ã¨ãã¯äº‹å‰åˆ†å¸ƒãŒçµæœã‚’å¤§ããå·¦å³ã™ã‚‹ã€‚

ã“ã®ç¾è±¡ã‚’ã€Œäº‹å¾Œä¸€è‡´æ€§ï¼ˆposterior consistencyï¼‰ã€ã¨å‘¼ã¶ã€‚$N \to \infty$ ã§äº‹å¾Œåˆ†å¸ƒã¯çœŸã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«é›†ä¸­ã™ã‚‹ â€” å¤§æ•°ã®æ³•å‰‡ã®ãƒ™ã‚¤ã‚ºç‰ˆã ã€‚

### 5.5b å®Ÿè£…æ¼”ç¿’: å…±å½¹äº‹å‰åˆ†å¸ƒã®è§£æçš„æ›´æ–°

ã‚°ãƒªãƒƒãƒ‰è¿‘ä¼¼ãŒã€Œæ•°å€¤çš„ã€ãªã‚‰ã°ã€å…±å½¹äº‹å‰åˆ†å¸ƒã¯ã€Œè§£æçš„ã€ã ã€‚

**Gaussian-Gaussian å…±å½¹ï¼ˆæ—¢çŸ¥åˆ†æ•£ã€æœªçŸ¥å¹³å‡ï¼‰**:

äº‹å‰: $\theta \sim \mathcal{N}(\mu_0, \tau_0^2)$ã€å°¤åº¦: $X_i \mid \theta \sim \mathcal{N}(\theta, \sigma^2)$

$$
\frac{1}{\tau_N^2} = \frac{1}{\tau_0^2} + \frac{N}{\sigma^2}, \quad
\mu_N = \tau_N^2 \left(\frac{\mu_0}{\tau_0^2} + \frac{N \bar{x}}{\sigma^2}\right)
$$

ç²¾åº¦ï¼ˆåˆ†æ•£ã®é€†æ•°ï¼‰ãŒåŠ æ³•çš„ã«æ›´æ–°ã•ã‚Œã‚‹ã€‚$N \to \infty$ ã§ $\mu_N \to \bar{x}$ï¼ˆMLEï¼‰ã€$\tau_N^2 \to 0$ã€‚

è¨˜å· â†” å¤‰æ•°å¯¾å¿œ:
- $\mu_0, \tau_0^2$ â†” äº‹å‰åˆ†å¸ƒã®å¹³å‡ãƒ»åˆ†æ•£ï¼ˆå…ˆé¨“çš„çŸ¥è­˜ï¼‰
- $\sigma^2$ â†” å°¤åº¦ã®åˆ†æ•£ï¼ˆæ—¢çŸ¥ã¨ä»®å®šï¼‰
- $\bar{x}, N$ â†” æ¨™æœ¬å¹³å‡ãƒ»ã‚µãƒ³ãƒ—ãƒ«æ•°
- $\mu_N, \tau_N^2$ â†” äº‹å¾Œåˆ†å¸ƒã®å¹³å‡ãƒ»åˆ†æ•£ï¼ˆè¦³æ¸¬å¾Œã®æ›´æ–°ã•ã‚ŒãŸä¿¡å¿µï¼‰

**ç²¾åº¦ï¼ˆprecisionï¼‰ã®åŠ æ³•æ€§**:

ç²¾åº¦ $\lambda = 1/\tau^2$ï¼ˆåˆ†æ•£ã®é€†æ•°ï¼‰ã§æ›¸ãã¨æ›´æ–°å¼ã¯ç¾ã—ããªã‚‹:

$$
\lambda_N = \underbrace{\lambda_0}_{\text{äº‹å‰ã®ç²¾åº¦}} + \underbrace{\frac{N}{\sigma^2}}_{\text{ãƒ‡ãƒ¼ã‚¿ã®ç²¾åº¦}}
$$

ç²¾åº¦ã¯**åŠ æ³•çš„ã«æ›´æ–°ã•ã‚Œã‚‹**ã€‚1å€‹ã®ãƒ‡ãƒ¼ã‚¿ãŒ $1/\sigma^2$ ã®ç²¾åº¦ã‚’è¿½åŠ ã™ã‚‹ â€” ç²¾åº¦ã®ç©ã¿é‡ã­ãŒä¿¡å¿µã®å¼·åŒ–ã ã€‚

$N \to \infty$: $\lambda_N \to \infty$ï¼ˆç²¾åº¦ç„¡é™å¤§ â†’ åˆ†æ•£0 â†’ ç¢ºä¿¡ã«åæŸï¼‰ã€$\mu_N \to \bar{x}$ï¼ˆMLE ã«åæŸï¼‰ã€‚  
äº‹å‰åˆ†å¸ƒ $\tau_0^2 \to \infty$ï¼ˆç„¡æƒ…å ±ï¼‰: $\mu_N \to \bar{x}$ã€$\tau_N^2 \to \sigma^2/N$ï¼ˆMLE ã®æ¨™æº–èª¤å·®ã®äºŒä¹—ï¼‰ã€‚

**å¼·äº‹å‰ vs å¼±äº‹å‰ã®æ¯”è¼ƒ**:

$\mu_0 = 0$ï¼ˆäº‹å‰ã®ä¿¡å¿µï¼šå¹³å‡ã¯0ï¼‰ã€çœŸå€¤ $\theta^* = 3$:
- å¼·äº‹å‰ï¼ˆ$\tau_0^2 = 0.5$ï¼‰: $N=1$ ã§ã¯äº‹å‰ã«å¼·ãå¼•ã£å¼µã‚‰ã‚Œ $\mu_N \approx 0.5$ã€‚$N=100$ ã§ $\mu_N \approx 2.9$ï¼ˆã»ã¼åæŸï¼‰
- å¼±äº‹å‰ï¼ˆ$\tau_0^2 = 100$ï¼‰: å° $N$ ã§ã‚‚ MLE ã«è¿‘ã„å€¤ã€‚äº‹å‰ã®å½±éŸ¿ãŒæœ€åˆã‹ã‚‰è–„ã„

ã“ã‚ŒãŒã€ŒL2 æ­£å‰‡åŒ– = Gaussian äº‹å‰åˆ†å¸ƒã€ã®ç›´æ„Ÿã ã€‚æ­£å‰‡åŒ–ä¿‚æ•° $\lambda$ ã¯äº‹å‰ç²¾åº¦ $\lambda_0 = \lambda \sigma^2$ ã«å¯¾å¿œã™ã‚‹ã€‚æ­£å‰‡åŒ–ã‚’å¼·ãã™ã‚‹ï¼ˆ$\lambda \uparrow$ï¼‰= äº‹å‰åˆ†å¸ƒã‚’å¼·ãã™ã‚‹ï¼ˆ$\tau_0^2 \downarrow$ï¼‰= ãƒ‡ãƒ¼ã‚¿ã‚ˆã‚Šäº‹å‰çŸ¥è­˜ã‚’ä¿¡ã˜ã‚‹ã€‚

**3æ¨å®šé‡ã®æ¯”è¼ƒ**:

| æ¨å®šé‡ | å¼ | ç‰¹å¾´ |
|:-------|:---|:-----|
| MLE | $\bar{x}$ | ãƒã‚¤ã‚¢ã‚¹ãªã—ã€å°ãƒ‡ãƒ¼ã‚¿ä¸å®‰å®š |
| MAP | $\mu_N$ | äº‹å‰+å°¤åº¦ã€æ­£å‰‡åŒ–ã¨ç­‰ä¾¡ |
| äº‹å¾Œå¹³å‡ | $\mu_N$ï¼ˆGaussianäº‹å¾Œï¼‰| MAP=äº‹å¾Œå¹³å‡ |

### 5.5a KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ â€” åˆ†å¸ƒé–“ã®ã€Œè·é›¢ã€å®Ÿè£…

KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¯ç¢ºç‡è«–ã®å…¨ã¦ã®æ­¦å™¨ãŒé›†çµã™ã‚‹å ´æ‰€ã ã€‚VAEã®ELBOã€diffusion modelã®ç›®çš„é–¢æ•°ã€æƒ…å ±ç†è«–ã®åŸºç¤ â€” å…¨ã¦ã“ã“ã«é€šã˜ã‚‹ã€‚

$$
D_{\mathrm{KL}}(p \| q) = \int p(x) \log \frac{p(x)}{q(x)} dx = \mathbb{E}_{p}\left[\log \frac{p(X)}{q(X)}\right]
$$

**åŸºæœ¬æ€§è³ª**:
- $D_{\mathrm{KL}}(p \| q) \geq 0$ï¼ˆGibbsä¸ç­‰å¼ã€Jensenä¸ç­‰å¼ã‹ã‚‰ï¼‰
- $D_{\mathrm{KL}}(p \| q) = 0 \iff p = q$ï¼ˆã»ã¼è‡³ã‚‹æ‰€ã§ï¼‰
- éå¯¾ç§°: $D_{\mathrm{KL}}(p \| q) \neq D_{\mathrm{KL}}(q \| p)$ï¼ˆè·é›¢å…¬ç†ã‚’æº€ãŸã•ãªã„ï¼‰

**2ã¤ã®Gaussiané–“ã®KLï¼ˆé–‰å½¢å¼ï¼‰**:

$$
D_{\mathrm{KL}}(\mathcal{N}(\mu_1, \sigma_1^2) \| \mathcal{N}(\mu_2, \sigma_2^2)) =
\log\frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2 + (\mu_1-\mu_2)^2}{2\sigma_2^2} - \frac{1}{2}
$$

è¨˜å· â†” å¤‰æ•°å¯¾å¿œ:
- $\mu_1, \sigma_1^2$ â†” `mu1, var1`ï¼ˆåˆ†å¸ƒ $p$ï¼‰
- $\mu_2, \sigma_2^2$ â†” `mu2, var2`ï¼ˆåˆ†å¸ƒ $q$ï¼‰
- $D_{\mathrm{KL}}(p\|q)$ â†” `kl_pq`ï¼ˆã‚¹ã‚«ãƒ©ãƒ¼ã€éè² ï¼‰

shape: scalar inputs â†’ scalar outputã€‚Block 1 ã§ã¯ `D.kl_divergence(p, q)` ãŒé–‰å½¢å¼ã‚’è‡ªå‹•è¨ˆç®—ã™ã‚‹ã€‚

**éè² æ€§ã®è¨¼æ˜ â€” Gibbs ä¸ç­‰å¼**:

$\log x \leq x - 1$ï¼ˆ$x > 0$ã€ç­‰å·ã¯ $x=1$ ã®ã¿ï¼‰ã‚’ä½¿ã†:

$$
-D_{\mathrm{KL}}(p \| q) = \mathbb{E}_p\!\left[\log \frac{q(X)}{p(X)}\right] \leq \mathbb{E}_p\!\left[\frac{q(X)}{p(X)} - 1\right] = \int q(x)\,dx - \int p(x)\,dx = 1 - 1 = 0
$$

ã‚ˆã£ã¦ $D_{\mathrm{KL}}(p \| q) \geq 0$ã€ç­‰å·ã¯ $p = q$ ã®ã¨ãï¼ˆ$\log(q/p) = 0$ a.e.ï¼‰ã€‚

**é–‰å½¢å¼ã®å°å‡ºï¼ˆGaussian-Gaussianï¼‰**:

$$
\begin{aligned}
D_{\mathrm{KL}}(\mathcal{N}_1 \| \mathcal{N}_2) &= \int p_1 \log \frac{p_1}{p_2}\,dx \\
&= \int p_1 \left[\log \frac{\sigma_2}{\sigma_1} + \frac{(x-\mu_1)^2}{2\sigma_1^2} - \frac{(x-\mu_2)^2}{2\sigma_2^2}\right] dx \\
&= \log\frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2}{2\sigma_2^2} + \frac{(\mu_1-\mu_2)^2}{2\sigma_2^2} - \frac{1}{2}
\end{aligned}
$$

3è¡Œç›®ã§ $\mathbb{E}_{p_1}[(X-\mu_1)^2] = \sigma_1^2$ã€$\mathbb{E}_{p_1}[(X-\mu_2)^2] = \sigma_1^2 + (\mu_1-\mu_2)^2$ ã‚’ä½¿ã£ãŸã€‚

**éå¯¾ç§°æ€§ã®ç›´æ„Ÿ**: $D_{\mathrm{KL}}(p \| q)$ ã¯ã€Œ$p$ ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¤ã¤ $q$ ã¨ã®é•ã„ã‚’æ¸¬ã‚‹ã€ã€‚$q$ ã®è£¾ãŒè»½ãã¦ $p$ ã®è£¾ãŒé‡ã„å ´åˆï¼ˆ$q(x) \ll p(x)$ ã§ $p(x) > 0$ï¼‰ã€$\log(p/q) \to +\infty$ ã¨ãªã‚Š KL ãŒçˆ†ç™ºã™ã‚‹ã€‚é€†æ–¹å‘ $D_{\mathrm{KL}}(q \| p)$ ã¯ $q$ ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ãŸã‚ã€ã“ã®çˆ†ç™ºã¯èµ·ããªã„ã€‚

VAE ã® ELBO ã§ $D_{\mathrm{KL}}(q_\phi \| p)$ ã‚’ä½¿ã†ï¼ˆ$p$ ã‚’å¤–å´ã«ç½®ãï¼‰ã®ã¯ã€$q_\phi$ ã®é ˜åŸŸå¤–ã§ã®çˆ†ç™ºã‚’é¿ã‘ã‚‹ãŸã‚ã  â€” ã€Œmode-seekingã€ã¨å‘¼ã°ã‚Œã‚‹æ€§è³ªã€‚

**Block 1 ã§ã®ç¢ºèª**: `D.kl_divergence(p, p)` ãŒ $10^{-6}$ ä»¥ä¸‹ï¼ˆæ•°å€¤ç²¾åº¦ã®ç¯„å›²ã§ 0ï¼‰ã«ãªã‚‹ã“ã¨ã‚’ assert ã§æ¤œè¨¼ã—ãŸã€‚

**VAEã¨ã®æ¥ç¶š**: VAEã®ELBOã«ã¯ $D_{\mathrm{KL}}(q_\phi(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z}))$ ãŒç™»å ´ã™ã‚‹ã€‚$p(\mathbf{z}) = \mathcal{N}(\mathbf{0}, \mathbf{I})$ã€$q_\phi = \mathcal{N}(\boldsymbol{\mu}, \text{diag}(\boldsymbol{\sigma}^2))$ ãªã‚‰ã€æ¬¡å…ƒç‹¬ç«‹ãªGaussian KLã®é–‰å½¢å¼ãŒä½¿ãˆã‚‹:

$$
D_{\mathrm{KL}}(q \| p) = \frac{1}{2} \sum_{j=1}^d (\sigma_j^2 + \mu_j^2 - 1 - \log \sigma_j^2)
$$

ç¬¬8å›ï¼ˆVAEï¼‰ã§ã“ã®å¼ãŒæå¤±é–¢æ•°ã«ç›´æ¥ç¾ã‚Œã‚‹ã€‚

### 5.5c Fisheræƒ…å ±é‡ â€” CramÃ©r-Raoä¸‹ç•Œã®å®Ÿè£…æ¤œè¨¼

Fisheræƒ…å ±é‡ $I(\theta) = \mathbb{E}\left[\left(\frac{\partial \log p(x;\theta)}{\partial \theta}\right)^2\right]$ ã¯æ¨å®šã®é›£ã—ã•ã‚’å®šé‡åŒ–ã™ã‚‹ã€‚

ç­‰ä¾¡ãªè¡¨ç¾ï¼ˆå¯¾æ•°å°¤åº¦ã®æ›²ç‡ï¼‰:

$$
I(\theta) = -\mathbb{E}\left[\frac{\partial^2 \log p(x; \theta)}{\partial \theta^2}\right]
$$

**CramÃ©r-Raoä¸‹ç•Œ**: ä»»æ„ã®ä¸åæ¨å®šé‡ã®åˆ†æ•£ã¯ $1/(n I(\theta))$ ã‚ˆã‚Šå°ã•ãã§ããªã„:

$$
\text{Var}(\hat{\theta}) \geq \frac{1}{n \cdot I(\theta)}
$$

è¨˜å· â†” å¤‰æ•°å¯¾å¿œ:
- $\theta$ â†” `theta: float`ï¼ˆæ¨å®šå¯¾è±¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
- ã‚¹ã‚³ã‚¢é–¢æ•° $s(x;\theta) = \partial_\theta \log p(x;\theta)$ â†” ã‚¹ã‚«ãƒ©ãƒ¼é–¢æ•°ï¼ˆå„ãƒ‡ãƒ¼ã‚¿ç‚¹ã§è©•ä¾¡ï¼‰
- $I(\theta) = \mathbb{E}[s^2]$ â†” ã‚¹ã‚³ã‚¢ã®äºŒä¹—æœŸå¾…å€¤
- CR ä¸‹ç•Œ $1/(nI(\theta))$ â†” ä»»æ„ã®ä¸åæ¨å®šé‡ã®åˆ†æ•£ã®ä¸‹é™

shape: `score(x): (N,)`, Fisher info: scalar, CR bound: scalar

**CramÃ©r-Rao ä¸‹ç•Œã®å°å‡ºã‚¹ã‚±ãƒƒãƒ**:

ä»»æ„ã®ä¸åæ¨å®šé‡ $\hat{\theta}(X)$ï¼ˆ$\mathbb{E}[\hat{\theta}] = \theta$ï¼‰ã«ã¤ã„ã¦:

$$
\frac{\partial}{\partial \theta}\mathbb{E}[\hat{\theta}] = 1 \implies \int \hat{\theta}(x) \frac{\partial}{\partial \theta} p(x;\theta)\,dx = 1
$$

$\frac{\partial \log p}{\partial \theta} = \frac{1}{p}\frac{\partial p}{\partial \theta}$ ã‚’ä»£å…¥:

$$
\mathbb{E}[\hat{\theta}(X) \cdot s(X;\theta)] = 1
$$

ã‚¹ã‚³ã‚¢ã®æœŸå¾…å€¤ã‚¼ãƒ­ï¼ˆ$\mathbb{E}[s] = 0$ï¼‰ã‹ã‚‰ $\text{Cov}(\hat{\theta}, s) = \mathbb{E}[\hat{\theta} \cdot s] = 1$ã€‚Cauchy-Schwarz ä¸ç­‰å¼ã‚ˆã‚Š:

$$
1 = \text{Cov}(\hat{\theta}, s)^2 \leq \text{Var}(\hat{\theta}) \cdot \text{Var}(s) = \text{Var}(\hat{\theta}) \cdot I(\theta)
$$

ã‚ˆã£ã¦ $\text{Var}(\hat{\theta}) \geq 1/I(\theta)$ã€‚$n$ å€‹ã® i.i.d. ã‚µãƒ³ãƒ—ãƒ«ãªã‚‰ Fisher æƒ…å ±ãŒåŠ æ³•çš„ï¼ˆ$I_n = n I(\theta)$ï¼‰ãªã®ã§ $\text{Var}(\hat{\theta}_n) \geq 1/(n I(\theta))$ã€‚

**Gaussian MLE ã®åŠ¹ç‡æ€§**: $\hat{\mu} = \bar{X}$ ã®åˆ†æ•£ã¯ $\sigma^2/n$ã€‚Fisher æƒ…å ± $I(\mu) = 1/\sigma^2$ ãªã®ã§ CR ä¸‹ç•Œã¯ $1/(n \cdot 1/\sigma^2) = \sigma^2/n$ã€‚æ¯”ç‡ = 1.0 â€” æ¨™æœ¬å¹³å‡ã¯ **Fisher åŠ¹ç‡çš„æ¨å®šé‡**ï¼ˆCR ä¸‹ç•Œã‚’é”æˆã™ã‚‹ï¼‰ã€‚

**CR ä¸‹ç•ŒãŒé”æˆã•ã‚Œãªã„ä¾‹**: åˆ†æ•£ $\sigma^2$ ã®æ¨å®šã§ã¯ã€æ¨™æœ¬åˆ†æ•£ $\hat{\sigma}^2 = \frac{1}{n}\sum(x_i-\bar{x})^2$ ã®åˆ†æ•£ã¯ $2\sigma^4/n$ã€CR ä¸‹ç•Œã¯ $2\sigma^4/n$ï¼ˆFisher æƒ…å ± $I(\sigma^2) = n/(2\sigma^4)$ï¼‰â€” åŒã˜ãåŠ¹ç‡çš„ã€‚ã—ã‹ã—ã€åˆ†æ•£ã®å¹³æ–¹æ ¹ $\hat{\sigma}$ ã®æ¨å®šé‡ã¯ä¸€èˆ¬ã« CR ä¸‹ç•Œã‚’é”æˆã—ãªã„ï¼ˆãƒ‡ãƒ«ã‚¿æ³•ã«ã‚ˆã‚‹å¤‰æ›ã§ä¸‹ç•ŒãŒå¤‰ã‚ã‚‹ãŸã‚ï¼‰ã€‚

**Fisher æƒ…å ±è¡Œåˆ—ï¼ˆå¤šæ¬¡å…ƒï¼‰**: $\mathbf{I}(\boldsymbol{\theta})_{ij} = \mathbb{E}[\partial_i \log p \cdot \partial_j \log p]$ã€‚ã“ã‚ŒãŒ Riemannian è¨ˆé‡ã«ãªã‚Šã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã®ã€ŒKL å¹¾ä½•å­¦ã€ã‚’å®šç¾©ã™ã‚‹ã€‚è‡ªç„¶å‹¾é… $\tilde{\nabla} = \mathbf{I}^{-1} \nabla$ ã¯ã“ã®å¹¾ä½•å­¦ã«æ²¿ã£ãŸæœ€æ€¥é™ä¸‹ã ã€‚

**æ¤œè¨¼**: æ¨™æœ¬å¹³å‡ã¯CramÃ©r-Raoä¸‹ç•Œã‚’**ã´ã£ãŸã‚Šé”æˆ**ã™ã‚‹ï¼ˆFisheråŠ¹ç‡çš„æ¨å®šé‡ï¼‰ã€‚æ¯”ç‡ãŒå…¨ã¦â‰ˆ1.0ã«ãªã‚‹ã€‚

**ã‚¹ã‚³ã‚¢ã®æœŸå¾…å€¤ã¯ã‚¼ãƒ­**: $\mathbb{E}[s(X;\theta)] = 0$ã€‚$\int p(x;\theta) dx = 1$ ã‚’ $\theta$ ã§å¾®åˆ†ã™ã‚‹ã¨å°ã‘ã‚‹ï¼ˆæ­£è¦åŒ–æ¡ä»¶ã®å¾®åˆ†ï¼‰ã€‚Fisheræƒ…å ±é‡ã¯ã‚¹ã‚³ã‚¢ã®åˆ†æ•£ã ã€‚

$$
\mathbb{E}[s] = \int \frac{\partial \log p}{\partial \theta} p \, dx = \frac{\partial}{\partial \theta} \int p \, dx = \frac{\partial}{\partial \theta} 1 = 0
$$

**å¤šæ¬¡å…ƒFisheræƒ…å ±è¡Œåˆ— (FIM)**: $\mathbf{I}(\boldsymbol{\theta})_{ij} = \mathbb{E}[\partial_i \log p \cdot \partial_j \log p]$ã€‚è‡ªç„¶å‹¾é…æ³• $\tilde{\nabla}_\theta \mathcal{L} = \mathbf{I}^{-1} \nabla_\theta \mathcal{L}$ ã¯FIMã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã®æ›²ç‡ã‚’è£œæ­£ã—ã€ç¢ºç‡å¤šæ§˜ä½“ä¸Šã®æœ€é©è§£ã«æœ€çŸ­çµŒè·¯ã§åˆ°é”ã™ã‚‹ã€‚

### 5.6 ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæ¯é–¢æ•°ã¨ç‰¹æ€§é–¢æ•°

**ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæ¯é–¢æ•°ï¼ˆMGFï¼‰**: $M_X(t) = \mathbb{E}[e^{tX}]$

MGFã® $k$ æ¬¡å¾®åˆ†ã¯ $k$ æ¬¡ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆã‚’ä¸ãˆã‚‹: $M_X^{(k)}(0) = \mathbb{E}[X^k]$


MGFãŒå­˜åœ¨ã—ãªã„åˆ†å¸ƒã‚‚ã‚ã‚‹ï¼ˆCauchyåˆ†å¸ƒãªã©ï¼‰ã€‚ãã®å ´åˆã¯**ç‰¹æ€§é–¢æ•°** $\varphi_X(t) = \mathbb{E}[e^{itX}]$ ã‚’ä½¿ã†ã€‚ç‰¹æ€§é–¢æ•°ã¯å¸¸ã«å­˜åœ¨ã—ã€åˆ†å¸ƒã‚’ä¸€æ„ã«æ±ºå®šã™ã‚‹ã€‚CLTã®è¨¼æ˜ã¯ã—ã°ã—ã°ç‰¹æ€§é–¢æ•°ã‚’ç”¨ã„ã¦è¡Œã‚ã‚Œã‚‹ã€‚

Gaussianã®å ´åˆ: $M_X(t) = \exp(\mu t + \frac{\sigma^2 t^2}{2})$ã€‚

**ç‹¬ç«‹å’Œã®æ€§è³ª**: $X, Y$ ãŒç‹¬ç«‹ãªã‚‰ $M_{X+Y}(t) = M_X(t) M_Y(t)$ã€‚ã“ã‚ŒãŒCLTè¨¼æ˜ã®æ ¸å¿ƒã  â€” ã‚µãƒ³ãƒ—ãƒ«å’Œã®ç‰¹æ€§é–¢æ•°ãŒå…ƒã®ç‰¹æ€§é–¢æ•°ã®ç©ã«ãªã‚Šã€$N \to \infty$ ã§æ­£è¦åˆ†å¸ƒã®ç‰¹æ€§é–¢æ•°ã«åæŸã™ã‚‹ã€‚

$$
M_X(t) = \mathbb{E}[e^{tX}] = \int e^{tx} p(x) \, dx
$$

è¨˜å· â†” å¤‰æ•°å¯¾å¿œ:
- $t$ â†” Laplace å¤‰æ›ã®å¤‰æ•°ï¼ˆå®Ÿæ•°ï¼‰
- $M_X^{(k)}(0) = \mathbb{E}[X^k]$ â†” $k$ æ¬¡å¾®åˆ†ã§ $k$ æ¬¡ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆã‚’å–ã‚Šå‡ºã›ã‚‹
- ç‰¹æ€§é–¢æ•° $\varphi_X(t) = M_X(it)$ â†” $t$ ã‚’è™šæ•°è»¸ã«ã—ãŸ MGF ã®è§£ææ¥ç¶š

**CLT ã®è¨¼æ˜ã‚¹ã‚±ãƒƒãƒï¼ˆç‰¹æ€§é–¢æ•°çµŒç”±ï¼‰**:

$S_N = \frac{1}{\sqrt{N}}\sum_{i=1}^N (X_i - \mu)/\sigma$ ã®ç‰¹æ€§é–¢æ•°ã‚’è¨ˆç®—ã™ã‚‹:

$$
\varphi_{S_N}(t) = \left[\varphi_{(X-\mu)/\sigma}\!\!\left(\frac{t}{\sqrt{N}}\right)\right]^N
$$

$(X-\mu)/\sigma$ ã®ç‰¹æ€§é–¢æ•°ã‚’ $u = t/\sqrt{N}$ å‘¨ã‚Šã§ãƒ†ã‚¤ãƒ©ãƒ¼å±•é–‹ï¼ˆ$u \to 0$ ã¨ã—ã¦ï¼‰:

$$
\varphi(u) = 1 + iu\mathbb{E}\left[\frac{X-\mu}{\sigma}\right] - \frac{u^2}{2}\mathbb{E}\left[\frac{(X-\mu)^2}{\sigma^2}\right] + O(u^3) = 1 - \frac{u^2}{2} + O(u^3)
$$

ï¼ˆ$\mathbb{E}[(X-\mu)/\sigma] = 0$ã€$\mathbb{E}[(X-\mu)^2/\sigma^2] = 1$ ã‚’ä½¿ç”¨ã€‚ï¼‰

$$
\varphi_{S_N}(t) = \left(1 - \frac{t^2}{2N} + O(N^{-3/2})\right)^N \xrightarrow{N \to \infty} e^{-t^2/2}
$$

$e^{-t^2/2}$ ã¯ $\mathcal{N}(0,1)$ ã®ç‰¹æ€§é–¢æ•°ã  â€” CLT ã®è¨¼æ˜å®Œäº†ã€‚

**MGF ãŒå­˜åœ¨ã—ãªã„åˆ†å¸ƒ**: Cauchy åˆ†å¸ƒ $p(x) = \frac{1}{\pi(1+x^2)}$ ã® MGF ã¯åæŸã—ãªã„ï¼ˆé‡è£¾ã®ãŸã‚ $\mathbb{E}[e^{tX}] = \infty$ï¼‰ã€‚ã ãŒç‰¹æ€§é–¢æ•°ã¯å¸¸ã«å­˜åœ¨ã™ã‚‹: $\varphi_X(t) = e^{-|t|}$ã€‚Cauchy åˆ†å¸ƒã¯ CLT ãŒé©ç”¨ã•ã‚Œãªã„ä»£è¡¨ä¾‹ã§ã€$N$ å€‹ã®å¹³å‡ã¯åŒã˜ Cauchy åˆ†å¸ƒã«å¾“ã„ã€ŒåæŸã—ãªã„ã€ã€‚

**Gaussian ã® MGF ã¨ç©ç‡**:

$M_X(t) = e^{\mu t + \sigma^2 t^2/2}$ ã‚’ $t$ ã§å±•é–‹ã™ã‚‹ã¨ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆãŒå¾—ã‚‰ã‚Œã‚‹:

$$
\begin{aligned}
\mathbb{E}[X] &= \mu \\
\mathbb{E}[X^2] &= \mu^2 + \sigma^2 \\
\mathbb{E}[X^3] &= \mu^3 + 3\mu\sigma^2 \\
\mathbb{E}[X^4] &= \mu^4 + 6\mu^2\sigma^2 + 3\sigma^4
\end{aligned}
$$

ç‹¬ç«‹å’Œã®æ€§è³ª $M_{X+Y}(t) = M_X(t) M_Y(t)$ ã‹ã‚‰: $X \sim \mathcal{N}(\mu_1, \sigma_1^2)$, $Y \sim \mathcal{N}(\mu_2, \sigma_2^2)$ ãŒç‹¬ç«‹ãªã‚‰ $X+Y \sim \mathcal{N}(\mu_1+\mu_2, \sigma_1^2+\sigma_2^2)$ã€‚ã“ã‚ŒãŒ Gaussian ã®å†ç”Ÿæ€§ï¼ˆreproductive propertyï¼‰ã ã€‚



### 5.7 è‡ªå·±å›å¸°å°¤åº¦ã®å®Œå…¨å®Ÿè£… â€” Topic 5

è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã®ã€Œå…¨ã¦ã€ã¯ã“ã®ä¸€å¼ã«åã¾ã‚‹:

$$
\log p(\mathbf{x}) = \sum_{t=1}^{T} \log p(x_t \mid x_1, \ldots, x_{t-1})
$$

å„ã‚¹ãƒ†ãƒƒãƒ—ãŒ Categorical åˆ†å¸ƒã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° + å¯¾æ•°ç¢ºç‡ã®åŠ ç®—ã€‚

**è¨˜å· â†” å¤‰æ•°å¯¾å¿œï¼ˆBlock 2ï¼‰**:
- $\mathbf{x} = (x_1,\ldots,x_T)$: ãƒˆãƒ¼ã‚¯ãƒ³åˆ— â†’ `x: (T,)` int tensor
- $\mathbf{z}_t = f_\theta(x_{<t})$: ãƒ¢ãƒ‡ãƒ«å‡ºåŠ› logit â†’ `logits: (T, V)` float tensor
- $\log p(x_t \mid x_{<t}) = \log \text{softmax}(\mathbf{z}_t)_{x_t}$ â†’ `log_probs.gather(-1, x)`
- $\log p(\mathbf{x}) = \sum_t \log p(x_t \mid x_{<t})$ â†’ `.sum()` over `(T,)` tensor
- Perplexity $= \exp(-\frac{1}{T}\log p(\mathbf{x}))$ â†’ `torch.exp(-log_prob / T)`

shape: `logits: (T, V)`, `x: (T,)`, `log_p_tokens: (T,)`, `log_prob: scalar`

**æ•°å€¤å®‰å®šåŒ–**: `F.log_softmax(logits, dim=-1)` ãŒ `logits - log Z` ã‚’è¨ˆç®—ã€‚`log Z = torch.logsumexp(logits, dim=-1)` ãŒ max-shift trick ã§å®‰å®šåŒ–ã•ã‚Œã‚‹:

$$
\log \sum_j e^{z_j} = c + \log \sum_j e^{z_j - c}, \quad c = \max_k z_k
$$

**è½ã¨ã—ç©´**: `logits[:-1]` ã¨ `x[1:]` ã®ãƒšã‚¢ã«ã™ã‚‹ã“ã¨ï¼ˆæ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ï¼‰ã€‚`F.nll_loss` ã¯ `log_softmax` é©ç”¨æ¸ˆã¿ãƒ†ãƒ³ã‚½ãƒ«ã‚’è¦æ±‚ã™ã‚‹ç‚¹ã«æ³¨æ„ï¼ˆå†…éƒ¨ã§å†ã³ `log_softmax` ã‚’ã‹ã‘ãªã„ï¼‰ã€‚

æ¤œç®—: `log_prob <= 0`ï¼ˆç¢ºç‡ã¯ [0,1] â†’ å¯¾æ•°ã¯éæ­£ï¼‰ã€`perplexity >= 1.0`ã€`logsumexp` çµŒç”±ã®æ‰‹å‹•è¨ˆç®—ã¨ assert ã§ä¸€è‡´ã‚’ç¢ºèªã™ã‚‹ã€‚

```python
import torch
import torch.nn.functional as F
import torch.distributions as D

torch.manual_seed(0)

# --- Block 2 / 3: MLE + autoregressive log-likelihood ---
# log p(x) = sum_{t=1}^{T} log p(x_t | x_{<t})

def autoregressive_log_prob(logits: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
    """
    logits: (T, V) â€” raw scores at each position (teacher-forcing)
    x:      (T,)   â€” token ids in {0, ..., V-1}
    returns: log p(x_1, ..., x_T) as a scalar tensor
    """
    # F.log_softmax = z_k - logsumexp(z)  [numerically stable]
    log_probs = F.log_softmax(logits, dim=-1)          # (T, V)
    # gather: pick log p(x_t) for actual token at each position
    log_p_t = log_probs.gather(dim=-1, index=x.unsqueeze(-1)).squeeze(-1)  # (T,)
    return log_p_t.sum()                                # scalar: log p(x)

def nll_loss(logits: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
    """Next-token prediction NLL: logits[:-1] predicts x[1:]."""
    return F.nll_loss(F.log_softmax(logits[:-1], dim=-1), x[1:], reduction='mean')

def perplexity(logits: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
    T = x.shape[0]
    return torch.exp(-autoregressive_log_prob(logits, x) / T)

# numerical verification
V, T = 50, 20
logits = torch.randn(T, V)
x = torch.randint(0, V, (T,))

lp  = autoregressive_log_prob(logits, x)
ppl = perplexity(logits, x)
nll = nll_loss(logits, x)

assert lp.item() <= 0,          "log probability <= 0"
assert ppl.item() >= 1.0,       "perplexity >= 1"

# cross-check via logsumexp
log_z   = torch.logsumexp(logits, dim=-1)              # (T,)  = log Z per position
ll_manual = (logits[range(T), x] - log_z).sum()
assert abs(lp.item() - ll_manual.item()) < 1e-5,       "logsumexp cross-check"

print(f"log_prob={lp:.4f}, perplexity={ppl:.2f}, NLL={nll:.4f}")
# log_prob <= 0,  perplexity >= 1,  NLL >= 0
```

**æ•°å€¤æ¤œç®—**: `softmax` ã®å’Œ = 1 ã¯ `F.softmax(logits, dim=-1).sum(dim=-1)` ãŒå…¨ã¦ `â‰ˆ 1.0` ã§ç¢ºèªã§ãã‚‹ã€‚Perplexity = 1 ã¯ãƒ¢ãƒ‡ãƒ«ãŒç¢ºä¿¡ã‚’æŒã£ã¦æ­£è§£ã‚’äºˆæ¸¬ã™ã‚‹å ´åˆï¼ˆå®Ÿéš›ã¯èªå½™ã‚µã‚¤ã‚º $V = 50$ ã®ä¸€æ§˜åˆ†å¸ƒã§ Perplexity â‰ˆ $e^{\log 50} = 50$ ãŒæœŸå¾…å€¤ï¼‰ã€‚

---

ã“ã‚Œã§ç¢ºç‡è«–ã®5ãƒˆãƒ”ãƒƒã‚¯å…¨ã¦ã®å®Ÿè£…ãŒå®Œäº†ã—ãŸã€‚æ¬¡ã¯ Triton ã‚«ãƒ¼ãƒãƒ«ã§ä¸¦åˆ—åŒ–ã™ã‚‹ã€‚

### 5.8 Triton ã‚«ãƒ¼ãƒãƒ« â€” ä¸¦åˆ—å¯¾æ•°å°¤åº¦è¨ˆç®—

ç¢ºç‡ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã«ãŠã„ã¦ã€Œå…¨ãƒ‡ãƒ¼ã‚¿ç‚¹ã® $\log p(x^{(i)} \mid \theta)$ ã‚’è¨ˆç®—ã™ã‚‹ã€æ“ä½œã¯**å„ãƒ‡ãƒ¼ã‚¿ç‚¹ãŒç‹¬ç«‹**ãªã®ã§ã€GPU ä¸Šã§ embarrassingly parallel ã«å®Ÿè¡Œã§ãã‚‹ã€‚

å„ã‚¹ãƒ¬ãƒƒãƒ‰ãŒ1ãƒ‡ãƒ¼ã‚¿ç‚¹ã®å¯¾æ•°å°¤åº¦ã‚’æ‹…å½“ã™ã‚‹ã‚«ãƒ¼ãƒãƒ«ã‚’å®Ÿè£…ã™ã‚‹ã€‚

$$
\ell_i(\mu, \sigma) = \log \mathcal{N}(x^{(i)} \mid \mu, \sigma) = -\frac{1}{2}\left(\frac{x^{(i)}-\mu}{\sigma}\right)^2 - \log\sigma - \frac{1}{2}\log(2\pi)
$$

**è¨˜å· â†” å¤‰æ•°å¯¾å¿œï¼ˆBlock 3ï¼‰**:
- $x^{(i)}$ â†” `x[pid * BLOCK + arange(BLOCK)]`ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ãŒæ‹…å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ï¼‰
- $z_i = (x^{(i)}-\mu)/\sigma$ â†” `z = (x - mu) / sigma`
- $\ell_i$ â†” `log_p` â†’ `out[...]` ã«æ›¸ãè¾¼ã‚€
- $\frac{1}{2}\log(2\pi) \approx 0.9189385$ â†” `0.9189385`ï¼ˆå®šæ•°ï¼‰

shape: `x_ptr: (N,)`, `out_ptr: (N,)`, å„ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ãŒ `BLOCK` ç‚¹ã‚’å‡¦ç†

**æ•°å€¤å®‰å®šåŒ–**: Gaussian log-prob ã§ã¯ `exp` ã¯ä¸è¦ â€” å¯¾æ•°ç©ºé–“ã®ã¾ã¾è¨ˆç®—ã§ãã‚‹ã€‚`z * z` ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ã¯å®Ÿç”¨çš„ãª $\sigma$ ã®ç¯„å›²ã§ã¯å•é¡Œã«ãªã‚‰ãªã„ï¼ˆFP32 ã®æœ€å¤§å€¤ $\approx 3.4 \times 10^{38}$ã€$z^2 < 10^{76}$ ã¾ã§å®‰å…¨ï¼‰ã€‚

æ¤œç®—: Triton ã‚«ãƒ¼ãƒãƒ«ã®å‡ºåŠ› sum ã¨ `torch.distributions.Normal.log_prob(x).sum()` ã‚’æ¯”è¼ƒã—ã€èª¤å·® $< 10^{-2}$ ã‚’ assert ã™ã‚‹ã€‚

```python
import triton
import triton.language as tl
import torch
import torch.distributions as D

@triton.jit
def gaussian_log_lik_kernel(
    x_ptr,              # (N,) data points
    out_ptr,            # (N,) output log-likelihoods
    mu,                 # scalar mean
    sigma,              # scalar std (> 0)
    N,                  # number of points
    BLOCK: tl.constexpr,
):
    pid  = tl.program_id(0)
    offs = pid * BLOCK + tl.arange(0, BLOCK)   # (BLOCK,)
    mask = offs < N

    x = tl.load(x_ptr + offs, mask=mask, other=0.0)   # (BLOCK,)

    # log N(x | mu, sigma) = -0.5*z^2 - log(sigma) - 0.5*log(2*pi)
    z     = (x - mu) / sigma                           # standardized
    log_p = -0.5 * z * z - tl.log(sigma) - 0.9189385  # 0.9189385 = 0.5*log(2*pi)

    tl.store(out_ptr + offs, log_p, mask=mask)


def gaussian_log_lik(x: torch.Tensor, mu: float, sigma: float) -> torch.Tensor:
    """Parallel log N(x_i | mu, sigma) for all i."""
    N   = x.numel()
    out = torch.empty(N, device=x.device, dtype=x.dtype)
    BLOCK = 1024
    grid  = (triton.cdiv(N, BLOCK),)
    gaussian_log_lik_kernel[grid](x, out, mu, sigma, N, BLOCK=BLOCK)
    return out


# numerical check: Triton sum vs torch.distributions
torch.manual_seed(0)
x_test = torch.randn(10_000)
mu_val, sigma_val = 0.5, 1.2

ll_triton = gaussian_log_lik(x_test, mu_val, sigma_val).sum()
ll_torch  = D.Normal(mu_val, sigma_val).log_prob(x_test).sum()

assert abs(ll_triton.item() - ll_torch.item()) < 1.0   # float32 accumulation tolerance
print(f"triton sum={ll_triton:.4f},  torch sum={ll_torch:.4f}")
print(f"|diff| = {abs(ll_triton - ll_torch):.4f}  (float32 accumulation)")
```

### 5.9 ç†è§£åº¦ãƒã‚§ãƒƒã‚¯ â€” Z5 å®Œäº†ç¢ºèª

<details>
<summary>Q1: SciPyã§å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã®æ¡ä»¶ä»˜ãåˆ†å¸ƒã‚’è¨ˆç®—ã™ã‚‹éš›ã®æ•°å€¤å®‰å®šæ€§ã®æ³¨æ„ç‚¹ã¯ï¼Ÿ</summary>

**A**: å…±åˆ†æ•£è¡Œåˆ— $\Sigma$ ãŒç‰¹ç•°ã«è¿‘ã„å ´åˆã€é€†è¡Œåˆ—è¨ˆç®—ãŒä¸å®‰å®šã«ãªã‚‹ã€‚å¯¾ç­–ï¼š(1) `torch.linalg.solve` ã‚’ä½¿ã„ç›´æ¥é€†è¡Œåˆ—ã‚’é¿ã‘ã‚‹ã€(2) Cholesky åˆ†è§£ã§æ­£å®šå€¤æ€§ã‚’ç¢ºèªï¼ˆ`torch.linalg.cholesky` ãŒå¤±æ•— â†’ åŠæ­£å®šå€¤ï¼‰ã€(3) æ­£å‰‡åŒ–é … $\Sigma + \epsilon I$ ã‚’è¿½åŠ ï¼ˆ$\epsilon \sim 10^{-6}$ï¼‰ã€(4) æ¡ä»¶æ•° $\kappa(\Sigma)$ ã‚’ç¢ºèªï¼ˆ$> 10^{10}$ ãªã‚‰å±é™ºï¼‰ã€‚`D.MultivariateNormal` ã¯ `scale_tril` å¼•æ•°ã§ Cholesky å› å­ã‚’ç›´æ¥å—ã‘å–ã‚Œã‚‹ã€‚

</details>

<details>
<summary>Q2: ãƒ™ã‚¤ã‚ºæ¨è«–ã®ã‚°ãƒªãƒƒãƒ‰è¿‘ä¼¼ãŒå®Ÿç”¨çš„ã§ãªã„ç†ç”±ã¨ä»£æ›¿æ‰‹æ³•ã‚’èª¬æ˜ã›ã‚ˆã€‚</summary>

**A**: ã‚°ãƒªãƒƒãƒ‰è¿‘ä¼¼ã¯æ¬¡å…ƒã®å‘ªã„ï¼ˆ$d$ æ¬¡å…ƒã§ $N^d$ ç‚¹å¿…è¦ï¼‰ã€‚10æ¬¡å…ƒã§å„è»¸100ç‚¹ãªã‚‰ $100^{10} = 10^{20}$ ç‚¹ã€‚ä»£æ›¿æ‰‹æ³•ï¼š(1) MCMCï¼ˆMetropolis-Hastingsã€HMCï¼‰ã§äº‹å¾Œåˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€(2) å¤‰åˆ†æ¨è«–ï¼ˆELBOæœ€å¤§åŒ–ï¼‰ã§è¿‘ä¼¼åˆ†å¸ƒ $q(\theta)$ ã‚’æœ€é©åŒ–ã€(3) Laplaceè¿‘ä¼¼ã§äº‹å¾Œã®ãƒ¢ãƒ¼ãƒ‰å‘¨ã‚Šã‚’æ­£è¦è¿‘ä¼¼ã€‚

</details>

---

### 5.9 åˆ†å¸ƒãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®å…¨ä½“åƒã¨ç›¸äº’é–¢ä¿‚

ç¬¬4å›ã§ç™»å ´ã—ãŸåˆ†å¸ƒãŸã¡ã®é–¢ä¿‚ã‚’æ•´ç†ã™ã‚‹ã€‚ã“ã‚Œã‚’çŸ¥ã£ã¦ã„ã‚‹ã¨ã€æ–°ã—ã„å•é¡Œã«ç›´é¢ã—ãŸã¨ãã€Œã©ã®åˆ†å¸ƒã‚’ä½¿ã†ã¹ãã‹ã€ãŒè¦‹ãˆã‚„ã™ããªã‚‹ã€‚

```mermaid
flowchart TD
  B["Bernoulli(p)\nP(X=1)=p"] --> C["Binomial(n,p)\nnå›è©¦è¡Œã®æˆåŠŸæ•°"]
  B --> CAT["Categorical(Ï€)\nKå€¤ã®é›¢æ•£åˆ†å¸ƒ"]
  CAT --> MULT["Multinomial(n,Ï€)\nnå›è©¦è¡Œã®å‡ºç¾æ•°"]
  N1["Normal(Î¼,ÏƒÂ²)"] --> MVN["Multivariate Normal\n(Î¼,Î£)"]
  MVN --> GMM["Gaussian Mixture\nÎ£_k Ï€_k N(Î¼_k,Î£_k)"]
  BETA["Beta(a,b)\nâˆˆ[0,1]"] --> B
  GAMMA["Gamma(Î±,Î²)"] --> N1
  GAMMA --> POISSON["Poisson(Î»)\néè² æ•´æ•°"]
  N1 --> CHI2["Chi-squared(k)\n=Gamma(k/2,2)"]
  EF["æŒ‡æ•°å‹åˆ†å¸ƒæ—\np(x|Î·)=h(x)exp(Î·^T T(x)-A(Î·))"] --> B
  EF --> N1
  EF --> BETA
  EF --> GAMMA
  EF --> POISSON
  EF --> CAT
```

**è¦šãˆã¦ãŠãã¹ãå¤‰æ›**:

| å¤‰æ› | æ•°å¼ | ç”¨é€” |
|:-----|:-----|:-----|
| $X \sim \mathcal{N}(0,1)$ â†’ $X^2 \sim \chi^2(1)$ | 2ä¹—å¤‰æ› | æ¤œå®šçµ±è¨ˆé‡ |
| $\sum_{k=1}^n Z_k^2 \sim \chi^2(n)$ | åŠ æ³•æ€§ | åˆ†æ•£æ¨å®š |
| $\text{Bernoulli}(p) = \text{Binomial}(1, p)$ | ç‰¹æ®Šã‚±ãƒ¼ã‚¹ | LLMå‡ºåŠ› |
| $\text{Categorical}(\boldsymbol{\pi}) = \text{Multinomial}(1, \boldsymbol{\pi})$ | ç‰¹æ®Šã‚±ãƒ¼ã‚¹ | ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ |
| $X \sim \text{Poisson}(\lambda)$ ã¨ã—ã¦ $\lambda \to \infty$: $\mathcal{N}(\lambda, \lambda)$ | CLT | æ­£è¦è¿‘ä¼¼ |

**ç¬¬4å›ã®ãƒˆãƒ”ãƒƒã‚¯å…¨ã‚«ãƒãƒ¬ãƒƒã‚¸ç¢ºèª**:

| ãƒˆãƒ”ãƒƒã‚¯ | å®Ÿè£…å®Œäº† | é‡è¦åº¦ |
|:---------|:---------|:-------|
| ç¢ºç‡åˆ†å¸ƒï¼ˆGaussian/Categorical/Betaï¼‰ | 5.1 âœ… | â­â­â­ |
| å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒãƒ»æ¡ä»¶ä»˜ãåˆ†å¸ƒ | 5.2 âœ… | â­â­â­ |
| æŒ‡æ•°å‹åˆ†å¸ƒæ— | 5.3 âœ… | â­â­â­ |
| GMMãƒ»EMç®—æ³•ã®å‰æ®µ | 5.4 âœ… | â­â­â­ |
| ãƒ™ã‚¤ã‚ºæ¨è«–ï¼ˆã‚°ãƒªãƒƒãƒ‰ï¼‰| 5.5a âœ… | â­â­ |
| å…±å½¹äº‹å‰åˆ†å¸ƒï¼ˆGaussian-Gaussianï¼‰| 5.5b âœ… | â­â­â­ |
| KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ | 5.5a-KL âœ… | â­â­â­ |
| Fisheræƒ…å ±é‡ãƒ»CRä¸‹ç•Œ | 5.5c âœ… | â­â­â­ |
| LLNãƒ»CLT | 5.1è£œè¶³ âœ… | â­â­ |
| è‡ªå·±å›å¸°å°¤åº¦ | 5.7 âœ… | â­â­â­ |

> Progress: 85%

---

## ğŸ”¬ Z6. æ–°ãŸãªå†’é™ºã¸ï¼ˆ20åˆ†ï¼‰â€” ç¢ºç‡è«–ã®ç ”ç©¶ç³»è­œ

### 6.1 VAE â€” ç¢ºç‡çš„ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®çµ±ä¸€

Kingma & Welling (2013)[^2] ã¯ç¢ºç‡è«–ã®å…¨æ­¦å™¨ã‚’ä¸€ç‚¹ã«é›†ç´„ã—ãŸã€‚

è¦³æ¸¬ $\mathbf{x}$ã€æ½œåœ¨å¤‰æ•° $\mathbf{z}$ã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ« $p_\theta(\mathbf{x} \mid \mathbf{z})$ã€‚å•é¡Œ: äº‹å¾Œåˆ†å¸ƒ $p_\theta(\mathbf{z} \mid \mathbf{x})$ ãŒ intractableã€‚

**è§£æ±º**: å¤‰åˆ†åˆ†å¸ƒ $q_\phi(\mathbf{z} \mid \mathbf{x}) \approx p_\theta(\mathbf{z} \mid \mathbf{x})$ ã§è¿‘ä¼¼ã—ã€ELBOï¼ˆEvidence Lower BOundï¼‰ã‚’æœ€å¤§åŒ–:

$$
\log p_\theta(\mathbf{x}) \geq \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}[\log p_\theta(\mathbf{x} \mid \mathbf{z})] - D_{\mathrm{KL}}(q_\phi(\mathbf{z} \mid \mathbf{x}) \| p(\mathbf{z}))
$$

å·¦è¾ºã¨å³è¾ºã®å·®ã¯ $D_{\mathrm{KL}}(q \| p_\theta(\mathbf{z}|\mathbf{x})) \geq 0$ ã ã‹ã‚‰ã€ç­‰å·ã¯KLãŒã‚¼ãƒ­ã®ã¨ãã€‚

**ç¬¬4å›ã¨ã®æ¥ç¶š**:
- ç¬¬1é … $\mathbb{E}_{q}[\log p_\theta(\mathbf{x}|\mathbf{z})]$ = Gaussian MLE ã®æœŸå¾…å€¤ç‰ˆ
- ç¬¬2é … $D_{\mathrm{KL}}(q \| p)$ = KL divergenceï¼ˆæƒ…å ±ç†è«–ã€ç¬¬5å›ä»¥é™ï¼‰
- äº‹å‰ $p(\mathbf{z}) = \mathcal{N}(\mathbf{0}, I)$ = å…±å½¹Gaussian ã®å¿œç”¨

### 6.2 Bayesian Deep Learning â€” åˆ†å¸ƒã¨ã—ã¦ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯

ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®é‡ã¿ $\mathbf{w}$ ã‚’ç‚¹æ¨å®šã§ã¯ãªãåˆ†å¸ƒã¨ã—ã¦æ‰±ã†ã€‚

$$
p(\mathbf{w} \mid \mathcal{D}) \propto p(\mathcal{D} \mid \mathbf{w}) \cdot p(\mathbf{w})
$$

ã“ã‚Œã¯ç¬¬4å› Â§3 ã®ãƒ™ã‚¤ã‚ºæ›´æ–°ã®ç›´æ¥é©ç”¨ã ã€‚å•é¡Œ: $\mathbf{w}$ ãŒä½•ç™¾ä¸‡æ¬¡å…ƒã§ã‚‚ã‚°ãƒªãƒƒãƒ‰è¿‘ä¼¼ã¯ä¸å¯èƒ½ â†’ å¤‰åˆ†æ¨è«–ï¼ˆVIï¼‰ã‹MCMCãŒå¿…è¦ã€‚

**Bayes by Backprop**: é‡ã¿ã‚’ $q(\mathbf{w}) = \mathcal{N}(\boldsymbol{\mu}, \text{diag}(\boldsymbol{\sigma}^2))$ ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã—ã€ELBOã‚’å‹¾é…é™ä¸‹ã§æœ€å¤§åŒ–ã€‚ã€Œé‡ã¿ã®ä¸ç¢ºå®Ÿæ€§ã€ãŒäºˆæ¸¬ã®ä¸ç¢ºå®Ÿæ€§ã«å¤‰æ›ã•ã‚Œã‚‹ã€‚

**ãªãœä»Šã€å†æ³¨ç›®ã•ã‚Œã‚‹ã®ã‹**: LLMã®Calibrationå•é¡Œã€‚ã€Œãƒ¢ãƒ‡ãƒ«ãŒé«˜ç¢ºä¿¡åº¦ã§èª¤ç­”ã™ã‚‹ã€ç¾è±¡ã‚’Bayesianæ‰‹æ³•ã§ç·©å’Œã§ãã‚‹å¯èƒ½æ€§ã€‚

### 6.3 è‡ªå·±å›å¸°ã®æ™®éæ€§ â€” Malach (2023)

$$
\log p(\mathbf{x}) = \sum_{t=1}^{T} \log p(x_t \mid x_{<t})
$$

ã“ã®é€£é–è¦å‰‡ã¯**ä»»æ„ã®åˆ†å¸ƒ**ã«å¯¾ã—ã¦å³å¯†ã«æˆç«‹ã™ã‚‹ï¼ˆç¢ºç‡ã®ä¹—æ³•å®šç†ï¼‰ã€‚Malach (2023)[^5] ã¯ã€Œååˆ†ãªè¡¨ç¾åŠ›ã‚’æŒã¤è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã¯ã‚ã‚‰ã‚†ã‚‹ç¢ºç‡åˆ†å¸ƒã‚’è¿‘ä¼¼ã§ãã‚‹ã€ã“ã¨ã‚’ç†è«–åŒ–ã—ãŸã€‚

ã€ŒGPTç³»LLMãŒç”»åƒãƒ»éŸ³å£°ãƒ»ã‚¿ãƒ³ãƒ‘ã‚¯è³ªãƒ»ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã§ãã‚‹ã€ã®ç†è«–çš„æ ¹æ‹ ã¯ã“ã“ã«ã‚ã‚‹ã€‚é€£é–è¦å‰‡ã®ã‚·ãƒ³ãƒ—ãƒ«ã•ãŒã€é©ç”¨ç¯„å›²ã®åºƒå¤§ã•ã«ç›´çµã™ã‚‹ã€‚

### 6.4 Diffusion Models â€” ç¢ºç‡éç¨‹ã¨é€†æ‹¡æ•£

DDPM (Ho et al. 2020)[^6] ã¯ç¢ºç‡è«–ã®ç•°ãªã‚‹å´é¢ã‚’ä½¿ã†ã€‚

**Forward process** (æ‹¡æ•£: ãƒ‡ãƒ¼ã‚¿ â†’ ãƒã‚¤ã‚º):

$$
q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t;\, \sqrt{1-\beta_t}\,\mathbf{x}_{t-1},\, \beta_t I)
$$

å„ã‚¹ãƒ†ãƒƒãƒ—ã§å°‘é‡ã®ãƒã‚¤ã‚ºã‚’åŠ ãˆã‚‹ã€‚$T$ ã‚¹ãƒ†ãƒƒãƒ—å¾Œ: $\mathbf{x}_T \approx \mathcal{N}(\mathbf{0}, I)$ã€‚

**Reverse process** (ç”Ÿæˆ: ãƒã‚¤ã‚º â†’ ãƒ‡ãƒ¼ã‚¿): ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ $p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)$ ã‚’å­¦ç¿’ã€‚

**ç¬¬4å›ã¨ã®æ¥ç¶š**: Forward processã¯Gaussianã®é€£ç¶šç©ã€‚ELBO ã®æœ€é©åŒ–ã¯VAEã¨åŒã˜æ§‹é€ ã€‚ç¬¬4å›ã§å­¦ã‚“ã ã€ŒGaussianåŒå£«ã®å‘¨è¾ºåŒ–ã®é–‰å½¢å¼ã€ãŒ $q(\mathbf{x}_t \mid \mathbf{x}_0)$ ã®åˆ†æçš„è¨ˆç®—ã‚’å¯èƒ½ã«ã™ã‚‹ã€‚

### 6.5 ç ”ç©¶ç³»è­œå›³

```mermaid
flowchart TD
  KO["Kolmogorov (1933)<br/>ç¢ºç‡ç©ºé–“ã®å…¬ç†åŒ–"]
  BE["Bayes & Price (1763)<br/>ãƒ™ã‚¤ã‚ºã®å®šç†"]
  CR["CramÃ©r-Rao (1945/46)<br/>Fisheræƒ…å ±é‡ãƒ»æ¨å®šé™ç•Œ"]
  EF["æŒ‡æ•°å‹åˆ†å¸ƒæ—<br/>ååˆ†çµ±è¨ˆé‡ãƒ»å…±å½¹äº‹å‰åˆ†å¸ƒ"]
  CLT["ä¸­å¿ƒæ¥µé™å®šç†<br/>ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®æ™®éæ€§"]
  EM["EMç®—æ³• (Dempster 1977)<br/>æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«å­¦ç¿’"]
  VAE["VAE (Kingma 2013)<br/>æ·±å±¤ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
  BBB["Bayes by Backprop (2015)<br/>Bayesian Deep Learning"]
  DDPM["DDPM (Ho 2020)<br/>æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«"]
  AR["è‡ªå·±å›å¸°æ™®éæ€§ (Malach 2023)"]

  KO --> CLT
  BE --> EF
  CR --> EF
  EF --> EM
  CLT --> DDPM
  EM --> VAE
  VAE --> DDPM
  EF --> VAE
  BE --> BBB
  KO --> AR
```

> Progress: 95%

---

## ğŸ“ Z7. ã‚¨ãƒ”ãƒ­ãƒ¼ã‚°ï¼ˆ10åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨æ¬¡å›äºˆå‘Š

### 7.0 æ•°å¼â†”å®Ÿè£…å¯¾å¿œè¡¨

| æ•°å¼ | å®Ÿè£… | ã‚»ã‚¯ã‚·ãƒ§ãƒ³ |
|:-----|:-----|:-----------|
| $f(x;\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp(-\frac{(x-\mu)^2}{2\sigma^2})$ | `D.Normal(mu, sigma).log_prob(x)` | 5.1 Block 1 |
| $\hat{\mu} = \bar{x}$, $\hat{\sigma}^2 = \frac{1}{N}\sum(x_i-\bar{x})^2$ | `x.mean()`, `x.std(unbiased=False)` | 5.1 Block 1 |
| $\mathcal{H}(\boldsymbol{\pi}) \leq \log K$ | `D.Categorical(probs).entropy()` | 5.1 Block 1 |
| $D_{\mathrm{KL}}(p\|q)$ (Gaussian é–‰å½¢å¼) | `D.kl_divergence(p, q)` | 5.1 Block 1 |
| $\mathcal{N}(\mathbf{x}\mid\boldsymbol{\mu},\boldsymbol{\Sigma})$ | `D.MultivariateNormal(mu, cov).log_prob(x)` | 5.1 Block 1 |
| $\boldsymbol{\mu}_{1\mid 2}, \boldsymbol{\Sigma}_{1\mid 2}$ï¼ˆSchur è£œè¡Œåˆ—ï¼‰ | å°å‡ºå‚ç…§ Â§5.2 | 5.2 |
| $p(x\mid\boldsymbol{\eta}) = h(x)\exp(\boldsymbol{\eta}^\top T(x) - A(\boldsymbol{\eta}))$ | è‡ªç„¶å‹¾é…ãƒ»æœŸå¾…å€¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è§£æ | 5.3 |
| $p(\mathbf{x}\mid\theta) = \pi\mathcal{N}_1 + (1-\pi)\mathcal{N}_2$ | EM ç®—æ³•ã¸ã®æ©‹æ¸¡ã—ï¼ˆÂ§5.4ï¼‰ | 5.4 |
| $p(\theta\mid\mathbf{x}) \propto \theta^{a+h-1}(1-\theta)^{b+t-1}$ | Beta-Bernoulli å…±å½¹æ›´æ–° | 5.5a |
| $\lambda_N = \lambda_0 + N/\sigma^2$ï¼ˆç²¾åº¦åŠ æ³•æ€§ï¼‰ | ç²¾åº¦æ›´æ–°å…¬å¼ | 5.5b |
| $D_{\mathrm{KL}}(p\|q) \geq 0$ï¼ˆGibbs ä¸ç­‰å¼ï¼‰ | `D.kl_divergence` | 5.5a-KL |
| $\mathrm{Var}(\hat{\theta}) \geq 1/(nI(\theta))$ï¼ˆCR ä¸‹ç•Œï¼‰ | CR ä¸‹ç•Œå°å‡º Â§5.5c | 5.5c |
| $M_X(t) = e^{\mu t + \sigma^2 t^2/2}$ | CLT è¨¼æ˜ã‚¹ã‚±ãƒƒãƒ Â§5.6 | 5.6 |
| $\log p(\mathbf{x}) = \sum_t \log p(x_t\mid x_{<t})$ | `autoregressive_log_prob(logits, x)` | 5.7 Block 2 |
| Perplexity $= \exp(-\frac{1}{T}\log p)$ | `perplexity(logits, x)` | 5.7 Block 2 |
| $\ell_i = \log \mathcal{N}(x^{(i)}\mid\mu,\sigma)$ ä¸¦åˆ— | `gaussian_log_lik_kernel[grid](...)` | 5.8 Block 3 |

### 7.1 æœ¬è¬›ç¾©ã®æ ¸å¿ƒ â€” 3ã¤ã®æŒã¡å¸°ã‚Š

1. **ç¢ºç‡ã¯ã€Œã‚ã‹ã‚‰ãªã•ã€ã®è¨€èªã§ã‚ã‚‹ã€‚** ç¢ºç‡ç©ºé–“ $(\Omega, \mathcal{F}, P)$ ã¨ã„ã†å³å¯†ãªæ çµ„ã¿ã®ä¸Šã«ã€ç¢ºç‡å¤‰æ•°ãƒ»æœŸå¾…å€¤ãƒ»æ¡ä»¶ä»˜ãç¢ºç‡ãŒå®šç¾©ã•ã‚Œã‚‹ã€‚ã“ã®è¨€èªãªã—ã«ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯è¨˜è¿°ã§ããªã„ã€‚

2. **ãƒ™ã‚¤ã‚ºã®å®šç†ã¯ã€Œå­¦ç¿’ã€ã®æ•°å¼ã ã€‚** äº‹å‰åˆ†å¸ƒï¼ˆä¿¡å¿µï¼‰+ å°¤åº¦ï¼ˆãƒ‡ãƒ¼ã‚¿ï¼‰â†’ äº‹å¾Œåˆ†å¸ƒï¼ˆæ›´æ–°ã•ã‚ŒãŸä¿¡å¿µï¼‰ã€‚VAEã®ELBOã‚‚ã€LLMã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚‚ã€ã“ã®æ§‹é€ ã®å¤‰ç¨®ã ã€‚

3. **MLEã¯æ¡ä»¶ä»˜ãCategoricalåˆ†å¸ƒã®æœ€é©åŒ–ã«å¸°ç€ã™ã‚‹ã€‚** LLMã®å­¦ç¿’ã¯ã€å„æ™‚åˆ» $t$ ã§ $p(x_t \mid x_{<t})$ ã‚’Categoricalåˆ†å¸ƒã¨ã—ã¦MLEæ¨å®šã™ã‚‹ã“ã¨ã€‚æœ¬è¬›ç¾©ã§å­¦ã‚“ã å…¨ã¦ã®é“å…·ãŒã“ã“ã«é›†ç´„ã•ã‚Œã‚‹ã€‚

### 7.2 FAQ

<details><summary>Q: ãƒ™ã‚¤ã‚ºã¨é »åº¦ä¸»ç¾©ã€çµå±€ã©ã¡ã‚‰ãŒæ­£ã—ã„ã®ã‹ï¼Ÿ</summary>

ã€Œæ­£ã—ã•ã€ã®åŸºæº–ãŒç•°ãªã‚‹ã€‚é »åº¦ä¸»ç¾©ã¯ã€Œæ¨å®šé‡ã®é•·æœŸçš„æŒ¯ã‚‹èˆã„ã€ï¼ˆç¹°ã‚Šè¿”ã—å®Ÿé¨“ï¼‰ã§è©•ä¾¡ã—ã€ãƒ™ã‚¤ã‚ºã¯ã€Œç¾åœ¨ã®çŸ¥è­˜ã®ä¸‹ã§ã®ç¢ºä¿¡åº¦ã€ã§è©•ä¾¡ã™ã‚‹ã€‚MLã®æ–‡è„ˆã§ã¯:

- **MLE**ï¼ˆé »åº¦ä¸»ç¾©å¯„ã‚Šï¼‰: è¨ˆç®—ãŒç°¡å˜ã€æ¼¸è¿‘çš„ã«æœ€é©ã€å¤§ãƒ‡ãƒ¼ã‚¿å‘ã
- **ãƒ™ã‚¤ã‚ºæ¨è«–**: ä¸ç¢ºå®Ÿæ€§ã®å®šé‡åŒ–ãŒè‡ªç„¶ã€å°ãƒ‡ãƒ¼ã‚¿å‘ãã€äº‹å‰çŸ¥è­˜ã‚’æ´»ç”¨å¯èƒ½

å®Ÿç”¨ä¸Šã¯ã€Œã©ã¡ã‚‰ã‹ä¸€æ–¹ã€ã§ã¯ãªãã€å•é¡Œã«å¿œã˜ã¦ä½¿ã„åˆ†ã‘ã‚‹ã€‚VAEã¯å¤‰åˆ†ãƒ™ã‚¤ã‚ºã€LLMã®æå¤±é–¢æ•°ã¯MLEã ã€‚
</details>

<details><summary>Q: ãªãœæ­£è¦åˆ†å¸ƒãŒã“ã‚“ãªã«é »å‡ºã™ã‚‹ã®ã‹ï¼Ÿ</summary>

3ã¤ã®ç†ç”±ãŒã‚ã‚‹:

1. **ä¸­å¿ƒæ¥µé™å®šç†**: å¤šæ•°ã®ç‹¬ç«‹ãªå¾®å°åŠ¹æœã®å’Œã¯æ­£è¦åˆ†å¸ƒã«è¿‘ã¥ã
2. **æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼**: å¹³å‡ã¨åˆ†æ•£ã‚’å›ºå®šã—ãŸã¨ãã€ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æœ€å¤§ã®åˆ†å¸ƒãŒæ­£è¦åˆ†å¸ƒ
3. **è¨ˆç®—ã®éƒ½åˆ**: æ­£è¦åˆ†å¸ƒã®ç©ãƒ»å’Œãƒ»æ¡ä»¶ä»˜ããŒå…¨ã¦é–‰ã˜ãŸå½¢ã«ãªã‚‹

3ã¤ç›®ãŒå®Ÿç”¨ä¸Šæœ€ã‚‚é‡è¦ã ã€‚GANã®æ½œåœ¨ç©ºé–“ $\mathbf{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ ã‚„VAEã®äº‹å‰åˆ†å¸ƒã‚‚ã€è¨ˆç®—ã®å®¹æ˜“ã•ãŒé¸æŠã®ä¸»å› ã ã€‚
</details>

<details><summary>Q: æŒ‡æ•°å‹åˆ†å¸ƒæ—ã¯å®Ÿéš›ã«ã©ã“ã§ä½¿ã†ã®ã‹ï¼Ÿ</summary>

è‡³ã‚‹æ‰€ã§ã€‚

- **VAE**: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ›ã¯ã‚¬ã‚¦ã‚¹åˆ†å¸ƒï¼ˆæŒ‡æ•°å‹åˆ†å¸ƒæ—ï¼‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- **EBM**: $p(\mathbf{x}) = \frac{1}{Z}\exp(-E(\mathbf{x}))$ ã¯æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®ä¸€èˆ¬åŒ–
- **GLM**: ä¸€èˆ¬åŒ–ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”åˆ†å¸ƒã¯æŒ‡æ•°å‹åˆ†å¸ƒæ—
- **Softmax**: Categoricalåˆ†å¸ƒã¯æŒ‡æ•°å‹åˆ†å¸ƒæ—ã€‚LLMã®å‡ºåŠ›åˆ†å¸ƒãã®ã‚‚ã®

ç¬¬27å›ï¼ˆEBMï¼‰ã¨ç¬¬9å›ï¼ˆå¤‰åˆ†æ¨è«–ï¼‰ã§æœ¬æ ¼çš„ã«æ´»ç”¨ã™ã‚‹ã€‚
</details>

<details><summary>Q: CramÃ©r-Raoä¸‹ç•Œã‚’çŸ¥ã£ã¦ä½•ã®å½¹ã«ç«‹ã¤ã®ã‹ï¼Ÿ</summary>

ã€Œã“ã®æ¨å®šå•é¡Œã§ã“ã‚Œä»¥ä¸Šã®ç²¾åº¦ã¯åŸç†çš„ã«ä¸å¯èƒ½ã€ã¨ã„ã†é™ç•Œã‚’çŸ¥ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚

- ãƒ¢ãƒ‡ãƒ«è¨­è¨ˆ: æ¨å®šé‡ã®åˆ†æ•£ãŒCRä¸‹ç•Œã«è¿‘ã‘ã‚Œã°ã€ã“ã‚Œä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿ã¯ä¸è¦
- å®Ÿé¨“è¨ˆç”»: Fisheræƒ…å ±é‡ãŒå¤§ãã„å®Ÿé¨“æ¡ä»¶ã‚’é¸ã¶ã“ã¨ã§ã€å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§ç²¾å¯†ãªæ¨å®šãŒå¯èƒ½
- ç†è«–è§£æ: NNã®è¡¨ç¾åŠ›ã¨Fisheræƒ…å ±é‡ã®é–¢ä¿‚ã¯æ´»ç™ºãªç ”ç©¶åˆ†é‡
</details>

<details><summary>Q: ã€Œç¢ºç‡å¯†åº¦é–¢æ•°ã®å€¤ãŒ1ã‚’è¶…ãˆã‚‹ã€ã®ã¯é–“é•ã„ã§ã¯ï¼Ÿ</summary>

ã„ã„ãˆã€æ­£ã—ã„ã€‚PDFã¯ç¢ºç‡ã§ã¯ãªã„ã€‚ç¢ºç‡ã¯å¯†åº¦ã®**ç©åˆ†**ã§å¾—ã‚‰ã‚Œã‚‹:

$$
P(a \leq X \leq b) = \int_a^b f(x) dx
$$

$f(x)$ è‡ªä½“ã¯éè² ã§ã‚ã‚Œã°ã„ãã‚‰ã§ã‚‚å¤§ããã¦ã‚ˆã„ã€‚ä¾‹ãˆã° $\mathcal{N}(0, 0.01)$ ã®ãƒ”ãƒ¼ã‚¯ã¯ $f(0) = \frac{1}{\sqrt{2\pi \cdot 0.01}} \approx 3.99$ ã§ã€1ã‚’å¤§ããè¶…ãˆã‚‹ã€‚ç©åˆ†ã™ã‚‹ã¨å¿…ãš1ã«ãªã‚‹ãŒã€å¯†åº¦å€¤ãŒ1ã‚’è¶…ãˆã‚‹ã“ã¨è‡ªä½“ã¯ä½•ã®å•é¡Œã‚‚ãªã„ã€‚

</details>

<details><summary>Q: Multinomialåˆ†å¸ƒã¨Categoricalåˆ†å¸ƒã®é•ã„ã¯ï¼Ÿ</summary>

Categoricalåˆ†å¸ƒã¯ã€Œã‚µã‚¤ã‚³ãƒ­ã‚’1å›æŒ¯ã‚‹ã€: $x \in \{1, \ldots, K\}$, $P(x=k) = \pi_k$ã€‚

Multinomialåˆ†å¸ƒã¯ã€Œã‚µã‚¤ã‚³ãƒ­ã‚’ $n$ å›æŒ¯ã£ã¦ã€å„é¢ã®å‡ºãŸå›æ•°ã‚’è¨˜éŒ²ã™ã‚‹ã€: $(c_1, \ldots, c_K) \sim \text{Multi}(n, \boldsymbol{\pi})$, $\sum_k c_k = n$ã€‚

LLMã®æ–‡è„ˆã§ã¯:
- 1ãƒˆãƒ¼ã‚¯ãƒ³ã®äºˆæ¸¬ = Categoricalåˆ†å¸ƒ
- ãƒãƒƒãƒå†…ã®å…¨ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã®çµ±è¨ˆ = Multinomialåˆ†å¸ƒ

Categorical = Multinomial($n=1$, $\boldsymbol{\pi}$) ã ã€‚
</details>

<details><summary>Q: ã€Œå°¤åº¦ã€ã¨ã€Œç¢ºç‡ã€ã¯ä½•ãŒé•ã†ã®ã‹ï¼Ÿ</summary>

**ç¢ºç‡**: ãƒ‡ãƒ¼ã‚¿ $x$ ãŒå¯å¤‰ã§ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ãŒå›ºå®š â†’ $P(X=x \mid \theta)$

**å°¤åº¦**: ãƒ‡ãƒ¼ã‚¿ $x$ ãŒå›ºå®šã§ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ãŒå¯å¤‰ â†’ $L(\theta; x) = P(X=x \mid \theta)$

æ•°å¼ã¯å…¨ãåŒã˜ã€‚è¦–ç‚¹ã®é•ã„ã ã‘ã ã€‚ç¢ºç‡ã¨ã—ã¦è¦‹ã‚‹ã¨ $\sum_x P(x \mid \theta) = 1$ï¼ˆãƒ‡ãƒ¼ã‚¿ã«é–¢ã—ã¦æ­£è¦åŒ–ï¼‰ã€‚å°¤åº¦ã¨ã—ã¦è¦‹ã‚‹ã¨ $\int L(\theta; x) d\theta$ ã¯ä¸€èˆ¬ã«1ã«ãªã‚‰ãªã„ã€‚

MLEã¯ã€Œã“ã®ãƒ‡ãƒ¼ã‚¿ãŒæœ€ã‚‚ã‚ˆãç”Ÿæˆã•ã‚Œã‚‹ã‚ˆã†ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ã‚’æ¢ã™ â†’ å°¤åº¦é–¢æ•°ã®æœ€å¤§åŒ–ã€‚

</details>

<details><summary>Q: æ¡ä»¶ä»˜ãæœŸå¾…å€¤ E[X|Y] ã¯ãªãœç¢ºç‡å¤‰æ•°ãªã®ã‹ï¼Ÿ</summary>

$\mathbb{E}[X \mid Y=y]$ ã¯ $y$ ã®é–¢æ•°ã¨ã—ã¦è¨ˆç®—ã§ãã‚‹ã€‚ä¾‹ãˆã° $(X,Y) \sim \mathcal{N}$ ãªã‚‰ $\mathbb{E}[X \mid Y=y] = \mu_X + \rho \frac{\sigma_X}{\sigma_Y}(y - \mu_Y)$ï¼ˆç·šå½¢ï¼‰ã€‚

$Y$ ãŒç¢ºç‡å¤‰æ•°ã ã‹ã‚‰ $\mathbb{E}[X \mid Y]$ ã‚‚ç¢ºç‡å¤‰æ•°ã«ãªã‚‹ã€‚é‡è¦ãªæ€§è³ª: **ç¹°ã‚Šè¿”ã—æœŸå¾…å€¤ã®æ³•å‰‡**

$$
\mathbb{E}[\mathbb{E}[X \mid Y]] = \mathbb{E}[X]
$$

ã“ã‚Œã¯ELBOã®å°å‡ºã§ã‚‚ä½¿ã‚ã‚Œã‚‹: $\log p(\mathbf{x}) = \mathbb{E}_{q(\mathbf{z})}[\log p(\mathbf{x}, \mathbf{z})/q(\mathbf{z})] + D_{\mathrm{KL}}(q \| p)$ã€‚

</details>

<details><summary>Q: ã“ã®ç¢ºç‡è«–ã®çŸ¥è­˜ã¯ç¬¬5å›ï¼ˆæ¸¬åº¦è«–ï¼‰ã§ã©ã†æ‹¡å¼µã•ã‚Œã‚‹ã®ã‹ï¼Ÿ</summary>

æœ¬è¬›ç¾©ã§ã¯ã€Œç¢ºç‡å¯†åº¦é–¢æ•° $f(x)$ ãŒå­˜åœ¨ã™ã‚‹ã€ã¨æš—é»™ã«ä»®å®šã—ãŸã€‚ã ãŒ:

- é›¢æ•£ã¨é€£ç¶šãŒæ··ã˜ã£ãŸåˆ†å¸ƒã¯ï¼Ÿ
- $\mathbb{R}^d$ ä¸Šã®å…¨ã¦ã®éƒ¨åˆ†é›†åˆã«ç¢ºç‡ã‚’å®šç¾©ã§ãã‚‹ã‹ï¼Ÿ
- ã€Œã»ã¨ã‚“ã©ç¢ºå®Ÿã«ã€ã¨ã¯ä½•ã‹ï¼Ÿ

ç¬¬5å›ã§ã¯æ¸¬åº¦è«–ã®è¨€è‘‰ã§ $f(x) = \frac{dP}{d\lambda}$ ï¼ˆRadon-Nikodymå°é–¢æ•°ï¼‰ã¨ã—ã¦å¯†åº¦é–¢æ•°ã‚’å³å¯†ã«å®šç¾©ã™ã‚‹ã€‚ç¢ºç‡éç¨‹ï¼ˆMarkové€£é–ã€Browné‹å‹•ï¼‰ã‚‚å°å…¥ã—ã€æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®SDEå®šå¼åŒ–ã¸ã®æ©‹æ¸¡ã—ã‚’è¡Œã†ã€‚
</details>

### 7.3 ç¢ºç‡è«–ã§ã‚ˆãã‚ã‚‹ã€Œç½ ã€

<details><summary>ç½ 6: å¤šæ¬¡å…ƒGaussianã®ã€Œã»ã¨ã‚“ã©ã®ç¢ºç‡è³ªé‡ã€ã¯æ®»ã«ã‚ã‚‹</summary>

1æ¬¡å…ƒã§ã¯ Gaussian ã®ç¢ºç‡è³ªé‡ã¯å¹³å‡ä»˜è¿‘ã«é›†ä¸­ã™ã‚‹ï¼ˆ$\pm 2\sigma$ ã«95%ï¼‰ã€‚

$d$ æ¬¡å…ƒã§ã¯å…¨ãé•ã†ã€‚$\mathbf{x} \sim \mathcal{N}(\mathbf{0}, I_d)$ ã®ãƒãƒ«ãƒ  $\|\mathbf{x}\|$ ã¯:

$$
\mathbb{E}[\|\mathbf{x}\|^2] = d, \quad \text{Var}(\|\mathbf{x}\|) = O(1)
$$

ã¤ã¾ã‚Š $\|\mathbf{x}\| \approx \sqrt{d}$ ã«é›†ä¸­ã™ã‚‹ï¼ˆæ¬¡å…ƒã®å‘ªã„ ã®ç¾ã‚Œï¼‰ã€‚$d=1000$ ã§ã¯å…¨ã‚µãƒ³ãƒ—ãƒ«ãŒåŠå¾„ $\approx 31.6$ ã®è–„ã„çƒæ®»ä¸Šã«ã‚ã‚‹ã€‚

VAEã®æ½œåœ¨ç©ºé–“ $\mathbf{z} \sim \mathcal{N}(\mathbf{0}, I_{100})$ ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã¨ã€$\|\mathbf{z}\| \approx 10$ ã®çƒæ®»ã‹ã‚‰ã—ã‹ã‚µãƒ³ãƒ—ãƒ«ãŒæ¥ãªã„ã€‚ã“ã‚ŒãŒVAEã®ã€Œposterior collapseã€å•é¡Œã®ä¸€å› ã ã€‚

</details>



<details><summary>ç½ 1: P(A|B) â‰  P(B|A) â€” æ¡ä»¶ã®é€†è»¢</summary>

ã€Œé›¨ã®ã¨ãå‚˜ã‚’æŒã¤ç¢ºç‡90%ã€ã¨ã€Œå‚˜ã‚’æŒã£ã¦ã„ã‚‹ã¨ãé›¨ã®ç¢ºç‡ã€ã¯å…¨ãé•ã†ã€‚ãƒ™ã‚¤ã‚ºã®å®šç†ãªã—ã«ã“ã®2ã¤ã‚’æ··åŒã™ã‚‹ã®ãŒã€Œæ¤œå¯Ÿå®˜ã®èª¤è¬¬ã€ã ã€‚DNAé‘‘å®šã§ã€Œä¸€è‡´ã—ãŸ = çŠ¯äººã€ã¨çµè«–ã™ã‚‹ã®ã¯ $P(\text{ä¸€è‡´} \mid \text{çŠ¯äºº})$ ã¨ $P(\text{çŠ¯äºº} \mid \text{ä¸€è‡´})$ ã®æ··åŒã€‚
</details>

<details><summary>ç½ 2: ç‹¬ç«‹ã¨ç„¡ç›¸é–¢ã¯é•ã†</summary>

ç„¡ç›¸é–¢: $\text{Cov}(X, Y) = 0$ï¼ˆç·šå½¢é–¢ä¿‚ãŒãªã„ï¼‰
ç‹¬ç«‹: $P(X, Y) = P(X)P(Y)$ï¼ˆã‚ã‚‰ã‚†ã‚‹é–¢ä¿‚ãŒãªã„ï¼‰

ç‹¬ç«‹ â†’ ç„¡ç›¸é–¢ã ãŒã€é€†ã¯æˆã‚Šç«‹ãŸãªã„ã€‚$X \sim \mathcal{N}(0,1)$, $Y = X^2$ ã¯ç„¡ç›¸é–¢ã ãŒç‹¬ç«‹ã§ã¯ãªã„ã€‚
</details>

<details><summary>ç½ 3: åˆ†æ•£0ã§ã‚‚åˆ†å¸ƒã¯æ±ºã¾ã‚‰ãªã„</summary>

CramÃ©r-Raoä¸‹ç•Œ $\text{Var} \geq 1/(nI)$ ã¯ä¸åæ¨å®šé‡ã«ã—ã‹é©ç”¨ã•ã‚Œãªã„ã€‚ãƒã‚¤ã‚¢ã‚¹ã®ã‚ã‚‹æ¨å®šé‡ã¯CRä¸‹ç•Œã‚’ä¸‹å›ã‚‹ã“ã¨ãŒã‚ã‚‹ï¼ˆJames-Steinã®ç¸®å°æ¨å®šé‡ï¼‰ã€‚ã€Œãƒã‚¤ã‚¢ã‚¹ã‚’è¨±å®¹ã™ã‚‹ä»£ã‚ã‚Šã«MSEã‚’ä¸‹ã’ã‚‹ã€ã®ã¯ã€MLã§ã¯æ­£å‰‡åŒ–ã¨ã—ã¦æ—¥å¸¸çš„ã«è¡Œã‚ã‚Œã‚‹ã€‚
</details>

<details><summary>ç½ 4: MLEã¯å¸¸ã«æœ€è‰¯ã§ã¯ãªã„</summary>

å°ã‚µãƒ³ãƒ—ãƒ«ã§ã¯MLEã®ãƒã‚¤ã‚¢ã‚¹ãŒå•é¡Œã«ãªã‚‹ã€‚åˆ†æ•£æ¨å®šé‡ $\hat{\sigma}^2_{\text{MLE}} = \frac{1}{N}\sum(x_i - \bar{x})^2$ ã¯ $\sigma^2$ ã‚’éå°è©•ä¾¡ã™ã‚‹ã€‚James-Steinã®å®šç†ãŒç¤ºã™ã®ã¯ã€3æ¬¡å…ƒä»¥ä¸Šã§ã¯MLEãŒã€Œè¨±å®¹å¯èƒ½ã§ãªã„ã€ï¼ˆadmissible ã§ãªã„ï¼‰ã¨ã„ã†è¡æ’ƒçš„äº‹å®Ÿã ã€‚
</details>

<details><summary>ç½ 5: äº‹å‰åˆ†å¸ƒãŒã€Œä¸»è¦³çš„ã€ã¯æ¬ ç‚¹ã‹ï¼Ÿ</summary>

é »åº¦ä¸»ç¾©è€…ã¯ãƒ™ã‚¤ã‚ºã®ã€Œä¸»è¦³æ€§ã€ã‚’æ‰¹åˆ¤ã™ã‚‹ã€‚ã ãŒ:
- ã€Œäº‹å‰åˆ†å¸ƒãªã—ã€ã¯ã€Œä¸€æ§˜äº‹å‰åˆ†å¸ƒã€ã¨ç­‰ä¾¡ â€” ã“ã‚Œã‚‚ä¸»è¦³çš„
- å¼±æƒ…å ±äº‹å‰åˆ†å¸ƒã¯ã€ç‰©ç†çš„åˆ¶ç´„ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç¯„å›²ç­‰ï¼‰ã‚’è‡ªç„¶ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
- ãƒ‡ãƒ¼ã‚¿ãŒååˆ†ã‚ã‚Œã°äº‹å‰åˆ†å¸ƒã®å½±éŸ¿ã¯æ¶ˆãˆã‚‹ï¼ˆäº‹å¾Œä¸€è‡´æ€§ï¼‰

å®Ÿç”¨çš„ã«ã¯ã€äº‹å‰åˆ†å¸ƒã¯ã€Œæ­£å‰‡åŒ–ã®ä¸€å½¢æ…‹ã€ã¨å‰²ã‚Šåˆ‡ã£ã¦ã‚ˆã„ã€‚
</details>

### 7.4 æ¬¡å›äºˆå‘Š â€” ç¬¬5å›: æ¸¬åº¦è«–çš„ç¢ºç‡è«–ãƒ»ç¢ºç‡éç¨‹å…¥é–€

ç¬¬4å›ã§ç¢ºç‡åˆ†å¸ƒã‚’ã€Œä½¿ãˆã‚‹ã€ã‚ˆã†ã«ãªã£ãŸã€‚ã ãŒã€ä»¥ä¸‹ã®å•ã„ã«ç­”ãˆã‚‰ã‚Œã‚‹ã ã‚ã†ã‹:

- ã€Œç¢ºç‡å¯†åº¦é–¢æ•°ã€ã¨ã¯å³å¯†ã«ä½•ã‹ï¼Ÿ ãªãœç‚¹ $x$ ã§ã® $f(x)$ ã¯ç¢ºç‡ã§ã¯ãªã„ã®ã‹ï¼Ÿ
- é›¢æ•£ã¨é€£ç¶šãŒæ··ã˜ã£ãŸåˆ†å¸ƒã‚’ã©ã†æ‰±ã†ã‹ï¼Ÿ
- ã€Œã»ã¨ã‚“ã©ç¢ºå®Ÿã«åæŸã™ã‚‹ã€ã®ã€Œã»ã¨ã‚“ã©ã€ã¨ã¯ï¼Ÿ
- Browné‹å‹•ã¯ãªãœå¾®åˆ†ä¸å¯èƒ½ãªã®ã‹ï¼Ÿ
- æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®forward processã‚’è¨˜è¿°ã™ã‚‹SDEã¨ã¯ä½•ã‹ï¼Ÿ

ç¬¬5å›ã§ã¯**æ¸¬åº¦è«–**ã®è¨€è‘‰ã§ç¢ºç‡è«–ã‚’å†æ§‹ç¯‰ã™ã‚‹ã€‚Lebesgueç©åˆ†ã€Radon-Nikodymå°é–¢æ•°ã€ç¢ºç‡éç¨‹ã€Markové€£é–ã€Browné‹å‹• â€” æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®æ•°å­¦çš„åŸºç›¤ãŒã“ã“ã«åŸ‹ã¾ã£ã¦ã„ã‚‹ã€‚

ãã—ã¦ `%timeit` ãŒåˆç™»å ´ã™ã‚‹ã€‚Monte Carloç©åˆ†ã®è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’æ¸¬ã‚Šå§‹ã‚ã‚‹ã¨ã€Pythonã®ã€Œé…ã•ã€ãŒå°‘ã—ãšã¤è¦‹ãˆã¦ãã‚‹......ã€‚

> **Note:** **é€²æ—: 100% å®Œäº†** ç¬¬4å›: ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦ â€” å…¨ã‚¾ãƒ¼ãƒ³ã‚¯ãƒªã‚¢ã€‚ãŠç–²ã‚Œã•ã¾ã§ã—ãŸã€‚ç¢ºç‡ã®è¨€èªã‚’æ‰‹ã«å…¥ã‚ŒãŸä»Šã€ç¬¬5å›ã§æ¸¬åº¦è«–ã¨ã„ã†ã€Œç¢ºç‡ã®æ–‡æ³•ã€ã‚’å³å¯†ã«å®šç¾©ã™ã‚‹æ—…ã«å‡ºã‚ˆã†ã€‚

---


**ç¬¬5å›ã®æ ¸å¿ƒæ¦‚å¿µãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼**:

```mermaid
flowchart LR
  P4["ç¬¬4å›\nç¢ºç‡åˆ†å¸ƒ\nMLE/ãƒ™ã‚¤ã‚º\nGaussian"] --> P5
  P5["ç¬¬5å›\næ¸¬åº¦è«–\nç¢ºç‡éç¨‹\nSDE"] --> P8
  P8["ç¬¬8å›\nVAE\nELBO\nå¤‰åˆ†æ¨è«–"] --> P15
  P15["ç¬¬15å›\nDiffusion\nSDEé€†å•é¡Œ\nScore matching"]
```

ç‰¹ã«ã€ŒRadon-Nikodymå°é–¢æ•°ã€ã¯ Score matching ã®æ•°å­¦çš„åŸºç¤ã ã€‚ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_x \log p(x)$ ã¯ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®å‹¾é…ã‚’è¡¨ã—ã€æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ã‚ºé™¤å»ãƒ—ãƒ­ã‚»ã‚¹ã¨ç›´æ¥å¯¾å¿œã™ã‚‹ã€‚

### 7.5 ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **ç¾å®Ÿã®ãƒ‡ãƒ¼ã‚¿ã¯æ­£è¦åˆ†å¸ƒã«å¾“ã‚ãªã„ã€‚ãã‚Œã§ã‚‚ä»®å®šã™ã‚‹"æœ¬å½“ã®ç†ç”±"ã¯ä½•ã‹ï¼Ÿ**

CLTãŒã€Œå¤šæ•°ã®ç‹¬ç«‹å¾®å°åŠ¹æœã®å’Œâ†’æ­£è¦åˆ†å¸ƒã€ã‚’ä¿è¨¼ã™ã‚‹ã‹ã‚‰ï¼Ÿ ãã‚Œã¯ç†ç”±ã®ä¸€ã¤ã ã€‚ã ãŒæœ¬è³ªã¯ã‚‚ã£ã¨æ·±ã„ã€‚

- ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã¯**æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åˆ†å¸ƒ**ã ã€‚å¹³å‡ã¨åˆ†æ•£ã ã‘ã‚’çŸ¥ã£ã¦ã„ã‚‹ã¨ãã€ãã‚Œä»¥ä¸Šã®ä»®å®šã‚’ç½®ã‹ãªã„ã€Œæœ€ã‚‚æƒ…å ±é‡ã®å°‘ãªã„ã€åˆ†å¸ƒãŒã‚¬ã‚¦ã‚¹ã 
- ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®æ¼”ç®—ã¯**é–‰ã˜ã¦ã„ã‚‹**ã€‚å’Œãƒ»æ¡ä»¶ä»˜ããƒ»å‘¨è¾ºãŒå…¨ã¦ã‚¬ã‚¦ã‚¹ã®ã¾ã¾ã€‚ã“ã‚Œã¯è¨ˆç®—ä¸Šã®å¥‡è·¡ã¨è¨€ã£ã¦ã‚ˆã„
- ãã—ã¦ã€æ­£è¦åˆ†å¸ƒãŒã€Œé–“é•ã£ã¦ã„ã‚‹ã€ã“ã¨ã¯**ã‚ã‹ã£ã¦ã„ã‚‹**ä¸Šã§ä½¿ã†ã€‚é‡è¦ãªã®ã¯ã€Œã©ã®ç¨‹åº¦é–“é•ã£ã¦ã„ã‚‹ã‹ã€ã‚’å®šé‡åŒ–ã™ã‚‹ã“ã¨ â€” KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ï¼ˆç¬¬6å›ï¼‰ãŒãã®é“å…·ã 

<details><summary>ãƒ™ã‚¤ã‚ºè„³ä»®èª¬ â€” è„³ã¯ç¢ºç‡è¨ˆç®—æ©Ÿã‹ï¼Ÿ</summary>

èªçŸ¥ç§‘å­¦ã«ã¯ã€Œè„³ã¯ãƒ™ã‚¤ã‚ºæ¨è«–ã‚’è¡Œã£ã¦ã„ã‚‹ã€ã¨ã„ã†ä»®èª¬ãŒã‚ã‚‹ã€‚æ„Ÿè¦šå…¥åŠ›ï¼ˆå°¤åº¦ï¼‰ã¨çµŒé¨“ï¼ˆäº‹å‰åˆ†å¸ƒï¼‰ã‚’çµ„ã¿åˆã‚ã›ã¦ä¸–ç•Œã®çŠ¶æ…‹ï¼ˆäº‹å¾Œåˆ†å¸ƒï¼‰ã‚’æ¨å®šã™ã‚‹ã€‚

éŒ¯è¦–ç¾è±¡ã¯ã€å¼·ã„äº‹å‰åˆ†å¸ƒãŒå¼±ã„å°¤åº¦ã‚’ä¸Šæ›¸ãã™ã‚‹ä¾‹ã¨ã—ã¦è§£é‡ˆã•ã‚Œã‚‹ã€‚VAEã®ãƒ‡ã‚³ãƒ¼ãƒ€ãŒã€Œã¼ã‚„ã‘ãŸã€ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã®ã¯ã€äº‹å‰åˆ†å¸ƒ $p(\mathbf{z}) = \mathcal{N}(\mathbf{0}, \mathbf{I})$ ãŒéåº¦ã«æ»‘ã‚‰ã‹ãªæ½œåœ¨ç©ºé–“ã‚’å¼·åˆ¶ã™ã‚‹ãŸã‚ â€” ã‚ã‚‹æ„å‘³ã€è„³ã®éŒ¯è¦–ã¨åŒã˜æ§‹é€ ã ã€‚

ã€Œæ­£è¦åˆ†å¸ƒã‚’ä»®å®šã™ã‚‹ã€ã®ã¯ã€è„³ãŒã€Œä¸–ç•Œã¯æ»‘ã‚‰ã‹ã ã€ã¨ä»®å®šã™ã‚‹ã®ã¨åŒã˜ã‹ã‚‚ã—ã‚Œãªã„ã€‚
</details>

è€ƒãˆã¦ã¿ã‚ˆã†:

- **LLMã®å‡ºåŠ›åˆ†å¸ƒã¯Categoricalã€‚** æ­£è¦åˆ†å¸ƒã§ã¯ãªã„ã€‚ã ãŒCategoricalåˆ†å¸ƒã®è‡ªç„¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆlogitï¼‰ã¯é€£ç¶šå€¤ã§ã€ãã®ç©ºé–“ã§ã¯æ­£è¦åˆ†å¸ƒçš„ãªä»®å®šãŒä½¿ã‚ã‚Œã‚‹
- **æ¬¡å…ƒã®å‘ªã„**: 100æ¬¡å…ƒã®ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®ã‚µãƒ³ãƒ—ãƒ«ã¯ã€ã»ã¼ç¢ºå®Ÿã«åŸç‚¹ã‹ã‚‰ $\sqrt{100} = 10$ ã®è·é›¢ã«ã‚ã‚‹ã€‚ã€Œé«˜æ¬¡å…ƒã®ã‚¬ã‚¦ã‚¹ã¯çƒæ®»ã«é›†ä¸­ã™ã‚‹ã€â€” ã“ã‚ŒãŒæ­£è¦åˆ†å¸ƒã®ç›´æ„ŸãŒå´©å£Šã™ã‚‹ç¬é–“ã 
- **æ­£è¦åˆ†å¸ƒã¯"æœ€ã‚‚ç„¡çŸ¥ãª"åˆ†å¸ƒ**: æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åŸç†ã«ã‚ˆã‚Šã€å¹³å‡ã¨åˆ†æ•£ã—ã‹çŸ¥ã‚‰ãªã„ã¨ãã€ä½™è¨ˆãªä»®å®šã‚’æœ€ã‚‚å°‘ãªãã™ã‚‹åˆ†å¸ƒãŒã‚¬ã‚¦ã‚¹ã€‚ã€ŒçŸ¥ã‚‰ãªã„ã“ã¨ã‚’æ­£ç›´ã«èªã‚ã‚‹åˆ†å¸ƒã€ã¨ã‚‚è¨€ãˆã‚‹


---

### 7.6 æœ€æ–°ç ”ç©¶ (2020-2026)

#### 6.9.1 Fisheræƒ…å ±é‡ã®ç†è«–çš„é€²å±•

Fisheræƒ…å ±é‡ã¯çµ±è¨ˆçš„æ¨æ¸¬ã®åŸºç¤ã§ã‚ã‚Šã€æœ€è¿‘ã®ç ”ç©¶ã¯ãã®å¿œç”¨ç¯„å›²ã‚’æ‹¡å¤§ã—ã¦ã„ã‚‹ã€‚

**æœŸå¾…Fisheræƒ…å ± vs è¦³æ¸¬Fisheræƒ…å ±**

Fisheræƒ…å ±é‡ã«ã¯2ã¤ã®è¡¨ç¾ãŒã‚ã‚‹:

$$
I(\theta) = \mathbb{E}\left[\left(\frac{\partial \log p(X; \theta)}{\partial \theta}\right)^2\right] = -\mathbb{E}\left[\frac{\partial^2 \log p(X; \theta)}{\partial \theta^2}\right]
$$

å‰è€…ã¯ã€ŒæœŸå¾…ã€ã€å¾Œè€…ã¯ã€Œè¦³æ¸¬ã€ã¨å‘¼ã°ã‚Œã‚‹ã€‚2013å¹´ã®arXivè«–æ–‡[^13]ã¯ã€**æœŸå¾…Fisheræƒ…å ±ã‚’ä½¿ã£ãŸä¿¡é ¼åŒºé–“ãŒè¦³æ¸¬Fisheræƒ…å ±ã‚’ä½¿ã£ãŸå ´åˆã‚ˆã‚Šå¹³å‡äºŒä¹—èª¤å·®ã®æ„å‘³ã§ç²¾åº¦ãŒé«˜ã„**ã“ã¨ã‚’è¨¼æ˜ã—ãŸã€‚2021å¹´ã®ç¶šç·¨[^14]ã§ã¯ã€ã“ã®çµæœã‚’åŒºé–“æ¨å®šã®ç›¸å¯¾æ€§èƒ½è©•ä¾¡ã«æ‹¡å¼µã—ã¦ã„ã‚‹ã€‚

**æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«ã¸ã®æ‹¡å¼µ**

2024å¹´ã®ç ”ç©¶[^15]ã¯ã€æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã™ã‚‹Fisheræƒ…å ±é‡ã®æ˜ç¤ºçš„å®šç¾©ã‚’å¯èƒ½ã«ã™ã‚‹æ–°ã—ã„æœ€å°¤æ¨å®šãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ãŸã€‚å¾“æ¥ã€æ½œåœ¨å¤‰æ•° $\mathbf{z}$ ã‚’ç©åˆ†æ¶ˆå»ã—ãŸå‘¨è¾ºå°¤åº¦ $p(\mathbf{x}; \theta) = \int p(\mathbf{x}, \mathbf{z}; \theta) d\mathbf{z}$ ã§ã¯Fisheræƒ…å ±é‡ã®è¨ˆç®—ãŒå›°é›£ã ã£ãŸã€‚ã“ã®ç ”ç©¶ã¯ã€å¤‰åˆ†è¿‘ä¼¼ã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§åŠ¹ç‡çš„ãªæ¨å®šã‚’å®Ÿç¾ã—ã¦ã„ã‚‹ã€‚

**ãƒ†ãƒ³ã‚½ãƒ«ãƒ¢ãƒ‡ãƒ«ã®Fisheræƒ…å ±**

2025å¹´ã®æœ€æ–°è«–æ–‡[^16]ã¯ã€ãƒã‚¢ã‚½ãƒ³Canonical Polyadic (CP) ãƒ†ãƒ³ã‚½ãƒ«åˆ†è§£ã®Fisheræƒ…å ±é‡ã‚’å°å‡ºã—ãŸã€‚3æ¬¡å…ƒä»¥ä¸Šã®ãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆä¾‹: æ™‚é–“Ã—ç©ºé–“Ã—å‘¨æ³¢æ•°ï¼‰ã®çµ±è¨ˆçš„æ€§è³ªã‚’å®šé‡åŒ–ã™ã‚‹ã“ã¨ã§ã€CramÃ©r-Raoä¸‹ç•Œã«åŸºã¥ãæ¨å®šé‡ã®è©•ä¾¡ãŒå¯èƒ½ã«ãªã‚‹ã€‚


#### 6.9.2 æ¸¬åº¦è«–çš„ç¢ºç‡è«–ã®å®Ÿç”¨åŒ–

æ¸¬åº¦è«–ã¯ç¢ºç‡è«–ã®å³å¯†ãªåŸºç¤ã‚’ä¸ãˆã‚‹ãŒã€ã€ŒæŠ½è±¡çš„ã™ãã¦å®Ÿç”¨çš„ã§ãªã„ã€ã¨ã„ã†èª¤è§£ãŒã‚ã‚‹ã€‚æœ€è¿‘ã®ç ”ç©¶ã¯ã€æ¸¬åº¦è«–çš„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®å®Ÿç”¨çš„å¿œç”¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚

**Tayloræ¸¬åº¦ã¨ç¢ºç‡éç¨‹**

2025å¹´ã®arXivè«–æ–‡[^17]ã¯ã€Tayloræ¸¬åº¦ã¨ã„ã†æ¦‚å¿µã‚’å°å…¥ã—ã€Browné‹å‹•ã€ãƒãƒ«ãƒãƒ³ã‚²ãƒ¼ãƒ«ã€ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ã€æ™‚ç³»åˆ—ãƒ¢ãƒ‡ãƒ«ã‚’çµ±ä¸€çš„ã«æ‰±ã†æ çµ„ã¿ã‚’ææ¡ˆã—ãŸã€‚ã“ã‚Œã¯Taylorå±•é–‹ã®ä¸€èˆ¬åŒ–ã§ã‚ã‚Šã€ç¢ºç‡éç¨‹ã®å±€æ‰€çš„æ€§è³ªã‚’æ‰ãˆã‚‹ã€‚

**é€£ç¶šæ™‚é–“ç¢ºç‡éç¨‹ã®Metric Temporal Logic**

2023å¹´ã®ç ”ç©¶[^18]ã¯ã€é€£ç¶šæ™‚é–“ç¢ºç‡éç¨‹ãŒMetric Temporal Logic (MTL) ã®è«–ç†å¼ã‚’æº€ãŸã™ã‹ã©ã†ã‹ã®å¯æ¸¬æ€§ã‚’ç¢ºç«‹ã—ãŸã€‚ã“ã‚Œã¯å½¢å¼æ¤œè¨¼ã¨ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã‚’æ©‹æ¸¡ã—ã™ã‚‹æˆæœã§ã€è‡ªå‹•é‹è»¢è»Šã®å®‰å…¨æ€§æ¤œè¨¼ãªã©ã«å¿œç”¨ã•ã‚Œã¦ã„ã‚‹ã€‚

**ç¢ºç‡ç©ºé–“ã®æ§‹æˆ**

arXivè«–æ–‡[^19]ã¯ã€æ±ºå®šè«–çš„éç¨‹ã‹ã‚‰å‡ºç™ºã—ã¦æŠ½è±¡çš„ç¢ºç‡ç©ºé–“ã‚’æ§‹æˆã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã—ãŸã€‚ã“ã‚Œã¯ã€Œç¢ºç‡çš„ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¯æ±ºå®šè«–çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã‚ã‚‹ã€ã¨ã„ã†å“²å­¦çš„æ´å¯Ÿã‚’å½¢å¼åŒ–ã—ã¦ã„ã‚‹ã€‚

#### 6.9.3 æƒ…å ±ç†è«–ã®æœ€æ–°å±•é–‹

KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¨ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã¯æ©Ÿæ¢°å­¦ç¿’ã®ä¸­å¿ƒæ¦‚å¿µã ãŒã€ãã®ç†è«–ã¯ã¾ã ç™ºå±•é€”ä¸Šã ã€‚

**Î±-ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¨ãƒ™ã‚¤ã‚ºæœ€é©åŒ–**

2024å¹´ã®è«–æ–‡[^20]ã¯ã€KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã‚’ä¸€èˆ¬åŒ–ã—ãŸÎ±-ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã«åŸºã¥ãæ–°ã—ã„ãƒ™ã‚¤ã‚ºæœ€é©åŒ–æ‰‹æ³•ã€ŒAlpha Entropy Search (AES)ã€ã‚’ææ¡ˆã—ãŸã€‚Î±-ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¯:

$$
D_\alpha(p \| q) = \frac{1}{\alpha(\alpha-1)} \left( \int p(x)^\alpha q(x)^{1-\alpha} dx - 1 \right)
$$

$\alpha \to 1$ ã§KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã«åæŸã™ã‚‹ã€‚AESã¯ç²å¾—é–¢æ•°ã¨ã—ã¦ã€æ¬¡ã®è©•ä¾¡ç‚¹ã§ã®ç›®çš„é–¢æ•°å€¤ã¨å¤§åŸŸçš„æœ€å¤§å€¤ã®ã€Œä¾å­˜åº¦ã€ã‚’æœ€å¤§åŒ–ã™ã‚‹ã€‚ã“ã®ä¾å­˜åº¦ã‚’Î±-ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã§æ¸¬ã‚‹ã“ã¨ã§ã€KLãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã‚ˆã‚Šæ¢ç´¢ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹ã‚’æŸ”è»Ÿã«åˆ¶å¾¡ã§ãã‚‹ã€‚

**Jensen-Shannonã¨KLã®é–¢ä¿‚**

2025å¹´ã®è«–æ–‡[^21]ã¯ã€Jensen-Shannon (JS) ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¨KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®æœ€é©ãªä¸‹ç•Œã‚’ç¢ºç«‹ã—ãŸ:

$$
\text{JS}(p \| q) = \frac{1}{2} D_{\text{KL}}(p \| m) + \frac{1}{2} D_{\text{KL}}(q \| m), \quad m = \frac{p + q}{2}
$$

JSãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¯GANã®ç›®çš„é–¢æ•°ã¨ã—ã¦çŸ¥ã‚‰ã‚Œã¦ã„ã‚‹ãŒã€KLã¨ã®å®šé‡çš„é–¢ä¿‚ã¯é•·å¹´ä¸æ˜ã ã£ãŸã€‚ã“ã®æˆæœã«ã‚ˆã‚Šã€GANã®åæŸæ€§ç†è«–ãŒæ”¹å–„ã•ã‚ŒãŸã€‚

**å¹¾ä½•å­¦çš„æƒ…å ±ç†è«– (GAIT)**

å¾“æ¥ã®KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¯ç¢ºç‡åˆ†å¸ƒã‚’ã€Œç‚¹ã€ã¨ã—ã¦æ‰±ã„ã€ç©ºé–“ã®å¹¾ä½•ã‚’ç„¡è¦–ã™ã‚‹ã€‚2019å¹´ã®è«–æ–‡[^22]ã¯ã€ç¢ºç‡åˆ†å¸ƒã®å°ï¼ˆsupportï¼‰ã®å¹¾ä½•å­¦çš„æ§‹é€ ã‚’è€ƒæ…®ã—ãŸæ–°ã—ã„ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã€ŒGeometric Informationã€ã‚’ææ¡ˆã—ãŸã€‚ã“ã‚Œã¯æœ€é©è¼¸é€ç†è«–ã¨KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã‚’çµ±åˆã™ã‚‹è©¦ã¿ã ã€‚


#### 6.9.4 çµ±è¨ˆçš„æ¨æ¸¬ã®æ–°ç†è«–

**Extended Likelihoodã¨ãƒ©ãƒ³ãƒ€ãƒ æœªçŸ¥é‡**

2023å¹´ã®è«–æ–‡[^23]ã¯ã€å¾“æ¥ã®å°¤åº¦ç†è«–ã‚’ã€Œå›ºå®šã•ã‚ŒãŸæœªçŸ¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ã‹ã‚‰ã€Œãƒ©ãƒ³ãƒ€ãƒ ãªæœªçŸ¥é‡ã€ã¸æ‹¡å¼µã—ãŸã€‚ã“ã‚Œã¯é »åº¦ä¸»ç¾©ã¨ãƒ™ã‚¤ã‚ºä¸»ç¾©ã®ä¸­é–“çš„ç«‹å ´ã§ã€äº‹å‰åˆ†å¸ƒã‚’ä»®å®šã›ãšã«ãƒ©ãƒ³ãƒ€ãƒ åŠ¹æœã‚’æ‰±ãˆã‚‹ã€‚

**Maximum Ideal Likelihood**

2024å¹´ã®ç ”ç©¶[^24]ã¯ã€æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã™ã‚‹æ–°ã—ã„æ¨å®šãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€ŒMaximum Ideal Likelihood (MIL)ã€ã‚’ææ¡ˆã—ãŸã€‚å¾“æ¥ã®MLEã¯å‘¨è¾ºåŒ– $p(\mathbf{x}) = \int p(\mathbf{x}, \mathbf{z}) d\mathbf{z}$ ãŒå›°é›£ã ã£ãŸãŒã€MILã¯æ½œåœ¨å¤‰æ•°ã‚’ã€Œç†æƒ³çš„ãªè¦³æ¸¬ã€ã¨ã—ã¦æ‰±ã†ã“ã¨ã§ã€è¨ˆç®—å¯èƒ½ãªç›®çš„é–¢æ•°ã‚’å°å‡ºã™ã‚‹ã€‚æ¼¸è¿‘çš„ã«MLEã¨ç­‰ä¾¡ã§ã‚ã‚Šã€ä¿¡é ¼åŒºé–“ã‚‚æ§‹æˆã§ãã‚‹ã€‚


#### 6.9.5 éæ­£è¦åŒ–çµ±è¨ˆãƒ¢ãƒ‡ãƒ«ã¨ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°

ç¢ºç‡å¯†åº¦é–¢æ•°ã‚’æ­£è¦åŒ–å®šæ•°è¾¼ã¿ã§è¨ˆç®—ã™ã‚‹ã®ã¯å›°é›£ãªå ´åˆãŒå¤šã„ã€‚Energy-Based Model (EBM) ã§ã¯ $p(x) = \frac{1}{Z}\exp(-E(x))$ ã¨è¡¨ç¾ã™ã‚‹ãŒã€åˆ†é…é–¢æ•° $Z = \int \exp(-E(x))dx$ ã®è¨ˆç®—ãŒæŒ‡æ•°çš„ã«å›°é›£ã ã€‚

**ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°** [^9] ã¯ã€æ­£è¦åŒ–å®šæ•°ã‚’è¨ˆç®—ã›ãšã«ç¢ºç‡ãƒ¢ãƒ‡ãƒ«ã‚’æ¨å®šã™ã‚‹æ‰‹æ³•ã ã€‚ã‚¹ã‚³ã‚¢é–¢æ•° $s(x) = \nabla_x \log p(x)$ ã¯æ­£è¦åŒ–å®šæ•°ã«ä¾å­˜ã—ãªã„ã“ã¨ã‚’åˆ©ç”¨ã™ã‚‹:

$$
s(x) = \nabla_x \log p(x) = \nabla_x \log \frac{1}{Z}\exp(-E(x)) = \nabla_x [-E(x) - \log Z] = -\nabla_x E(x)
$$

ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°ç›®çš„é–¢æ•°:

$$
J(\theta) = \frac{1}{2}\mathbb{E}_{p_{\text{data}}(x)}\left[\| \nabla_x \log p_\theta(x) - \nabla_x \log p_{\text{data}}(x) \|^2\right]
$$

ã“ã‚Œã¯æ­£è¦åŒ–å®šæ•°ãªã—ã§è¨ˆç®—å¯èƒ½ãªå½¢ã«å¤‰å½¢ã§ãã‚‹ï¼ˆéƒ¨åˆ†ç©åˆ†ã‚’ç”¨ã„ãŸæ’ç­‰å¼ï¼‰ã€‚æ‹¡æ•£ãƒ¢ãƒ‡ãƒ« [^10] ã®ç†è«–çš„åŸºç›¤ã®ä¸€ã¤ã§ã‚‚ã‚ã‚‹ã€‚


#### 6.9.6 ç¢ºç‡è«–ã¨LLMã®æ·±ã„æ¥ç¶š

LLMã®è¨“ç·´ã¯ã€æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã¨ã„ã†ç¢ºç‡çš„ã‚¿ã‚¹ã‚¯ã«å¸°ç€ã™ã‚‹ã€‚ã“ã®æ¥ç¶šã‚’æ˜ç¢ºã«ã—ã‚ˆã†ã€‚

**è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã¨é€£é–è¦å‰‡**:

$$
p(\mathbf{x}) = \prod_{t=1}^{T} p(x_t \mid x_{<t})
$$

å„æ™‚åˆ»ã§ã®æ¡ä»¶ä»˜ãåˆ†å¸ƒ $p(x_t \mid x_{<t})$ ã¯Categoricalåˆ†å¸ƒã§ã‚ã‚Šã€Softmaxã§å®šç¾©ã•ã‚Œã‚‹:

$$
p(x_t = k \mid x_{<t}) = \frac{\exp(z_k)}{\sum_{j=1}^{V} \exp(z_j)}, \quad z = f_\theta(x_{<t})
$$

**Cross-Entropyæå¤±ã¨MLE**:

$$
\mathcal{L} = -\frac{1}{T}\sum_{t=1}^{T} \log p_\theta(x_t \mid x_{<t}) = -\frac{1}{T} \log p_\theta(\mathbf{x})
$$

ã“ã‚Œã¯è² ã®å¯¾æ•°å°¤åº¦ã§ã‚ã‚Šã€æœ€å°åŒ–ã¯MLEã¨ç­‰ä¾¡ã ã€‚

**Perplexityã¨æ¡ä»¶ä»˜ãã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼**:

$$
\text{Perplexity} = \exp(\mathcal{L}) = \exp\left(-\frac{1}{T}\sum_{t=1}^{T} \log p(x_t \mid x_{<t})\right)
$$

ã“ã‚Œã¯æ¡ä»¶ä»˜ãã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ $H(X_t \mid X_{<t})$ ã®æŒ‡æ•°ã§ã‚ã‚‹ã€‚Perplexity=10ã¯ã€Œå„æ™‚åˆ»ã§å¹³å‡10å€‹ã®å€™è£œã‹ã‚‰é¸æŠã—ã¦ã„ã‚‹ã€ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã€‚

**ç¢ºç‡çš„ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã¨Top-k/Nucleus Sampling**:

æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\tau$ ã‚’å°å…¥ã—ãŸç¢ºç‡åˆ†å¸ƒ:

$$
p_\tau(x_t = k) = \frac{\exp(z_k/\tau)}{\sum_j \exp(z_j/\tau)}
$$

- $\tau \to 0$: æ±ºå®šè«–çš„ï¼ˆargmaxï¼‰
- $\tau = 1$: å…ƒã®åˆ†å¸ƒ
- $\tau > 1$: ã‚ˆã‚Šå¹³å¦ï¼ˆå¤šæ§˜æ€§å¢—åŠ ï¼‰

Nucleus samplingï¼ˆTop-pï¼‰ã¯ç´¯ç©ç¢ºç‡ $\sum_{k \in \text{top-p}} p(k) \geq p$ ã‚’æº€ãŸã™æœ€å°é›†åˆã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚ã“ã‚Œã¯ã€Œç¢ºç‡è³ªé‡ã®ä¸Šä½p%ã€ã¨ã„ã†å‹•çš„é–¾å€¤ã ã€‚


> **Note:** **LLMã®ç¢ºç‡è«–çš„è§£é‡ˆ**: æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã¯ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æ¡ä»¶ä»˜ãç¢ºç‡åˆ†å¸ƒã‚’å­¦ç¿’ã—ã¦ã„ã‚‹ã€‚ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ï¼ˆtemperature, top-k, nucleusï¼‰ã¯ã€ã“ã®ç¢ºç‡åˆ†å¸ƒã‹ã‚‰ã®ã€Œåˆ¶å¾¡ã•ã‚ŒãŸãƒ©ãƒ³ãƒ€ãƒ åŒ–ã€ã ã€‚æ±ºå®šè«–çš„ç”Ÿæˆï¼ˆgreedyï¼‰ã¯æœ€å°¤æ¨å®šã€ç¢ºç‡çš„ç”Ÿæˆã¯ãƒ™ã‚¤ã‚ºæ¨è«–ã®è¦–ç‚¹ã¨å¯¾å¿œã™ã‚‹ã€‚

### 7.7 ç¢ºç‡è«–ã‹ã‚‰ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¸ã®æ©‹

ç¬¬4å›ã§å­¦ã‚“ã å…¨ã¦ãŒã€æ·±å±¤ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æ•°å­¦çš„åœŸå°ã ã€‚ã“ã®æ©‹ã‚’æ˜ç¤ºçš„ã«ç¤ºã—ã¦ãŠãã€‚

**VAEï¼ˆç¬¬8å›ï¼‰ã¸ã®ç›´æ¥æ¥ç¶š**:

| ç¬¬4å›ã®æ¦‚å¿µ | VAEã§ã®å½¹å‰² |
|:------------|:------------|
| MLE | ãƒ‡ã‚³ãƒ¼ãƒ€ $p_\theta(\mathbf{x}\mid\mathbf{z})$ ã®æœ€å¤§åŒ– |
| KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ | ELBOã®æ­£å‰‡åŒ–é … $D_{\mathrm{KL}}(q_\phi\|p)$ |
| Gaussian MLE | ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ $\mu_\phi, \sigma_\phi$ ã®å‡ºåŠ› |
| æŒ‡æ•°å‹åˆ†å¸ƒæ— | ãƒ‡ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ›åˆ†å¸ƒè¨­è¨ˆ |
| å¤‰åˆ†æ¨è«–ï¼ˆäº‹å¾Œä¸€è‡´æ€§ï¼‰| ELBOæœ€å¤§åŒ–ã«ã‚ˆã‚‹è¿‘ä¼¼äº‹å¾Œåˆ†å¸ƒã®å­¦ç¿’ |

**Diffusion Modelsï¼ˆç¬¬15å›ï¼‰ã¸ã®æ¥ç¶š**:

| ç¬¬4å›ã®æ¦‚å¿µ | Diffusionã§ã®å½¹å‰² |
|:------------|:------------------|
| Gaussianç©ã®é–‰å½¢å¼ | $q(\mathbf{x}_t\mid\mathbf{x}_0)$ ã®åˆ†æçš„è¨ˆç®— |
| æ¡ä»¶ä»˜ãGaussian | é€†ãƒ—ãƒ­ã‚»ã‚¹ $p_\theta(\mathbf{x}_{t-1}\mid\mathbf{x}_t)$ ã®å½¢ |
| KLæœ€å°åŒ– | ELBO = $\sum_t \mathbb{E}[D_{\mathrm{KL}}(q_t\|p_{t-1})]$ |

**LLMï¼ˆç¬¬20å›ï¼‰ã¸ã®æ¥ç¶š**:

| ç¬¬4å›ã®æ¦‚å¿µ | LLMã§ã®å½¹å‰² |
|:------------|:------------|
| Categoricalåˆ†å¸ƒ | softmaxå‡ºåŠ›å±¤ |
| é€£é–è¦å‰‡ $\log p(\mathbf{x}) = \sum_t \log p(x_t\mid x_{<t})$ | è‡ªå·±å›å¸°ç›®çš„é–¢æ•° |
| MLE | æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã®æœ€å¤§åŒ–ï¼ˆäº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼‰|
| æŒ‡æ•°å‹åˆ†å¸ƒæ— | logitç©ºé–“ã®å¹¾ä½•å­¦ |

ç¢ºç‡è«–ã¯ã€Œç©ã¿æœ¨ã€ã ã€‚ã“ã“ã§ç©ã‚“ã æ¦‚å¿µãŒã€å¾ŒåŠã®å…¨ã¦ã®è¬›ç¾©ã§å‘¼ã³æˆ»ã•ã‚Œã‚‹ã€‚

> Progress: 100%

> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. ãƒ™ã‚¤ã‚ºã®å®šç† $p(\theta|\mathcal{D}) \propto p(\mathcal{D}|\theta)p(\theta)$ ã«ãŠã„ã¦ã€äº‹å‰åˆ†å¸ƒãƒ»å°¤åº¦ãƒ»äº‹å¾Œåˆ†å¸ƒã‚’ãã‚Œãã‚ŒåŒå®šã›ã‚ˆã€‚
> 2. æ­£è¦åˆ†å¸ƒ $\mathcal{N}(\mu, \sigma^2)$ ã®å¹³å‡ãƒ»åˆ†æ•£ã‚’ $\mu, \sigma$ ã§æ›¸ãã€$\sigma \to 0$ ã®ã¨ãåˆ†å¸ƒã¯ã©ã†å¤‰åŒ–ã™ã‚‹ã‹èª¬æ˜ã›ã‚ˆã€‚

---
> **ğŸ“– å‰ç·¨ã‚‚ã‚ã‚ã›ã¦ã”è¦§ãã ã•ã„**
> [ã€å‰ç·¨ã€‘ç¬¬4å›: ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦](/articles/ml-lecture-04-part1) ã§ã¯ã€ç¢ºç‡è«–ãƒ»ãƒ™ã‚¤ã‚ºã®å®šç†ãƒ»æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®ç†è«–ã‚’å­¦ã³ã¾ã—ãŸã€‚

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Blei, D. M., Kucukelbir, A., & McAuliffe, J. D. (2016). Variational Inference: A Review for Statisticians.
<https://arxiv.org/abs/1601.00670>

[^2]: Kingma, D. P., & Welling, M. (2013). Auto-Encoding Variational Bayes.
<https://arxiv.org/abs/1312.6114>

[^3]: Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the Knowledge in a Neural Network.
<https://arxiv.org/abs/1503.02531>

[^4]: Ho, J., Jain, A., & Abbeel, P. (2020). Denoising Diffusion Probabilistic Models.
<https://arxiv.org/abs/2006.11239>

[^5]: Malach, E. (2023). Auto-Regressive Next-Token Predictors are Universal Learners.
<https://arxiv.org/abs/2309.06979>

[^6]: Song, Y., & Ermon, S. (2019). Generative Modeling by Estimating Gradients of the Data Distribution.
<https://arxiv.org/abs/1907.05600>

[^7]: Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., & Poole, B. (2021). Score-Based Generative Modeling through Stochastic Differential Equations.
<https://arxiv.org/abs/2011.13456>

[^10]: Song, Y., Sohl-Dickstein, J., Kingma, D.P., Kumar, A., Ermon, S., Poole, B. (2020). "Score-Based Generative Modeling through Stochastic Differential Equations." *ICLR 2021 (Oral)*.
[https://arxiv.org/abs/2011.13456](https://arxiv.org/abs/2011.13456)

[^11]: Rezende, D.J., Mohamed, S. (2015). "Variational Inference with Normalizing Flows." *ICML 2015*.
[https://arxiv.org/abs/1505.05770](https://arxiv.org/abs/1505.05770)

[^12]: Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W. (2021). "LoRA: Low-Rank Adaptation of Large Language Models." *ICLR 2022*.
[https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)

[^13]: Relative Performance of Expected and Observed Fisher Information in Covariance Estimation for Maximum Likelihood Estimates. (2013). *arXiv preprint*.
[https://arxiv.org/abs/1305.1056](https://arxiv.org/abs/1305.1056)

[^14]: Relative Performance of Fisher Information in Interval Estimation. (2021). *arXiv preprint*.
[https://arxiv.org/abs/2107.04620](https://arxiv.org/abs/2107.04620)

[^15]: Maximum Ideal Likelihood Estimation: A Unified Inference Framework for Latent Variable Models. (2024). *arXiv preprint*.
[https://arxiv.org/abs/2410.01194](https://arxiv.org/abs/2410.01194)

[^16]: A Latent-Variable Formulation of the Poisson Canonical Polyadic Tensor Model: Maximum Likelihood Estimation and Fisher Information. (2025). *arXiv preprint*.
[https://arxiv.org/abs/2511.05352](https://arxiv.org/abs/2511.05352)

[^17]: The Taylor Measure and its Applications. (2025). *arXiv preprint*.
[https://arxiv.org/abs/2508.04760](https://arxiv.org/abs/2508.04760)

[^18]: On the Metric Temporal Logic for Continuous Stochastic Processes. (2023). *arXiv preprint*.
[https://arxiv.org/abs/2308.00984](https://arxiv.org/abs/2308.00984)

[^19]: A Probability Space at Inception of Stochastic Process. (2025). *arXiv preprint*.
[https://arxiv.org/abs/2510.20824](https://arxiv.org/abs/2510.20824)

[^20]: Alpha Entropy Search for New Information-based Bayesian Optimization. (2024). *arXiv preprint*.
[https://arxiv.org/abs/2411.16586](https://arxiv.org/abs/2411.16586)

[^21]: Dorent, R., et al. (2025). Connecting Jensen-Shannon and Kullback-Leibler Divergences: A New Bound for Representation Learning. *arXiv preprint*.
[https://arxiv.org/abs/2510.20644](https://arxiv.org/abs/2510.20644)

[^22]: GAIT: A Geometric Approach to Information Theory. (2019). *arXiv preprint*.
[https://arxiv.org/abs/1906.08325](https://arxiv.org/abs/1906.08325)

[^23]: Statistical Inference for Random Unknowns via Modifications of Extended Likelihood. (2023). *arXiv preprint*.
[https://arxiv.org/abs/2310.09955](https://arxiv.org/abs/2310.09955)

[^24]: Maximum Ideal Likelihood Estimation: A Unified Inference Framework for Latent Variable Models. (2024). *arXiv preprint*.
[https://arxiv.org/abs/2410.01194](https://arxiv.org/abs/2410.01194)

---

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

---


---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
