---
title: "ç¬¬32å›: Productionçµ±åˆã€å‰ç·¨ã€‘ç†è«–ç·¨: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œ"
emoji: "ğŸ”„"
type: "tech"
topics: ["machinelearning"]
published: true
slug: "ml-lecture-32-part1"
---
---

# ç¬¬32å›: Production & ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ— + çµ±åˆPJ ğŸ†

:::message
**å‰æçŸ¥è­˜**: ç¬¬31å›ã§MLOpsåŸºç›¤ã‚’æ•´ãˆãŸã€‚ã“ã®ç¬¬32å›ã¯Course IIIæœ€çµ‚å› â€” 14å›ã®å…¨æŠ€è¡“ã‚’çµ±åˆã—ã¦E2Eã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
:::

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” 3è¡Œã§E2Eã‚·ã‚¹ãƒ†ãƒ ã‚’ä½“æ„Ÿ

ç¬¬31å›ã§MLOpsãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã—ãŸã€‚æœ€çµ‚å›ã®ä»Šå›ã€**å…¨ã¦ã‚’çµ±åˆã—ãŸProduction E2Eã‚·ã‚¹ãƒ†ãƒ **ã‚’3è¡Œã®ã‚³ãƒ¼ãƒ‰ã§ä½“æ„Ÿã—ã‚ˆã†ã€‚

```julia
# SmolVLM2-256Mæ¨è«– â†’ Elixir API â†’ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›† â†’ Juliaå†è¨“ç·´
using SmolVLM2Inference, ElixirGateway, FeedbackLoop
result = deploy_e2e_system("models/smolvlm2-256m.onnx", port=4000)
# => "E2E system deployed: Juliaè¨“ç·´â†’Rustæ¨è«–â†’Elixiré…ä¿¡â†’Feedbackâ†’å†è¨“ç·´"
```

**å‡ºåŠ›**:
```
ğŸ¯ E2E System Status:
  âš¡ Julia Training Pipeline: Ready (SmolVLM2-256M, VAE, GANçµ±åˆ)
  ğŸ¦€ Rust Inference Server: Running on port 8080 (Axum, ONNX Runtime)
  ğŸ”® Elixir API Gateway: Running on port 4000 (Phoenix, JWT auth, Rate limit)
  ğŸ“Š Monitoring: Prometheus metrics at :9090
  ğŸ”„ Feedback Loop: Active (implicit+explicit feedback collected)

âœ… System Health: All components operational
ğŸ“ˆ Current throughput: 1,247 req/s (95th %ile latency: 12ms)
```

**ã“ã®è£ã«ã‚ã‚‹æ•°å¼**: ç¬¬19å›ã‹ã‚‰ç¬¬31å›ã§å­¦ã‚“ã **å…¨ã¦ã®æŠ€è¡“ãŒçµ±åˆã•ã‚Œã¦ã„ã‚‹**:

$$
\text{Production System} = \underbrace{\text{Train}_{\text{Julia}}}_{\text{ç¬¬20,23å›}} \xrightarrow{\text{Export}_{\text{ONNX}}} \underbrace{\text{Infer}_{\text{Rust}}}_{\text{ç¬¬26å›}} \xrightarrow{\text{Serve}_{\text{Elixir}}} \underbrace{\text{Feedback}}_{\text{ç¬¬32å›}} \circlearrowleft
$$

ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—ã®æ•°å¼:

$$
\theta_{t+1} \leftarrow \theta_t - \eta \nabla_\theta \mathcal{L}(\theta_t; \mathcal{D}_{\text{feedback}})
$$

3è¡Œã®ã‚³ãƒ¼ãƒ‰ã®è£ã§ã€**Juliaè¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**ãŒVAE/GAN/GPTã‚’è¨“ç·´ã—ã€**Rustæ¨è«–ã‚µãƒ¼ãƒãƒ¼**ãŒONNXãƒ¢ãƒ‡ãƒ«ã‚’é«˜é€Ÿæ¨è«–ã€**Elixir APIã‚²ãƒ¼ãƒˆã‚¦ã‚§ã‚¤**ãŒåˆ†æ•£é…ä¿¡ã¨èªè¨¼ã‚’æ‹…å½“ã€**ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—**ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è©•ä¾¡ã‚’åé›†ã—ã¦å†è¨“ç·´ã«ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã™ã‚‹ â€” å…¨ã¦ãŒè‡ªå‹•çš„ã«å‹•ä½œã™ã‚‹ã€‚

**ã“ã‚ŒãŒCourse III 14å›ã®é›†å¤§æˆã ã€‚**

:::message
**é€²æ—: 3%å®Œäº†ï¼** ç¬¬32å›ã®ã‚´ãƒ¼ãƒ«ã¯ã€ŒProduction E2Eã‚·ã‚¹ãƒ†ãƒ ã‚’è‡ªåŠ›ã§æ§‹ç¯‰ãƒ»é‹ç”¨ã§ãã‚‹ã€ã“ã¨ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” AIã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆ & ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’è§¦ã‚‹

### 1.1 AIã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã®è¨­è¨ˆ

AIã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã®æœ¬è³ªã¯**å•ã„åˆã‚ã›ã®è‡ªå‹•åˆ†é¡**ã¨**äººé–“ã¸ã®ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æˆ¦ç•¥**ã ã€‚

```julia
using CustomerSupport, Embeddings

# å•ã„åˆã‚ã›ã‚’è‡ªå‹•åˆ†é¡
inquiry = "å•†å“ãŒå±Šã‹ãªã„ã€‚æ³¨æ–‡ç•ªå·ã¯12345ã§ã™ã€‚"
category, confidence = classify_inquiry(inquiry)
# => ("é…é€å•é¡Œ", 0.92)

if confidence < 0.7
    escalate_to_human(inquiry, reason="ä½ä¿¡é ¼åº¦")
elseif category == "è¿”é‡‘è¦æ±‚"
    escalate_to_human(inquiry, reason="é«˜ãƒªã‚¹ã‚¯")
else
    auto_response = generate_faq_response(category, inquiry)
    send_response(auto_response)
end
```

**æ•°å¼**: å•ã„åˆã‚ã›åˆ†é¡ã¯Softmaxåˆ†é¡

$$
p(c_i | \mathbf{x}) = \frac{\exp(\mathbf{w}_i^\top \mathbf{x})}{\sum_{j=1}^C \exp(\mathbf{w}_j^\top \mathbf{x})}
$$

ã“ã“ã§ $\mathbf{x}$ ã¯å•ã„åˆã‚ã›ã®Embeddingã€$\mathbf{w}_i$ ã¯ã‚«ãƒ†ã‚´ãƒª $c_i$ ã®é‡ã¿ãƒ™ã‚¯ãƒˆãƒ«ã€‚

**ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æˆ¦ç•¥**:

| æ¡ä»¶ | ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ | ç†ç”± |
|:-----|:----------|:-----|
| `confidence < 0.7` | äººé–“ã«ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ | ãƒ¢ãƒ‡ãƒ«ãŒè‡ªä¿¡ã‚’æŒã¦ãªã„ |
| `category == "è¿”é‡‘"` | äººé–“ã«ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ | é«˜ãƒªã‚¹ã‚¯ãƒ»é«˜ã‚³ã‚¹ãƒˆåˆ¤æ–­ |
| `sentiment < -0.5` | äººé–“ã«ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ | æ€’ã£ã¦ã„ã‚‹é¡§å®¢ |
| ãã®ä»– | è‡ªå‹•å¿œç­” | æ¨™æº–çš„ãªå•ã„åˆã‚ã› |

### 1.2 ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†: æš—é»™çš„ vs æ˜ç¤ºçš„

ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«ã¯**æš—é»™çš„**ã¨**æ˜ç¤ºçš„**ã®2ç¨®é¡ãŒã‚ã‚‹ã€‚

```julia
# æš—é»™çš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯: ã‚¯ãƒªãƒƒã‚¯ãƒ»æ»åœ¨æ™‚é–“ãƒ»ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«æ·±åº¦
implicit_feedback = collect_implicit_feedback(
    click_through=true,
    dwell_time=45.3,  # ç§’
    scroll_depth=0.78  # 78%ã¾ã§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
)
# => ImplicitFeedback(positive_signal=0.82)

# æ˜ç¤ºçš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯: è©•ä¾¡ãƒœã‚¿ãƒ³ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆãƒ»NPS
explicit_feedback = collect_explicit_feedback(
    rating=4,  # 1-5 stars
    comment="å›ç­”ã¯å½¹ç«‹ã£ãŸãŒã€ã‚‚ã†å°‘ã—å…·ä½“ä¾‹ãŒæ¬²ã—ã‹ã£ãŸ",
    nps=8      # Net Promoter Score (0-10)
)
# => ExplicitFeedback(sentiment=0.65, topics=["å…·ä½“ä¾‹ä¸è¶³"])
```

**æ•°å¼**: æš—é»™çš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®ã‚¹ã‚³ã‚¢é–¢æ•°

$$
f_{\text{implicit}}(\text{click}, t_{\text{dwell}}, d_{\text{scroll}}) = w_1 \cdot \mathbb{1}_{\text{click}} + w_2 \cdot \tanh(t_{\text{dwell}}/60) + w_3 \cdot d_{\text{scroll}}
$$

ã“ã“ã§ $\mathbb{1}_{\text{click}}$ ã¯ã‚¯ãƒªãƒƒã‚¯ã®æœ‰ç„¡ï¼ˆ0 or 1ï¼‰ã€$w_1, w_2, w_3$ ã¯é‡ã¿ï¼ˆä¾‹: $w_1=0.4, w_2=0.4, w_3=0.2$ï¼‰ã€‚

**æ˜ç¤ºçš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ**:

$$
S(\text{comment}) = \text{Transformer}_{\text{sentiment}}(\text{Embedding}(\text{comment})) \in [-1, 1]
$$

### 1.3 ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†æ: ãƒˆãƒ”ãƒƒã‚¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°

åé›†ã—ãŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚³ãƒ¡ãƒ³ãƒˆã‚’**ãƒˆãƒ”ãƒƒã‚¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°**ã—ã¦æ ¹æœ¬åŸå› ã‚’åˆ†æã™ã‚‹ã€‚

```julia
using UMAP, HDBSCAN

# 1,000ä»¶ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚³ãƒ¡ãƒ³ãƒˆã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
comments = load_feedback_comments(n=1000)
embeddings = embed_comments(comments)  # (1000, 384) Embedding

# UMAPæ¬¡å…ƒå‰Šæ¸› â†’ HDBSCAN ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
umap_emb = umap(embeddings, n_components=2)
clusters = hdbscan(umap_emb, min_cluster_size=20)

# ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã®ä»£è¡¨çš„ãªã‚³ãƒ¡ãƒ³ãƒˆ
for (cluster_id, representative_comments) in clusters
    println("Cluster $cluster_id:")
    println("  ", join(representative_comments[1:3], "\n  "))
end
```

**å‡ºåŠ›ä¾‹**:
```
Cluster 1: "é…é€ãŒé…ã„"ç³»
  "å•†å“ãŒå±Šã‹ãªã„"
  "é…é€çŠ¶æ³ãŒæ›´æ–°ã•ã‚Œãªã„"
  "é…é€æ¥­è€…ã«é€£çµ¡ãŒã¤ã‹ãªã„"

Cluster 2: "å…·ä½“ä¾‹ä¸è¶³"ç³»
  "ã‚‚ã£ã¨å…·ä½“çš„ãªæ‰‹é †ãŒæ¬²ã—ã„"
  "ç”»åƒä»˜ãã§èª¬æ˜ã—ã¦æ¬²ã—ã„"
  "ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ãŒæ¬²ã—ã„"
```

**æ•°å¼**: UMAPæ¬¡å…ƒå‰Šæ¸›

$$
\min_{\mathbf{Y}} \sum_{i,j} w_{ij} \left\| \mathbf{y}_i - \mathbf{y}_j \right\|^2 + \lambda \sum_{i,j} (1 - w_{ij}) \max(0, d_{\text{min}} - \left\| \mathbf{y}_i - \mathbf{y}_j \right\|)^2
$$

ã“ã“ã§ $\mathbf{Y} \in \mathbb{R}^{n \times 2}$ ã¯2æ¬¡å…ƒåŸ‹ã‚è¾¼ã¿ã€$w_{ij}$ ã¯é«˜æ¬¡å…ƒç©ºé–“ã§ã®è¿‘å‚é‡ã¿ã€‚

### 1.4 PyTorchã¨ã®å¯¾å¿œ â€” ãƒ¢ãƒ‡ãƒ«è¨“ç·´

```python
import torch
import torch.nn as nn

# ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ä½¿ã£ãŸFine-tuning
class FeedbackClassifier(nn.Module):
    def __init__(self, embedding_dim=384, num_classes=10):
        super().__init__()
        self.classifier = nn.Linear(embedding_dim, num_classes)

    def forward(self, x):
        return self.classifier(x)

model = FeedbackClassifier()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
criterion = nn.CrossEntropyLoss()

# ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´
for epoch in range(10):
    for batch in feedback_dataloader:
        embeddings, labels = batch
        logits = model(embeddings)
        loss = criterion(logits, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

**Juliaå¯¾å¿œ** (æ•°å¼ â†” ã‚³ãƒ¼ãƒ‰ 1:1):

```julia
using Lux, Optimisers, Zygote

# Lux.jl ã§ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†é¡å™¨
struct FeedbackClassifier <: Lux.AbstractExplicitLayer
    embedding_dim::Int
    num_classes::Int
end

function (m::FeedbackClassifier)(x, ps, st)
    W = ps.W  # (num_classes, embedding_dim)
    b = ps.b  # (num_classes,)
    return W * x .+ b, st
end

# è¨“ç·´ãƒ«ãƒ¼ãƒ—
model = FeedbackClassifier(384, 10)
ps, st = Lux.setup(rng, model)
opt_state = Optimisers.setup(AdamW(1e-4), ps)

for epoch in 1:10
    for (embeddings, labels) in feedback_dataloader
        # Forward + Backward
        loss, grads = Zygote.withgradient(ps) do p
            logits, _ = model(embeddings, p, st)
            cross_entropy_loss(logits, labels)
        end

        # Update
        opt_state, ps = Optimisers.update(opt_state, ps, grads[1])
    end
end
```

**æ¥ç¶šå›³**:

```mermaid
graph LR
    A[ãƒ¦ãƒ¼ã‚¶ãƒ¼å•ã„åˆã‚ã›] --> B[Embedding]
    B --> C[åˆ†é¡ãƒ¢ãƒ‡ãƒ«]
    C --> D{ä¿¡é ¼åº¦ > 0.7?}
    D -->|Yes| E[è‡ªå‹•å¿œç­”]
    D -->|No| F[äººé–“ã«ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³]
    E --> G[ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†]
    F --> G
    G --> H[ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†æ]
    H --> I[ãƒ¢ãƒ‡ãƒ«æ”¹å–„]
    I --> C
```

:::message
**é€²æ—: 10%å®Œäº†ï¼** AIã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã®è¨­è¨ˆã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†ã®åŸºç¤ã‚’ä½“é¨“ã—ãŸã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœProductionã‚·ã‚¹ãƒ†ãƒ ãŒå¿…è¦ã‹

### 2.1 Course IIIã®åœ°å›³: ç¬¬19-32å›ã®æŒ¯ã‚Šè¿”ã‚Š

Course IIIã¯**ç†è«–ã‚’å‹•ãã‚·ã‚¹ãƒ†ãƒ ã«å¤‰ãˆã‚‹14å›**ã ã£ãŸã€‚å„è¬›ç¾©ã‚’æŒ¯ã‚Šè¿”ã‚ã†ã€‚

| å› | ã‚¿ã‚¤ãƒˆãƒ« | ç²å¾—ã—ãŸæ­¦å™¨ | è¨€èª |
|:---|:---------|:-------------|:-----|
| ç¬¬19å› | ç’°å¢ƒæ§‹ç¯‰ & FFI | FFIå¢ƒç•Œè¨­è¨ˆ / C-ABIçµ±ä¸€ç†è«– | ğŸ¦€âš¡ğŸ”® |
| ç¬¬20å› | å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ | VAE/GAN/Transformerå®Ÿè£…ã®å‹ | âš¡ğŸ¦€ |
| ç¬¬21å› | ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ | ETL/ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°/å¯è¦–åŒ– | âš¡ |
| ç¬¬22å› | ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ« | VLM/ç”»åƒ-ãƒ†ã‚­ã‚¹ãƒˆçµ±åˆ | âš¡ğŸ¦€ |
| ç¬¬23å› | Fine-tuning & PEFT | LoRA/QLoRA/AdaLoRA | âš¡ğŸ¦€ |
| ç¬¬24å› | çµ±è¨ˆå­¦ | ä»®èª¬æ¤œå®š/A/Bãƒ†ã‚¹ãƒˆ/ä¿¡é ¼åŒºé–“ | âš¡ |
| ç¬¬25å› | å› æœæ¨è«– | RCT/DID/IV/å‚¾å‘ã‚¹ã‚³ã‚¢ | âš¡ |
| ç¬¬26å› | æ¨è«–æœ€é©åŒ– | é‡å­åŒ–/è’¸ç•™/ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° | ğŸ¦€âš¡ |
| ç¬¬27å› | è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ | FID/CLIP Score/Human Eval | âš¡ |
| ç¬¬28å› | ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ | Few-shot/CoT/ReAct/Self-Consistency | âš¡ |
| ç¬¬29å› | RAG | Retrieval/Rerank/Hybrid Search | âš¡ğŸ¦€ |
| ç¬¬30å› | ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ | ReAct/Tool Use/Multi-Agent | ğŸ”®âš¡ |
| ç¬¬31å› | MLOps | CI/CD/Monitoring/A/Bãƒ†ã‚¹ãƒˆ | ğŸ¦€âš¡ğŸ”® |
| **ç¬¬32å›** | **Productionçµ±åˆ** | **E2Eã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰** | **ğŸ¦€âš¡ğŸ”®** |

**å…¨ã¦ã‚’çµ±åˆã—ãŸã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**:

```mermaid
graph TD
    A[ãƒ‡ãƒ¼ã‚¿åé›†] --> B[âš¡ Juliaè¨“ç·´PL]
    B --> C[ãƒ¢ãƒ‡ãƒ«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ ONNX]
    C --> D[ğŸ¦€ Rustæ¨è«–ã‚µãƒ¼ãƒãƒ¼]
    D --> E[ğŸ”® Elixir APIã‚²ãƒ¼ãƒˆã‚¦ã‚§ã‚¤]
    E --> F[ãƒ¦ãƒ¼ã‚¶ãƒ¼]
    F --> G[ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†]
    G --> H[ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†æ]
    H --> A
    D --> I[ğŸ“Š Monitoring Prometheus]
    E --> I
    I --> J[ã‚¢ãƒ©ãƒ¼ãƒˆ & ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰]
```

### 2.2 Productionã®æœ¬è³ª: Trainâ†’Feedbacké–‰ãƒ«ãƒ¼ãƒ—

Productionã‚·ã‚¹ãƒ†ãƒ ã®æœ¬è³ªã¯**é–‰ãƒ«ãƒ¼ãƒ—**ã ã€‚

**å¾“æ¥ã®MLé–‹ç™º** (é–‹ãƒ«ãƒ¼ãƒ—):
```
ãƒ‡ãƒ¼ã‚¿åé›† â†’ è¨“ç·´ â†’ è©•ä¾¡ â†’ ãƒ‡ãƒ—ãƒ­ã‚¤ â†’ [çµ‚äº†]
```

**Productionã‚·ã‚¹ãƒ†ãƒ ** (é–‰ãƒ«ãƒ¼ãƒ—):
```
ãƒ‡ãƒ¼ã‚¿åé›† â†’ è¨“ç·´ â†’ è©•ä¾¡ â†’ ãƒ‡ãƒ—ãƒ­ã‚¤ â†’ Feedbackåé›† â†º
                                          â†“
                                      åˆ†æ & æ”¹å–„
```

**é–‰ãƒ«ãƒ¼ãƒ—ã®æ•°å¼**:

$$
\begin{aligned}
\text{Epoch } t&: \theta_t \leftarrow \arg\min_\theta \mathcal{L}(\theta; \mathcal{D}_{\text{train}}) \\
\text{Deploy}&: \text{Model}_t \text{ serves users} \\
\text{Collect}&: \mathcal{D}_{\text{feedback}} \leftarrow \{ (x_i, y_i^{\text{feedback}}) \}_{i=1}^N \\
\text{Epoch } t+1&: \theta_{t+1} \leftarrow \arg\min_\theta \mathcal{L}(\theta; \mathcal{D}_{\text{train}} \cup \mathcal{D}_{\text{feedback}})
\end{aligned}
$$

**ãªãœé–‰ãƒ«ãƒ¼ãƒ—ãŒå¿…è¦ã‹ï¼Ÿ**

1. **ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆ**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•ã¯æ™‚é–“ã¨ã¨ã‚‚ã«å¤‰åŒ–ã™ã‚‹
2. **åˆ†å¸ƒã‚·ãƒ•ãƒˆ**: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨æœ¬ç•ªãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒãŒç•°ãªã‚‹
3. **ç¶™ç¶šçš„æ”¹å–„**: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æ´»ç”¨ã—ã¦æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹

### 2.3 æ¾å°¾ç ”ã¨ã®å¯¾æ¯”

| é …ç›® | æ¾å°¾ç ” (æ•™ç§‘æ›¸ãƒ¬ãƒ™ãƒ«) | æœ¬ã‚·ãƒªãƒ¼ã‚º Course III |
|:-----|:---------------------|:---------------------|
| **è¨“ç·´** | PyTorchã§è¨“ç·´ | âš¡ Juliaé«˜é€Ÿè¨“ç·´ (ç¬¬20å›) |
| **æ¨è«–** | Pythonã§æ¨è«– | ğŸ¦€ Rusté«˜é€Ÿæ¨è«– (ç¬¬26å›) |
| **é…ä¿¡** | Flask/FastAPI | ğŸ”® Elixiråˆ†æ•£é…ä¿¡ (ç¬¬30å›) |
| **ç›£è¦–** | ãªã— | Prometheus/Grafana (ç¬¬31å›) |
| **ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯** | ãªã— | **Active Learning + HITL** (ç¬¬32å›) |
| **E2Eçµ±åˆ** | ãªã— | **å…¨è¨€èªçµ±åˆã‚·ã‚¹ãƒ†ãƒ ** (ç¬¬32å›) |

**æ¾å°¾ç ”ãŒæ•™ãˆãªã„ã“ã¨**:
- 3è¨€èªçµ±åˆ (ğŸ¦€âš¡ğŸ”®)
- Productionå“è³ªè¨­è¨ˆ (ç¬¬26å›ã®æ¨è«–æœ€é©åŒ–, ç¬¬31å›ã®MLOps)
- ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ— (ç¬¬32å›)
- E2Eã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰ (ç¬¬32å›)

### 2.4 3ã¤ã®æ¯”å–©ã§æ‰ãˆã‚‹ã€ŒProductionã€

**æ¯”å–©1: ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³çµŒå–¶**
- è¨“ç·´ = ãƒ¬ã‚·ãƒ”é–‹ç™º
- æ¨è«– = æ–™ç†æä¾›
- ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ = é¡§å®¢ãƒ¬ãƒ“ãƒ¥ãƒ¼
- æ”¹å–„ = ãƒ¬ã‚·ãƒ”æ”¹è‰¯

**æ¯”å–©2: è‡ªå‹•è»Šè£½é€ **
- è¨“ç·´ = è©¦ä½œè»Šé–‹ç™º
- æ¨è«– = é‡ç”£ãƒ©ã‚¤ãƒ³
- ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ = å“è³ªæ¤œæŸ» + é¡§å®¢ã‚¯ãƒ¬ãƒ¼ãƒ 
- æ”¹å–„ = è¨­è¨ˆå¤‰æ›´

**æ¯”å–©3: ç”Ÿæ…‹ç³»**
- è¨“ç·´ = ç¨®ã®é€²åŒ–
- æ¨è«– = å€‹ä½“ã®ç”Ÿå­˜
- ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ = è‡ªç„¶é¸æŠ
- æ”¹å–„ = é©å¿œé€²åŒ–

**Productionã®3æ¯”å–©ãŒç¤ºã™ã“ã¨**:
1. **ç¶™ç¶šçš„ãƒ—ãƒ­ã‚»ã‚¹**: ä¸€åº¦ä½œã£ã¦çµ‚ã‚ã‚Šã§ã¯ãªã„
2. **ç’°å¢ƒé©å¿œ**: å¤–éƒ¨ç’°å¢ƒã®å¤‰åŒ–ã«å¯¾å¿œã™ã‚‹
3. **ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é§†å‹•**: ãƒ‡ãƒ¼ã‚¿ãŒæ”¹å–„ã‚’å°ã

### 2.5 Trojan Horse: ğŸâ†’ğŸ¦€â†’âš¡â†’ğŸ”® å®Œå…¨çµ±åˆ

ç¬¬9å›ã§RustãŒç™»å ´ã—ã€ç¬¬10å›ã§JuliaãŒç™»å ´ã—ã€ç¬¬19å›ã§ElixirãŒç™»å ´ã—ãŸã€‚**3è¨€èªãŒæƒã£ãŸä»Šã€ãã‚Œãã‚Œã®å½¹å‰²ãŒæ˜ç¢ºã«ãªã£ãŸ**ã€‚

| è¨€èª | å½¹å‰² | ç†ç”± | ç™»å ´å› |
|:-----|:-----|:-----|:-------|
| ğŸ¦€ Rust | æ¨è«–ãƒ»ã‚¤ãƒ³ãƒ•ãƒ©ãƒ»æœ¬ç•ª | ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ / å‹å®‰å…¨ / é«˜é€Ÿ | ç¬¬9å› |
| âš¡ Julia | ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ãƒ»è¨“ç·´ | æ•°å¼â†”ã‚³ãƒ¼ãƒ‰1:1 / å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒ | ç¬¬10å› |
| ğŸ”® Elixir | åˆ†æ•£é…ä¿¡ãƒ»è€éšœå®³æ€§ | OTP / Actor / let it crash | ç¬¬19å› |
| ğŸ Python | æŸ»èª­ç”¨ (èª­ã‚€ã ã‘) | ç ”ç©¶è€…ã®ã‚³ãƒ¼ãƒ‰ç†è§£ | ç¬¬1å› |

**ç¬¬32å›ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸**: **Pythonã¯å’æ¥­ã—ãŸ**ã€‚Productionç’°å¢ƒã§ã¯ğŸ¦€âš¡ğŸ”®ãŒå½“ãŸã‚Šå‰ã€‚

:::message
**é€²æ—: 20%å®Œäº†ï¼** Productionã‚·ã‚¹ãƒ†ãƒ ã®å…¨ä½“åƒã¨Course IIIã®ä½ç½®ã¥ã‘ã‚’ç†è§£ã—ãŸã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ— & Active Learningç†è«–

### 3.1 ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—ã®æ•°å¼åŒ–

#### 3.1.1 æš—é»™çš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®å®šå¼åŒ–

æš—é»™çš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¯**ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•ã‹ã‚‰é–“æ¥çš„ã«å“è³ªã‚’æ¨å®š**ã™ã‚‹ã€‚

**å®šç¾©**: ã‚¯ãƒªãƒƒã‚¯ã‚¹ãƒ«ãƒ¼ç‡ (CTR) ã®è¨ˆç®—

$$
\text{CTR} = \frac{\text{ã‚¯ãƒªãƒƒã‚¯æ•°}}{\text{è¡¨ç¤ºå›æ•°}}
$$

**æ»åœ¨æ™‚é–“ãƒ¢ãƒ‡ãƒ«**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒ $t$ ç§’æ»åœ¨ã—ãŸå ´åˆã®æº€è¶³åº¦

$$
s_{\text{dwell}}(t) = 1 - \exp(-\lambda t)
$$

ã“ã“ã§ $\lambda > 0$ ã¯æ¸›è¡°ç‡ã€‚$t \to \infty$ ã§ $s \to 1$ã€$t=0$ ã§ $s=0$ã€‚

**ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«æ·±åº¦ãƒ¢ãƒ‡ãƒ«**: ãƒšãƒ¼ã‚¸ã® $d \in [0,1]$ ã¾ã§è¦‹ãŸå ´åˆã®æº€è¶³åº¦

$$
s_{\text{scroll}}(d) = d
$$

**çµ±åˆã‚¹ã‚³ã‚¢**: 3ã¤ã®æŒ‡æ¨™ã‚’é‡ã¿ä»˜ãå’Œã§çµåˆ

$$
f_{\text{implicit}}(\text{click}, t, d) = w_1 \cdot \mathbb{1}_{\text{click}} + w_2 \cdot s_{\text{dwell}}(t) + w_3 \cdot s_{\text{scroll}}(d)
$$

å…¸å‹çš„ãªé‡ã¿: $w_1=0.4, w_2=0.4, w_3=0.2$ã€‚

**æ•°å€¤æ¤œè¨¼** (Julia):

```julia
Î» = 0.05  # 20ç§’ã§ s â‰ˆ 0.63
s_dwell(t) = 1 - exp(-Î» * t)

# æ»åœ¨æ™‚é–“45.3ç§’ã€ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«78%ã€ã‚¯ãƒªãƒƒã‚¯ã‚ã‚Š
t = 45.3
d = 0.78
click = 1

s_t = s_dwell(t)  # â‰ˆ 0.90
score = 0.4 * click + 0.4 * s_t + 0.2 * d
# => 0.4 + 0.36 + 0.156 = 0.916
```

#### 3.1.2 æ˜ç¤ºçš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®å®šå¼åŒ–

æ˜ç¤ºçš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¯**ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç›´æ¥è©•ä¾¡ã‚’å…¥åŠ›**ã™ã‚‹ã€‚

**è©•ä¾¡ã‚¹ã‚³ã‚¢æ­£è¦åŒ–**:

$$
r_{\text{norm}} = \frac{r - r_{\min}}{r_{\max} - r_{\min}}
$$

5æ®µéšè©•ä¾¡ (1-5) ã®å ´åˆ: $r_{\text{norm}} = (r-1)/4$ã€‚

**ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ**: ã‚³ãƒ¡ãƒ³ãƒˆ $c$ ã‹ã‚‰æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ $S(c) \in [-1, 1]$ ã‚’æŠ½å‡º

$$
S(c) = \text{Classifier}_{\text{sentiment}}(\text{Embedding}(c))
$$

Transformerãƒ™ãƒ¼ã‚¹ã®ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†é¡å™¨ã‚’ä½¿ç”¨ã€‚

**Net Promoter Score (NPS)**: é¡§å®¢ãƒ­ã‚¤ãƒ¤ãƒ«ãƒ†ã‚£æŒ‡æ¨™

$$
\text{NPS} = \frac{\text{æ¨å¥¨è€… (9-10ç‚¹)} - \text{æ‰¹åˆ¤è€… (0-6ç‚¹)}}{\text{ç·å›ç­”æ•°}} \times 100
$$

**çµ±åˆãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚¹ã‚³ã‚¢**:

$$
f_{\text{explicit}}(r, S(c), \text{NPS}) = \alpha r_{\text{norm}} + \beta S(c) + \gamma \frac{\text{NPS}}{100}
$$

å…¸å‹çš„ãªé‡ã¿: $\alpha=0.5, \beta=0.3, \gamma=0.2$ã€‚

#### 3.1.3 ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é§†å‹•ã®ç¶™ç¶šå­¦ç¿’

ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ä½¿ã£ãŸãƒ¢ãƒ‡ãƒ«æ›´æ–°ã®æ•°å¼ã€‚

**ç›®çš„é–¢æ•°**: å…ƒã®è¨“ç·´æå¤±ã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æå¤±ã®é‡ã¿ä»˜ãå’Œ

$$
\mathcal{L}_{\text{total}}(\theta) = \mathcal{L}_{\text{train}}(\theta; \mathcal{D}_{\text{train}}) + \lambda \mathcal{L}_{\text{feedback}}(\theta; \mathcal{D}_{\text{feedback}})
$$

ã“ã“ã§ $\lambda > 0$ ã¯ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®é‡è¦åº¦ã€‚

**ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æå¤±**: ãƒ¦ãƒ¼ã‚¶ãƒ¼è©•ä¾¡ã¨ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ã®å·®

$$
\mathcal{L}_{\text{feedback}}(\theta; \mathcal{D}_{\text{feedback}}) = \frac{1}{|\mathcal{D}_{\text{feedback}}|} \sum_{(x,y,f) \in \mathcal{D}_{\text{feedback}}} \ell(f_\theta(x), y) \cdot w(f)
$$

ã“ã“ã§:
- $f_\theta(x)$ ã¯ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬
- $y$ ã¯æ­£è§£ãƒ©ãƒ™ãƒ«
- $f$ ã¯ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚¹ã‚³ã‚¢
- $w(f)$ ã¯ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«åŸºã¥ãé‡ã¿: $w(f) = f$ (é«˜è©•ä¾¡ã»ã©é‡è¦–)

**å‹¾é…é™ä¸‹æ›´æ–°**:

$$
\theta_{t+1} \leftarrow \theta_t - \eta \nabla_\theta \mathcal{L}_{\text{total}}(\theta_t)
$$

### 3.2 Active Learningå®Œå…¨ç‰ˆ

#### 3.2.1 ä¸ç¢ºå®Ÿæ€§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ç†è«–

Active Learningã®ç›®æ¨™: **æœ€å°ã®ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚³ã‚¹ãƒˆã§æœ€å¤§ã®æ€§èƒ½å‘ä¸Š**ã‚’é”æˆã™ã‚‹ã€‚

**ä¸ç¢ºå®Ÿæ€§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**: ãƒ¢ãƒ‡ãƒ«ãŒæœ€ã‚‚è‡ªä¿¡ã‚’æŒã¦ãªã„ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠ

$$
x^* = \arg\max_{x \in \mathcal{U}} U(x; \theta)
$$

ã“ã“ã§ $\mathcal{U}$ ã¯ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿ã€$U(x; \theta)$ ã¯ä¸ç¢ºå®Ÿæ€§æŒ‡æ¨™ã€‚

**3ã¤ã®ä¸ç¢ºå®Ÿæ€§æŒ‡æ¨™**:

1. **Least Confidence**: æœ€å¤§ç¢ºç‡ãŒä½ã„ã‚µãƒ³ãƒ—ãƒ«

$$
U_{\text{LC}}(x; \theta) = 1 - \max_c p_\theta(c | x)
$$

2. **Margin Sampling**: ä¸Šä½2ã‚¯ãƒ©ã‚¹ã®ç¢ºç‡å·®ãŒå°ã•ã„ã‚µãƒ³ãƒ—ãƒ«

$$
U_{\text{M}}(x; \theta) = - \left( p_\theta(c_1 | x) - p_\theta(c_2 | x) \right)
$$

ã“ã“ã§ $c_1, c_2$ ã¯ç¢ºç‡ä¸Šä½2ã‚¯ãƒ©ã‚¹ã€‚

3. **Entropy**: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãŒæœ€å¤§ã®ã‚µãƒ³ãƒ—ãƒ«

$$
U_{\text{Ent}}(x; \theta) = H(p_\theta(\cdot | x)) = - \sum_{c=1}^C p_\theta(c | x) \log p_\theta(c | x)
$$

**ã©ã‚Œã‚’ä½¿ã†ã¹ãã‹ï¼Ÿ**

| æŒ‡æ¨™ | é•·æ‰€ | çŸ­æ‰€ | é©ç”¨å ´é¢ |
|:-----|:-----|:-----|:---------|
| Least Confidence | è¨ˆç®—ãŒè»½ã„ | 2ç•ªç›®ã®ç¢ºç‡ã‚’ç„¡è¦– | 2ã‚¯ãƒ©ã‚¹åˆ†é¡ |
| Margin | æ±ºå®šå¢ƒç•Œã‚’é‡è¦– | å¤šã‚¯ãƒ©ã‚¹ã§æƒ…å ±æå¤± | 2ã‚¯ãƒ©ã‚¹ or ãƒãƒ©ãƒ³ã‚¹è‰¯å¥½ |
| Entropy | å…¨ã‚¯ãƒ©ã‚¹ã®æƒ…å ±ã‚’ä½¿ã† | è¨ˆç®—ã‚³ã‚¹ãƒˆã‚„ã‚„é«˜ | å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ |

**æ•°å€¤æ¤œè¨¼** (Julia):

```julia
# 3ã‚¯ãƒ©ã‚¹åˆ†é¡ã®ä¾‹
p = [0.6, 0.3, 0.1]  # ã‚¯ãƒ©ã‚¹ç¢ºç‡

# Least Confidence
U_LC = 1 - maximum(p)  # => 0.4

# Margin
p_sorted = sort(p, rev=true)
U_M = -(p_sorted[1] - p_sorted[2])  # => -(0.6 - 0.3) = -0.3

# Entropy
H(p) = -sum(p .* log.(p .+ 1e-10))
U_Ent = H(p)  # => 0.897

println("LC: $U_LC, Margin: $U_M, Entropy: $U_Ent")
```

#### 3.2.2 MSAL (Maximally Separated Active Learning)

arXiv:2411.17444 "Maximally Separated Active Learning" (Nov 2024)[^1] ã§ææ¡ˆã•ã‚ŒãŸæ‰‹æ³•ã€‚

**èª²é¡Œ**: å¾“æ¥ã®ä¸ç¢ºå®Ÿæ€§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¯**é¡ä¼¼ã—ãŸã‚µãƒ³ãƒ—ãƒ«ã°ã‹ã‚Šé¸ã‚“ã§ã—ã¾ã†** (sampling bias)ã€‚

**è§£æ±ºç­–**: ä¸ç¢ºå®Ÿæ€§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«**å¤šæ§˜æ€§åˆ¶ç´„**ã‚’è¿½åŠ ã€‚

**MSALç›®çš„é–¢æ•°**:

$$
x^* = \arg\max_{x \in \mathcal{U}} \left[ U(x; \theta) + \alpha \cdot D(x; \mathcal{L}) \right]
$$

ã“ã“ã§:
- $U(x; \theta)$ ã¯ä¸ç¢ºå®Ÿæ€§ã‚¹ã‚³ã‚¢
- $D(x; \mathcal{L})$ ã¯æ—¢ã«ãƒ©ãƒ™ãƒ«ä»˜ã‘ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ $\mathcal{L}$ ã¨ã®å¤šæ§˜æ€§
- $\alpha > 0$ ã¯å¤šæ§˜æ€§ã®é‡è¦åº¦

**å¤šæ§˜æ€§ã‚¹ã‚³ã‚¢**: æœ€è¿‘å‚ã¨ã®è·é›¢

$$
D(x; \mathcal{L}) = \min_{x' \in \mathcal{L}} \left\| \phi(x) - \phi(x') \right\|_2
$$

ã“ã“ã§ $\phi(x)$ ã¯Embedding (ä¾‹: BERTæœ€çµ‚å±¤)ã€‚

**Equiangular Prototypes**: MSALã¯å„ã‚¯ãƒ©ã‚¹ã®**ç­‰è§’è¶…çƒé¢ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—**ã‚’ä½¿ã†ã€‚

$C$ ã‚¯ãƒ©ã‚¹ã®å ´åˆã€$d$ æ¬¡å…ƒçƒé¢ä¸Šã« $C$ å€‹ã®ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã‚’ç­‰é–“éš”é…ç½®:

$$
\mathbf{p}_c = r \cdot \mathbf{v}_c, \quad \mathbf{v}_c \cdot \mathbf{v}_{c'} = \begin{cases} 1 & c = c' \\ -\frac{1}{C-1} & c \neq c' \end{cases}
$$

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

```julia
function msal_select_batch(model, unlabeled_pool, labeled_data, batch_size, Î±=0.5)
    selected = []

    for _ in 1:batch_size
        scores = []
        for x in unlabeled_pool
            # ä¸ç¢ºå®Ÿæ€§ã‚¹ã‚³ã‚¢
            U = entropy(model(x))

            # å¤šæ§˜æ€§ã‚¹ã‚³ã‚¢: æ—¢é¸æŠã‚µãƒ³ãƒ—ãƒ«ã¨ã®æœ€å°è·é›¢
            Ï†_x = embedding(x)
            D = minimum([norm(Ï†_x - embedding(x')) for x' in labeled_data âˆª selected])

            # çµ±åˆã‚¹ã‚³ã‚¢
            score = U + Î± * D
            push!(scores, (x, score))
        end

        # æœ€é«˜ã‚¹ã‚³ã‚¢ã‚’é¸æŠ
        x_best = argmax(s -> s[2], scores)[1]
        push!(selected, x_best)
        unlabeled_pool = filter(x -> x != x_best, unlabeled_pool)
    end

    return selected
end
```

#### 3.2.3 Human-in-the-Loop (HITL) è¨­è¨ˆ

arXiv:2409.09467 "Keeping Humans in the Loop" (Sep 2024)[^2] ã§è­°è«–ã•ã‚ŒãŸãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã€‚

**HITLã®3åŸå‰‡**:

1. **Selective Annotation**: äººé–“ã¯é›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã®ã¿ã‚¢ãƒãƒ†ãƒ¼ãƒˆ
2. **Quality Control**: è¤‡æ•°ã‚¢ãƒãƒ†ãƒ¼ã‚¿ãƒ¼é–“ã®ä¸€è‡´åº¦ã‚’æ¸¬å®š
3. **Feedback Integration**: ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å³åº§ã«è¨“ç·´ã«åæ˜ 

**ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å“è³ªã®å®šé‡åŒ–**: Cohen's Kappa

$$
\kappa = \frac{p_o - p_e}{1 - p_e}
$$

ã“ã“ã§:
- $p_o$ ã¯è¦³æ¸¬ä¸€è‡´ç‡
- $p_e$ ã¯å¶ç„¶ã®ä¸€è‡´ç‡

$\kappa > 0.6$ ã§ã€Œå®Ÿè³ªçš„ãªä¸€è‡´ã€ã€$\kappa > 0.8$ ã§ã€Œã»ã¼å®Œå…¨ãªä¸€è‡´ã€ã€‚

**Disagreement Resolution**: 2äººã®ã‚¢ãƒãƒ†ãƒ¼ã‚¿ãƒ¼ãŒç•°ãªã‚‹ãƒ©ãƒ™ãƒ«ã‚’ä»˜ã‘ãŸå ´åˆ

```julia
function resolve_disagreement(x, label_A, label_B, model)
    if label_A == label_B
        return label_A  # ä¸€è‡´
    else
        # ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’å‚è€ƒã«å°‚é–€å®¶ãŒåˆ¤æ–­
        pred = model(x)
        println("Disagreement: A=$label_A, B=$label_B, Model=$pred")
        return expert_review(x, label_A, label_B, pred)
    end
end
```

**å°‚é–€å®¶ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°**:

| æ¡ä»¶ | ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ |
|:-----|:----------|
| $\kappa < 0.6$ | å…¨ã‚µãƒ³ãƒ—ãƒ«ã‚’å°‚é–€å®¶ãƒ¬ãƒ“ãƒ¥ãƒ¼ |
| $0.6 \leq \kappa < 0.8$ | Disagreementã®ã¿ãƒ¬ãƒ“ãƒ¥ãƒ¼ |
| $\kappa \geq 0.8$ | ãƒ¬ãƒ“ãƒ¥ãƒ¼ä¸è¦ |

#### 3.2.4 âš”ï¸ Boss Battle: Active LearningåæŸä¿è¨¼

arXiv:2110.15784 "Convergence of Uncertainty Sampling" (Oct 2021)[^3] ã®å®šç†ã‚’å®Œå…¨ç†è§£ã™ã‚‹ã€‚

**å®šç† (Simplified)**: ã‚ã‚‹æ¡ä»¶ä¸‹ã§ã€ä¸ç¢ºå®Ÿæ€§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¯**æœ€é©æ±ºå®šå¢ƒç•Œã«åæŸ**ã™ã‚‹ã€‚

**ä»®å®š**:
1. ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p(x, y)$ ã¯å›ºå®š
2. ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹ $\mathcal{F}$ ã¯ååˆ†ãªè¡¨ç¾åŠ›ã‚’æŒã¤ (VCæ¬¡å…ƒ $d_{VC} < \infty$)
3. ã‚µãƒ³ãƒ—ãƒ«é¸æŠã¯æ±ºå®šå¢ƒç•Œä»˜è¿‘ã«é›†ä¸­

**åæŸãƒ¬ãƒ¼ãƒˆ**: $T$ ãƒ©ã‚¦ãƒ³ãƒ‰å¾Œã®èª¤å·®

$$
\mathbb{E}[\text{Error}(\theta_T)] \leq \mathcal{O}\left( \frac{d_{VC}}{T} \log T \right)
$$

ã“ã“ã§ $d_{VC}$ ã¯VCæ¬¡å…ƒã€‚

**è¨¼æ˜ã®ã‚¹ã‚±ãƒƒãƒ**:

1. **æ±ºå®šå¢ƒç•Œã®å®šç¾©**: $\{ x : p_\theta(c_1 | x) = p_\theta(c_2 | x) \}$
2. **ä¸ç¢ºå®Ÿæ€§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®æ€§è³ª**: Entropyæœ€å¤§ = æ±ºå®šå¢ƒç•Œä¸Š
3. **PACå­¦ç¿’ç†è«–**: $N$ ã‚µãƒ³ãƒ—ãƒ«ã§èª¤å·® $\epsilon$ ä»¥ä¸‹ã«ãªã‚‹ç¢ºç‡

$$
P(\text{Error}(\theta) > \epsilon) \leq 2 \mathcal{M}(\mathcal{F}, N) e^{-N \epsilon^2 / 8}
$$

ã“ã“ã§ $\mathcal{M}(\mathcal{F}, N)$ ã¯æˆé•·é–¢æ•°ã€‚

4. **VCæ¬¡å…ƒã¨ã®é–¢ä¿‚**: $\mathcal{M}(\mathcal{F}, N) \leq N^{d_{VC}}$
5. **çµè«–**: $N = \mathcal{O}(d_{VC} / \epsilon^2 \log(1/\delta))$ ã‚µãƒ³ãƒ—ãƒ«ã§ååˆ†

**æ•°å€¤æ¤œè¨¼** (Julia):

```julia
# ç·šå½¢åˆ†é¡å™¨ (VCæ¬¡å…ƒ = d+1)
d = 10  # ç‰¹å¾´é‡æ¬¡å…ƒ
d_VC = d + 1

# ç›®æ¨™èª¤å·® Îµ = 0.01, ç¢ºç‡ Î´ = 0.05
Îµ = 0.01
Î´ = 0.05

# å¿…è¦ã‚µãƒ³ãƒ—ãƒ«æ•°
N_required = ceil(Int, d_VC / Îµ^2 * log(1/Î´))
# => ç´„ 32,919 ã‚µãƒ³ãƒ—ãƒ«

println("VCæ¬¡å…ƒ: $d_VC")
println("å¿…è¦ã‚µãƒ³ãƒ—ãƒ«æ•°: $N_required")
```

**ãƒœã‚¹æ’ƒç ´ã®è¨¼**: ä¸ç¢ºå®Ÿæ€§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®åæŸãƒ¬ãƒ¼ãƒˆ $\mathcal{O}(d_{VC}/T \log T)$ ã‚’å°å‡ºã—ã€æ•°å€¤æ¤œè¨¼ã§ç¢ºèªã—ãŸã€‚

### 3.3 ãƒ¢ãƒ‡ãƒ«æ”¹å–„ã‚µã‚¤ã‚¯ãƒ«ã®æ•°å¼

#### 3.3.1 Continuous Learning (ç¶™ç¶šå­¦ç¿’)

**å®šç¾©**: æœ¬ç•ªç’°å¢ƒã§ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ä½¿ã£ã¦**ãƒ¢ãƒ‡ãƒ«ã‚’ç¶™ç¶šçš„ã«æ›´æ–°**ã™ã‚‹ã€‚

**Naive Approach** (ç ´æ»…çš„å¿˜å´):

$$
\theta_{t+1} \leftarrow \arg\min_\theta \mathcal{L}(\theta; \mathcal{D}_{\text{new}})
$$

å•é¡Œ: å¤ã„ãƒ‡ãƒ¼ã‚¿ $\mathcal{D}_{\text{old}}$ ã®æ€§èƒ½ãŒåŠ£åŒ– (Catastrophic Forgetting)ã€‚

**Elastic Weight Consolidation (EWC)**: é‡è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¤‰åŒ–ã‚’æŠ‘åˆ¶

$$
\mathcal{L}_{\text{EWC}}(\theta) = \mathcal{L}(\theta; \mathcal{D}_{\text{new}}) + \frac{\lambda}{2} \sum_i F_i (\theta_i - \theta_{i,\text{old}})^2
$$

ã“ã“ã§:
- $F_i$ ã¯Fisheræƒ…å ±é‡: $F_i = \mathbb{E}_{x \sim \mathcal{D}_{\text{old}}} \left[ \left( \frac{\partial \log p_{\theta_{\text{old}}}(y|x)}{\partial \theta_i} \right)^2 \right]$
- $\lambda > 0$ ã¯æ­£å‰‡åŒ–å¼·åº¦

**Experience Replay**: å¤ã„ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒãƒ•ã‚¡ã‚’ä¿æŒ

$$
\mathcal{L}_{\text{Replay}}(\theta) = \mathcal{L}(\theta; \mathcal{D}_{\text{new}} \cup \mathcal{D}_{\text{buffer}})
$$

ã“ã“ã§ $\mathcal{D}_{\text{buffer}}$ ã¯å¤ã„ãƒ‡ãƒ¼ã‚¿ã®ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒ«ã€‚

**ã©ã¡ã‚‰ã‚’ä½¿ã†ã¹ãã‹ï¼Ÿ**

| æ‰‹æ³• | ãƒ¡ãƒ¢ãƒª | è¨ˆç®—é‡ | æ€§èƒ½ | é©ç”¨å ´é¢ |
|:-----|:------|:-------|:-----|:---------|
| EWC | å° (Fisheræƒ…å ±é‡ã®ã¿) | ä¸­ | ä¸­ | ãƒ¡ãƒ¢ãƒªåˆ¶ç´„ |
| Replay | å¤§ (ãƒãƒƒãƒ•ã‚¡ä¿æŒ) | å¤§ | é«˜ | é«˜æ€§èƒ½å„ªå…ˆ |

#### 3.3.2 Hidden Feedback Loop Effect

arXiv:2405.02726 "Mathematical Model of the Hidden Feedback Loop Effect"[^4] ã§è­°è«–ã•ã‚ŒãŸå•é¡Œã€‚

**å•é¡Œ**: ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ãŒæ¬¡ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹**éš ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—**ã€‚

**æ•°å¼ãƒ¢ãƒ‡ãƒ«**: æ™‚åˆ» $t$ ã§ã®ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p_t(x, y)$ ãŒå‰å›ã®ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ã«ä¾å­˜

$$
p_{t+1}(x, y) = (1 - \alpha) p_{\text{true}}(x, y) + \alpha \cdot \delta_{y = \hat{y}_t(x)} p_t(x)
$$

ã“ã“ã§:
- $p_{\text{true}}(x, y)$ ã¯çœŸã®åˆ†å¸ƒ
- $\hat{y}_t(x)$ ã¯æ™‚åˆ» $t$ ã®ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬
- $\alpha \in [0, 1]$ ã¯ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å¼·åº¦

**çµæœ**: $\alpha > 0.5$ ã§ãƒ¢ãƒ‡ãƒ«ãŒ**è‡ªå·±å¼·åŒ–ãƒã‚¤ã‚¢ã‚¹**ã«é™¥ã‚‹ã€‚

**æ•°å€¤ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³** (Julia):

```julia
# 2ã‚¯ãƒ©ã‚¹åˆ†é¡ã®ä¾‹
p_true = [0.5, 0.5]  # çœŸã®åˆ†å¸ƒ
Î± = 0.6  # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å¼·åº¦

p_t = copy(p_true)
for t in 1:10
    # ãƒ¢ãƒ‡ãƒ«ã¯å¸¸ã«ã‚¯ãƒ©ã‚¹1ã‚’äºˆæ¸¬ (simplified)
    y_pred = 1

    # æ¬¡ã®åˆ†å¸ƒ: ã‚¯ãƒ©ã‚¹1ãŒå¢—ãˆã‚‹
    p_t = (1 - Î±) .* p_true + Î± .* [y_pred == 1 ? 1.0 : 0.0, y_pred == 2 ? 1.0 : 0.0]

    println("t=$t: p(y=1)=$(p_t[1])")
end
# => t=10: p(y=1) â‰ˆ 0.94 (å¤§ããåã‚‹)
```

**å¯¾ç­–**: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å¼·åº¦ $\alpha$ ã‚’åˆ¶å¾¡ or ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§çœŸã®åˆ†å¸ƒã‚’ä¿æŒã€‚

#### 3.3.3 RLHF (Reinforcement Learning from Human Feedback)

arXiv:2504.12501 "RLHF" (2025)[^5] ã§ä½“ç³»åŒ–ã•ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é§†å‹•è¨“ç·´ã€‚

**3ã‚¹ãƒ†ãƒƒãƒ—**:

1. **Supervised Fine-tuning (SFT)**: äººé–“ã®ä¾‹ã§äº‹å‰è¨“ç·´

$$
\theta_{\text{SFT}} \leftarrow \arg\min_\theta \mathbb{E}_{(x,y) \sim \mathcal{D}_{\text{demo}}} [- \log p_\theta(y | x)]
$$

2. **Reward Model Training**: äººé–“ã®å¥½ã¿ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–

$$
r_\phi(x, y) = \mathbb{E}_{\text{human}}[\text{preference}(x, y)]
$$

è¨“ç·´ãƒ‡ãƒ¼ã‚¿: $(x, y_w, y_l)$ (win/lose pair)

$$
\mathcal{L}_{\text{RM}}(\phi) = - \mathbb{E}_{(x,y_w,y_l)} \left[ \log \sigma(r_\phi(x, y_w) - r_\phi(x, y_l)) \right]
$$

3. **RL Fine-tuning**: Rewardæœ€å¤§åŒ–

$$
\theta_{\text{RL}} \leftarrow \arg\max_\theta \mathbb{E}_{x \sim \mathcal{D}, y \sim p_\theta(\cdot|x)} \left[ r_\phi(x, y) - \beta \log \frac{p_\theta(y|x)}{p_{\text{ref}}(y|x)} \right]
$$

ã“ã“ã§ $\beta > 0$ ã¯KLæ­£å‰‡åŒ–ä¿‚æ•°ã€$p_{\text{ref}}$ ã¯å‚ç…§ãƒ¢ãƒ‡ãƒ« (SFT)ã€‚

**PPO (Proximal Policy Optimization)** ã§RLã‚’å®‰å®šåŒ–:

$$
\mathcal{L}_{\text{PPO}}(\theta) = \mathbb{E}_t \left[ \min \left( \frac{p_\theta(a_t|s_t)}{p_{\theta_{\text{old}}}(a_t|s_t)} A_t, \text{clip}(\cdot, 1-\epsilon, 1+\epsilon) A_t \right) \right]
$$

ã“ã“ã§ $A_t$ ã¯Advantageã€$\epsilon=0.2$ ã¯å…¸å‹å€¤ã€‚

### 3.4 E2Eã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ç†è«–

#### 3.4.1 ã‚µãƒ¼ãƒ“ã‚¹é–“é€šä¿¡ã®æ•°å¼

**REST API**: ãƒªã‚¯ã‚¨ã‚¹ãƒˆ $r$ ã«å¯¾ã™ã‚‹ãƒ¬ã‚¹ãƒãƒ³ã‚¹ $s$

$$
s = f_{\text{API}}(r; \theta)
$$

**ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·**: å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å‡¦ç†æ™‚é–“ã®å’Œ

$$
t_{\text{total}} = t_{\text{gateway}} + t_{\text{inference}} + t_{\text{postprocess}}
$$

**ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**: å˜ä½æ™‚é–“ã‚ãŸã‚Šã®å‡¦ç†æ•°

$$
\text{Throughput} = \frac{1}{t_{\text{total}} + t_{\text{queue}}}
$$

ã“ã“ã§ $t_{\text{queue}}$ ã¯ã‚­ãƒ¥ãƒ¼ã‚¤ãƒ³ã‚°æ™‚é–“ã€‚

**Little's Law**: å¹³å‡ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•° $L$ã€å¹³å‡åˆ°ç€ç‡ $\lambda$ã€å¹³å‡å‡¦ç†æ™‚é–“ $W$

$$
L = \lambda W
$$

ä¾‹: $\lambda = 100$ req/sã€$W = 0.05$ s â†’ $L = 5$ ä¸¦è¡Œãƒªã‚¯ã‚¨ã‚¹ãƒˆã€‚

#### 3.4.2 Circuit Breakerç†è«–

**çŠ¶æ…‹é·ç§»**:

```
Closed â†’ (å¤±æ•—ç‡ > threshold) â†’ Open â†’ (timeoutçµŒé) â†’ Half-Open â†’ (æˆåŠŸ) â†’ Closed
```

**æ•°å¼ãƒ¢ãƒ‡ãƒ«**: å¤±æ•—ç‡ $p_{\text{fail}}$ã€é–¾å€¤ $\theta_{\text{CB}}$

$$
\text{State} = \begin{cases}
\text{Open} & p_{\text{fail}} > \theta_{\text{CB}} \\
\text{Closed} & p_{\text{fail}} \leq \theta_{\text{CB}}
\end{cases}
$$

**Exponential Backoff**: OpençŠ¶æ…‹ã‹ã‚‰ã®å¾©å¸°æ™‚é–“

$$
t_{\text{wait}} = t_0 \cdot 2^n
$$

ã“ã“ã§ $n$ ã¯å¤±æ•—å›æ•°ã€$t_0$ ã¯åˆæœŸå¾…ã¡æ™‚é–“ã€‚

#### 3.4.3 Rate Limiting (Token Bucket)

**Token Bucket Algorithm**: å®¹é‡ $B$ã€è£œå……ãƒ¬ãƒ¼ãƒˆ $r$

$$
\text{tokens}(t) = \min(B, \text{tokens}(t-1) + r \Delta t - c)
$$

ã“ã“ã§ $c$ ã¯ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§æ¶ˆè²»ã—ãŸãƒˆãƒ¼ã‚¯ãƒ³æ•°ã€‚

**è¨±å¯æ¡ä»¶**:

$$
\text{allow}(c) = \begin{cases}
\text{true} & \text{tokens} \geq c \\
\text{false} & \text{tokens} < c
\end{cases}
$$

**æ•°å€¤ä¾‹**:

```julia
# Token Bucket ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
B = 100  # ãƒã‚±ãƒƒãƒˆå®¹é‡
r = 10   # è£œå……ãƒ¬ãƒ¼ãƒˆ (tokens/sec)

tokens = B
t = 0

for i in 1:15
    # 1ç§’ã”ã¨ã«7ãƒˆãƒ¼ã‚¯ãƒ³è¦æ±‚
    t += 1
    tokens = min(B, tokens + r * 1 - 7)

    println("t=$t: tokens=$tokens")
end
# => t=15: tokens=145 - 105 = 40 (ãƒã‚±ãƒƒãƒˆå®¹é‡ã§ã‚­ãƒ£ãƒƒãƒ—)
```

:::message
**é€²æ—: 50%å®Œäº†ï¼** ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—æ•°å¼ã¨Active Learningç†è«–ã‚’ç¿’å¾—ã—ãŸã€‚æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ã‚¯ãƒªã‚¢ï¼
:::

---

### 3.5 Direct Preference Optimization (DPO)ã®æ•°å­¦çš„åŸºç¤

RLHF (Reinforcement Learning from Human Feedback) ã¯3æ®µéšã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ (SFT â†’ Reward Model â†’ PPO) ã‚’å¿…è¦ã¨ã—ã€è¨“ç·´ãŒè¤‡é›‘ã§ä¸å®‰å®šã ã£ãŸã€‚**DPO (Direct Preference Optimization)** [^5] ã¯ã€Reward Modelã¨RL finetuningã‚’**å˜ä¸€ã®æ•™å¸«ã‚ã‚Šå­¦ç¿’**ã«çµ±åˆã—ãŸé©å‘½çš„æ‰‹æ³•ã ã€‚

#### 3.5.1 RLHFã®ç†è«–çš„å†å®šå¼åŒ–

æ¨™æº–çš„ãªRLHFç›®çš„é–¢æ•°:

$$
\max_\theta \mathbb{E}_{x \sim \mathcal{D}, y \sim p_\theta(\cdot|x)} \left[ r_\phi(x, y) - \beta \log \frac{p_\theta(y|x)}{p_{\text{ref}}(y|x)} \right]
$$

ã“ã“ã§ $r_\phi(x,y)$ ã¯å­¦ç¿’ã•ã‚ŒãŸReward Modelã€$\beta > 0$ ã¯KL penaltyä¿‚æ•°ã€‚

**Key Insight**: æœ€é©æ–¹ç­– $p^*_\theta$ ã¯è§£æçš„ã«è§£ã‘ã‚‹:

$$
p^*_\theta(y|x) = \frac{1}{Z(x)} p_{\text{ref}}(y|x) \exp\left( \frac{1}{\beta} r_\phi(x,y) \right)
$$

ã“ã“ã§ $Z(x) = \sum_y p_{\text{ref}}(y|x) \exp(r_\phi(x,y)/\beta)$ ã¯åˆ†é…é–¢æ•°ã€‚

ã“ã‚Œã‚’é€†ã«è§£ãã¨ã€Reward ModelãŒæ–¹ç­–ã§è¡¨ç¾ã§ãã‚‹:

$$
r_\phi(x,y) = \beta \log \frac{p^*_\theta(y|x)}{p_{\text{ref}}(y|x)} + \beta \log Z(x)
$$

#### 3.5.2 Bradley-Terry Preference Modelã¨DPO Loss

äººé–“ã®é¸å¥½ãƒ‡ãƒ¼ã‚¿ã¯ $(x, y_w, y_l)$ (chosen vs rejected)ã€‚Bradley-Terry Modelã§é¸å¥½ç¢ºç‡ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–:

$$
p(y_w \succ y_l | x) = \frac{\exp(r_\phi(x, y_w))}{\exp(r_\phi(x, y_w)) + \exp(r_\phi(x, y_l))} = \sigma(r_\phi(x, y_w) - r_\phi(x, y_l))
$$

Reward Modelã®é–‰å½¢å¼è§£ (3.5.1ç¯€) ã‚’ä»£å…¥ã™ã‚‹ã¨:

$$
p(y_w \succ y_l | x) = \sigma\left( \beta \log \frac{p_\theta(y_w|x)}{p_{\text{ref}}(y_w|x)} - \beta \log \frac{p_\theta(y_l|x)}{p_{\text{ref}}(y_l|x)} \right)
$$

**åˆ†é…é–¢æ•° $Z(x)$ ãŒæ¶ˆãˆãŸï¼** ã“ã‚ŒãŒDPOã®æ ¸å¿ƒçš„ç™ºè¦‹ã ã€‚

**DPO Loss**:

$$
\mathcal{L}_{\text{DPO}}(\theta) = - \mathbb{E}_{(x,y_w,y_l)} \left[ \log \sigma\left( \beta \log \frac{p_\theta(y_w|x)}{p_{\text{ref}}(y_w|x)} - \beta \log \frac{p_\theta(y_l|x)}{p_{\text{ref}}(y_l|x)} \right) \right]
$$

Reward Modelã‚‚ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚‚ä¸è¦ â€” **ç›´æ¥LMã®å°¤åº¦æ¯”ã‚’æœ€é©åŒ–**ã™ã‚‹ã€‚

#### 3.5.3 DPO vs RLHF: ç†è«–çš„æ¯”è¼ƒ

| é …ç›® | RLHF | DPO |
|:-----|:-----|:----|
| **è¨“ç·´æ®µéš** | 3æ®µéš (SFTâ†’RMâ†’PPO) | 1æ®µéš (æ•™å¸«ã‚ã‚Šå­¦ç¿’) |
| **Reward Model** | å¿…è¦ ($r_\phi$ ã‚’æ˜ç¤ºçš„ã«å­¦ç¿’) | ä¸è¦ (æš—é»™çš„ã«æ–¹ç­–ã§è¡¨ç¾) |
| **æœ€é©åŒ–** | RL (PPO, Actor-Critic) | Cross-Entropy (MLE) |
| **å®‰å®šæ€§** | ä½ (RLç‰¹æœ‰ã®ä¸å®‰å®šæ€§) | é«˜ (æ•™å¸«ã‚ã‚Šå­¦ç¿’) |
| **è¨ˆç®—ã‚³ã‚¹ãƒˆ** | é«˜ (ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°+RMæ¨è«–) | ä½ (å°¤åº¦è¨ˆç®—ã®ã¿) |
| **æ•°å­¦çš„åŸºç¤** | RLç†è«– | å¤‰åˆ†æ¨è«– + Bradley-Terry |

**å®Ÿé¨“çµæœ** (Rafailov et al. 2023 [^5]):
- Anthropic Helpful-Harmlessãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: DPO = RLHF-PPO (win rate 50-55%)
- TL;DRè¦ç´„ã‚¿ã‚¹ã‚¯: DPO preferred by humans 58% vs PPO 42%
- GPT-4è©•ä¾¡: DPOç”Ÿæˆ > PPOç”Ÿæˆ (60% win rate)

#### 3.5.4 Productionç’°å¢ƒã§ã®RLUF (Reinforcement Learning from User Feedback)

å®ŸProductionã§ã¯ã€å°‚é–€å®¶ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ãªã**å®Ÿãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æš—é»™çš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯**ã‚’æ´»ç”¨ã™ã‚‹ã€‚arXiv:2505.14946 "Reinforcement Learning from User Feedback" [^6] ãŒææ¡ˆã—ãŸRLUFãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€‚

**èª²é¡Œ**:
1. **Binary Feedback**: ãƒ¦ãƒ¼ã‚¶ãƒ¼åå¿œã¯ğŸ‘ğŸ‘emojiç¨‹åº¦ (preference pairã§ã¯ãªã„)
2. **Sparse**: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç‡ < 5%
3. **Adversarial**: ãƒœãƒƒãƒˆã‚„æ‚ªæ„ã‚ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼

**RLUFç›®çš„é–¢æ•°**:

$$
\mathcal{L}_{\text{RLUF}}(\theta) = \mathbb{E}_{x,y,f} \left[ f \cdot \log p_\theta(y|x) + (1-f) \cdot \log(1 - p_\theta(y|x)) \right]
$$

ã“ã“ã§ $f \in \{0,1\}$ ã¯binary feedbackã€‚

**Adversarial Filtering**: ç•°å¸¸æ¤œå‡ºã§ãƒã‚¤ã‚ºé™¤å»

$$
\text{Keep}(x,y,f) = \mathbb{1}\left[ \left| f - \mathbb{E}_{(x',y') \sim N(x,y)}[f'] \right| < \tau \right]
$$

è¿‘å‚ãƒ‡ãƒ¼ã‚¿ã¨æ¯”è¼ƒã—ã¦å¤–ã‚Œå€¤ã‚’é™¤å¤– ($\tau=0.3$ ãŒå…¸å‹å€¤)ã€‚

**æ•°å€¤ä¾‹** (Julia):

```julia
# Binary feedback ã‹ã‚‰ preference pair ã‚’ç”Ÿæˆ
positive_responses = [(x1, y1), (x2, y2), ...]  # f=1
negative_responses = [(x3, y3), (x4, y4), ...]  # f=0

# Construct pairs: (x, y_w=positive, y_l=negative)
pairs = [(x, y_pos, y_neg) for (x, y_pos) in positive_responses, (_, y_neg) in negative_responses if same_prompt(x, _)]

# DPO training with constructed pairs
loss_dpo = dpo_loss(model, pairs, Î²=0.1)
```

#### 3.5.5 OpenRLHF Framework (2024)

arXiv:2405.11143 "OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework" [^7] ã¯ã€Production-grade RLHFã®åŒ…æ‹¬çš„å®Ÿè£…ã‚’æä¾›ã€‚

**ã‚µãƒãƒ¼ãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:
- **SFT** (Supervised Fine-Tuning)
- **DPO** (Direct Preference Optimization)
- **RM** (Reward Model)
- **PPO** (Proximal Policy Optimization)
- **PRM** (Process Reward Model) â€” ä¸­é–“ã‚¹ãƒ†ãƒƒãƒ—ã®è©•ä¾¡

**ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**:
- Ray + vLLMçµ±åˆ â†’ 70B modelã®åˆ†æ•£è¨“ç·´
- DeepSpeed ZeRO-3 â†’ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
- Hybrid Engine â†’ è¨“ç·´ä¸­ã®ãƒãƒƒãƒæ¨è«–æœ€é©åŒ–

**Benchmark** (7B model, A100 x 8):

| Method | Throughput (samples/s) | GPU Memory (GB/GPU) |
|:-------|:----------------------|:--------------------|
| Naive PPO | 12 | 78 |
| OpenRLHF PPO | 47 | 42 |
| DPO (OpenRLHF) | 124 | 28 |

DPOã¯**10å€ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ + åŠåˆ†ã®ãƒ¡ãƒ¢ãƒª** â€” Productionç’°å¢ƒã§ã®å®Ÿç”¨æ€§ãŒé«˜ã„ã€‚

### 3.6 MLOpsæˆç†Ÿåº¦ãƒ¢ãƒ‡ãƒ«ã¨ç¶™ç¶šçš„æ”¹å–„

#### 3.6.1 MLOps Maturity Levels

Google Cloud [^8] ãŒææ¡ˆã™ã‚‹æˆç†Ÿåº¦ãƒ¢ãƒ‡ãƒ«:

**Level 0: Manual** (æ‰‹ä½œæ¥­)
- Data scientistä¸»å°
- Jupyter notebookãƒ™ãƒ¼ã‚¹
- ãƒ‡ãƒ—ãƒ­ã‚¤ã¯æ‰‹å‹•
- ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãªã—

**Level 1: ML Pipeline Automation** (è¨“ç·´è‡ªå‹•åŒ–)
- CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
- è‡ªå‹•è¨“ç·´ãƒˆãƒªã‚¬ãƒ¼
- ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒª
- åŸºæœ¬çš„ãªãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

**Level 2: CI/CD Pipeline Automation** (å®Œå…¨è‡ªå‹•åŒ–)
- è‡ªå‹•å†è¨“ç·´
- A/Bãƒ†ã‚¹ãƒˆçµ±åˆ
- Feature Store
- ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º â†’ è‡ªå‹•ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

**æˆç†Ÿåº¦ã®æ•°å­¦çš„æŒ‡æ¨™**:

$$
\text{MLOps Score} = w_1 \cdot \text{Automation\%} + w_2 \cdot \text{TestCoverage} + w_3 \cdot \frac{1}{\text{MTTR}} + w_4 \cdot \text{DeployFreq}
$$

ã“ã“ã§:
- Automation%: æ‰‹ä½œæ¥­ã‚¹ãƒ†ãƒƒãƒ—ã®å‰²åˆ (0-100%)
- TestCoverage: ãƒ‡ãƒ¼ã‚¿+ãƒ¢ãƒ‡ãƒ«+ã‚³ãƒ¼ãƒ‰ã®ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸
- MTTR (Mean Time To Recovery): éšœå®³ã‹ã‚‰å¾©æ—§ã¾ã§ã®å¹³å‡æ™‚é–“
- DeployFreq: ãƒ‡ãƒ—ãƒ­ã‚¤é »åº¦ (deploys/week)

å…¸å‹çš„ãªé‡ã¿: $w_1=0.3, w_2=0.2, w_3=0.25, w_4=0.25$ã€‚

#### 3.6.2 Data Drift Detection ã®ç†è«–

**å®šç¾©**: æœ¬ç•ªãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p_{\text{prod}}(x)$ ãŒè¨“ç·´ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p_{\text{train}}(x)$ ã‹ã‚‰ä¹–é›¢ã€‚

**æ¤œå‡ºæŒ‡æ¨™**:

1. **KL Divergence**:

$$
D_{KL}(p_{\text{prod}} \| p_{\text{train}}) = \mathbb{E}_{x \sim p_{\text{prod}}} \left[ \log \frac{p_{\text{prod}}(x)}{p_{\text{train}}(x)} \right]
$$

2. **Kolmogorov-Smirnov Test**:

$$
D_{KS} = \sup_x \left| F_{\text{prod}}(x) - F_{\text{train}}(x) \right|
$$

ã“ã“ã§ $F$ ã¯ç´¯ç©åˆ†å¸ƒé–¢æ•°ã€‚

3. **Population Stability Index (PSI)**:

$$
\text{PSI} = \sum_{i=1}^B \left( p_{\text{prod},i} - p_{\text{train},i} \right) \log \frac{p_{\text{prod},i}}{p_{\text{train},i}}
$$

ã“ã“ã§ $B$ ã¯ãƒ“ãƒ³æ•° (ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ åˆ†å‰²)ã€‚

**é–¾å€¤ãƒ™ãƒ¼ã‚¹ã‚¢ãƒ©ãƒ¼ãƒˆ**:

$$
\text{Alert} = \begin{cases}
\text{Critical} & \text{PSI} > 0.25 \\
\text{Warning} & 0.1 < \text{PSI} \leq 0.25 \\
\text{OK} & \text{PSI} \leq 0.1
\end{cases}
$$

**æ•°å€¤æ¤œè¨¼** (Julia):

```julia
using Statistics, StatsBase

# Training data: N(0, 1)
x_train = randn(10000)

# Production data: N(0.5, 1.2) (shifted + wider)
x_prod = 0.5 .+ 1.2 .* randn(10000)

# Histogram binning
bins = range(-4, 4, length=10)
h_train = fit(Histogram, x_train, bins).weights
h_prod = fit(Histogram, x_prod, bins).weights

# Normalize to probabilities
p_train = h_train ./ sum(h_train)
p_prod = h_prod ./ sum(h_prod)

# PSI calculation
psi = sum((p_prod .- p_train) .* log.((p_prod .+ 1e-10) ./ (p_train .+ 1e-10)))
println("PSI: $psi")  # => 0.34 (Critical drift detected!)
```

#### 3.6.3 Model Performance Degradation ã®æ•°å­¦

**æ¦‚å¿µãƒ‰ãƒªãƒ•ãƒˆ vs ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆ**:

- **ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆ**: $p(x)$ ãŒå¤‰åŒ– (å…¥åŠ›åˆ†å¸ƒã®å¤‰åŒ–)
- **æ¦‚å¿µãƒ‰ãƒªãƒ•ãƒˆ**: $p(y|x)$ ãŒå¤‰åŒ– (å…¥åŠ›-å‡ºåŠ›é–¢ä¿‚ã®å¤‰åŒ–)

**Performance decay model**:

$$
\text{Acc}(t) = \text{Acc}_0 \cdot e^{-\lambda t} + \text{Acc}_\infty (1 - e^{-\lambda t})
$$

ã“ã“ã§:
- $\text{Acc}_0$: åˆæœŸç²¾åº¦ (ãƒ‡ãƒ—ãƒ­ã‚¤æ™‚)
- $\text{Acc}_\infty$: é•·æœŸå¹³è¡¡ç²¾åº¦
- $\lambda > 0$: åŠ£åŒ–ç‡

**å†è¨“ç·´ãƒˆãƒªã‚¬ãƒ¼æ¡ä»¶**:

$$
\text{Retrain} = \mathbb{1}\left[ \text{Acc}(t) < \theta_{\text{min}} \lor \Delta\text{Acc} > \tau \right]
$$

ã“ã“ã§:
- $\theta_{\text{min}}$: è¨±å®¹æœ€å°ç²¾åº¦ (ä¾‹: 0.85)
- $\Delta\text{Acc} = \text{Acc}(t-1) - \text{Acc}(t)$: ç²¾åº¦ä½ä¸‹å¹…
- $\tau$: ä½ä¸‹é–¾å€¤ (ä¾‹: 0.05)

**æ•°å€¤ä¾‹**:

```julia
# Model deployed with Acc_0 = 0.95
Acc_0 = 0.95
Acc_âˆ = 0.75  # Long-term equilibrium (concept drift)
Î» = 0.01  # Decay rate (per day)

# Performance over 100 days
t_days = 0:100
Acc_t = @. Acc_0 * exp(-Î» * t_days) + Acc_âˆ * (1 - exp(-Î» * t_days))

# Find first day to retrigger
Î¸_min = 0.85
first_retrain_day = findfirst(Acc_t .< Î¸_min)
println("Retrain after $first_retrain_day days")  # => Day 57
```

### 3.7 Continuous Integration for ML (CI/ML)

#### 3.7.1 MLãƒ†ã‚¹ãƒˆæˆ¦ç•¥ã®åˆ†é¡å­¦

å¾“æ¥ã®ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ãƒ†ã‚¹ãƒˆã«åŠ ãˆã€MLã‚·ã‚¹ãƒ†ãƒ ã¯**ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ†ã‚¹ãƒˆ**ã¨**ãƒ¢ãƒ‡ãƒ«å“è³ªãƒ†ã‚¹ãƒˆ**ãŒå¿…è¦ã€‚

**ãƒ†ã‚¹ãƒˆãƒ”ãƒ©ãƒŸãƒƒãƒ‰ (MLç‰ˆ)**:

```
        /\
       /  \  Model Validation Tests (å°‘æ•°ãƒ»é…ã„)
      /____\
     /      \  Integration Tests (ä¸­ç¨‹åº¦)
    /________\
   /          \
  /__Data_QA__\ Unit Tests + Data Schema Tests (å¤šæ•°ãƒ»é«˜é€Ÿ)
```

**Data Quality Tests**:

1. **Schema Validation**: å‹ãƒ»ç¯„å›²ãƒ»å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
   ```julia
   @test all(0 .<= data.age .<= 120)  # Age range check
   @test eltype(data.price) <: Real   # Type check
   ```

2. **Statistical Tests**: åˆ†å¸ƒã®ä¸€è²«æ€§
   ```julia
   # Mean should be within 3Ïƒ of training data
   @test abs(mean(prod_data) - Î¼_train) < 3 * Ïƒ_train
   ```

3. **Feature Correlation**: ç‰¹å¾´é‡é–“ã®ç›¸é–¢å¤‰åŒ–æ¤œå‡º
   $$
   \rho_{\text{prod}}(X_i, X_j) \approx \rho_{\text{train}}(X_i, X_j) \pm \epsilon
   $$

**Model Quality Tests**:

1. **Invariance Tests**: å…¥åŠ›å¤‰æ›ã«å¯¾ã™ã‚‹ä¸å¤‰æ€§
   ```julia
   # Image rotation shouldn't change class (for rotation-invariant tasks)
   @test predict(model, rotate(img, 10Â°)) == predict(model, img)
   ```

2. **Directional Expectation Tests**: å…¥åŠ›å¤‰åŒ–ã®æ–¹å‘æ€§
   ```julia
   # Increasing income should not decrease loan approval probability
   @test predict_prob(model, income=100k) â‰¥ predict_prob(model, income=50k)
   ```

3. **Minimum Functionality Tests**: ç°¡å˜ãªã‚±ãƒ¼ã‚¹ã¯100%æ­£è§£
   ```julia
   @test all(predict(model, obvious_cases) .== ground_truth)
   ```

#### 3.7.2 Shadow Deployment ã¨ Traffic Splitting

**Shadow Mode**: æ–°ãƒ¢ãƒ‡ãƒ«ã¯æ¨è«–ã™ã‚‹ãŒçµæœã¯è¿”ã•ãªã„ï¼ˆãƒ­ã‚°ã®ã¿ï¼‰

$$
\text{Response} = \begin{cases}
f_{\text{old}}(x) & \text{(return to user)} \\
f_{\text{new}}(x) & \text{(log only, no return)}
\end{cases}
$$

**Canary Deployment**: ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚’æ®µéšçš„ã«åˆ†å‰²

$$
p_{\text{route}}(x) = \begin{cases}
f_{\text{new}}(x) & \text{with prob } \alpha \\
f_{\text{old}}(x) & \text{with prob } 1-\alpha
\end{cases}
$$

å…¸å‹çš„ãª $\alpha$ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«: $0.01 \to 0.05 \to 0.25 \to 0.50 \to 1.0$

**A/B Test Statistical Power**:

å¿…è¦ã‚µãƒ³ãƒ—ãƒ«æ•° $n$ (å„ã‚°ãƒ«ãƒ¼ãƒ—):

$$
n = \frac{2(z_{\alpha/2} + z_\beta)^2 \sigma^2}{\delta^2}
$$

ã“ã“ã§:
- $z_{\alpha/2}$: æœ‰æ„æ°´æº– (Î±=0.05 â†’ z=1.96)
- $z_\beta$: æ¤œå‡ºåŠ› (Î²=0.8 â†’ z=0.84)
- $\sigma^2$: ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆ†æ•£
- $\delta$: æ¤œå‡ºã—ãŸã„åŠ¹æœã‚µã‚¤ã‚º

**æ•°å€¤ä¾‹**:

```julia
# CTR A/B test
Ïƒ = 0.05  # CTR standard deviation
Î´ = 0.01  # Detect 1% improvement
Î± = 0.05
Î² = 0.8

z_Î± = 1.96
z_Î² = 0.84

n = ceil(Int, 2 * (z_Î± + z_Î²)^2 * Ïƒ^2 / Î´^2)
println("Required sample size per group: $n")  # => 768 samples
```

:::message
**é€²æ—: 70%å®Œäº†ï¼** DPOç†è«–ã€MLOpsæˆç†Ÿåº¦ã€CI/MLã‚’å®Œå…¨ç†è§£ã—ãŸã€‚Production E2Eã‚·ã‚¹ãƒ†ãƒ ã®æ•°å­¦çš„åŸºç›¤ãŒæƒã£ãŸï¼
:::

---

### 3.8 Model Monitoring & Observability ã®æ•°ç†

#### 3.8.1 RED Metrics (Rate, Errors, Duration)

**Request Rate**: å˜ä½æ™‚é–“ã‚ãŸã‚Šã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°

$$
R(t) = \frac{\Delta N}{\Delta t}
$$

ã“ã“ã§ $\Delta N$ ã¯æ™‚é–“çª“ $\Delta t$ å†…ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã€‚

**Error Rate**: ã‚¨ãƒ©ãƒ¼ã®å‰²åˆ

$$
E(t) = \frac{N_{\text{error}}(t)}{N_{\text{total}}(t)}
$$

**Duration (Latency) Percentiles**: P50, P95, P99

$$
D_{p} = \inf \{ d : P(\text{Latency} \leq d) \geq p \}
$$

**SLI (Service Level Indicator)**: RED metricsã®çµ„ã¿åˆã‚ã›

$$
\text{SLI}_{\text{availability}} = \frac{N_{\text{success}}}{N_{\text{total}}} \times 100\%
$$

$$
\text{SLI}_{\text{latency}} = \frac{N_{\text{latency} < \theta}}{N_{\text{total}}} \times 100\%
$$

**SLO (Service Level Objective)**: ç›®æ¨™å€¤

- Availability SLO: 99.9% (3 nines)
- Latency SLO: 95% of requests < 100ms

**Error Budget**: SLOã‹ã‚‰ã®è¨±å®¹èª¤å·®

$$
\text{ErrorBudget}_{\text{month}} = (1 - \text{SLO}) \times N_{\text{total/month}}
$$

ä¾‹: SLO = 99.9%, æœˆé–“100ä¸‡ãƒªã‚¯ã‚¨ã‚¹ãƒˆ â†’ Error Budget = 1,000ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

**æ•°å€¤æ¤œè¨¼** (Julia):

```julia
# Monthly requests
N_total = 1_000_000
SLO_availability = 0.999

# Error budget
error_budget = (1 - SLO_availability) * N_total  # => 1,000

# Current errors
N_errors = 450

# Remaining budget
remaining = error_budget - N_errors  # => 550
burn_rate = N_errors / (N_total * 0.5)  # Current month half-way

println("Error Budget: $error_budget")
println("Used: $N_errors ($(100*N_errors/error_budget)%)")
println("Remaining: $remaining")
println("Burn Rate: $(round(burn_rate, digits=4))")
```

#### 3.8.2 Prediction Drift vs Label Drift

**Prediction Drift**: ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›åˆ†å¸ƒã®å¤‰åŒ–

$$
D_{\text{pred}} = D_{KL}(p_{\text{prod}}(\hat{y}) \| p_{\text{train}}(\hat{y}))
$$

**Label Drift**: çœŸã®ãƒ©ãƒ™ãƒ«åˆ†å¸ƒã®å¤‰åŒ– (Ground truthãŒå¾—ã‚‰ã‚Œã‚‹å ´åˆ)

$$
D_{\text{label}} = D_{KL}(p_{\text{prod}}(y) \| p_{\text{train}}(y))
$$

**é–¢ä¿‚æ€§**: Covariate Shift ($p(x)$ å¤‰åŒ–) vs Prior Probability Shift ($p(y)$ å¤‰åŒ–)

$$
p_{\text{prod}}(y|x) = p_{\text{train}}(y|x) \quad \text{(Covariate Shift)}
$$

$$
p_{\text{prod}}(y|x) \neq p_{\text{train}}(y|x) \quad \text{(Concept Drift)}
$$

**æ¤œå‡ºæˆ¦ç•¥**:

1. **Immediate Detection** (Prediction Drift): ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã€ãƒ©ãƒ™ãƒ«ä¸è¦
2. **Delayed Detection** (Label Drift): ãƒ©ãƒ™ãƒ«å–å¾—å¾Œ (æ•°æ—¥ã€œæ•°é€±é–“)

**æ•°å€¤ä¾‹**:

```julia
# Training: Binary classification, p(y=1) = 0.3
p_train_pos = 0.3

# Production: p(y=1) = 0.5 (label drift!)
p_prod_pos = 0.5

# KL divergence for label distribution
function binary_kl(p, q)
    return p * log(p / q) + (1 - p) * log((1 - p) / (1 - q))
end

D_label = binary_kl(p_prod_pos, p_train_pos)
println("Label Drift KL: $(round(D_label, digits=4))")  # => 0.0513
```

#### 3.8.3 Explainability Monitoring (SHAPå€¤ã®åˆ†å¸ƒå¤‰åŒ–)

ãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜å¯èƒ½æ€§ã‚‚ç›£è¦–å¯¾è±¡ã€‚**SHAPå€¤ã®åˆ†å¸ƒå¤‰åŒ–**ã§ãƒ¢ãƒ‡ãƒ«ã®å†…éƒ¨ãƒ­ã‚¸ãƒƒã‚¯å¤‰åŒ–ã‚’æ¤œå‡ºã€‚

**SHAPå€¤** (Shapley Additive exPlanations):

$$
\phi_i(x) = \sum_{S \subseteq F \setminus \{i\}} \frac{|S|! (|F| - |S| - 1)!}{|F|!} \left[ f(S \cup \{i\}) - f(S) \right]
$$

ã“ã“ã§:
- $F$: å…¨ç‰¹å¾´é‡é›†åˆ
- $S$: ç‰¹å¾´é‡éƒ¨åˆ†é›†åˆ
- $f(S)$: ç‰¹å¾´é‡ $S$ ã‚’ä½¿ã£ãŸäºˆæ¸¬å€¤

**SHAP Drift Detection**: ç‰¹å¾´é‡ $i$ ã®SHAPå€¤åˆ†å¸ƒãŒå¤‰åŒ–

$$
D_{\text{SHAP},i} = D_{KS}(\phi_{i,\text{train}}, \phi_{i,\text{prod}})
$$

KSçµ±è¨ˆé‡ (Kolmogorov-Smirnov) ã§åˆ†å¸ƒã®å·®ã‚’æ¤œå‡ºã€‚

**ã‚¢ãƒ©ãƒ¼ãƒˆæ¡ä»¶**:

$$
\text{Alert}_{\text{SHAP}} = \bigvee_{i=1}^{|F|} \left[ D_{\text{SHAP},i} > \tau_{\text{KS}} \right]
$$

å…¸å‹çš„ãªé–¾å€¤: $\tau_{\text{KS}} = 0.15$

**æ•°å€¤æ¤œè¨¼** (Julia with SHAP.jl concept):

```julia
using HypothesisTests

# Training SHAP values for feature "age" (N=1000 samples)
shap_age_train = randn(1000) .* 0.5 .+ 0.2

# Production SHAP values (shifted distribution)
shap_age_prod = randn(1000) .* 0.6 .+ 0.35

# KS test
ks_test = ApproximateTwoSampleKSTest(shap_age_train, shap_age_prod)
D_KS = ks_test.Î´

println("KS Statistic for SHAP(age): $(round(D_KS, digits=4))")
println("Alert: $(D_KS > 0.15)")  # => true if significant shift
```

#### 3.8.4 Feedback Loop Stability Analysis

ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—ãŒ**ä¸å®‰å®šåŒ–**ã™ã‚‹æ¡ä»¶ã‚’è§£æã€‚

**System Dynamics Model**: ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ $\hat{y}_t$ ãŒãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p_{t+1}(x)$ ã«å½±éŸ¿

$$
p_{t+1}(x) = T(p_t(x), \hat{y}_t)
$$

ã“ã“ã§ $T$ ã¯çŠ¶æ…‹é·ç§»é–¢æ•°ã€‚

**å®‰å®šæ€§æ¡ä»¶** (Lyapunov Stability):

$$
\exists V(p) \geq 0 \text{ s.t. } V(T(p, \hat{y})) < V(p) \quad \forall p \neq p^*
$$

$V$ ã¯Lyapunové–¢æ•°ã€$p^*$ ã¯å¹³è¡¡ç‚¹ã€‚

**ç°¡æ˜“ãƒ¢ãƒ‡ãƒ«** (Linear Approximation):

$$
p_{t+1} = A p_t + B \hat{y}_t
$$

å®‰å®šæ€§æ¡ä»¶: è¡Œåˆ— $A$ ã®å›ºæœ‰å€¤ãŒå˜ä½å††å†…

$$
|\lambda_i(A)| < 1 \quad \forall i
$$

**ä¸å®‰å®šåŒ–ã®ä¾‹** (Positive Feedback):

1. ãƒ¢ãƒ‡ãƒ«ãŒåºƒå‘Šã‚¯ãƒªãƒƒã‚¯ã‚’äºˆæ¸¬
2. é«˜äºˆæ¸¬ã‚¹ã‚³ã‚¢ â†’ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é »ç¹ã«è¡¨ç¤º
3. è¡¨ç¤ºå¢— â†’ ã‚¯ãƒªãƒƒã‚¯å¢— (exposure bias)
4. æ–°è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒé«˜ã‚¯ãƒªãƒƒã‚¯ç‡ã«åã‚‹
5. ãƒ¢ãƒ‡ãƒ«ãŒéä¿¡ â†’ ã•ã‚‰ã«é«˜äºˆæ¸¬ â†’ **ç™ºæ•£**

**ç·©å’Œç­–**:

- **Exploration**: ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° ($\epsilon$-greedy)
  $$
  a_t = \begin{cases}
  \arg\max_a Q(s,a) & \text{with prob } 1-\epsilon \\
  \text{random}(A) & \text{with prob } \epsilon
  \end{cases}
  $$

- **Inverse Propensity Score Weighting**:
  $$
  \mathcal{L}_{\text{IPW}}(\theta) = \mathbb{E}_{(x,y,a) \sim \mathcal{D}} \left[ \frac{1}{p(a|x)} \ell(f_\theta(x), y) \right]
  $$

#### 3.8.5 Cost-Aware Monitoring (ROI-driven alerting)

å…¨ã¦ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ç›£è¦–ã™ã‚‹ã®ã¯ã‚³ã‚¹ãƒˆãŒé«˜ã„ã€‚**ROI (Return on Investment)** ãƒ™ãƒ¼ã‚¹ã§ç›£è¦–å¯¾è±¡ã‚’é¸æŠã€‚

**Monitoring Cost**: ãƒ‡ãƒ¼ã‚¿åé›†ãƒ»ä¿å­˜ãƒ»å¯è¦–åŒ–ã®ã‚³ã‚¹ãƒˆ

$$
C_{\text{monitor}} = c_{\text{storage}} \cdot N_{\text{metrics}} + c_{\text{compute}} \cdot N_{\text{samples}}
$$

**Expected Value of Monitoring**: éšœå®³æ¤œå‡ºã«ã‚ˆã‚‹æå¤±å›é¿

$$
\text{EV}_{\text{monitor}} = p_{\text{detect}} \cdot (\text{Loss}_{\text{failure}} - C_{\text{fix}})
$$

ã“ã“ã§:
- $p_{\text{detect}}$: ç›£è¦–ã«ã‚ˆã‚‹éšœå®³æ¤œå‡ºç¢ºç‡
- $\text{Loss}_{\text{failure}}$: éšœå®³ã«ã‚ˆã‚‹æå¤± (ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ ã€ãƒ¬ãƒ”ãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³)
- $C_{\text{fix}}$: ä¿®æ­£ã‚³ã‚¹ãƒˆ

**ç›£è¦–å°å…¥æ¡ä»¶**:

$$
\text{Deploy Monitoring} \iff \text{EV}_{\text{monitor}} > C_{\text{monitor}}
$$

**å„ªå…ˆåº¦ä»˜ã‘**: ROIé †ã«ã‚½ãƒ¼ãƒˆ

$$
\text{ROI}_i = \frac{\text{EV}_{\text{monitor},i}}{C_{\text{monitor},i}}
$$

é«˜ROIãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰é †ã«ç›£è¦–ã‚’è¿½åŠ ã€‚

**æ•°å€¤ä¾‹**:

```julia
# Metrics candidates
metrics = [
    (name="Latency P99", EV=50_000, cost=200),
    (name="SHAP Drift", EV=30_000, cost=800),
    (name="Feature Correlation", EV=10_000, cost=500),
    (name="Prediction Entropy", EV=5_000, cost=100)
]

# Calculate ROI
roi_list = [(m.name, m.EV / m.cost) for m in metrics]
sort!(roi_list, by=x->x[2], rev=true)

println("Monitoring Priority (by ROI):")
for (name, roi) in roi_list
    println("  $name: ROI = $(round(roi, digits=2))")
end
# Output:
#   Latency P99: ROI = 250.0
#   Prediction Entropy: ROI = 50.0
#   SHAP Drift: ROI = 37.5
#   Feature Correlation: ROI = 20.0
```

### 3.9 Continual Learning ã®ç†è«–çš„æ·±æ˜ã‚Š

#### 3.9.1 Catastrophic Forgetting ã®æ•°å­¦çš„å®šå¼åŒ–

**å®šç¾©**: æ–°ã‚¿ã‚¹ã‚¯å­¦ç¿’æ™‚ã«ã€æ—§ã‚¿ã‚¹ã‚¯ã®æ€§èƒ½ãŒåŠ‡çš„ã«ä½ä¸‹ã™ã‚‹ç¾è±¡ã€‚

**Loss Landscapeè¦–ç‚¹**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã®æå¤±é–¢æ•°

$$
\mathcal{L}_{\text{total}}(\theta) = \mathcal{L}_{\text{old}}(\theta; \mathcal{D}_{\text{old}}) + \mathcal{L}_{\text{new}}(\theta; \mathcal{D}_{\text{new}})
$$

Naive SGDã¯ $\mathcal{L}_{\text{new}}$ ã®ã¿æœ€å°åŒ– â†’ $\mathcal{L}_{\text{old}}$ ãŒå¢—å¤§ã€‚

**Fisher Information Matrix (FIM)**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®é‡è¦åº¦

$$
F_{ij} = \mathbb{E}_{x \sim \mathcal{D}_{\text{old}}} \left[ \frac{\partial \log p_{\theta_{\text{old}}}(y|x)}{\partial \theta_i} \frac{\partial \log p_{\theta_{\text{old}}}(y|x)}{\partial \theta_j} \right]
$$

å¯¾è§’è¿‘ä¼¼: $F \approx \text{diag}(F_{11}, F_{22}, \ldots)$

**EWC (Elastic Weight Consolidation)** [^9]:

$$
\mathcal{L}_{\text{EWC}}(\theta) = \mathcal{L}_{\text{new}}(\theta) + \frac{\lambda}{2} \sum_i F_{ii} (\theta_i - \theta_{i,\text{old}})^2
$$

é‡è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (å¤§ããª $F_{ii}$) ã®å¤‰æ›´ã«ãƒšãƒŠãƒ«ãƒ†ã‚£ã€‚

**Memory Efficiency**: FIMã¯ $O(d)$ (å¯¾è§’ã®ã¿ä¿å­˜)ã€å…ƒãƒ‡ãƒ¼ã‚¿ã¯ä¸è¦ã€‚

**æ•°å€¤æ¤œè¨¼**:

```julia
# Simplified FIM calculation (diagonal)
function compute_fisher_diagonal(model, data, params)
    fisher = zero(params)

    for (x, y) in data
        # Gradient of log-likelihood
        grads = gradient(params) do p
            logp = log_likelihood(model, x, y, p)
            return logp
        end

        # Square of gradients
        fisher .+= grads .^ 2
    end

    fisher ./= length(data)  # Average
    return fisher
end

# EWC loss
function ewc_loss(params, params_old, fisher, Î», new_loss)
    penalty = Î» / 2 * sum(fisher .* (params .- params_old).^2)
    return new_loss + penalty
end
```

#### 3.9.2 Progressive Neural Networks

**ã‚¢ã‚¤ãƒ‡ã‚¢**: æ–°ã‚¿ã‚¹ã‚¯ã”ã¨ã«æ–°ã—ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’è¿½åŠ ã€æ—§ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯å‡çµã€‚

**ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**:

$$
h_i^{(k)} = f\left( W_i^{(k)} h_{i-1}^{(k)} + \sum_{j=1}^{k-1} U_i^{(k \leftarrow j)} h_{i-1}^{(j)} \right)
$$

ã“ã“ã§:
- $h_i^{(k)}$: ã‚¿ã‚¹ã‚¯ $k$ ã®å±¤ $i$ ã®æ´»æ€§åŒ–
- $W_i^{(k)}$: åŒã‚¿ã‚¹ã‚¯å†…ã®é‡ã¿ (å­¦ç¿’)
- $U_i^{(k \leftarrow j)}$: ã‚¿ã‚¹ã‚¯ $j$ ã‹ã‚‰ã®æ¨ªæ–¹å‘æ¥ç¶š (å­¦ç¿’)

**ãƒ¡ãƒ¢ãƒªã‚³ã‚¹ãƒˆ**: ã‚¿ã‚¹ã‚¯æ•° $K$ ã«å¯¾ã—ã¦ $O(K \cdot d)$ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¢—åŠ ã€‚

**Transferå­¦ç¿’åŠ¹æœ**: æ—§ã‚¿ã‚¹ã‚¯ã®çŸ¥è­˜ã‚’æ¨ªæ–¹å‘æ¥ç¶šã§æ´»ç”¨ã€‚

#### 3.9.3 Meta-Learning for Fast Adaptation (MAML)

**Model-Agnostic Meta-Learning (MAML)** [^10]: å°‘æ•°ã‚µãƒ³ãƒ—ãƒ«ã§é«˜é€Ÿé©å¿œã§ãã‚‹åˆæœŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã€‚

**ç›®çš„é–¢æ•°**:

$$
\theta^* = \arg\min_\theta \mathbb{E}_{\mathcal{T} \sim p(\mathcal{T})} \left[ \mathcal{L}_{\mathcal{T}}(\theta - \alpha \nabla_\theta \mathcal{L}_{\mathcal{T}}(\theta)) \right]
$$

ã“ã“ã§:
- $\mathcal{T}$: ã‚¿ã‚¹ã‚¯åˆ†å¸ƒ
- $\alpha$: å†…ãƒ«ãƒ¼ãƒ—å­¦ç¿’ç‡
- $\theta - \alpha \nabla_\theta \mathcal{L}_{\mathcal{T}}(\theta)$: 1ã‚¹ãƒ†ãƒƒãƒ—æ›´æ–°å¾Œã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

**2éšå¾®åˆ†**: ãƒ¡ã‚¿å‹¾é…

$$
\nabla_\theta \mathcal{L}_{\mathcal{T}}(\theta') \approx \nabla_\theta \mathcal{L}_{\mathcal{T}}(\theta) - \alpha \nabla_\theta^2 \mathcal{L}_{\mathcal{T}}(\theta)
$$

è¨ˆç®—ã‚³ã‚¹ãƒˆé«˜ã„ãŒã€First-Order MAML (FOMAML) ã§è¿‘ä¼¼å¯èƒ½ã€‚

**Productionå¿œç”¨**: æ–°ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ»æ–°ãƒ‰ãƒ¡ã‚¤ãƒ³ã¸ã®é«˜é€Ÿé©å¿œã€‚

```julia
# MAML pseudocode (conceptual)
function maml_meta_update(tasks, Î¸, Î±, Î²)
    meta_grad = zero(Î¸)

    for task in tasks
        # Inner loop: 1-step adaptation
        Î¸_adapted = Î¸ - Î± * gradient(task.loss, Î¸)

        # Outer loop: meta gradient
        meta_grad .+= gradient(task.loss, Î¸_adapted)
    end

    # Meta update
    Î¸_new = Î¸ - Î² * meta_grad / length(tasks)
    return Î¸_new
end
```

:::message
**é€²æ—: 90%å®Œäº†ï¼** Monitoringç†è«–ã€Continual Learningã€MAMLã¾ã§å®Œå…¨ç†è§£ã—ãŸã€‚Production E2Eã‚·ã‚¹ãƒ†ãƒ ã®å…¨æ•°å­¦ãŒæƒã£ãŸï¼
:::

---


## è¨˜æ³•è¦ç´„

### æ•°å­¦è¨˜æ³•

| è¨˜å· | æ„å‘³ | ä¾‹ |
|------|------|-----|
| $\theta$ | ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | $\theta \in \mathbb{R}^d$ |
| $\mathcal{L}$ | æå¤±é–¢æ•° | $\mathcal{L}(\theta) = \text{MSE}$ |
| $\nabla_\theta$ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«é–¢ã™ã‚‹å‹¾é… | $\nabla_\theta \mathcal{L}$ |
| $\mathbb{E}_{x \sim p}$ | åˆ†å¸ƒ$p$ã«é–¢ã™ã‚‹æœŸå¾…å€¤ | $\mathbb{E}_{x \sim \mathcal{D}}[f(x)]$ |
| $\mathcal{D}_{\text{pool}}$ | ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¼ãƒ« | Active Learningç”¨ |
| $x^{(i)}$ | $i$ç•ªç›®ã®ã‚µãƒ³ãƒ—ãƒ« | $(x^{(1)}, y^{(1)}), \ldots$ |
| $\mathcal{H}$ | ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ | $\mathcal{H}(p) = -\sum p \log p$ |
| $\text{MI}(X;Y)$ | ç›¸äº’æƒ…å ±é‡ | $\text{MI}(y;\theta \mid x, \mathcal{D})$ |

### ã‚³ãƒ¼ãƒ‰è¦ç´„

**Julia**:
```julia
# é–¢æ•°å: snake_case
function train_model(data::Matrix, labels::Vector)
    # ...
end

# å‹å: PascalCase
struct TrainingPipeline
    model::Lux.AbstractExplicitLayer
end

# å®šæ•°: UPPER_CASE
const BATCH_SIZE = 32
```

**Rust**:
```rust
// é–¢æ•°å: snake_case
pub fn run_inference(input: &[f32]) -> Vec<f32> {
    // ...
}

// å‹å: PascalCase
pub struct InferenceEngine {
    session: Session,
}

// å®šæ•°: SCREAMING_SNAKE_CASE
const MAX_BATCH_SIZE: usize = 128;
```

**Elixir**:
```elixir
# é–¢æ•°å: snake_case
def process_request(request) do
  # ...
end

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å: PascalCase
defmodule FeedbackCollector do
  # ...
end

# ã‚¢ãƒˆãƒ : lowercase
:ok, :error, :rate_limited
```

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å›³è¨˜æ³•

```mermaid
graph LR
    A[Component A] -->|REST API| B[Component B]
    B -->|gRPC| C[Component C]
    C -.->|Async| D[(Database)]

    style A fill:#4ecdc4,stroke:#1a535c
    style B fill:#ffe66d,stroke:#ff6b6b
    style C fill:#95e1d3,stroke:#38ada9
    style D fill:#f38181,stroke:#aa4465
```

- **å®Ÿç·š**: åŒæœŸé€šä¿¡ (REST, gRPC)
- **ç‚¹ç·š**: éåŒæœŸé€šä¿¡ (Message Queue, Event)
- **å††æŸ±**: ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆã‚¢ (DB, Cache)
- **è‰²**: è¨€èªåˆ¥ (ğŸ¦€ Rust=é’, âš¡ Julia=é»„, ğŸ”® Elixir=ç·‘)

---

:::message
**ğŸ“ Course IIIå®Œå…¨åˆ¶è¦‡ãŠã‚ã§ã¨ã†ï¼**

ã‚ãªãŸã¯ä»Šã€ä»¥ä¸‹ã®ã‚¹ã‚­ãƒ«ã‚’ç²å¾—ã—ãŸ:
1. âœ… ç†è«–ï¼ˆCourse I-IIï¼‰â†’ å®Ÿè£…ï¼ˆCourse IIIï¼‰ã®å®Œå…¨æ©‹æ¸¡ã—
2. âœ… Julia/Rust/Elixir 3è¨€èªã§ã®Production E2Eã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰åŠ›
3. âœ… è¨“ç·´â†’æ¨è«–â†’é…ä¿¡â†’ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯â†’ç¶™ç¶šå­¦ç¿’ã®å®Ÿè£…
4. âœ… è² è·ãƒ†ã‚¹ãƒˆãƒ»Chaos Engineeringãƒ»MLOpsã®å®Ÿè·µçŸ¥è­˜

**ã“ã“ã‹ã‚‰2ã¤ã®ãƒ«ãƒ¼ãƒˆãŒåˆ†å²ã™ã‚‹**:

**ğŸŒŠ Course IV: æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ç†è«–æ·±åŒ–ï¼ˆç¬¬33-42å›ã€å…¨10å›ï¼‰**
- Normalizing Flows â†’ EBM â†’ Score Matching â†’ DDPM â†’ SDE â†’ Flow Matching â†’ LDM â†’ Consistency Models â†’ World Models â†’ çµ±ä¸€ç†è«–
- ã€Œæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«è«–æ–‡ã®ç†è«–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒå°å‡ºã§ãã‚‹ã€æ•°å­¦åŠ›ã‚’ç²å¾—
- å¯†åº¦ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®è«–ç†çš„ãƒã‚§ãƒ¼ãƒ³ã‚’å®Œå…¨è¸ç ´

**ğŸ¨ Course V: ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–å¿œç”¨ï¼ˆç¬¬43-50å›ã€å…¨8å›ï¼‰**
- Visionãƒ»Audioãƒ»RLãƒ»Proteinãƒ»Moleculeãƒ»Climateãƒ»Robotãƒ»Simulation
- å„ãƒ‰ãƒ¡ã‚¤ãƒ³ã®æœ€æ–°SOTAæŠ€è¡“ã‚’å®Ÿè£…
- å®Ÿä¸–ç•Œå•é¡Œã¸ã®é©ç”¨åŠ›ã‚’é›ãˆã‚‹

**Course IVã¨Vã¯ç‹¬ç«‹** â€” ã©ã¡ã‚‰ã‹ã‚‰å§‹ã‚ã¦ã‚‚è‰¯ã„ã€‚ä¸¡æ–¹å±¥ä¿®ã§å…¨50å›å®Œå…¨åˆ¶è¦‡ã€‚

**æ¬¡å›äºˆå‘Š: ç¬¬33å› Normalizing Flows â€” å¯é€†å¤‰æ›ã§å³å¯†å°¤åº¦ã‚’æ‰‹ã«å…¥ã‚Œã‚‹**
:::

---

### ä¸»è¦è«–æ–‡

[^1]: Wang, K. et al. (2024). Maximally Separated Active Learning. arXiv:2411.17444.
@[card](https://arxiv.org/abs/2411.17444)

[^2]: Kingma, D. P., & Welling, M. (2013). Auto-Encoding Variational Bayes. arXiv:1312.6114.
@[card](https://arxiv.org/abs/1312.6114)

[^3]: Goodfellow, I. et al. (2014). Generative Adversarial Networks. arXiv:1406.2661.
@[card](https://arxiv.org/abs/1406.2661)

[^4]: Perdomo, J. et al. (2024). Mathematical Model of the Hidden Feedback Loop Effect. arXiv:2405.02726.
@[card](https://arxiv.org/abs/2405.02726)

[^5]: Rafailov, R. et al. (2023). Direct Preference Optimization: Your Language Model is Secretly a Reward Model. arXiv:2305.18290.
@[card](https://arxiv.org/abs/2305.18290)

[^6]: Liu, T. et al. (2025). Reinforcement Learning from User Feedback. arXiv:2505.14946.
@[card](https://arxiv.org/abs/2505.14946)

[^7]: Hu, J. et al. (2024). OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework. arXiv:2405.11143.
@[card](https://arxiv.org/abs/2405.11143)

[^8]: Google Cloud. (2021). MLOps: Continuous delivery and automation pipelines in machine learning.
@[card](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)

[^9]: Kirkpatrick, J. et al. (2017). Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13), 3521-3526.

[^10]: Finn, C., Abbeel, P., & Levine, S. (2017). Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. ICML 2017. arXiv:1703.03400.
@[card](https://arxiv.org/abs/1703.03400)

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
