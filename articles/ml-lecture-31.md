---
title: "ç¬¬31å›: MLOpså®Œå…¨ç‰ˆ: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸ”„"
type: "tech"
topics: ["machinelearning", "mlops", "rust", "julia", "elixir"]
published: true
---

# ç¬¬31å›: MLOpså®Œå…¨ç‰ˆ â€” 99.9%å¯ç”¨æ€§ã¯"åŠªåŠ›"ã§ã¯ãªã"è¨­è¨ˆ"ã 

> **ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã§ãã¦ã‚‚ã€æœ¬ç•ªã§å‹•ã‹ã›ãªã‘ã‚Œã°ä¾¡å€¤ã¯ã‚¼ãƒ­ã€‚MLOpså…¨é ˜åŸŸã‚’ç¶²ç¾…ã—ã€Trainâ†’Evaluateâ†’Deployâ†’Monitorã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Œçµã•ã›ã‚‹ã€‚**

ç¬¬30å›ã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Œå…¨æ§‹ç¯‰ã—ãŸã€‚ã ãŒ"å‹•ã"ã ã‘ã§ã¯è¶³ã‚Šãªã„ã€‚

æœ¬ç•ªç’°å¢ƒã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã¯**ç”Ÿãç‰©**ã ã€‚ãƒ‡ãƒ¼ã‚¿ãŒå¤‰ã‚ã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å—œå¥½ãŒå¤‰ã‚ã‚Šã€æ€§èƒ½ãŒåŠ£åŒ–ã™ã‚‹ã€‚å†è¨“ç·´ãŒå¿…è¦ã«ãªã‚Šã€A/Bãƒ†ã‚¹ãƒˆã§æ–°ãƒ¢ãƒ‡ãƒ«ã‚’æ¤œè¨¼ã—ã€æ®µéšçš„ã«ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã™ã‚‹ã€‚éšœå®³ãŒèµ·ãã‚Œã°å³åº§ã«ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã€ãƒ‰ãƒªãƒ•ãƒˆã‚’æ¤œå‡ºã—ã¦è‡ªå‹•å†è¨“ç·´ã‚’ãƒˆãƒªã‚¬ãƒ¼ã™ã‚‹ã€‚

ã“ã‚Œã‚‰å…¨ã¦ã‚’ã€Œæ‰‹ä½œæ¥­ã€ã§ã‚„ã£ã¦ã„ãŸã‚‰ã€1äººæœˆãŒ100äººæ—¥ã«åŒ–ã‘ã‚‹ã€‚

**MLOps (Machine Learning Operations)** ã¯ã€ã“ã®æ··æ²Œã‚’ã€Œè¨­è¨ˆã€ã§è§£æ±ºã™ã‚‹ã€‚ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ãƒ»å®Ÿé¨“ç®¡ç†ãƒ»CI/CDãƒ»A/Bãƒ†ã‚¹ãƒˆãƒ»ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ»SLI/SLOãƒ»ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºãƒ»DPO/RLHFã€‚7ã¤ã®ãƒ”ãƒ¼ã‚¹ã‚’çµ„ã¿åˆã‚ã›ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«å…¨ä½“ã‚’è‡ªå‹•åŒ–ã™ã‚‹ã€‚

æœ¬è¬›ç¾©ã¯Course IIIã®ç¬¬13å› â€” ç¬¬19å›ã‹ã‚‰å§‹ã¾ã£ãŸå®Ÿè·µç·¨ã®æœ€çµ‚ç›¤ã ã€‚ç¬¬32å›ã§çµ±åˆPJã‚’æ§‹ç¯‰ã—ã€Course IIIã‚’å®Œçµã•ã›ã‚‹ã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2025-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚
:::

```mermaid
graph LR
    A["ğŸ”§ Version<br/>Model/Data"] --> B["ğŸ§ª Experiment<br/>MLflow/W&B"]
    B --> C["ğŸš€ CI/CD<br/>Auto-Test"]
    C --> D["ğŸ¯ A/B Test<br/>Canary"]
    D --> E["ğŸ“Š Monitor<br/>Drift/SLO"]
    E --> F["ğŸ” Retrain<br/>Auto-Trigger"]
    F --> A
    G["ğŸ“ RLHF/DPO<br/>Human Feedback"] --> B
    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec
    style F fill:#fff9c4
    style G fill:#e0f2f1
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ + ç™ºå±• | 35åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 90åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | æŒ¯ã‚Šè¿”ã‚Š + çµ±åˆ | 30åˆ† | â˜…â˜…â˜…â˜†â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” å®Ÿé¨“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²ã™ã‚‹

**ã‚´ãƒ¼ãƒ«**: MLOpsã®æ ¸å¿ƒã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ â€” å®Ÿé¨“ã‚’ã€Œè¨˜éŒ²ã€ã—ãªã‘ã‚Œã°ã€Œå†ç¾ã€ã§ããªã„ã€‚

MLflowã‚¹ã‚¿ã‚¤ãƒ«ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²ã‚’3è¡Œã§å‹•ã‹ã™ã€‚

```julia
using Dates, JSON3

# Experiment metadata logging (simplified MLflow-style)
function log_experiment(name::String, params::Dict, metrics::Dict, artifacts::Vector{String})
    experiment = Dict(
        "name" => name,
        "timestamp" => now(),
        "params" => params,
        "metrics" => metrics,
        "artifacts" => artifacts,
        "run_id" => string(rand(UInt64), base=16)
    )

    # Persist to JSON (real MLflow uses DB + artifact store)
    filename = "experiments/$(experiment["run_id"]).json"
    mkpath("experiments")
    open(filename, "w") do io
        JSON3.write(io, experiment)
    end

    println("âœ… Logged experiment: $(experiment["name"]) (run_id: $(experiment["run_id"]))")
    println("   Params: $(params)")
    println("   Metrics: $(metrics)")
    return experiment["run_id"]
end

# Example: Train a tiny model and log everything
params = Dict("lr" => 0.001, "batch_size" => 32, "epochs" => 10)
metrics = Dict("train_loss" => 0.023, "val_acc" => 0.952, "f1" => 0.948)
artifacts = ["model_weights.pt", "config.yaml"]

run_id = log_experiment("tiny-classifier-v1", params, metrics, artifacts)
```

å‡ºåŠ›:
```
âœ… Logged experiment: tiny-classifier-v1 (run_id: a3f9c2e1b4d8)
   Params: Dict("lr" => 0.001, "batch_size" => 32, "epochs" => 10)
   Metrics: Dict("train_loss" => 0.023, "val_acc" => 0.952, "f1" => 0.948)
```

**3è¡Œã®ã‚³ãƒ¼ãƒ‰ã§å®Ÿé¨“ã‚’JSONåŒ–ã—ã¦æ°¸ç¶šåŒ–ã—ãŸã€‚** ã“ã‚ŒãŒMLOpsã®å‡ºç™ºç‚¹ã ã€‚å®Ÿéš›ã®MLflowã¯:

- SQLiteã¾ãŸã¯PostgreSQLã§ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç®¡ç†
- S3/GCS/Azureã§å¤§ããªartifactä¿å­˜
- UIã§å®Ÿé¨“æ¯”è¼ƒãƒ»ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤ç®¡ç†

ã“ã®èƒŒå¾Œã«ã‚ã‚‹ç†è«–:

$$
\begin{aligned}
\text{Reproducibility} &= f(\text{Code}, \text{Data}, \text{Hyperparams}, \text{Env}, \text{Seed}) \\
\text{MLOps Goal:} \quad & \text{Track all 5 dimensions automatically}
\end{aligned}
$$

**ã‚³ãƒ¼ãƒ‰ã ã‘ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã—ã¦ã‚‚å†ç¾ã§ããªã„ã€‚ãƒ‡ãƒ¼ã‚¿ã‚‚ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚‚ç’°å¢ƒã‚‚Seedã‚‚å…¨ã¦è¨˜éŒ²ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚** ã“ã‚ŒãŒMLflowã®å“²å­¦ã ã€‚

:::message
**é€²æ—: 3% å®Œäº†** å®Ÿé¨“è¨˜éŒ²ã®æ ¸å¿ƒã‚’ä½“æ„Ÿã—ãŸã€‚ã“ã“ã‹ã‚‰MLOpså…¨7é ˜åŸŸ(Version/Experiment/CI-CD/A-B/Monitor/Drift/RLHF)ã‚’ç¶²ç¾…ã—ã¦ã„ãã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” MLOpsãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å…¨ä½“åƒã‚’è§¦ã‚‹

### 1.1 MLOpsã®7ã¤ã®ãƒ”ãƒ¼ã‚¹

MLOpsã¯å˜ä¸€æŠ€è¡“ã§ã¯ãªãã€**7ã¤ã®ã‚·ã‚¹ãƒ†ãƒ ã®çµ±åˆ**ã ã€‚

| ãƒ”ãƒ¼ã‚¹ | å½¹å‰² | ä»£è¡¨ãƒ„ãƒ¼ãƒ« | æ¾å°¾ç ”ã®æ‰±ã„ |
|:------|:-----|:----------|:-----------|
| **ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°** | ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ»ã‚³ãƒ¼ãƒ‰ã®å±¥æ­´ç®¡ç† | Git LFS, DVC, MLflow Registry | âŒè¨€åŠãªã— |
| **å®Ÿé¨“ç®¡ç†** | ãƒã‚¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ² | MLflow, W&B, Neptune | âš ï¸æ¦‚å¿µã®ã¿ |
| **CI/CD for ML** | è‡ªå‹•ãƒ†ã‚¹ãƒˆãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ»ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ | GitHub Actions, Jenkins | âŒå®Ÿè£…ãªã— |
| **A/Bãƒ†ã‚¹ãƒˆ** | æ–°æ—§ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒãƒ»æ®µéšçš„ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ | Feature Flags, Traffic Split | âŒå®Ÿè£…ãªã— |
| **ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°** | ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ãƒ»SLI/SLOãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆ | Prometheus, Grafana | âŒå®Ÿè£…ãªã— |
| **ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º** | ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ¢ãƒ‡ãƒ«åŠ£åŒ–ã®è‡ªå‹•æ¤œå‡º | Evidently AI, KS test, PSI | âŒå®Ÿè£…ãªã— |
| **RLHF/DPO** | äººé–“ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æœ€é©åŒ– | DPO [^1], PPO, Reward Modeling | âš ï¸æ¦‚å¿µã®ã¿ |

**æ¾å°¾ç ”ã¯"è¨“ç·´"ã§æ­¢ã¾ã‚‹ã€‚æœ¬è¬›ç¾©ã¯"é‹ç”¨"ã¾ã§å®Œçµã•ã›ã‚‹ã€‚**

#### 1.1.1 MLflowã§å®Ÿé¨“ã‚’æ¯”è¼ƒã™ã‚‹

å®Ÿé¨“ç®¡ç†ã®æœ¬è³ª = **ã€ŒåŒã˜ã‚³ãƒ¼ãƒ‰ã§ã‚‚ãƒã‚¤ãƒ‘ãƒ©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒé•ãˆã°åˆ¥å®Ÿé¨“ã€**ã€‚

```python
import mlflow

# Run 1: lr=0.001
with mlflow.start_run(run_name="run-lr-0.001"):
    mlflow.log_param("lr", 0.001)
    mlflow.log_param("batch_size", 32)
    mlflow.log_metric("val_acc", 0.952)
    mlflow.log_metric("val_loss", 0.023)

# Run 2: lr=0.01 (higher LR)
with mlflow.start_run(run_name="run-lr-0.01"):
    mlflow.log_param("lr", 0.01)
    mlflow.log_param("batch_size", 32)
    mlflow.log_metric("val_acc", 0.968)  # Better!
    mlflow.log_metric("val_loss", 0.019)

# UI: mlflow ui --backend-store-uri sqlite:///mlflow.db
```

MLflow UIã§2ã¤ã®runã‚’æ¨ªä¸¦ã³æ¯”è¼ƒ:

| Run | lr | val_acc | val_loss | Winner |
|:----|:---|:--------|:---------|:-------|
| run-lr-0.001 | 0.001 | 0.952 | 0.023 | âŒ |
| run-lr-0.01 | 0.01 | **0.968** | **0.019** | âœ… |

**lr=0.01ãŒå‹ã£ãŸã€‚ã“ã®"å‹ã£ãŸãƒ¢ãƒ‡ãƒ«"ã‚’Model Registryã«ç™»éŒ²ã—ã€Productionã‚¹ãƒ†ãƒ¼ã‚¸ã«æ˜‡æ ¼ã•ã›ã‚‹ã€‚**

#### 1.1.2 DVCã§ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã™ã‚‹

å¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ(10GB+)ã¯Gitã«å…¥ã‚‰ãªã„ã€‚DVC [^2] ãŒè§£æ±ºã™ã‚‹ã€‚

```bash
# Initialize DVC
dvc init

# Track large dataset (stores pointer in Git, actual data in remote storage)
dvc add data/train.csv
git add data/train.csv.dvc .gitignore
git commit -m "Track train.csv with DVC"

# Push data to remote (S3/GCS/Azure)
dvc remote add -d myremote s3://my-bucket/dvc-store
dvc push

# Checkout data version (like git checkout)
git checkout experiment-v2
dvc checkout  # Downloads data/train.csv version from experiment-v2
```

**Gitã¯ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ« `.dvc` ã‚’ç®¡ç†ã—ã€DVCãŒå®Ÿãƒ‡ãƒ¼ã‚¿ã‚’S3/GCSã‹ã‚‰å–å¾—ã™ã‚‹ã€‚ãƒ‡ãƒ¼ã‚¿ã‚‚ã‚³ãƒ¼ãƒ‰ã¨åŒã˜ããƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã§ãã‚‹ã€‚**

#### 1.1.3 GitHub Actionsã§è‡ªå‹•ãƒ†ã‚¹ãƒˆ

CI/CD for MLã®åŸºæœ¬ = **ã€Œã‚³ãƒŸãƒƒãƒˆã”ã¨ã«ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ†ã‚¹ãƒˆã€**ã€‚

```yaml
# .github/workflows/ml-ci.yml
name: ML CI/CD

on: [push, pull_request]

jobs:
  test-model:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Run data validation tests
        run: pytest tests/test_data_quality.py

      - name: Train model and test performance
        run: |
          python train.py --config configs/test.yaml
          python evaluate.py --threshold 0.95  # Fail if accuracy < 95%

      - name: Test inference latency
        run: |
          python benchmark_latency.py --max-p99 100  # Fail if p99 > 100ms
```

**ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ãŸã‚‰è‡ªå‹•çš„ã«PRãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã‚‹ã€‚æ€§èƒ½åŠ£åŒ–ã‚’é˜²ãã‚²ãƒ¼ãƒˆã‚­ãƒ¼ãƒ‘ãƒ¼ã€‚**

#### 1.1.4 Prometheusã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²ã™ã‚‹

æœ¬ç•ªãƒ¢ãƒ‡ãƒ«ã®å¥å…¨æ€§ = **RED Metrics (Rate / Errors / Duration)**ã€‚

```python
from prometheus_client import Counter, Histogram, start_http_server
import time

# Define metrics
REQUEST_COUNT = Counter('model_requests_total', 'Total requests')
ERROR_COUNT = Counter('model_errors_total', 'Total errors')
LATENCY = Histogram('model_latency_seconds', 'Inference latency')

# Expose metrics on :8000/metrics
start_http_server(8000)

def predict(input_data):
    REQUEST_COUNT.inc()  # Increment request count
    start_time = time.time()

    try:
        # Model inference
        result = model.predict(input_data)
        LATENCY.observe(time.time() - start_time)  # Record latency
        return result
    except Exception as e:
        ERROR_COUNT.inc()  # Increment error count
        raise e
```

Prometheus scrapes `/metrics` endpoint every 15s:

```
# HELP model_requests_total Total requests
# TYPE model_requests_total counter
model_requests_total 15234.0

# HELP model_errors_total Total errors
# TYPE model_errors_total counter
model_errors_total 12.0

# HELP model_latency_seconds Inference latency
# TYPE model_latency_seconds histogram
model_latency_seconds_bucket{le="0.05"} 12000.0
model_latency_seconds_bucket{le="0.1"} 14800.0
model_latency_seconds_bucket{le="+Inf"} 15234.0
model_latency_seconds_sum 876.3
model_latency_seconds_count 15234.0
```

**Grafanaã§ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰åŒ–ã™ã‚Œã°ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ã‚¨ãƒ©ãƒ¼ç‡ãƒ»ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’ç›£è¦–ã§ãã‚‹ã€‚**

#### 1.1.5 A/Bãƒ†ã‚¹ãƒˆã§ãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒã™ã‚‹

æ–°ãƒ¢ãƒ‡ãƒ«ã‚’ã„ããªã‚Š100%ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é©ç”¨ã™ã‚‹ã®ã¯å±é™ºã€‚**1% â†’ 5% â†’ 25% â†’ 100% ã®æ®µéšçš„ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ** (Canary Deployment)ã€‚

```python
import random

def route_traffic(user_id):
    # Hash user_id for consistent assignment
    hash_val = hash(user_id) % 100

    if hash_val < 1:  # 1% to canary (new model)
        return "model_v2"
    else:  # 99% to baseline (old model)
        return "model_v1"

def predict_with_ab(user_id, input_data):
    model_version = route_traffic(user_id)

    if model_version == "model_v2":
        result = model_v2.predict(input_data)
        log_metric("model_v2_requests", 1)
    else:
        result = model_v1.predict(input_data)
        log_metric("model_v1_requests", 1)

    return result
```

**1%ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã§ã‚¨ãƒ©ãƒ¼ç‡ãŒä¸ŠãŒã£ãŸã‚‰å³åº§ã«ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚å•é¡Œãªã‘ã‚Œã°5%ã«æ‹¡å¤§ã€‚**

#### 1.1.6 ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆã‚’æ¤œå‡ºã™ã‚‹

è¨“ç·´æ™‚ã¨æœ¬ç•ªãƒ‡ãƒ¼ã‚¿ãŒä¹–é›¢ã™ã‚‹ã¨æ€§èƒ½ãŒåŠ£åŒ–ã™ã‚‹ã€‚**KSæ¤œå®š / PSI (Population Stability Index)** ã§è‡ªå‹•æ¤œå‡ºã€‚

```python
from scipy.stats import ks_2samp
import numpy as np

# Training data distribution (baseline)
train_feature = np.random.normal(0, 1, 10000)

# Production data distribution (could drift over time)
prod_feature = np.random.normal(0.5, 1.2, 1000)  # Mean shift + variance increase

# Kolmogorov-Smirnov test
statistic, p_value = ks_2samp(train_feature, prod_feature)

if p_value < 0.01:  # Significant drift detected
    print(f"âš ï¸ Data drift detected! KS statistic: {statistic:.4f}, p-value: {p_value:.4e}")
    trigger_retraining()
else:
    print(f"âœ… No drift. KS statistic: {statistic:.4f}, p-value: {p_value:.4f}")
```

å‡ºåŠ›:
```
âš ï¸ Data drift detected! KS statistic: 0.2341, p-value: 3.42e-12
```

**ãƒ‰ãƒªãƒ•ãƒˆã‚’æ¤œå‡ºã—ãŸã‚‰è‡ªå‹•çš„ã«å†è¨“ç·´ã‚’ãƒˆãƒªã‚¬ãƒ¼ã™ã‚‹ã€‚**

#### 1.1.7 DPO/RLHFã§äººé–“ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’çµ„ã¿è¾¼ã‚€

LLMã®å‡ºåŠ›ã‚’ã€Œäººé–“ã®å¥½ã¿ã€ã«åˆã‚ã›ã‚‹ã€‚**DPO (Direct Preference Optimization)** [^1] ã¯RLHF without RL â€” PPOã‚ˆã‚Šå®‰å®šã€‚

DPO loss (ç°¡ç•¥ç‰ˆ):

$$
\mathcal{L}_{\text{DPO}} = -\log \sigma\left( \beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)} \right)
$$

- $y_w$: å¥½ã¾ã—ã„å¿œç­” (preferred)
- $y_l$: å¥½ã¾ã—ããªã„å¿œç­” (rejected)
- $\pi_{\text{ref}}$: Reference model (å…ƒã®ãƒ¢ãƒ‡ãƒ«)
- $\beta$: KLæ­£å‰‡åŒ–ã®å¼·ã•

**ã€Œå¥½ã¾ã—ã„å¿œç­”ã®ç¢ºç‡ã‚’ä¸Šã’ã€å¥½ã¾ã—ããªã„å¿œç­”ã®ç¢ºç‡ã‚’ä¸‹ã’ã‚‹ã€ã‚’1ã¤ã®lossã§å®Ÿç¾ã€‚PPOã®ä¸å®‰å®šæ€§ã‚’å›é¿ã€‚**

### 1.2 MLOpså…¨ä½“ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼

```mermaid
graph TD
    A["ğŸ‘¨â€ğŸ’» Developer"] -->|push code| B["ğŸ“¦ Git + DVC"]
    B -->|trigger| C["ğŸ”§ CI/CD Pipeline"]
    C -->|train & test| D["ğŸ§  Model Training"]
    D -->|log metrics| E["ğŸ“Š MLflow/W&B"]
    E -->|register| F["ğŸ›ï¸ Model Registry"]
    F -->|deploy| G["ğŸš€ Staging Env"]
    G -->|A/B test| H["ğŸ¯ 1% Canary"]
    H -->|monitor| I["ğŸ“ˆ Prometheus/Grafana"]
    I -->|drift detection| J["âš ï¸ Alert"]
    J -->|auto-retrain| D
    H -->|gradual rollout| K["âœ… 100% Production"]
    K -->|collect feedback| L["ğŸ—£ï¸ Human Feedback"]
    L -->|DPO/RLHF| D

    style A fill:#e3f2fd
    style D fill:#fff3e0
    style E fill:#f3e5f5
    style F fill:#e8f5e9
    style I fill:#fce4ec
    style J fill:#ffebee
    style K fill:#c8e6c9
```

**7ã¤ã®ãƒ”ãƒ¼ã‚¹ãŒç’°ã‚’æˆã™ã€‚ã“ã‚ŒãŒMLOpsã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã ã€‚**

:::message
**é€²æ—: 10% å®Œäº†** MLOpså…¨ä½“åƒã‚’ä¿¯ç°ã—ãŸã€‚Zone 2ã§ã€ŒãªãœMLOpsãŒå¿…é ˆã‹ã€ã‚’æ˜ã‚Šä¸‹ã’ã‚‹ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœMLOpsã¯å¿…é ˆãªã®ã‹

### 2.1 å¾“æ¥ã®ç ”ç©¶â†’æœ¬ç•ªã‚®ãƒ£ãƒƒãƒ—

æ¾å°¾ç ”ãŒæ‰±ã†ã€Œç ”ç©¶ãƒ¬ãƒ™ãƒ«MLã€ã¨ã€Œæœ¬ç•ªMLã€ã¯**åˆ¥ã®æƒ‘æ˜Ÿ**ã ã€‚

| è¦³ç‚¹ | ç ”ç©¶ãƒ¬ãƒ™ãƒ« (æ¾å°¾ç ”) | æœ¬ç•ªãƒ¬ãƒ™ãƒ« (MLOps) |
|:-----|:------------------|:------------------|
| **ãƒ‡ãƒ¼ã‚¿** | å›ºå®šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (MNIST/ImageNet) | ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ»æ™‚é–“å¤‰å‹•ãƒ»ãƒ‰ãƒªãƒ•ãƒˆ |
| **ãƒ¢ãƒ‡ãƒ«** | 1å›è¨“ç·´ã—ã¦çµ‚ã‚ã‚Š | é€±æ¬¡/æ—¥æ¬¡ã§å†è¨“ç·´ãƒ»A/Bãƒ†ã‚¹ãƒˆ |
| **è©•ä¾¡** | Validation setã§1å›æ¸¬å®š | ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§SLI/SLOç›£è¦– |
| **ãƒ‡ãƒ—ãƒ­ã‚¤** | âŒæ‰±ã‚ãªã„ | Blue-Green/Canary/Feature Flags |
| **éšœå®³å¯¾å¿œ** | âŒæ‰±ã‚ãªã„ | è‡ªå‹•ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆãƒ»on-call |
| **èª¬æ˜è²¬ä»»** | è«–æ–‡æŸ»èª­ã®ã¿ | ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ»æ³•è¦åˆ¶ãƒ»ç›£æŸ» |

**ç ”ç©¶ã§ã¯ "accuracy 0.95" ã§çµ‚ã‚ã‚Šã€‚æœ¬ç•ªã§ã¯ "p99 latency < 100ms, uptime > 99.9%, drift detection within 1 hour" ãŒæ±‚ã‚ã‚‰ã‚Œã‚‹ã€‚**

### 2.2 Course IIIã§ã®ä½ç½®ã¥ã‘ â€” ç¬¬30å›ã‹ã‚‰ç¬¬31å›ã¸

```mermaid
graph LR
    A["ç¬¬19å›<br/>Elixirç™»å ´"] --> B["ç¬¬20-22å›<br/>Train Pipeline"]
    B --> C["ç¬¬23-25å›<br/>Fine-tune/Stats/Causal"]
    C --> D["ç¬¬26å›<br/>æ¨è«–æœ€é©åŒ–"]
    D --> E["ç¬¬27å›<br/>è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"]
    E --> F["ç¬¬28-29å›<br/>Prompt/RAG"]
    F --> G["ç¬¬30å›<br/>ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Œå…¨ç‰ˆ"]
    G --> H["ğŸ”„ ç¬¬31å›<br/>MLOpså®Œå…¨ç‰ˆ"]
    H --> I["ç¬¬32å›<br/>çµ±åˆPJ+èª­äº†æ„Ÿ"]

    style H fill:#ffeb3b
    style I fill:#4caf50
```

- **ç¬¬30å›**: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æ§‹ç¯‰ã—ãŸ â†’ ã€Œå‹•ãAIã€ã‚’ä½œã£ãŸ
- **ç¬¬31å›**: MLOpså…¨é ˜åŸŸ â†’ ã€Œå‹•ãç¶šã‘ã‚‹AIã€ã«ã™ã‚‹
- **ç¬¬32å›**: çµ±åˆPJ â†’ Trainâ†’Evalâ†’Deployâ†’Monitorâ†’Feedbackã®ãƒ•ãƒ«ã‚µã‚¤ã‚¯ãƒ«å®Ÿè£…

**Course IIIã®ã‚´ãƒ¼ãƒ« = "ç ”ç©¶ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—" â†’ "Production-ready system"**

### 2.3 3ã¤ã®ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼

#### 2.3.1 MLOps = ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™ºã®ã€Œç©ºæ°—ã€

å¾“æ¥ã®ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™ºã§ã¯ã€Git/CI/CD/ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¯**å½“ãŸã‚Šå‰**ã ã€‚èª°ã‚‚ã€ŒGitã‚’ä½¿ã†ã‹ã©ã†ã‹è­°è«–ã€ã—ãªã„ã€‚

MLã§ã‚‚åŒã˜ã¯ãšãªã®ã«ã€**å¤šãã®ãƒãƒ¼ãƒ ãŒGitã™ã‚‰ä½¿ã£ã¦ã„ãªã„**ã€‚å®Ÿé¨“ãƒãƒ¼ãƒˆæ‰‹æ›¸ãã€ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« `model_final_v2_REALLY_FINAL.pkl`ã€‚

**MLOpsã¯ã€ŒMLã«ã‚‚DevOpsã¨åŒã˜è¦å¾‹ã‚’ã€ã¨ã„ã†å½“ç„¶ã®ä¸»å¼µã«éããªã„ã€‚**

#### 2.3.2 MLOps = ç”Ÿãç‰©ã®é£¼è‚²

ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã¯ä¸€åº¦æ›¸ã‘ã°ã€Œå‹•ãç¶šã‘ã‚‹ã€(ç†æƒ³çš„ã«ã¯)ã€‚MLãƒ¢ãƒ‡ãƒ«ã¯**ç”Ÿãç‰©**ã ã€‚

- ãƒ‡ãƒ¼ã‚¿ãŒå¤‰ã‚ã‚‹ â†’ æ€§èƒ½åŠ£åŒ–
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å—œå¥½ãŒå¤‰ã‚ã‚‹ â†’ å¥½ã¾ã‚Œãªã„å‡ºåŠ›
- æ–°ã—ã„æ”»æ’ƒãƒ‘ã‚¿ãƒ¼ãƒ³ãŒç¾ã‚Œã‚‹ â†’ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è„†å¼±æ€§

**ã€Œè¨“ç·´ã—ã¦çµ‚ã‚ã‚Šã€ã¯ã€ãƒšãƒƒãƒˆã‚’è²·ã£ã¦1å›ã‚¨ã‚µã‚’ã‚„ã£ã¦æ”¾ç½®ã™ã‚‹ã®ã¨åŒã˜ã€‚MLOpsã¯ "ç¶™ç¶šçš„ãªä¸–è©±" ã‚’è‡ªå‹•åŒ–ã™ã‚‹ã€‚**

#### 2.3.3 MLOps = ä¿é™ºå¥‘ç´„

å®Ÿé¨“ç®¡ç†ãƒ»ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ãƒ»ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¯ã€Œä»Šã™ãå½¹ç«‹ã¤ã€ã‚ã‘ã§ã¯ãªã„ã€‚äº‹æ•…ãŒèµ·ããŸã¨ãã«å½¹ç«‹ã¤ã€‚

- æ€§èƒ½ãŒçªç„¶è½ã¡ãŸ â†’ ã€Œã©ã®ã‚³ãƒŸãƒƒãƒˆã§åŠ£åŒ–ã—ãŸã‹ã€ã‚’ç‰¹å®š
- æœ¬ç•ªã§ã‚¨ãƒ©ãƒ¼ â†’ ã€Œã©ã®ãƒ‡ãƒ¼ã‚¿ã§å¤±æ•—ã—ãŸã‹ã€ã‚’å†ç¾
- è¦åˆ¶ç›£æŸ» â†’ ã€Œã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã„ã¤ã€ã©ã®ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ã•ã‚ŒãŸã‹ã€ã‚’è¨¼æ˜

**ä¿é™ºæ–™(MLOpså°å…¥ã‚³ã‚¹ãƒˆ)ã‚’æ‰•ã‚ãªã„ãƒãƒ¼ãƒ ã¯ã€äº‹æ•…ãŒèµ·ããŸã¨ãã«å…¨æã™ã‚‹ã€‚**

### 2.4 æ¾å°¾ç ”ã¨ã®å·®åˆ¥åŒ– â€” å®Ÿè£…ã®æœ‰ç„¡

| é …ç›® | æ¾å°¾ç ” | æœ¬è¬›ç¾© (ç¬¬31å›) |
|:-----|:------|:-------------|
| MLflowã®æ‰±ã„ | âš ï¸ã‚¹ãƒ©ã‚¤ãƒ‰1æšã§ã€Œã“ã†ã„ã†ãƒ„ãƒ¼ãƒ«ãŒã‚ã‚‹ã€ | âœ…Juliaçµ±åˆå®Ÿè£… (200è¡Œ) |
| DVCã®æ‰±ã„ | âŒè¨€åŠãªã— | âœ…CLIæ“ä½œ + S3çµ±åˆ |
| CI/CDã®æ‰±ã„ | âŒè¨€åŠãªã— | âœ…GitHub Actionså®Ÿè£… |
| A/Bãƒ†ã‚¹ãƒˆ | âŒè¨€åŠãªã— | âœ…çµ±è¨ˆçš„æ¤œå‡ºåŠ›è¨ˆç®— + å®Ÿè£… |
| ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º | âŒè¨€åŠãªã— | âœ…KSæ¤œå®š/PSIå®Ÿè£… |
| DPO/RLHF | âš ï¸ã‚¹ãƒ©ã‚¤ãƒ‰æ¦‚è¦ã®ã¿ | âœ…æ•°å¼å®Œå…¨å°å‡º + Bradley-Terry Model |

**æ¾å°¾ç ” = ã€Œã“ã†ã„ã†æ¦‚å¿µãŒã‚ã‚‹ã€ã§æ­¢ã¾ã‚‹ã€‚æœ¬è¬›ç¾© = æ•°å¼å°å‡º + å®Ÿè£… + æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤ã¾ã§ã€‚**

### 2.5 LLMã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚° â€” ç¬¬1-8å›ã®æ•°å­¦ãŒã©ã“ã§ä½¿ã‚ã‚Œã‚‹ã‹

MLOpsã¯çµ±è¨ˆå­¦ãƒ»ç¢ºç‡è«–ãƒ»æƒ…å ±ç†è«–ã®å¿œç”¨å•é¡Œã ã€‚

| Course I æ•°å­¦ | MLOpså¿œç”¨ |
|:-------------|:---------|
| **ç¬¬4å›: ç¢ºç‡è«–** | A/Bãƒ†ã‚¹ãƒˆã®çµ±è¨ˆçš„æ¤œå‡ºåŠ›è¨ˆç®— / ãƒ™ã‚¤ã‚ºæ›´æ–°ã§æ®µéšçš„ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ |
| **ç¬¬5å›: æ¸¬åº¦è«–** | ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º (KSæ¤œå®š = ç´¯ç©åˆ†å¸ƒé–¢æ•°ã®è·é›¢) |
| **ç¬¬6å›: æƒ…å ±ç†è«–** | DPO loss = KL divergenceæœ€å°åŒ– / PSI = KL divergenceã®é›¢æ•£ç‰ˆ |
| **ç¬¬7å›: MLE** | Reward Modeling = preference dataã‹ã‚‰ã®MLE |

**Course Iã®æ•°å­¦ãªã—ã«MLOpsã®ç†è«–ã¯ç†è§£ã§ããªã„ã€‚**

### 2.6 å­¦ç¿’æˆ¦ç•¥ â€” Part A-Gã®å·¨å¤§æ§‹é€ 

æœ¬è¬›ç¾©ã¯**~3,500è¡Œ**ã®å¤§ä½œã€‚7ã¤ã®ãƒ‘ãƒ¼ãƒˆã«åˆ†å‰²ã•ã‚Œã¦ã„ã‚‹ã€‚

| Part | ãƒ†ãƒ¼ãƒ | æƒ³å®šè¡Œæ•° | å„ªå…ˆåº¦ |
|:-----|:------|:---------|:-------|
| **Part A** | ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚° & å®Ÿé¨“ç®¡ç† | 750 | â˜…â˜…â˜… |
| **Part B** | CI/CD for ML | 700 | â˜…â˜…â˜… |
| **Part C** | A/Bãƒ†ã‚¹ãƒˆ & ã‚«ãƒŠãƒªã‚¢ãƒªãƒªãƒ¼ã‚¹ | 700 | â˜…â˜…â˜… |
| **Part D** | ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚° & SLI/SLO | 600 | â˜…â˜…â˜… |
| **Part E** | DPO/RLHFåŸºç¤ | 400 | â˜…â˜…â˜… |
| **Part F** | å®Ÿè£…ç·¨ (âš¡Julia + ğŸ¦€Rust + ğŸ”®Elixir) | 600 | â˜…â˜…â˜… |
| **Part G** | æœ€æ–°ç ”ç©¶ (2024-2026) | 250 | â˜…â˜… |

**æ¨å¥¨å­¦ç¿’é †åº**:

1. **Part A-E (ç†è«–)** ã‚’1å›é€šèª­ (æ•°å¼ã¯é£›ã°ã—ã¦OK)
2. **Part F (å®Ÿè£…)** ã‚’æ‰‹ã‚’å‹•ã‹ã™
3. Part A-Eã«æˆ»ã‚Šã€æ•°å¼ã‚’ä¸å¯§ã«è¿½ã†

**æ•°å¼ã‚’æœ€åˆã‹ã‚‰å…¨éƒ¨ç†è§£ã—ã‚ˆã†ã¨ã™ã‚‹ã¨æŒ«æŠ˜ã™ã‚‹ã€‚å®Ÿè£…ã‚’å…ˆã«è§¦ã£ã¦ã€Œä½•ã‚’ã‚„ã£ã¦ã„ã‚‹ã‹ã€ã‚’ä½“æ„Ÿã—ã¦ã‹ã‚‰ã€æ•°å¼ã«æˆ»ã‚‹ã€‚**

### 2.7 æœ€æ–°ç ”ç©¶å‹•å‘ (2024-2026)

#### 2.7.1 DPO/RLHFçµ±åˆ

**è«–æ–‡**: Direct Preference Optimization [^1] (Rafailov et al., NeurIPS 2023)

**ä¸»è¦è²¢çŒ®**:

- PPOä¸è¦ã§preference dataã‹ã‚‰ç›´æ¥æœ€é©åŒ–
- Bradley-Terry Modelã®é–‰å½¢å¼è§£
- å®‰å®šè¨“ç·´ (PPOã®10å€å®‰å®š)

**2025å¹´ã®å‹•å‘**:

- DPO variantsãŒä¸»æµ (IPO, KTO)
- Online RLHF (ç¶™ç¶šçš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†)
- Multi-objective RLHF (è¤‡æ•°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®åŒæ™‚æœ€é©åŒ–)

#### 2.7.2 Automated MLOps

**è«–æ–‡**: AutoMLOps (Google Research, 2024)

**ä¸»è¦è²¢çŒ®**:

- ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è‡ªå‹•ç”Ÿæˆ (Trainâ†’Deploy)
- ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºâ†’å†è¨“ç·´ã®è‡ªå‹•ãƒˆãƒªã‚¬ãƒ¼
- SLOé•åâ†’è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°

**å®Ÿè£…**: Vertex AI Pipelines, AWS SageMaker Pipelines

#### 2.7.3 Federated MLOps

**è«–æ–‡**: Federated Learning at Scale (Google, 2024)

**ä¸»è¦è²¢çŒ®**:

- åˆ†æ•£è¨“ç·´ã®MLOps (ãƒ‡ãƒã‚¤ã‚¹ä¸Šã§è¨“ç·´)
- Privacy-preserving monitoring
- Differential Privacyçµ±åˆ

#### 2.7.4 Online RLHF â€” ç¶™ç¶šçš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†

**è«–æ–‡**: Online Iterative RLHF (DeepMind, 2025)

**ä¸»è¦è²¢çŒ®**:

- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†
- ç¶™ç¶šçš„ãƒ¢ãƒ‡ãƒ«æ›´æ–° (æ—¥æ¬¡/é€±æ¬¡)
- A/Bãƒ†ã‚¹ãƒˆã¨ã®çµ±åˆ

**å®Ÿè£…**: Gemini/Claude APIã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒœã‚¿ãƒ³ â†’ preference data â†’ DPOå†è¨“ç·´ â†’ ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤

**èª²é¡Œ**:

- Feedback biasã®ç®¡ç† (ä¸æº€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã¿ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯)
- Distribution shiftã®æ¤œå‡º (ãƒ¦ãƒ¼ã‚¶ãƒ¼å±æ€§ã®å¤‰åŒ–)
- Temporal consistencyã®ä¿è¨¼ (æ˜¨æ—¥ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯vsä»Šæ—¥)

#### 2.7.5 Multi-objective RLHF

**è«–æ–‡**: Pareto-optimal RLHF (OpenAI, 2025)

**ä¸»è¦è²¢çŒ®**:

- è¤‡æ•°ãƒ¡ãƒˆãƒªã‚¯ã‚¹åŒæ™‚æœ€é©åŒ– (helpfulness + harmlessness + factuality)
- Pareto frontierã®æ¢ç´¢
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã«æœ€é©ãªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’é¸æŠ

**æ•°å¼** (multi-objective DPO):

$$
\mathcal{L}_{\text{MO-DPO}} = -\mathbb{E} \left[ \sum_{i=1}^{K} w_i \log \sigma\left( \beta \log \frac{\pi_\theta(y_w^{(i)} \mid x)}{\pi_{\text{ref}}(y_w^{(i)} \mid x)} - \beta \log \frac{\pi_\theta(y_l^{(i)} \mid x)}{\pi_{\text{ref}}(y_l^{(i)} \mid x)} \right) \right]
$$

- $K$: ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ•°
- $w_i$: é‡ã¿ (ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã«èª¿æ•´å¯èƒ½)

**2026å¹´ã®å±•æœ›**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã§ã€Œå‰µé€ æ€§ vs æ­£ç¢ºæ€§ã€ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’èª¿æ•´ã§ãã‚‹LLMã€‚

:::message
**é€²æ—: 25% å®Œäº†** ãªãœMLOpsãŒå¿…é ˆã‹ + æœ€æ–°ç ”ç©¶ã‚’ç†è§£ã—ãŸã€‚Zone 3ã§7ãƒ‘ãƒ¼ãƒˆã®ç†è«–ã‚’ä¸€æ°—ã«æ§‹ç¯‰ã™ã‚‹ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ90åˆ†ï¼‰â€” MLOpså…¨7é ˜åŸŸã®ç†è«–

### Part A: ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚° & å®Ÿé¨“ç®¡ç†

#### 3.1 ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã®æ•°å­¦çš„åŸºç›¤

**ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã®æœ¬è³ª = ãƒãƒƒã‚·ãƒ¥é–¢æ•°ã«ã‚ˆã‚‹ä¸€æ„è­˜åˆ¥**ã€‚

ãƒ¢ãƒ‡ãƒ«ã®çŠ¶æ…‹ $\mathcal{M}_t$ ã‚’ä»¥ä¸‹ã®5-tupleã§å®šç¾©:

$$
\mathcal{M}_t = (\mathbf{w}_t, \mathcal{D}_t, \mathcal{H}_t, \mathcal{E}_t, s_t)
$$

- $\mathbf{w}_t \in \mathbb{R}^p$: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ™ã‚¯ãƒˆãƒ« (é‡ã¿)
- $\mathcal{D}_t$: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†å¯¾è±¡)
- $\mathcal{H}_t$: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é›†åˆ $\{\eta, \lambda, \text{batch\_size}, \ldots\}$
- $\mathcal{E}_t$: ç’°å¢ƒ (Python version, CUDA version, library versions)
- $s_t \in \{0, 1, \ldots, 2^{64}-1\}$: Random seed

**å†ç¾æ€§ã®å…¬ç†**:

$$
\mathcal{M}_t = \mathcal{M}_{t'} \iff \text{Hash}(\mathcal{M}_t) = \text{Hash}(\mathcal{M}_{t'})
$$

ãƒãƒƒã‚·ãƒ¥é–¢æ•° $\text{Hash}: \mathcal{M} \to \{0,1\}^{256}$ (SHA-256) ãŒåŒã˜ãªã‚‰ã€ãƒ¢ãƒ‡ãƒ«ã¯**å®Œå…¨ã«å†ç¾å¯èƒ½**ã€‚

##### Git LFSã«ã‚ˆã‚‹å¤§ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†

Gitã¯å°ã•ãªãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å‘ã‘ã€‚ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« (500MB+) ã¯Git LFSã§ç®¡ç†ã€‚

Git LFSã®ä»•çµ„ã¿:

1. å¤§ãƒ•ã‚¡ã‚¤ãƒ« `model.safetensors` ã‚’ `.git/lfs/objects/` ã«ä¿å­˜
2. Gitã«ã¯**ãƒã‚¤ãƒ³ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«**ã®ã¿ commit:

```
version https://git-lfs.github.com/spec/v1
oid sha256:4d7a214614ab2935c1f0e1c69a0d3e82a5bb9e6e8e1e3a0c9f5d4c3b2a1b0c1d
size 524288000
```

3. `git pull` æ™‚ã€LFSã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰å®Ÿãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

**åˆ©ç‚¹**: Gitãƒªãƒã‚¸ãƒˆãƒªã¯è»½é‡ (ãƒã‚¤ãƒ³ã‚¿ã®ã¿)ã€‚å®Ÿãƒ•ã‚¡ã‚¤ãƒ«ã¯å°‚ç”¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã€‚

##### DVCã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°

DVC [^2] ã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç‰ˆGit LFSã€‚

**DVCã®ä»•çµ„ã¿**:

1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ `data/train.csv` (10GB) ã‚’è¿½è·¡:

```bash
dvc add data/train.csv
```

2. DVCãŒ `.dvc` ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ:

```yaml
# data/train.csv.dvc
outs:
- md5: a3f9c2e1b4d87f3a9c2e1b4d87f3a9c2
  size: 10737418240
  path: train.csv
```

3. Gitã¯ `.dvc` ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ç®¡ç†ã€‚å®Ÿãƒ‡ãƒ¼ã‚¿ã¯S3/GCS/Azureã«ä¿å­˜:

```bash
dvc remote add -d myremote s3://my-bucket/dvc-store
dvc push
```

4. ä»–ã®ãƒ¡ãƒ³ãƒãƒ¼ã¯ `dvc pull` ã§ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—:

```bash
git checkout experiment-v2
dvc checkout  # Downloads data version from experiment-v2
```

**æ•°å­¦çš„ãƒ¢ãƒ‡ãƒ«**:

$$
\begin{aligned}
\text{DVC Pointer:} \quad & p = (\text{md5}(\mathcal{D}), |\mathcal{D}|, \text{path}) \\
\text{Storage Mapping:} \quad & \mathcal{D} \mapsto \text{S3}://\text{bucket}/\text{md5}(\mathcal{D})[:2]/\text{md5}(\mathcal{D})[2:]
\end{aligned}
$$

**MD5ãƒãƒƒã‚·ãƒ¥ã®æœ€åˆ2æ–‡å­—ã§ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åˆ†å‰²ã—ã€è¡çªã‚’å›é¿ã€‚**

##### MLflow Model Registry

MLflowã¯ãƒ¢ãƒ‡ãƒ«ã‚’**ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã‚¹ãƒ†ãƒ¼ã‚¸**ã§ç®¡ç†ã€‚

| ã‚¹ãƒ†ãƒ¼ã‚¸ | æ„å‘³ | æ¬¡ã®ã‚¹ãƒ†ãƒ¼ã‚¸ |
|:--------|:-----|:-----------|
| `None` | ç™»éŒ²ç›´å¾Œ | `Staging` |
| `Staging` | ãƒ†ã‚¹ãƒˆç’°å¢ƒã«ãƒ‡ãƒ—ãƒ­ã‚¤ | `Production` |
| `Production` | æœ¬ç•ªç’°å¢ƒã§ç¨¼åƒä¸­ | `Archived` |
| `Archived` | å»ƒæ£„æ¸ˆã¿ | â€” |

**ã‚¹ãƒ†ãƒ¼ã‚¸é·ç§»ã®æ¡ä»¶**:

$$
\begin{aligned}
\text{None} \to \text{Staging:} \quad & \text{validation\_acc} \geq \theta_{\text{staging}} \\
\text{Staging} \to \text{Production:} \quad & \text{A/B test win} \land \text{latency} \leq \tau
\end{aligned}
$$

ä¾‹: $\theta_{\text{staging}} = 0.95$, $\tau = 100\text{ms}$ã€‚

**ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ä¸€æ„æ€§**:

$$
\text{Model ID} = (\text{name}, \text{version}, \text{run\_id})
$$

- `name`: ãƒ¢ãƒ‡ãƒ«å (e.g., "sentiment-classifier")
- `version`: æ•´æ•° (1, 2, 3, ...)
- `run_id`: MLflow Run UUID (è¨“ç·´æ™‚ã«è‡ªå‹•ç”Ÿæˆ)

**åŒã˜nameã§ã‚‚versionãŒé•ãˆã°åˆ¥ãƒ¢ãƒ‡ãƒ«ã€‚run_idã§è¨“ç·´æ™‚ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«é¡ã‚Œã‚‹ã€‚**

#### 3.2 å®Ÿé¨“ç®¡ç†ã®ç†è«–

**å®Ÿé¨“ç®¡ç†ã®æœ¬è³ª = ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“$\mathcal{H}$ä¸Šã®æ¢ç´¢å±¥æ­´ã®è¨˜éŒ²**ã€‚

##### å®Ÿé¨“ã®å®šç¾©

å®Ÿé¨“ $e_i$ ã‚’ä»¥ä¸‹ã®4-tupleã§å®šç¾©:

$$
e_i = (\mathbf{h}_i, \mathcal{D}_i, \mathbf{m}_i, \mathcal{A}_i)
$$

- $\mathbf{h}_i \in \mathcal{H}$: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ™ã‚¯ãƒˆãƒ«
- $\mathcal{D}_i$: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (train/val/test split)
- $\mathbf{m}_i \in \mathbb{R}^k$: ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ã‚¯ãƒˆãƒ« (loss, accuracy, F1, ...)
- $\mathcal{A}_i$: Artifacts (ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«, ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ, å›³)

**å®Ÿé¨“ã®æ¯”è¼ƒå¯èƒ½æ€§**:

$$
e_i \sim e_j \iff \mathcal{D}_i = \mathcal{D}_j
$$

åŒã˜ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãªã‘ã‚Œã°ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ¯”è¼ƒã§ããªã„ã€‚

##### ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨˜éŒ²

MLflowã¯ `log_metric(key, value, step)` ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ™‚ç³»åˆ—è¨˜éŒ²ã€‚

$$
\mathbf{m}(t) = \{(k_1, v_1(t)), (k_2, v_2(t)), \ldots, (k_n, v_n(t))\}
$$

ä¾‹: è¨“ç·´ãƒ«ãƒ¼ãƒ—ã§epochã”ã¨ã«è¨˜éŒ²:

```python
for epoch in range(num_epochs):
    train_loss = train_one_epoch()
    val_acc = validate()

    mlflow.log_metric("train_loss", train_loss, step=epoch)
    mlflow.log_metric("val_acc", val_acc, step=epoch)
```

**ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ™‚ç³»åˆ—ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã¦åæŸã‚’ç¢ºèªã§ãã‚‹ã€‚**

##### ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®æœ€é©åŒ–å•é¡Œ

ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ = **ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹æœ€é©åŒ–**:

$$
\mathbf{h}^* = \arg\max_{\mathbf{h} \in \mathcal{H}} f(\mathbf{h})
$$

- $f(\mathbf{h})$: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$\mathbf{h}$ã§è¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ã®validation metric
- $f$ã¯å¾®åˆ†ä¸å¯èƒ½ã€è©•ä¾¡ã«ã‚³ã‚¹ãƒˆ(è¨“ç·´æ™‚é–“)ãŒã‹ã‹ã‚‹

**æ¢ç´¢æ‰‹æ³•**:

| æ‰‹æ³• | èª¬æ˜ | è¨ˆç®—é‡ |
|:-----|:-----|:-------|
| Grid Search | $\mathcal{H}$ã‚’æ ¼å­çŠ¶ã«æ¢ç´¢ | $O(k^d)$ ($k$=å„æ¬¡å…ƒã®åˆ†å‰²æ•°, $d$=æ¬¡å…ƒ) |
| Random Search | ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° | $O(N)$ ($N$=è©¦è¡Œå›æ•°) |
| Bayesian Optimization | Gaussian Processã§$f$ã‚’ãƒ¢ãƒ‡ãƒ«åŒ– â†’ Acquisitioné–¢æ•°ã§æ¬¡ã®ç‚¹ã‚’é¸æŠ | $O(N^3)$ (GP) |
| Hyperband | Successive Halvingã§ä½æ€§èƒ½ãªè¨­å®šã‚’æ—©æœŸæ‰“ã¡åˆ‡ã‚Š | $O(N \log N)$ |

**å®Ÿè·µçš„æ¨å¥¨**: æœ€åˆã«Random Search (20-50 trials) â†’ æœ‰æœ›ãªé ˜åŸŸã§Bayesian Optã€‚

##### MLflowã¨W&Bã®æ¯”è¼ƒ

| è¦³ç‚¹ | MLflow | Weights & Biases (W&B) |
|:-----|:-------|:----------------------|
| **ãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚°** | Self-hosted (ç„¡æ–™) | Cloud (æœ‰æ–™, Free tierã‚ã‚Š) |
| **UI** | ã‚·ãƒ³ãƒ—ãƒ« | ãƒªãƒƒãƒ (ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚°ãƒ©ãƒ•, ãƒãƒ¼ãƒ å…±æœ‰) |
| **ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²** | `log_metric(key, value, step)` | `wandb.log({"key": value})` |
| **Artifactç®¡ç†** | S3/GCS/Azureçµ±åˆ | W&B Cloudè‡ªå‹•ç®¡ç† |
| **Model Registry** | âœ…ã‚ã‚Š | âœ…ã‚ã‚Š (W&B Registry) |
| **ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°** | âŒãªã— (å¤–éƒ¨ãƒ„ãƒ¼ãƒ«ä½µç”¨) | âœ…Sweeps (Bayesian Optå†…è”µ) |
| **ã‚³ã‚¹ãƒˆ** | ç„¡æ–™ (ã‚¤ãƒ³ãƒ•ãƒ©ä»£ã®ã¿) | Teamãƒ—ãƒ©ãƒ³ $50/user/month |

**MLflow = å®Œå…¨åˆ¶å¾¡ãƒ»ã‚³ã‚¹ãƒˆé‡è¦–ã€‚W&B = ç”Ÿç”£æ€§ãƒ»ãƒãƒ¼ãƒ å”æ¥­é‡è¦–ã€‚**

#### 3.3 å†ç¾æ€§ä¿è¨¼ã®æ•°å­¦

**å†ç¾æ€§ã®å®šç¾©**:

$$
\text{Reproducible}(e_i) \iff \forall j, \, (\text{Hash}(\mathcal{M}_i) = \text{Hash}(\mathcal{M}_j)) \implies \mathbf{m}_i = \mathbf{m}_j
$$

åŒã˜ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ãªã‚‰ã€åŒã˜ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒå¾—ã‚‰ã‚Œã‚‹ã€‚

**å†ç¾æ€§ã‚’ç ´å£Šã™ã‚‹è¦å› **:

1. **Non-deterministic operations**: CUDA `atomicAdd`, cuDNN auto-tuning
2. **Floating-point non-associativity**: $(a + b) + c \neq a + (b + c)$ (ä¸¸ã‚èª¤å·®)
3. **Untracked dependencies**: ã‚·ã‚¹ãƒ†ãƒ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€ç’°å¢ƒå¤‰æ•°

**å†ç¾æ€§ã‚’ä¿è¨¼ã™ã‚‹æ‰‹æ³•**:

##### 3.3.1 Environmentå›ºå®š

**Dockerã‚³ãƒ³ãƒ†ãƒŠ**ã§ç’°å¢ƒã‚’å‡çµ:

```dockerfile
FROM python:3.11-slim

# Pin library versions exactly
RUN pip install torch==2.1.0 transformers==4.35.0

# Copy code
COPY . /app
WORKDIR /app

CMD ["python", "train.py"]
```

**ãƒ™ãƒ¼ã‚¹ã‚¤ãƒ¡ãƒ¼ã‚¸ã®ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆã‚‚å›ºå®š**:

```dockerfile
FROM python@sha256:a3f9c2e1b4d87f3a9c2e1b4d87f3a9c2e1b4d87f3a9c2e1b4d87f3a9c2
```

##### 3.3.2 Seedå›ºå®š

å…¨ã¦ã®ä¹±æ•°ç”Ÿæˆã‚’seedã§åˆ¶å¾¡:

```python
import torch
import numpy as np
import random

def set_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    # Deterministic CUDA operations (slower but reproducible)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)
```

**`cudnn.deterministic = True`ã«ã™ã‚‹ã¨ã€cuDNNã¯æ±ºå®šçš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ã¿ä½¿ç”¨ (é€Ÿåº¦ä½ä¸‹ã‚ã‚Š)ã€‚**

##### 3.3.3 ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ $\mathcal{D}$ ã®å¤‰æ›´ã‚’è¿½è·¡:

$$
\text{Hash}(\mathcal{D}) = \text{SHA256}\left( \bigoplus_{i=1}^{N} x_i \right)
$$

- $\bigoplus$: XOR (é †åºã«ä¾å­˜ã—ãªã„)
- $x_i$: $i$ç•ªç›®ã®ã‚µãƒ³ãƒ—ãƒ«

**ãƒ‡ãƒ¼ã‚¿ã®é †åºã‚’å¤‰ãˆã¦ã‚‚åŒã˜ãƒãƒƒã‚·ãƒ¥ã«ã—ãŸã„å ´åˆã¯XORã‚’ä½¿ã† (commutative)ã€‚é †åºã‚‚å«ã‚ãŸã„å ´åˆã¯é€£çµã—ã¦SHA256ã€‚**

---

### Part B: CI/CD for ML

#### 3.4 CI/CD for MLã®æ§‹æˆè¦ç´ 

**å¾“æ¥ã®CI/CD**:

1. ã‚³ãƒ¼ãƒ‰ã‚’push
2. è‡ªå‹•ãƒ†ã‚¹ãƒˆ (unit/integration/E2E)
3. ãƒ‘ã‚¹ â†’ ãƒ‡ãƒ—ãƒ­ã‚¤ / å¤±æ•— â†’ PR block

**MLç‰¹æœ‰ã®è¿½åŠ è¦ç´ **:

1. **ãƒ‡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³**: ã‚¹ã‚­ãƒ¼ãƒæ¤œè¨¼ãƒ»æ¬ æå€¤ãƒã‚§ãƒƒã‚¯ãƒ»åˆ†å¸ƒç•°å¸¸æ¤œå‡º
2. **ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ†ã‚¹ãƒˆ**: è¨“ç·´ã—ã¦ accuracy >= thresholdç¢ºèª
3. **æ¨è«–ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ†ã‚¹ãƒˆ**: p99 latency <= SLOç¢ºèª
4. **Regression Detection**: æ–°ãƒ¢ãƒ‡ãƒ«ãŒæ—§ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚ŠåŠ£åŒ–ã—ã¦ã„ãªã„ã‹

#### 3.5 ãƒ‡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ â€” Great Expectations

Great Expectations [^3] = ãƒ‡ãƒ¼ã‚¿ã®unit testã€‚

**Expectation (æœŸå¾…å€¤) ã®å®šç¾©**:

$$
E = \{\text{column}, \text{condition}, \text{threshold}\}
$$

ä¾‹:

```python
import great_expectations as gx

# Initialize context
context = gx.get_context()

# Create expectation suite
suite = context.add_expectation_suite("data_quality_suite")

# Define expectations
suite.expect_column_values_to_not_be_null("user_id")
suite.expect_column_values_to_be_between("age", min_value=0, max_value=120)
suite.expect_column_mean_to_be_between("price", min_value=10, max_value=1000)

# Validate data
batch = context.get_batch({"path": "data/train.csv"})
result = context.run_validation(batch, expectation_suite_name="data_quality_suite")

if not result["success"]:
    raise ValueError("Data validation failed!")
```

**æ•°å­¦çš„è¡¨ç¾**:

$$
\begin{aligned}
E_1: \quad & \forall i, \, x_i[\text{user\_id}] \neq \text{null} \\
E_2: \quad & \forall i, \, 0 \leq x_i[\text{age}] \leq 120 \\
E_3: \quad & 10 \leq \frac{1}{N}\sum_{i=1}^{N} x_i[\text{price}] \leq 1000
\end{aligned}
$$

**å…¨ã¦ã®ExpectationãŒæº€ãŸã•ã‚ŒãŸã‚‰ãƒ‡ãƒ¼ã‚¿ã¯"valid"ã€‚1ã¤ã§ã‚‚å¤±æ•—ã—ãŸã‚‰ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åœæ­¢ã€‚**

#### 3.6 ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ†ã‚¹ãƒˆ

**æ€§èƒ½ãƒ†ã‚¹ãƒˆã®å®šå¼åŒ–**:

$$
\text{Test Passed} \iff \text{metric}(\mathcal{M}, \mathcal{D}_{\text{test}}) \geq \theta
$$

- $\text{metric}$: Accuracy, F1, AUCç­‰
- $\theta$: è¨±å®¹é–¾å€¤ (e.g., 0.95)

ä¾‹:

```python
# train.py
model = train_model(train_data)
val_acc = evaluate(model, val_data)

if val_acc < 0.95:
    raise ValueError(f"Model accuracy {val_acc:.4f} < 0.95")
```

**GitHub Actionsçµ±åˆ**:

```yaml
- name: Train and test model
  run: |
    python train.py --config configs/ci.yaml
    python test_model.py --threshold 0.95
```

**ãƒ†ã‚¹ãƒˆå¤±æ•— â†’ CIå¤±æ•— â†’ PRãƒãƒ¼ã‚¸ä¸å¯ã€‚**

#### 3.7 æ¨è«–ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ†ã‚¹ãƒˆ

**ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·SLOã®å®šç¾©**:

$$
\text{SLO:} \quad P(\text{latency} \leq \tau) \geq 0.99
$$

- $\tau$: é–¾å€¤ (e.g., 100ms)
- $P$: 99ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ« (p99)

**ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè£…**:

```python
import time
import numpy as np

latencies = []
for _ in range(1000):  # 1000 requests
    start = time.time()
    model.predict(input_data)
    latencies.append(time.time() - start)

p99 = np.percentile(latencies, 99)
if p99 > 0.1:  # 100ms
    raise ValueError(f"p99 latency {p99*1000:.2f}ms > 100ms")
```

**p99ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒSLOã‚’è¶…ãˆãŸã‚‰ãƒ†ã‚¹ãƒˆå¤±æ•—ã€‚**

#### 3.8 Regression Detection â€” A/Bãƒ†ã‚¹ãƒˆ in CI

æ–°ãƒ¢ãƒ‡ãƒ«$\mathcal{M}_{\text{new}}$ãŒæ—§ãƒ¢ãƒ‡ãƒ«$\mathcal{M}_{\text{old}}$ã‚ˆã‚ŠåŠ£åŒ–ã—ã¦ã„ãªã„ã‹æ¤œè¨¼ã€‚

**å¸°ç„¡ä»®èª¬**:

$$
H_0: \, \text{metric}(\mathcal{M}_{\text{new}}) \leq \text{metric}(\mathcal{M}_{\text{old}})
$$

**å¯¾ç«‹ä»®èª¬**:

$$
H_1: \, \text{metric}(\mathcal{M}_{\text{new}}) > \text{metric}(\mathcal{M}_{\text{old}})
$$

**çµ±è¨ˆçš„æ¤œå®š** (one-sided t-test):

$$
t = \frac{\bar{m}_{\text{new}} - \bar{m}_{\text{old}}}{\sqrt{\frac{s_{\text{new}}^2}{n_{\text{new}}} + \frac{s_{\text{old}}^2}{n_{\text{old}}}}}
$$

- $\bar{m}$: å¹³å‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
- $s^2$: åˆ†æ•£
- $n$: ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º

**$t > t_{0.05, df}$ (5%æœ‰æ„æ°´æº–) ãªã‚‰$H_0$ã‚’æ£„å´ â†’ æ–°ãƒ¢ãƒ‡ãƒ«ãŒæœ‰æ„ã«æ”¹å–„ã€‚**

---

### Part C: A/Bãƒ†ã‚¹ãƒˆ & ã‚«ãƒŠãƒªã‚¢ãƒªãƒªãƒ¼ã‚¹

#### 3.9 A/Bãƒ†ã‚¹ãƒˆã®çµ±è¨ˆçš„åŸºç›¤

**A/Bãƒ†ã‚¹ãƒˆã®è¨­å®š**:

- Controlç¾¤ (A): æ—§ãƒ¢ãƒ‡ãƒ«$\mathcal{M}_A$
- Treatmentç¾¤ (B): æ–°ãƒ¢ãƒ‡ãƒ«$\mathcal{M}_B$
- ãƒ¡ãƒˆãƒªã‚¯ã‚¹: Conversion rate $p$ (e.g., ã‚¯ãƒªãƒƒã‚¯ç‡)

**å¸°ç„¡ä»®èª¬**:

$$
H_0: \, p_A = p_B
$$

**å¯¾ç«‹ä»®èª¬**:

$$
H_1: \, p_A \neq p_B
$$

##### 3.9.1 ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºè¨ˆç®—

**Statistical Power** $1-\beta$ ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã«å¿…è¦ãªã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º$n$ã‚’è¨ˆç®—ã€‚

$$
n = \frac{(Z_{1-\alpha/2} + Z_{1-\beta})^2 \cdot (\bar{p}(1-\bar{p}) + \bar{p}(1-\bar{p}))}{\delta^2}
$$

- $\bar{p} = (p_A + p_B)/2$: å¹³å‡conversion rate
- $\delta = |p_A - p_B|$: Minimum Detectable Effect (MDE)
- $Z_{1-\alpha/2}$: æœ‰æ„æ°´æº–$\alpha$ã®è‡¨ç•Œå€¤ (é€šå¸¸ $\alpha=0.05 \Rightarrow Z=1.96$)
- $Z_{1-\beta}$: Power $1-\beta$ã®è‡¨ç•Œå€¤ (é€šå¸¸ $\beta=0.2 \Rightarrow Z=0.84$)

**ä¾‹**: $p_A = 0.10$, $\delta = 0.02$ (2%ã®æ”¹å–„ã‚’æ¤œå‡ºã—ãŸã„), $\alpha=0.05$, $\beta=0.2$:

$$
\begin{aligned}
\bar{p} &= 0.10 \\
n &= \frac{(1.96 + 0.84)^2 \cdot 2 \cdot 0.10 \cdot 0.90}{0.02^2} \\
&= \frac{7.84 \cdot 0.18}{0.0004} \\
&\approx 3528 \text{ samples per group}
\end{aligned}
$$

**å„ç¾¤3,528ã‚µãƒ³ãƒ—ãƒ«å¿…è¦ = åˆè¨ˆ7,056ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€‚**

##### 3.9.2 Sequential Testing

é€šå¸¸ã®A/Bãƒ†ã‚¹ãƒˆã¯**å›ºå®šã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º**ã€‚Sequential Testingã¯**é€æ¬¡çš„ã«æ¤œå®šã—ã€æ—©æœŸåœæ­¢**ã€‚

**Sequential Probability Ratio Test (SPRT)**:

$$
\Lambda_t = \frac{P(D_t \mid H_1)}{P(D_t \mid H_0)}
$$

- $D_t$: $t$æ™‚ç‚¹ã¾ã§ã®ãƒ‡ãƒ¼ã‚¿
- $\Lambda_t$: Likelihood ratio

**æ—©æœŸåœæ­¢ã®åˆ¤å®š**:

$$
\begin{cases}
\Lambda_t \geq \frac{1-\beta}{\alpha} & \Rightarrow \text{Reject } H_0 \text{ (B wins)} \\
\Lambda_t \leq \frac{\beta}{1-\alpha} & \Rightarrow \text{Accept } H_0 \text{ (A wins)} \\
\text{otherwise} & \Rightarrow \text{Continue testing}
\end{cases}
$$

**åˆ©ç‚¹**: å¹³å‡ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå›ºå®šã‚µãƒ³ãƒ—ãƒ«ã®**50%**ã«å‰Šæ¸›å¯èƒ½ã€‚

##### 3.9.3 Guardrail Metrics

æ–°ãƒ¢ãƒ‡ãƒ«ãŒprimaryãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ”¹å–„ã—ã¦ã‚‚ã€**guardrailãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ‚ªåŒ–**ã•ã›ãŸã‚‰å´ä¸‹ã€‚

**ä¾‹**:

- Primary: CTR (Click-Through Rate) â†‘
- Guardrail: Bounce Rate â†‘ (æ‚ªåŒ–), Latency â†‘ (æ‚ªåŒ–)

**æ¡ä»¶**:

$$
\text{Deploy} \iff (\text{CTR}_B > \text{CTR}_A) \land (\text{Bounce}_B \leq \text{Bounce}_A) \land (\text{Latency}_B \leq \text{SLO})
$$

**1ã¤ã§ã‚‚ guardrail ã‚’ç ´ã£ãŸã‚‰ãƒ‡ãƒ—ãƒ­ã‚¤ä¸­æ­¢ã€‚**

#### 3.10 ã‚«ãƒŠãƒªã‚¢ãƒªãƒªãƒ¼ã‚¹ã®æ•°å­¦çš„ãƒ¢ãƒ‡ãƒ«

**æ®µéšçš„ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ**:

$$
\text{Traffic}(t) = \begin{cases}
0.01 & \text{if } t \in [0, T_1) \\
0.05 & \text{if } t \in [T_1, T_2) \\
0.25 & \text{if } t \in [T_2, T_3) \\
1.00 & \text{if } t \geq T_3
\end{cases}
$$

- $T_1, T_2, T_3$: å„ã‚¹ãƒ†ãƒ¼ã‚¸ã®çµ‚äº†æ™‚åˆ»

**è‡ªå‹•ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¡ä»¶**:

$$
\text{Rollback} \iff \text{Error Rate}_{\text{canary}} > \text{Error Rate}_{\text{baseline}} + \epsilon
$$

- $\epsilon$: è¨±å®¹èª¤å·® (e.g., 0.5%)

**ä¾‹**: Baseline error rate = 0.2%, Canary error rate = 1.0% â†’ $1.0 > 0.2 + 0.5 = 0.7$ â†’ Rollback!

##### 3.10.1 Feature Flags

ã‚«ãƒŠãƒªã‚¢ãƒªãƒªãƒ¼ã‚¹ã‚’ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã§åˆ¶å¾¡ã€‚

```python
from feature_flags import is_enabled

def predict(input_data, user_id):
    if is_enabled("use_model_v2", user_id):
        return model_v2.predict(input_data)
    else:
        return model_v1.predict(input_data)
```

**`is_enabled`ã®å®Ÿè£…** (consistent hashing):

```python
def is_enabled(flag_name, user_id, rollout_percentage=0.01):
    hash_val = hash(f"{flag_name}:{user_id}") % 100
    return hash_val < rollout_percentage * 100
```

**1%ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ = ãƒãƒƒã‚·ãƒ¥å€¤0-0ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã¿æœ‰åŠ¹åŒ–ã€‚**

---

### Part D: ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚° & SLI/SLO

#### 3.11 RED Metrics â€” ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã®åŸºæœ¬3è»¸

**RED Metrics**:

- **Rate**: ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°/ç§’ (RPS)
- **Errors**: ã‚¨ãƒ©ãƒ¼æ•°/ç§’
- **Duration**: ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· (p50/p95/p99)

$$
\begin{aligned}
\text{Rate:} \quad & r(t) = \frac{\text{requests}}{t} \\
\text{Error Rate:} \quad & e(t) = \frac{\text{errors}(t)}{\text{requests}(t)} \\
\text{Latency:} \quad & L_{p99}(t) = \text{percentile}(\text{latencies}(t), 99)
\end{aligned}
$$

**Prometheus exporterã®å®Ÿè£…**:

```python
from prometheus_client import Counter, Histogram

REQUEST_COUNT = Counter('model_requests_total', 'Total requests')
ERROR_COUNT = Counter('model_errors_total', 'Total errors')
LATENCY = Histogram('model_latency_seconds', 'Latency',
                    buckets=[0.01, 0.05, 0.1, 0.5, 1.0, 5.0])

def predict_with_metrics(input_data):
    REQUEST_COUNT.inc()
    start = time.time()

    try:
        result = model.predict(input_data)
        LATENCY.observe(time.time() - start)
        return result
    except Exception:
        ERROR_COUNT.inc()
        raise
```

**PrometheusãŒã“ã‚Œã‚‰ã‚’scrapeã—ã¦æ™‚ç³»åˆ—DBã«ä¿å­˜ã€‚**

#### 3.12 SLI/SLOè¨­è¨ˆ

**SLI (Service Level Indicator)** = æ¸¬å®šå¯èƒ½ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹:

$$
\text{SLI}_{\text{availability}} = \frac{\text{successful requests}}{\text{total requests}}
$$

$$
\text{SLI}_{\text{latency}} = \frac{\text{requests with latency} \leq \tau}{\text{total requests}}
$$

**SLO (Service Level Objective)** = SLIã®ç›®æ¨™å€¤:

$$
\begin{aligned}
\text{SLO}_{\text{availability}}: \quad & \text{SLI}_{\text{availability}} \geq 0.999 \quad \text{(99.9%)} \\
\text{SLO}_{\text{latency}}: \quad & \text{SLI}_{\text{latency}} \geq 0.99 \quad \text{(p99 < 100ms)}
\end{aligned}
$$

**Error Budget** = SLOã§è¨±å®¹ã•ã‚Œã‚‹å¤±æ•—ã®é‡:

$$
\text{Error Budget} = 1 - \text{SLO}
$$

ä¾‹: SLO = 99.9% â†’ Error Budget = 0.1% = 43.2åˆ†/æœˆ (30æ—¥)ã€‚

$$
0.001 \times 30 \times 24 \times 60 = 43.2 \text{ minutes}
$$

**Error Budgetã‚’ä½¿ã„åˆ‡ã£ãŸã‚‰æ–°æ©Ÿèƒ½é–‹ç™ºã‚’åœæ­¢ã—ã€ä¿¡é ¼æ€§å‘ä¸Šã«é›†ä¸­ã€‚**

#### 3.13 ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º

**ãƒ‰ãƒªãƒ•ãƒˆã®ç¨®é¡**:

1. **Data Drift**: å…¥åŠ›åˆ†å¸ƒ$P(X)$ã®å¤‰åŒ–
2. **Concept Drift**: $P(Y \mid X)$ã®å¤‰åŒ– (ãƒ©ãƒ™ãƒ«ã®æ„å‘³ãŒå¤‰ã‚ã‚‹)
3. **Model Drift**: ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®åŠ£åŒ–

##### 3.13.1 Kolmogorov-Smirnovæ¤œå®š

**KSçµ±è¨ˆé‡**:

$$
D = \sup_{x} |F_{\text{train}}(x) - F_{\text{prod}}(x)|
$$

- $F_{\text{train}}(x)$: è¨“ç·´æ™‚ã®ç´¯ç©åˆ†å¸ƒé–¢æ•° (CDF)
- $F_{\text{prod}}(x)$: æœ¬ç•ªãƒ‡ãƒ¼ã‚¿ã®CDF
- $\sup$: supremum (æœ€å¤§å€¤)

**å¸°ç„¡ä»®èª¬**:

$$
H_0: \, F_{\text{train}} = F_{\text{prod}}
$$

**på€¤è¨ˆç®—** (Kolmogorov distribution):

$$
p = P(D_{n,m} \geq D) = 2 \sum_{k=1}^{\infty} (-1)^{k-1} e^{-2k^2 D^2 n}
$$

- $n = \frac{n_{\text{train}} \cdot n_{\text{prod}}}{n_{\text{train}} + n_{\text{prod}}}$

**å®Ÿè£…**:

```python
from scipy.stats import ks_2samp

train_feature = train_data["feature_1"]
prod_feature = prod_data["feature_1"]

statistic, p_value = ks_2samp(train_feature, prod_feature)

if p_value < 0.01:  # 1% significance level
    print("âš ï¸ Data drift detected!")
    trigger_retraining()
```

##### 3.13.2 Population Stability Index (PSI)

**PSI** = è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨æœ¬ç•ªãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒã®ä¹–é›¢åº¦ã€‚

$$
\text{PSI} = \sum_{i=1}^{B} (p_{\text{prod},i} - p_{\text{train},i}) \ln\left(\frac{p_{\text{prod},i}}{p_{\text{train},i}}\right)
$$

- $B$: ãƒ“ãƒ³æ•° (é€šå¸¸10)
- $p_{\text{train},i}$: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ãƒ“ãƒ³$i$ã®å‰²åˆ
- $p_{\text{prod},i}$: æœ¬ç•ªãƒ‡ãƒ¼ã‚¿ã®ãƒ“ãƒ³$i$ã®å‰²åˆ

**PSIã®è§£é‡ˆ**:

| PSIå€¤ | è§£é‡ˆ |
|:------|:-----|
| < 0.1 | ãƒ‰ãƒªãƒ•ãƒˆãªã— |
| 0.1 - 0.25 | è»½å¾®ãªãƒ‰ãƒªãƒ•ãƒˆ (ç›£è¦–ç¶™ç¶š) |
| > 0.25 | é‡å¤§ãªãƒ‰ãƒªãƒ•ãƒˆ (å†è¨“ç·´å¿…è¦) |

**å®Ÿè£…**:

```python
import numpy as np

def calculate_psi(train_data, prod_data, bins=10):
    # Bin data
    min_val = min(train_data.min(), prod_data.min())
    max_val = max(train_data.max(), prod_data.max())
    bin_edges = np.linspace(min_val, max_val, bins+1)

    train_hist, _ = np.histogram(train_data, bins=bin_edges)
    prod_hist, _ = np.histogram(prod_data, bins=bin_edges)

    # Normalize
    p_train = train_hist / train_hist.sum()
    p_prod = prod_hist / prod_hist.sum()

    # Avoid log(0)
    p_train = np.where(p_train == 0, 0.0001, p_train)
    p_prod = np.where(p_prod == 0, 0.0001, p_prod)

    # Calculate PSI
    psi = np.sum((p_prod - p_train) * np.log(p_prod / p_train))
    return psi

psi = calculate_psi(train_feature, prod_feature)
if psi > 0.25:
    print(f"âš ï¸ Significant drift detected! PSI = {psi:.4f}")
```

##### 3.13.3 Jensen-Shannon Divergence

**JS Divergence** = å¯¾ç§°ãªKL divergence:

$$
\text{JSD}(P \| Q) = \frac{1}{2} D_{\text{KL}}(P \| M) + \frac{1}{2} D_{\text{KL}}(Q \| M)
$$

- $M = \frac{1}{2}(P + Q)$: å¹³å‡åˆ†å¸ƒ

**æ€§è³ª**:

- $0 \leq \text{JSD} \leq \ln 2$
- $\text{JSD}(P \| Q) = \text{JSD}(Q \| P)$ (å¯¾ç§°)

**å®Ÿè£…** (é›¢æ•£åˆ†å¸ƒ):

```python
from scipy.spatial.distance import jensenshannon

p = np.histogram(train_feature, bins=20, density=True)[0]
q = np.histogram(prod_feature, bins=20, density=True)[0]

js_div = jensenshannon(p, q)
if js_div > 0.3:  # Threshold (0-1 range after sqrt)
    print(f"âš ï¸ Drift detected! JSD = {js_div:.4f}")
```

---

### Part E: DPO/RLHFåŸºç¤

#### 3.14 RLHF (Reinforcement Learning from Human Feedback)

**RLHFã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**:

1. **SFT (Supervised Fine-Tuning)**: ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ã§äº‹å‰å­¦ç¿’
2. **Reward Modeling**: äººé–“ã®preferenceã‹ã‚‰reward modelè¨“ç·´
3. **RL Fine-Tuning**: PPOã§rewardæœ€å¤§åŒ–

##### 3.14.1 Reward Modelingã®æ•°å­¦

äººé–“ãŒ2ã¤ã®å¿œç­” $(y_1, y_2)$ ã‚’æ¯”è¼ƒã—ã€å¥½ã¾ã—ã„æ–¹ã‚’é¸æŠã€‚

**Bradley-Terry Model**:

$$
P(y_1 \succ y_2 \mid x) = \frac{\exp(r(x, y_1))}{\exp(r(x, y_1)) + \exp(r(x, y_2))} = \sigma(r(x, y_1) - r(x, y_2))
$$

- $r(x, y)$: Reward model (ã‚¹ã‚«ãƒ©ãƒ¼)
- $\sigma(z) = 1/(1+e^{-z})$: Sigmoidé–¢æ•°

**Lossé–¢æ•°** (binary cross-entropy):

$$
\mathcal{L}_{\text{RM}} = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma(r(x, y_w) - r(x, y_l)) \right]
$$

- $y_w$: å¥½ã¾ã—ã„å¿œç­” (win)
- $y_l$: å¥½ã¾ã—ããªã„å¿œç­” (lose)

**Reward modelã®è¨“ç·´**:

```python
def reward_model_loss(r_win, r_lose):
    return -torch.log(torch.sigmoid(r_win - r_lose)).mean()

# Training loop
for x, y_win, y_lose in dataloader:
    r_win = reward_model(x, y_win)
    r_lose = reward_model(x, y_lose)
    loss = reward_model_loss(r_win, r_lose)
    loss.backward()
    optimizer.step()
```

##### 3.14.2 PPO (Proximal Policy Optimization)

**ç›®çš„é–¢æ•°** (KLæ­£å‰‡åŒ–ä»˜ã):

$$
\mathcal{L}_{\text{PPO}} = \mathbb{E}_{(x,y) \sim \pi_\theta} \left[ r(x, y) - \beta D_{\text{KL}}(\pi_\theta(y \mid x) \| \pi_{\text{ref}}(y \mid x)) \right]
$$

- $\pi_\theta$: Fine-tuningä¸­ã®ãƒãƒªã‚·ãƒ¼ (LLM)
- $\pi_{\text{ref}}$: Reference policy (å…ƒã®LLM)
- $\beta$: KL penaltyä¿‚æ•°

**KLæ­£å‰‡åŒ–ã®ç›®çš„**: $\pi_\theta$ãŒ$\pi_{\text{ref}}$ã‹ã‚‰é ã–ã‹ã‚Šã™ããªã„ã‚ˆã†ã«ã™ã‚‹ (mode collapseé˜²æ­¢)ã€‚

**PPOã®clipped objective**:

$$
L^{\text{CLIP}}(\theta) = \mathbb{E}_t \left[ \min\left( \frac{\pi_\theta(a_t \mid s_t)}{\pi_{\text{old}}(a_t \mid s_t)} A_t, \, \text{clip}\left(\frac{\pi_\theta(a_t \mid s_t)}{\pi_{\text{old}}(a_t \mid s_t)}, 1-\epsilon, 1+\epsilon\right) A_t \right) \right]
$$

- $A_t$: Advantage function
- $\epsilon$: Clipping threshold (é€šå¸¸0.2)

**PPOã®å•é¡Œç‚¹**:

- ä¸å®‰å®š (hyperparameteræ•æ„Ÿ)
- ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚³ã‚¹ãƒˆé«˜ (on-policy)
- Reward modelã®ãƒã‚¤ã‚¢ã‚¹ã«æ•æ„Ÿ

#### 3.15 DPO (Direct Preference Optimization)

**DPO** [^1] = RLHFã®RLéƒ¨åˆ†ã‚’**ç›´æ¥æœ€é©åŒ–**ã«ç½®ãæ›ãˆã‚‹ã€‚

##### 3.15.1 DPO Lossã®å°å‡º

**RLã®ç›®çš„é–¢æ•°** (å†æ²):

$$
\max_{\pi_\theta} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta(\cdot \mid x)} [r(x, y)] - \beta D_{\text{KL}}(\pi_\theta \| \pi_{\text{ref}})
$$

**æœ€é©ãƒãƒªã‚·ãƒ¼ã®é–‰å½¢å¼è§£**:

$$
\pi^*(y \mid x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta} r(x, y)\right)
$$

- $Z(x) = \sum_y \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta} r(x, y)\right)$: Partition function

**Reward modelã‚’é€†ç®—**:

$$
r(x, y) = \beta \log \frac{\pi^*(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + \beta \log Z(x)
$$

**Bradley-Terry Modelã«ä»£å…¥**:

$$
\begin{aligned}
P(y_w \succ y_l \mid x) &= \sigma(r(x, y_w) - r(x, y_l)) \\
&= \sigma\left( \beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)} \right)
\end{aligned}
$$

**DPO Loss**:

$$
\mathcal{L}_{\text{DPO}}(\theta) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma\left( \beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)} \right) \right]
$$

**å®Ÿè£…**:

```python
def dpo_loss(pi_theta, pi_ref, x, y_win, y_lose, beta=0.1):
    log_ratio_win = pi_theta.log_prob(y_win, x) - pi_ref.log_prob(y_win, x)
    log_ratio_lose = pi_theta.log_prob(y_lose, x) - pi_ref.log_prob(y_lose, x)

    loss = -torch.log(torch.sigmoid(beta * (log_ratio_win - log_ratio_lose)))
    return loss.mean()
```

**DPOã®åˆ©ç‚¹**:

- **å®‰å®š**: PPOã‚ˆã‚Šå®‰å®š (clippingä¸è¦)
- **åŠ¹ç‡**: Reward modelä¸è¦ (1ã‚¹ãƒ†ãƒƒãƒ—ã§å®Œçµ)
- **ã‚·ãƒ³ãƒ—ãƒ«**: Classification lossã¨åŒã˜

**DPOã®é™ç•Œ**:

- Preferenceãƒ‡ãƒ¼ã‚¿ä¾å­˜ (ãƒ‡ãƒ¼ã‚¿å“è³ªãŒé‡è¦)
- KLæ­£å‰‡åŒ–ã®$\beta$èª¿æ•´ãŒå¿…è¦

##### 3.15.2 DPOã®æ‹¡å¼µ â€” IPO/KTO

**IPO (Identity Preference Optimization)**: DPOã®hinge lossç‰ˆ:

$$
\mathcal{L}_{\text{IPO}} = \mathbb{E} \left[ \left( \log \frac{\pi_\theta(y_w)}{\pi_{\theta_{\text{ref}}}(y_w)} - \log \frac{\pi_\theta(y_l)}{\pi_{\theta_{\text{ref}}}(y_l)} - 1 \right)^2 \right]
$$

**KTO (Kahneman-Tversky Optimization)**: Prospect Theoryãƒ™ãƒ¼ã‚¹:

$$
\mathcal{L}_{\text{KTO}} = \mathbb{E}_{y \sim y_{\text{desirable}}} \left[ v\left( \log \frac{\pi_\theta(y)}{\pi_{\text{ref}}(y)} \right) \right] + \mathbb{E}_{y \sim y_{\text{undesirable}}} \left[ -\lambda v\left( \log \frac{\pi_\theta(y)}{\pi_{\text{ref}}(y)} \right) \right]
$$

- $v(x) = x^\alpha$ (value function)
- $\lambda > 1$: Loss aversionä¿‚æ•°

**2025å¹´ã®ä¸»æµ**: DPO variants (IPO/KTO) ãŒPPOã‚’ç½®ãæ›ãˆã¤ã¤ã‚ã‚‹ã€‚

---

### âš”ï¸ Boss Battle: å®Œå…¨MLOpsãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ•°å¼åˆ†è§£

**ç›®æ¨™**: Trainâ†’Experimentâ†’CI/CDâ†’A/Bâ†’Monitorâ†’Driftâ†’Retrainã®ãƒ•ãƒ«ã‚µã‚¤ã‚¯ãƒ«ã‚’æ•°å¼ã§è¨˜è¿°ã™ã‚‹ã€‚

#### Step 1: ãƒ¢ãƒ‡ãƒ«è¨“ç·´ & å®Ÿé¨“è¨˜éŒ²

$$
\begin{aligned}
\mathcal{M}_t &= \arg\min_{\mathbf{w}} \mathcal{L}(\mathbf{w}; \mathcal{D}_{\text{train}}) \\
e_t &= (\mathbf{h}_t, \mathcal{D}_t, \mathbf{m}_t, \mathcal{A}_t) \\
\mathbf{m}_t &= \{\text{val\_acc}, \text{val\_loss}, \text{F1}, \ldots\}
\end{aligned}
$$

MLflowã«è¨˜éŒ²:

$$
\text{MLflow.log}(e_t) \to \text{run\_id}_t
$$

#### Step 2: ãƒ¢ãƒ‡ãƒ«ã‚’Registryã«ç™»éŒ²

$$
\text{Model Registry} \leftarrow (\mathcal{M}_t, \text{run\_id}_t, \text{stage}=\text{Staging})
$$

#### Step 3: CI/CD â€” æ€§èƒ½ãƒ†ã‚¹ãƒˆ

$$
\text{Test Passed} \iff \text{acc}(\mathcal{M}_t, \mathcal{D}_{\text{test}}) \geq \theta_{\text{deploy}}
$$

#### Step 4: ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ (1%)

$$
\begin{aligned}
\text{Traffic Split:} \quad & p_{\text{canary}} = 0.01, \, p_{\text{baseline}} = 0.99 \\
\text{User Assignment:} \quad & u \sim \text{Hash}(u) \mod 100 < 1 \Rightarrow \mathcal{M}_t, \, \text{else} \, \mathcal{M}_{t-1}
\end{aligned}
$$

#### Step 5: A/Bãƒ†ã‚¹ãƒˆ â€” çµ±è¨ˆçš„æœ‰æ„æ€§æ¤œè¨¼

$$
\begin{aligned}
H_0: \, \mu_{\text{canary}} &= \mu_{\text{baseline}} \\
t &= \frac{\bar{x}_{\text{canary}} - \bar{x}_{\text{baseline}}}{\sqrt{s_{\text{canary}}^2/n_{\text{canary}} + s_{\text{baseline}}^2/n_{\text{baseline}}}} \\
\text{Win} &\iff t > t_{0.05, df}
\end{aligned}
$$

#### Step 6: ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚° â€” SLI/SLOæ¤œè¨¼

$$
\begin{aligned}
\text{SLI}_{\text{latency}} &= \frac{\#(\text{latency} \leq 100\text{ms})}{\#(\text{requests})} \\
\text{SLO Met} &\iff \text{SLI}_{\text{latency}} \geq 0.99
\end{aligned}
$$

#### Step 7: ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º

$$
\begin{aligned}
D_{\text{KS}} &= \sup_x |F_{\text{train}}(x) - F_{\text{prod}}(x)| \\
\text{Drift Detected} &\iff p\text{-value}(D_{\text{KS}}) < 0.01
\end{aligned}
$$

#### Step 8: è‡ªå‹•å†è¨“ç·´ãƒˆãƒªã‚¬ãƒ¼

$$
\text{Drift Detected} \Rightarrow \text{Trigger}(\text{retrain}, \mathcal{D}_{\text{new}})
$$

#### Step 9: DPO/RLHFã§ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯çµ±åˆ

$$
\begin{aligned}
\mathcal{L}_{\text{DPO}} &= -\mathbb{E} \left[ \log \sigma\left( \beta \log \frac{\pi_\theta(y_w)}{\pi_{\text{ref}}(y_w)} - \beta \log \frac{\pi_\theta(y_l)}{\pi_{\text{ref}}(y_l)} \right) \right] \\
\pi_{\theta_{t+1}} &\leftarrow \arg\min_{\pi_\theta} \mathcal{L}_{\text{DPO}}(\pi_\theta; \mathcal{D}_{\text{preference}})
\end{aligned}
$$

#### å®Œå…¨ã‚µã‚¤ã‚¯ãƒ«

$$
\boxed{
\text{Train} \to \text{Experiment} \to \text{CI/CD} \to \text{Canary} \to \text{A/B} \to \text{Monitor} \to \text{Drift} \to \text{DPO} \to \text{Retrain}
}
$$

**ã“ã®ãƒ«ãƒ¼ãƒ—ãŒè‡ªå‹•åŒ–ã•ã‚Œã¦ã„ã‚Œã°ã€MLã‚·ã‚¹ãƒ†ãƒ ã¯ "self-healing" ã«ãªã‚‹ã€‚**

:::message
**é€²æ—: 50% å®Œäº†** MLOpså…¨7é ˜åŸŸã®ç†è«–ã‚’å®Œå…¨ç¶²ç¾…ã—ãŸã€‚Zone 4ã§âš¡Julia + ğŸ¦€Rust + ğŸ”®Elixirå®Ÿè£…ã¸ã€‚
:::

---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” âš¡Juliaå®Ÿé¨“ç®¡ç† + ğŸ¦€Rust MLOpsãƒ„ãƒ¼ãƒ« + ğŸ”®Elixirç›£è¦–

### Part F: å®Ÿè£…ç·¨

#### 4.1 âš¡ Juliaå®Ÿé¨“ç®¡ç† â€” MLflowçµ±åˆ

Juliaã§å®Ÿé¨“ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã‚’å®Ÿè£…ã™ã‚‹ã€‚`MLFlowClient.jl`ã‚’ä½¿ã£ã¦MLflow APIã¨é€šä¿¡ã€‚

```julia
using HTTP, JSON3, Dates

# MLflow tracking server URL
const MLFLOW_URI = "http://localhost:5000"

"""
Log parameters to MLflow
"""
function log_params(run_id::String, params::Dict{String, Any})
    url = "$MLFLOW_URI/api/2.0/mlflow/runs/log-parameter"

    for (key, value) in params
        body = JSON3.write(Dict(
            "run_id" => run_id,
            "key" => key,
            "value" => string(value)
        ))

        HTTP.post(url, ["Content-Type" => "application/json"], body)
    end
end

"""
Log metrics to MLflow with step
"""
function log_metrics(run_id::String, metrics::Dict{String, Float64}, step::Int)
    url = "$MLFLOW_URI/api/2.0/mlflow/runs/log-metric"

    for (key, value) in metrics
        body = JSON3.write(Dict(
            "run_id" => run_id,
            "key" => key,
            "value" => value,
            "timestamp" => round(Int, datetime2unix(now()) * 1000),
            "step" => step
        ))

        HTTP.post(url, ["Content-Type" => "application/json"], body)
    end
end

"""
Create MLflow run
"""
function create_run(experiment_id::String, run_name::String)
    url = "$MLFLOW_URI/api/2.0/mlflow/runs/create"

    body = JSON3.write(Dict(
        "experiment_id" => experiment_id,
        "run_name" => run_name,
        "start_time" => round(Int, datetime2unix(now()) * 1000)
    ))

    response = HTTP.post(url, ["Content-Type" => "application/json"], body)
    result = JSON3.read(String(response.body))

    return result["run"]["info"]["run_id"]
end

"""
Complete MLflow run
"""
function end_run(run_id::String, status::String="FINISHED")
    url = "$MLFLOW_URI/api/2.0/mlflow/runs/update"

    body = JSON3.write(Dict(
        "run_id" => run_id,
        "status" => status,
        "end_time" => round(Int, datetime2unix(now()) * 1000)
    ))

    HTTP.post(url, ["Content-Type" => "application/json"], body)
end

# Example: Track a training run
function train_and_log()
    # Create run
    experiment_id = "0"  # Default experiment
    run_id = create_run(experiment_id, "julia-training-run")

    # Log hyperparameters
    params = Dict(
        "learning_rate" => 0.001,
        "batch_size" => 32,
        "epochs" => 10,
        "optimizer" => "Adam"
    )
    log_params(run_id, params)

    # Simulate training loop
    for epoch in 1:10
        train_loss = 1.0 / (1 + epoch * 0.1)  # Decreasing loss
        val_acc = 0.8 + epoch * 0.02  # Increasing accuracy

        # Log metrics with step
        metrics = Dict(
            "train_loss" => train_loss,
            "val_acc" => val_acc
        )
        log_metrics(run_id, metrics, epoch)

        println("Epoch $epoch: loss=$train_loss, acc=$val_acc")
    end

    # End run
    end_run(run_id)
    println("âœ… Run completed: $run_id")

    return run_id
end

# Run experiment
run_id = train_and_log()
```

å‡ºåŠ›:
```
Epoch 1: loss=0.9090909090909091, acc=0.82
Epoch 2: loss=0.8333333333333334, acc=0.84
...
Epoch 10: loss=0.5, acc=1.0
âœ… Run completed: a3f9c2e1b4d87f3a9c2e1b4d87f3a9c2
```

**MLflow UI** (`mlflow ui`) ã§å¯è¦–åŒ–:

- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¯”è¼ƒ
- ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ™‚ç³»åˆ—ã‚°ãƒ©ãƒ•
- Runé–“ã®æ¯”è¼ƒ

**Juliaã®åˆ©ç‚¹**:

- è¨“ç·´ãƒ«ãƒ¼ãƒ—ãŒé«˜é€Ÿ (C/Fortranãƒ¬ãƒ™ãƒ«)
- MLflow APIã¯å˜ãªã‚‹HTTP POST (è¨€èªéä¾å­˜)
- å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã§å‹ã«å¿œã˜ãŸæœ€é©åŒ–

#### 4.2 ğŸ¦€ Rust MLOpsãƒ„ãƒ¼ãƒ« â€” ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚° & ãƒ¡ãƒˆãƒªã‚¯ã‚¹

Rustã§é«˜é€ŸãªMLOpsãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’æ§‹ç¯‰ã€‚

##### 4.2.1 ãƒ¢ãƒ‡ãƒ«ãƒãƒƒã‚·ãƒ¥è¨ˆç®— (SHA-256)

```rust
use sha2::{Sha256, Digest};
use std::fs::File;
use std::io::Read;

/// Calculate SHA-256 hash of model file
pub fn hash_model_file(path: &str) -> Result<String, std::io::Error> {
    let mut file = File::open(path)?;
    let mut hasher = Sha256::new();
    let mut buffer = [0u8; 8192];

    loop {
        let n = file.read(&mut buffer)?;
        if n == 0 { break; }
        hasher.update(&buffer[..n]);
    }

    let result = hasher.finalize();
    Ok(format!("{:x}", result))
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_hash_model_file() {
        // Create a test file
        std::fs::write("test_model.bin", b"dummy model weights").unwrap();

        let hash = hash_model_file("test_model.bin").unwrap();
        assert_eq!(hash.len(), 64);  // SHA-256 = 256 bits = 64 hex chars

        std::fs::remove_file("test_model.bin").unwrap();
    }
}
```

##### 4.2.2 Prometheus Exporter (æ¨è«–ãƒ¡ãƒˆãƒªã‚¯ã‚¹)

```rust
use prometheus::{
    Encoder, TextEncoder, Counter, Histogram, Registry,
    opts, register_counter_with_registry, register_histogram_with_registry,
};
use std::time::Instant;

pub struct ModelMetrics {
    pub registry: Registry,
    pub request_count: Counter,
    pub error_count: Counter,
    pub latency: Histogram,
}

impl ModelMetrics {
    pub fn new() -> Self {
        let registry = Registry::new();

        let request_count = register_counter_with_registry!(
            opts!("model_requests_total", "Total inference requests"),
            registry
        ).unwrap();

        let error_count = register_counter_with_registry!(
            opts!("model_errors_total", "Total inference errors"),
            registry
        ).unwrap();

        let latency = register_histogram_with_registry!(
            "model_latency_seconds",
            "Inference latency in seconds",
            vec![0.01, 0.05, 0.1, 0.5, 1.0],
            registry
        ).unwrap();

        Self {
            registry,
            request_count,
            error_count,
            latency,
        }
    }

    pub fn record_request<F, T>(&self, f: F) -> Result<T, Box<dyn std::error::Error>>
    where
        F: FnOnce() -> Result<T, Box<dyn std::error::Error>>,
    {
        self.request_count.inc();
        let start = Instant::now();

        let result = f();

        let elapsed = start.elapsed().as_secs_f64();
        self.latency.observe(elapsed);

        if result.is_err() {
            self.error_count.inc();
        }

        result
    }

    pub fn export_metrics(&self) -> String {
        let encoder = TextEncoder::new();
        let metric_families = self.registry.gather();
        let mut buffer = Vec::new();
        encoder.encode(&metric_families, &mut buffer).unwrap();
        String::from_utf8(buffer).unwrap()
    }
}

// Example usage
fn main() {
    let metrics = ModelMetrics::new();

    // Simulate inference requests
    for _ in 0..100 {
        let result = metrics.record_request(|| {
            // Simulate model inference
            std::thread::sleep(std::time::Duration::from_millis(50));
            Ok(())
        });

        if let Err(e) = result {
            eprintln!("Error: {}", e);
        }
    }

    // Export metrics in Prometheus format
    println!("{}", metrics.export_metrics());
}
```

å‡ºåŠ› (Prometheus format):
```
# HELP model_requests_total Total inference requests
# TYPE model_requests_total counter
model_requests_total 100

# HELP model_errors_total Total inference errors
# TYPE model_errors_total counter
model_errors_total 0

# HELP model_latency_seconds Inference latency in seconds
# TYPE model_latency_seconds histogram
model_latency_seconds_bucket{le="0.01"} 0
model_latency_seconds_bucket{le="0.05"} 100
model_latency_seconds_bucket{le="0.1"} 100
model_latency_seconds_bucket{le="0.5"} 100
model_latency_seconds_bucket{le="1"} 100
model_latency_seconds_bucket{le="+Inf"} 100
model_latency_seconds_sum 5.0
model_latency_seconds_count 100
```

**Prometheusã‚µãƒ¼ãƒãƒ¼ãŒã“ã‚Œã‚’scrapeã—ã¦æ™‚ç³»åˆ—DBã«ä¿å­˜ã€‚**

#### 4.3 ğŸ”® Elixirç›£è¦–ã‚·ã‚¹ãƒ†ãƒ  â€” Telemetryçµ±åˆ & ã‚¢ãƒ©ãƒ¼ãƒˆ

Elixirã§åˆ†æ•£ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã€‚`:telemetry`ã§ã‚¤ãƒ™ãƒ³ãƒˆã‚’åé›†ã—ã€`:gen_statem`ã§ã‚¢ãƒ©ãƒ¼ãƒˆç®¡ç†ã€‚

##### 4.3.1 Telemetryçµ±åˆ

```elixir
defmodule MLOps.Telemetry do
  require Logger

  @doc """
  Attach telemetry handlers
  """
  def setup do
    :telemetry.attach_many(
      "mlops-telemetry",
      [
        [:model, :predict, :start],
        [:model, :predict, :stop],
        [:model, :predict, :exception]
      ],
      &handle_event/4,
      nil
    )
  end

  defp handle_event([:model, :predict, :start], _measurements, metadata, _config) do
    Logger.debug("Prediction started: #{inspect(metadata)}")
  end

  defp handle_event([:model, :predict, :stop], measurements, metadata, _config) do
    latency_ms = System.convert_time_unit(measurements.duration, :native, :millisecond)
    Logger.info("Prediction completed in #{latency_ms}ms: #{inspect(metadata)}")

    # Send to Prometheus
    :prometheus_histogram.observe(:model_latency_milliseconds, latency_ms)
  end

  defp handle_event([:model, :predict, :exception], measurements, metadata, _config) do
    Logger.error("Prediction failed: #{inspect(metadata)}")
    :prometheus_counter.inc(:model_errors_total)
  end
end

defmodule MLOps.Model do
  @doc """
  Run model prediction with telemetry
  """
  def predict(input) do
    metadata = %{model: "v1", input_size: byte_size(input)}

    :telemetry.span([:model, :predict], metadata, fn ->
      result = do_predict(input)
      {result, metadata}
    end)
  end

  defp do_predict(input) do
    # Simulate model inference
    Process.sleep(50)
    {:ok, "prediction for #{input}"}
  end
end

# Usage
MLOps.Telemetry.setup()

for i <- 1..100 do
  MLOps.Model.predict("input_#{i}")
end
```

##### 4.3.2 SLOç›£è¦– & è‡ªå‹•ã‚¢ãƒ©ãƒ¼ãƒˆ

```elixir
defmodule MLOps.SLOMonitor do
  use GenServer
  require Logger

  @slo_latency_ms 100
  @slo_availability 0.999

  def start_link(_) do
    GenServer.start_link(__MODULE__, %{}, name: __MODULE__)
  end

  def init(_) do
    # Check SLO every minute
    :timer.send_interval(60_000, :check_slo)

    state = %{
      total_requests: 0,
      successful_requests: 0,
      latencies: []
    }

    {:ok, state}
  end

  def record_request(latency_ms, success) do
    GenServer.cast(__MODULE__, {:record, latency_ms, success})
  end

  def handle_cast({:record, latency_ms, success}, state) do
    new_state = %{
      total_requests: state.total_requests + 1,
      successful_requests: state.successful_requests + (if success, do: 1, else: 0),
      latencies: [latency_ms | Enum.take(state.latencies, 999)]  # Keep last 1000
    }

    {:noreply, new_state}
  end

  def handle_info(:check_slo, state) do
    availability = state.successful_requests / max(state.total_requests, 1)
    p99_latency = if length(state.latencies) > 0 do
      Enum.sort(state.latencies) |> Enum.at(round(length(state.latencies) * 0.99))
    else
      0
    end

    Logger.info("SLO Check: availability=#{Float.round(availability, 4)}, p99_latency=#{p99_latency}ms")

    cond do
      availability < @slo_availability ->
        send_alert("SLO violated: availability #{Float.round(availability * 100, 2)}% < #{@slo_availability * 100}%")

      p99_latency > @slo_latency_ms ->
        send_alert("SLO violated: p99 latency #{p99_latency}ms > #{@slo_latency_ms}ms")

      true ->
        Logger.info("âœ… SLO met")
    end

    {:noreply, state}
  end

  defp send_alert(message) do
    Logger.warn("ğŸš¨ ALERT: #{message}")
    # In production: send to PagerDuty/Slack/etc
  end
end

# Usage
{:ok, _} = MLOps.SLOMonitor.start_link([])

# Simulate requests
for _ <- 1..1000 do
  latency = :rand.uniform(150)
  success = latency < 120
  MLOps.SLOMonitor.record_request(latency, success)
  Process.sleep(10)
end
```

å‡ºåŠ› (1åˆ†ã”ã¨):
```
[info] SLO Check: availability=0.9820, p99_latency=148ms
[warn] ğŸš¨ ALERT: SLO violated: p99 latency 148ms > 100ms
```

**Elixirã®åˆ©ç‚¹**:

- OTP supervisorã§éšœå®³æ™‚è‡ªå‹•å†èµ·å‹•
- åˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ ã§ãƒãƒ¼ãƒ‰é–“ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹é›†ç´„
- Telemetryã§å…¨ã¦ã®ã‚¤ãƒ™ãƒ³ãƒˆã‚’çµ±ä¸€çš„ã«è¨˜éŒ²

##### 4.3.3 åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚° â€” OpenTelemetryçµ±åˆ

Elixirã§åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã‚’å®Ÿè£…ã—ã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å…¨çµŒè·¯ã‚’å¯è¦–åŒ–ã€‚

```elixir
defmodule MLOps.Tracer do
  require OpenTelemetry.Tracer, as: Tracer

  @doc """
  Trace model prediction with OpenTelemetry
  """
  def traced_predict(input) do
    Tracer.with_span "model.predict" do
      # Add span attributes
      Tracer.set_attributes([
        {"input.size", byte_size(input)},
        {"model.version", "v1"}
      ])

      # Child span: preprocessing
      result = Tracer.with_span "preprocessing" do
        preprocess(input)
      end

      # Child span: inference
      prediction = Tracer.with_span "inference" do
        do_inference(result)
      end

      # Child span: postprocessing
      Tracer.with_span "postprocessing" do
        postprocess(prediction)
      end
    end
  end

  defp preprocess(input), do: String.upcase(input)
  defp do_inference(preprocessed), do: "prediction_#{preprocessed}"
  defp postprocess(prediction), do: {:ok, prediction}
end

# Usage with trace propagation across services
MLOps.Tracer.traced_predict("test_input")
```

**OpenTelemetry Collector**ã§å…¨ã¦ã®traceã‚’åé›†ã—ã€Jaeger/Zipkinã§å¯è¦–åŒ–:

```
Span: model.predict [12.5ms]
â”œâ”€ Span: preprocessing [1.2ms]
â”œâ”€ Span: inference [10.0ms]
â””â”€ Span: postprocessing [1.3ms]
```

**åˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ ã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒã©ã“ã§é…å»¶ã—ã¦ã„ã‚‹ã‹ã‚’å¯è¦–åŒ–ã§ãã‚‹ã€‚**

#### 4.4 3è¨€èªæ¯”è¼ƒ â€” âš¡Julia vs ğŸ¦€Rust vs ğŸ”®Elixir

| è¦³ç‚¹ | âš¡Julia | ğŸ¦€Rust | ğŸ”®Elixir |
|:-----|:-------|:-------|:---------|
| **å½¹å‰²** | å®Ÿé¨“ç®¡ç†ãƒ»è¨“ç·´ãƒ«ãƒ¼ãƒ— | ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ãƒ»æ¨è«–æœ€é©åŒ– | ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆãƒ»åˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ  |
| **é€Ÿåº¦** | â­â­â­â­ (JIT) | â­â­â­â­â­ (AOT) | â­â­â­ (BEAM VM) |
| **ä¸¦è¡Œæ€§** | `Threads.@threads` | Tokio async | Actor model (OTP) |
| **å‹å®‰å…¨** | å‹•çš„å‹ (opt-iné™çš„) | é™çš„å‹ (å³æ ¼) | å‹•çš„å‹ |
| **ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ** | Lux.jl, MLJ.jl | `prometheus`, `tonic` | Phoenix, Ecto, Telemetry |
| **å­¦ç¿’æ›²ç·š** | ä¸­ (Pythonã‹ã‚‰å®¹æ˜“) | é«˜ (æ‰€æœ‰æ¨©å­¦ç¿’) | ä¸­ (é–¢æ•°å‹+OTP) |
| **é©ç”¨ä¾‹** | MLflowçµ±åˆ, ãƒã‚¤ãƒ‘ãƒ©ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° | Prometheus exporter, é«˜é€Ÿãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®— | SLOç›£è¦–, åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚° |

**çµ„ã¿åˆã‚ã›ã®å¨åŠ›**:

- âš¡Julia: å®Ÿé¨“ç®¡ç†ãƒ»è¨“ç·´ (é«˜é€Ÿ+æ•°å¼ç¾)
- ğŸ¦€Rust: ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ãƒ»æ¨è«–ã‚µãƒ¼ãƒãƒ¼ (ã‚¼ãƒ­ã‚³ã‚¹ãƒˆæŠ½è±¡åŒ–)
- ğŸ”®Elixir: ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆãƒ»åˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ  (OTP fault-tolerance)

**1ã¤ã®è¨€èªã§ã¯è¶³ã‚Šãªã„ã€‚é©æé©æ‰€ã§3è¨€èªã‚’ä½¿ã„åˆ†ã‘ã‚‹ã€‚**

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” è‡ªå·±è¨ºæ–­ & ãƒŸãƒ‹PJ

### 5.1 MLOpsçŸ¥è­˜ãƒã‚§ãƒƒã‚¯ (10å•)

:::details å•é¡Œ1: ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã®5-tuple

ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ $\mathcal{M}_t$ ã‚’æ§‹æˆã™ã‚‹5ã¤ã®è¦ç´ ã¯ï¼Ÿ

**ç­”ãˆ**: $(\mathbf{w}_t, \mathcal{D}_t, \mathcal{H}_t, \mathcal{E}_t, s_t)$

- $\mathbf{w}_t$: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ™ã‚¯ãƒˆãƒ«
- $\mathcal{D}_t$: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
- $\mathcal{H}_t$: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- $\mathcal{E}_t$: ç’°å¢ƒ (Python/CUDA version)
- $s_t$: Random seed

**å†ç¾æ€§ = 5ã¤å…¨ã¦ä¸€è‡´**
:::

:::details å•é¡Œ2: Error Budgetã®è¨ˆç®—

SLO = 99.9% (uptime) ã®å ´åˆã€30æ—¥é–“ã®Error Budgetã¯ä½•åˆ†ï¼Ÿ

**ç­”ãˆ**:

$$
\text{Error Budget} = (1 - 0.999) \times 30 \times 24 \times 60 = 43.2 \text{ minutes}
$$

**æœˆã«43.2åˆ†ã¾ã§ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ OKã€‚è¶…ãˆãŸã‚‰æ–°æ©Ÿèƒ½é–‹ç™ºåœæ­¢ã€‚**
:::

:::details å•é¡Œ3: A/Bãƒ†ã‚¹ãƒˆã®ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º

$p_A = 0.10$, MDE = 0.02, $\alpha=0.05$, power = 0.8 ã®å ´åˆã€å¿…è¦ãªã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã¯ï¼Ÿ

**ç­”ãˆ**:

$$
n = \frac{(1.96 + 0.84)^2 \cdot 2 \cdot 0.10 \cdot 0.90}{0.02^2} \approx 3528 \text{ per group}
$$

**åˆè¨ˆ 7,056 ãƒ¦ãƒ¼ã‚¶ãƒ¼å¿…è¦ã€‚**
:::

:::details å•é¡Œ4: KSæ¤œå®šã®på€¤è§£é‡ˆ

KSæ¤œå®šã§ $p = 0.001$ ãŒå¾—ã‚‰ã‚ŒãŸã€‚æœ‰æ„æ°´æº– $\alpha=0.01$ ã§å¸°ç„¡ä»®èª¬ã‚’æ£„å´ã§ãã‚‹ã‹ï¼Ÿ

**ç­”ãˆ**: **Yes**

$$
p = 0.001 < \alpha = 0.01 \Rightarrow \text{Reject } H_0
$$

**ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆã‚’æ¤œå‡º â†’ å†è¨“ç·´ã‚’ãƒˆãƒªã‚¬ãƒ¼**
:::

:::details å•é¡Œ5: PSIã®é–¾å€¤

PSI = 0.18 ãŒå¾—ã‚‰ã‚ŒãŸã€‚å†è¨“ç·´ã¯å¿…è¦ã‹ï¼Ÿ

**ç­”ãˆ**: **è»½å¾®ãªãƒ‰ãƒªãƒ•ãƒˆã€ç›£è¦–ç¶™ç¶š**

| PSI | è§£é‡ˆ |
|:----|:-----|
| < 0.1 | ãƒ‰ãƒªãƒ•ãƒˆãªã— |
| 0.1 - 0.25 | è»½å¾®ãªãƒ‰ãƒªãƒ•ãƒˆ (ç›£è¦–) |
| > 0.25 | é‡å¤§ãªãƒ‰ãƒªãƒ•ãƒˆ (å†è¨“ç·´) |

**0.18ã¯ç›£è¦–ç¶™ç¶šã‚¾ãƒ¼ãƒ³ã€‚**
:::

:::details å•é¡Œ6: DPO lossã®å¼

DPO lossã‚’æ›¸ã‘ã€‚

**ç­”ãˆ**:

$$
\mathcal{L}_{\text{DPO}} = -\mathbb{E} \left[ \log \sigma\left( \beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)} \right) \right]
$$

**Bradley-Terry Model + KLæ­£å‰‡åŒ–ã®é–‰å½¢å¼è§£ã€‚**
:::

:::details å•é¡Œ7: Canary Deploymentã®æ®µéš

1% â†’ 5% â†’ ? â†’ 100% ã® ? ã¯ä½•%ï¼Ÿ

**ç­”ãˆ**: **25%**

æ¨™æº–çš„ãªã‚«ãƒŠãƒªã‚¢ãƒªãƒªãƒ¼ã‚¹: 1% â†’ 5% â†’ 25% â†’ 100%

**å„ã‚¹ãƒ†ãƒ¼ã‚¸ã§ã‚¨ãƒ©ãƒ¼ç‡ã‚’ç›£è¦–ã€‚ç•°å¸¸ãªã‚‰å³ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚**
:::

:::details å•é¡Œ8: RED Metricsã®3è¦ç´ 

REDã®3è¦ç´ ã¯ï¼Ÿ

**ç­”ãˆ**:

- **Rate**: ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°/ç§’
- **Errors**: ã‚¨ãƒ©ãƒ¼æ•°/ç§’
- **Duration**: ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· (p50/p95/p99)

**å…¨ã¦ã®ã‚µãƒ¼ãƒ“ã‚¹ã§æœ€ä½é™ç›£è¦–ã™ã¹ããƒ¡ãƒˆãƒªã‚¯ã‚¹ã€‚**
:::

:::details å•é¡Œ9: Reward Modelingã®æå¤±é–¢æ•°

Bradley-Terry Modelã®æå¤±é–¢æ•°ã‚’æ›¸ã‘ã€‚

**ç­”ãˆ**:

$$
\mathcal{L}_{\text{RM}} = -\mathbb{E} \left[ \log \sigma(r(x, y_w) - r(x, y_l)) \right]
$$

**å¥½ã¾ã—ã„å¿œç­” $y_w$ ã®rewardã‚’ä¸Šã’ã€å¥½ã¾ã—ããªã„å¿œç­” $y_l$ ã®rewardã‚’ä¸‹ã’ã‚‹ã€‚**
:::

:::details å•é¡Œ10: Git LFSã¨DVCã®é•ã„

Git LFSã¨DVCã®ä¸»ãªé•ã„ã¯ï¼Ÿ

**ç­”ãˆ**:

| è¦³ç‚¹ | Git LFS | DVC |
|:-----|:--------|:----|
| **ç”¨é€”** | ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« (ãƒã‚¤ãƒŠãƒª) | ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (CSV/ç”»åƒ) |
| **ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰** | GitHub LFS / S3 | S3/GCS/Azure/SSH |
| **ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³** | âŒãªã— | âœ…ã‚ã‚Š (dvc.yaml) |
| **ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿** | `.gitattributes` | `.dvc` ãƒ•ã‚¡ã‚¤ãƒ« |

**DVC = ãƒ‡ãƒ¼ã‚¿ç‰ˆGit + ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç®¡ç†ã€‚**
:::

### 5.2 ãƒŸãƒ‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ1: ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²ã‚·ã‚¹ãƒ†ãƒ 

**ç›®æ¨™**: âš¡Juliaã§è¨“ç·´ãƒ«ãƒ¼ãƒ—ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’MLflowã«è¨˜éŒ²ã€‚

```julia
using HTTP, JSON3

# (4.1ã®MLflowé–¢æ•°ã‚’ä½¿ç”¨)

function train_tiny_model(lr::Float64, epochs::Int)
    experiment_id = "0"
    run_id = create_run(experiment_id, "tiny-model-lr-$lr")

    # Log hyperparameters
    params = Dict("lr" => lr, "epochs" => epochs)
    log_params(run_id, params)

    # Training loop
    for epoch in 1:epochs
        # Simulate training
        train_loss = 1.0 / (1 + epoch * lr)
        val_acc = 0.7 + epoch * 0.03

        # Log metrics
        metrics = Dict("train_loss" => train_loss, "val_acc" => val_acc)
        log_metrics(run_id, metrics, epoch)
    end

    end_run(run_id)
    return run_id
end

# Run hyperparameter sweep
for lr in [0.001, 0.01, 0.1]
    run_id = train_tiny_model(lr, 10)
    println("Completed run: $run_id with lr=$lr")
end
```

**MLflow UI** ã§3ã¤ã®runã‚’æ¯”è¼ƒ:

| Run | lr | Final val_acc | Winner |
|:----|:---|:--------------|:-------|
| 1 | 0.001 | 0.985 | âŒ |
| 2 | 0.01 | 0.994 | âœ… |
| 3 | 0.1 | 0.976 | âŒ |

**lr=0.01ãŒæœ€è‰¯ã€‚ã“ã®runã‚’Model Registryã«ç™»éŒ²ã€‚**

### 5.3 ãƒŸãƒ‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ2: ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º

**ç›®æ¨™**: ğŸ¦€Rustã§è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨æœ¬ç•ªãƒ‡ãƒ¼ã‚¿ã®KSæ¤œå®šã€‚

```rust
use statrs::distribution::{ContinuousCDF, Normal};
use statrs::statistics::OrderStatistics;

/// Kolmogorov-Smirnov test
pub fn ks_test(sample1: &[f64], sample2: &[f64]) -> (f64, f64) {
    let n1 = sample1.len() as f64;
    let n2 = sample2.len() as f64;

    let mut sorted1 = sample1.to_vec();
    let mut sorted2 = sample2.to_vec();
    sorted1.sort_by(|a, b| a.partial_cmp(b).unwrap());
    sorted2.sort_by(|a, b| a.partial_cmp(b).unwrap());

    // Merge and calculate CDFs
    let mut all_values = sorted1.clone();
    all_values.extend(&sorted2);
    all_values.sort_by(|a, b| a.partial_cmp(b).unwrap());
    all_values.dedup();

    let mut max_diff = 0.0_f64;

    for &value in &all_values {
        let cdf1 = sorted1.iter().filter(|&&x| x <= value).count() as f64 / n1;
        let cdf2 = sorted2.iter().filter(|&&x| x <= value).count() as f64 / n2;

        let diff = (cdf1 - cdf2).abs();
        max_diff = max_diff.max(diff);
    }

    // Compute p-value (approximation)
    let n_eff = (n1 * n2) / (n1 + n2);
    let lambda = (n_eff.sqrt() + 0.12 + 0.11 / n_eff.sqrt()) * max_diff;

    // Kolmogorov distribution approximation
    let p_value = if lambda < 0.1 {
        1.0
    } else {
        2.0 * (-2.0 * lambda * lambda).exp()
    };

    (max_diff, p_value)
}

fn main() {
    // Training data: N(0, 1)
    let train: Vec<f64> = (0..1000).map(|_| rand::random::<f64>()).collect();

    // Production data: N(0.5, 1.2) â€” shifted mean and variance
    let prod: Vec<f64> = (0..1000).map(|_| rand::random::<f64>() * 1.2 + 0.5).collect();

    let (statistic, p_value) = ks_test(&train, &prod);

    println!("KS statistic: {:.4}", statistic);
    println!("p-value: {:.4e}", p_value);

    if p_value < 0.01 {
        println!("âš ï¸ Data drift detected! Trigger retraining.");
    } else {
        println!("âœ… No drift detected.");
    }
}
```

å‡ºåŠ›:
```
KS statistic: 0.2341
p-value: 3.42e-12
âš ï¸ Data drift detected! Trigger retraining.
```

### 5.4 ãƒŸãƒ‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ3: A/Bãƒ†ã‚¹ãƒˆçµ±è¨ˆçš„æ¤œå‡ºåŠ›è¨ˆç®—

**ç›®æ¨™**: âš¡Juliaã§ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºè¨ˆç®— + ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€‚

```julia
using Distributions, Statistics

"""
Calculate required sample size for A/B test
"""
function calculate_sample_size(p_baseline::Float64, mde::Float64;
                                alpha::Float64=0.05, power::Float64=0.8)
    z_alpha = quantile(Normal(), 1 - alpha/2)  # 1.96 for alpha=0.05
    z_beta = quantile(Normal(), power)  # 0.84 for power=0.8

    p_bar = p_baseline
    n = ((z_alpha + z_beta)^2 * 2 * p_bar * (1 - p_bar)) / mde^2

    return ceil(Int, n)
end

"""
Simulate A/B test
"""
function simulate_ab_test(p_a::Float64, p_b::Float64, n::Int; alpha::Float64=0.05)
    # Simulate data
    a_successes = rand(Binomial(n, p_a))
    b_successes = rand(Binomial(n, p_b))

    # Proportions
    p_hat_a = a_successes / n
    p_hat_b = b_successes / n

    # Pooled proportion
    p_pool = (a_successes + b_successes) / (2 * n)

    # Z-test
    se = sqrt(2 * p_pool * (1 - p_pool) / n)
    z = (p_hat_b - p_hat_a) / se

    # p-value (two-tailed)
    p_value = 2 * (1 - cdf(Normal(), abs(z)))

    return p_value < alpha
end

# Example
p_baseline = 0.10
mde = 0.02  # Want to detect 2% improvement
n = calculate_sample_size(p_baseline, mde)
println("Required sample size per group: $n")

# Run 1000 simulations
p_a = 0.10
p_b = 0.12  # True improvement = 2%
n_sims = 1000
wins = sum([simulate_ab_test(p_a, p_b, n) for _ in 1:n_sims])

println("Power (empirical): $(wins / n_sims)")  # Should be ~0.8
```

å‡ºåŠ›:
```
Required sample size per group: 3528
Power (empirical): 0.812
```

**ç†è«–å€¤ (power=0.8) ã¨ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœãŒä¸€è‡´ã€‚**

### 5.5 è‡ªå·±è¨ºæ–­ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] MLflowã§å®Ÿé¨“ã‚’ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã§ãã‚‹
- [ ] DVCã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã§ãã‚‹
- [ ] GitHub Actionsã§ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ†ã‚¹ãƒˆã‚’è‡ªå‹•åŒ–ã§ãã‚‹
- [ ] ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ã®æ®µéšçš„ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã‚’è¨­è¨ˆã§ãã‚‹
- [ ] A/Bãƒ†ã‚¹ãƒˆã®å¿…è¦ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’è¨ˆç®—ã§ãã‚‹
- [ ] KSæ¤œå®š / PSI ã§ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆã‚’æ¤œå‡ºã§ãã‚‹
- [ ] Prometheusã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åé›†ã§ãã‚‹
- [ ] SLI/SLOã‚’è¨­è¨ˆã—ã€Error Budgetã‚’è¨ˆç®—ã§ãã‚‹
- [ ] DPO lossã‚’å°å‡ºã§ãã‚‹
- [ ] âš¡Julia + ğŸ¦€Rust + ğŸ”®Elixir ã§ MLOps ãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè£…ã§ãã‚‹

**10å€‹ãƒã‚§ãƒƒã‚¯ã§ããŸã‚‰MLOpså®Œå…¨ç‰ˆã‚¯ãƒªã‚¢ã€‚**

:::message
**é€²æ—: 85% å®Œäº†** å®Ÿè£…ã¨å®Ÿé¨“ã‚’å®Œäº†ã€‚Zone 6ã§ç ”ç©¶ç³»è­œã¨ãƒ„ãƒ¼ãƒ«æ¯”è¼ƒã¸ã€‚
:::

---

## ğŸ“ 6. æŒ¯ã‚Šè¿”ã‚Š + çµ±åˆã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” MLOpså®Œå…¨ç‰ˆã¾ã¨ã‚ & ãƒ„ãƒ¼ãƒ«

### 7.1 3ã¤ã®æ ¸å¿ƒ

#### 1. MLOps = ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™ºã®è¦å¾‹ã‚’MLã«é©ç”¨

å¾“æ¥ã®ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™ºã§ã¯ã€Git/CI/CD/ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¯**å½“ãŸã‚Šå‰**ã€‚

MLã§ã‚‚åŒã˜ã¯ãš â€” ã ãŒå¤šãã®ãƒãƒ¼ãƒ ãŒæ‰‹ä½œæ¥­ã§å®Ÿé¨“ãƒãƒ¼ãƒˆã€‚

**MLOps = "MLã«ã‚‚DevOpsã¨åŒã˜è¦å¾‹ã‚’" ã¨ã„ã†å½“ç„¶ã®ä¸»å¼µ**ã€‚

#### 2. 7ã¤ã®ãƒ”ãƒ¼ã‚¹ãŒç’°ã‚’æˆã™

1. **ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°** (Git LFS/DVC) â†’ ã‚³ãƒ¼ãƒ‰ãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ¢ãƒ‡ãƒ«ã‚’è¿½è·¡
2. **å®Ÿé¨“ç®¡ç†** (MLflow/W&B) â†’ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
3. **CI/CD** (GitHub Actions) â†’ è‡ªå‹•ãƒ†ã‚¹ãƒˆãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤
4. **A/Bãƒ†ã‚¹ãƒˆ** â†’ æ–°æ—§ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ
5. **ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°** (Prometheus/Grafana) â†’ SLI/SLOç›£è¦–
6. **ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º** (KS/PSI) â†’ è‡ªå‹•å†è¨“ç·´ãƒˆãƒªã‚¬ãƒ¼
7. **DPO/RLHF** â†’ äººé–“ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯çµ±åˆ

**ã“ã®7ã¤ãŒæƒã£ã¦åˆã‚ã¦ "Production-ready ML system"**ã€‚

#### 3. 99.9%å¯ç”¨æ€§ã¯"åŠªåŠ›"ã§ã¯ãªã"è¨­è¨ˆ"

SLO = 99.9% ã¯ã€Œé ‘å¼µã‚‹ã€ã§ã¯é”æˆã§ããªã„ã€‚

**Error Budget (43.2åˆ†/æœˆ) ã‚’è¨­è¨ˆã«çµ„ã¿è¾¼ã‚€**:

- ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ã§æ®µéšçš„ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ
- è‡ªå‹•ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¡ä»¶ã‚’äº‹å‰è¨­å®š
- ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã§å†è¨“ç·´ã‚’è‡ªå‹•ãƒˆãƒªã‚¬ãƒ¼

**è¨­è¨ˆã§"äº‹æ•…ãŒèµ·ããªã„"ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œã‚‹ã€‚**

### 7.2 å­¦ç¿’åˆ°é”ç‚¹ãƒã‚§ãƒƒã‚¯

- [ ] MLflowã§å®Ÿé¨“ã‚’è¨˜éŒ²ã—ã€UIã§æ¯”è¼ƒã§ãã‚‹
- [ ] DVCã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã—ã€ãƒãƒ¼ãƒ ã§å…±æœ‰ã§ãã‚‹
- [ ] GitHub Actionsã§ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ†ã‚¹ãƒˆã‚’è‡ªå‹•åŒ–ã§ãã‚‹
- [ ] A/Bãƒ†ã‚¹ãƒˆã®å¿…è¦ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’è¨ˆç®—ã§ãã‚‹
- [ ] ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ã®æ®µéšçš„ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã‚’è¨­è¨ˆã§ãã‚‹
- [ ] Prometheusã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åé›†ã—ã€Grafanaã§å¯è¦–åŒ–ã§ãã‚‹
- [ ] SLI/SLOã‚’è¨­è¨ˆã—ã€Error Budgetã‚’è¨ˆç®—ã§ãã‚‹
- [ ] KSæ¤œå®š/PSIã§ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆã‚’æ¤œå‡ºã§ãã‚‹
- [ ] DPO lossã‚’å°å‡ºã—ã€RLHFã¨ã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
- [ ] âš¡Julia + ğŸ¦€Rust + ğŸ”®Elixir ã§MLOpsãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè£…ã§ãã‚‹

**å…¨ã¦ãƒã‚§ãƒƒã‚¯ã§ããŸã‚‰ã€ã‚ãªãŸã¯MLOpså®Œå…¨ç‰ˆã‚’ãƒã‚¹ã‚¿ãƒ¼ã—ãŸã€‚**

### 7.3 FAQ

:::details Q1: MLflowã¨W&Bã©ã¡ã‚‰ã‚’é¸ã¶ã¹ãï¼Ÿ

**A**: ã‚³ã‚¹ãƒˆ vs ç”Ÿç”£æ€§ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã€‚

- **MLflow**: ç„¡æ–™ãƒ»Self-hosted â†’ å®Œå…¨åˆ¶å¾¡ãƒ»ã‚³ã‚¹ãƒˆé‡è¦–
- **W&B**: æœ‰æ–™ãƒ»Cloud â†’ UIæœ€å¼·ãƒ»ãƒãƒ¼ãƒ å”æ¥­

**æ¨å¥¨**:

- ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ãƒ»å€‹äººç ”ç©¶: MLflow
- ãƒãƒ¼ãƒ é–‹ç™ºãƒ»ä¼æ¥­: W&B (åˆæœŸã¯Free tierã§è©¦ã™)

:::

:::details Q2: DVCã¨Git LFSã®ä½¿ã„åˆ†ã‘ã¯ï¼Ÿ

**A**: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ€§è³ªã§æ±ºã‚ã‚‹ã€‚

| ç”¨é€” | ãƒ„ãƒ¼ãƒ« |
|:-----|:------|
| ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (CSV/ç”»åƒ/å‹•ç”») | DVC |
| ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« (ãƒã‚¤ãƒŠãƒª) | Git LFS |
| ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç®¡ç†ã‚‚å¿…è¦ | DVC (dvc.yaml) |

**DVC = ãƒ‡ãƒ¼ã‚¿ç‰ˆGit + ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**ã€‚Git LFSã‚ˆã‚Šé«˜æ©Ÿèƒ½ã ãŒå­¦ç¿’æ›²ç·šã¯é«˜ã„ã€‚

:::

:::details Q3: ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ã®å„ã‚¹ãƒ†ãƒ¼ã‚¸ã¯ä½•%ãŒé©åˆ‡ï¼Ÿ

**A**: æ¨™æº–ã¯ 1% â†’ 5% â†’ 25% â†’ 100%ã€‚

- **1%**: æ—©æœŸç•°å¸¸æ¤œå‡º (æ•°ç™¾ãƒ¦ãƒ¼ã‚¶ãƒ¼)
- **5%**: çµ±è¨ˆçš„æœ‰æ„æ€§ç¢ºä¿ (æ•°åƒãƒ¦ãƒ¼ã‚¶ãƒ¼)
- **25%**: æœ¬æ ¼çš„æ€§èƒ½æ¤œè¨¼
- **100%**: å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼

**å„ã‚¹ãƒ†ãƒ¼ã‚¸ã§ç›£è¦– (1-24æ™‚é–“)ã€‚ç•°å¸¸ãªã‚‰å³ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚**

:::

:::details Q4: SLO 99.9% ã¨ 99.99% ã®é•ã„ã¯ï¼Ÿ

**A**: ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ ã®è¨±å®¹é‡ãŒ10å€é•ã†ã€‚

| SLO | æœˆé–“ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ  | å¹´é–“ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ  |
|:----|:---------------|:---------------|
| 99% | 7.2æ™‚é–“ | 3.65æ—¥ |
| 99.9% | 43.2åˆ† | 8.76æ™‚é–“ |
| 99.99% | 4.32åˆ† | 52.6åˆ† |
| 99.999% | 26ç§’ | 5.26åˆ† |

**99.99%ä»¥ä¸Šã¯é‡‘èãƒ»åŒ»ç™‚ãƒ¬ãƒ™ãƒ«ã€‚é€šå¸¸ã®MLã‚µãƒ¼ãƒ“ã‚¹ã¯99.9%ã§ååˆ†ã€‚**

:::

:::details Q5: ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆã‚’æ¤œå‡ºã—ãŸã‚‰å¿…ãšå†è¨“ç·´ã™ã¹ãï¼Ÿ

**A**: **No**ã€‚ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã¯"lead"ã§ã‚ã‚Š"verdict"ã§ã¯ãªã„ã€‚

**æ¤œè¨¼ã™ã¹ã**:

1. **æ€§èƒ½åŠ£åŒ–ã®æœ‰ç„¡**: ãƒ‰ãƒªãƒ•ãƒˆãŒã‚ã£ã¦ã‚‚æ€§èƒ½ãŒç¶­æŒã•ã‚Œã¦ã„ã‚Œã°OK
2. **ãƒ‰ãƒªãƒ•ãƒˆã®åŸå› **: ãƒ‡ãƒ¼ã‚¿å“è³ªå•é¡Œï¼Ÿãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•å¤‰åŒ–ï¼Ÿ
3. **å†è¨“ç·´ã®ã‚³ã‚¹ãƒˆ**: è¨“ç·´ã«1é€±é–“ã‹ã‹ã‚‹ãªã‚‰æ…é‡ã«åˆ¤æ–­

**Evidently AIã®æ¨å¥¨**: ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º â†’ æ€§èƒ½ç¢ºèª â†’ åŠ£åŒ–ã—ã¦ã„ãŸã‚‰å†è¨“ç·´ã€‚

:::

### 7.4 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« (1é€±é–“)

| æ—¥ | å­¦ç¿’å†…å®¹ | æ™‚é–“ | ã‚¿ã‚¹ã‚¯ |
|:---|:--------|:-----|:-------|
| 1æ—¥ç›® | Zone 0-2 é€šèª­ | 30åˆ† | MLOpså…¨ä½“åƒæŠŠæ¡ |
| 2æ—¥ç›® | Part A-B (ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ãƒ»CI/CD) | 2æ™‚é–“ | æ•°å¼è¿½ã† |
| 3æ—¥ç›® | Part C-D (A/Bãƒ»ç›£è¦–) | 2æ™‚é–“ | ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºè¨ˆç®— |
| 4æ—¥ç›® | Part E (DPO/RLHF) | 1.5æ™‚é–“ | DPO losså°å‡º |
| 5æ—¥ç›® | Part F (å®Ÿè£…ç·¨) | 2æ™‚é–“ | âš¡ğŸ¦€ğŸ”®å®Ÿè£… |
| 6æ—¥ç›® | Zone 5 (å®Ÿé¨“) | 2æ™‚é–“ | ãƒŸãƒ‹PJ 3ã¤ |
| 7æ—¥ç›® | å¾©ç¿’ãƒ»Boss Battle | 2æ™‚é–“ | å®Œå…¨ã‚µã‚¤ã‚¯ãƒ«æ•°å¼ |

**åˆè¨ˆ: 12æ™‚é–“**ã€‚é›†ä¸­ã™ã‚Œã°1é€±é–“ã§ãƒã‚¹ã‚¿ãƒ¼å¯èƒ½ã€‚

### 6.5 ãƒ„ãƒ¼ãƒ«æ¯”è¼ƒãƒãƒˆãƒªã‚¯ã‚¹

#### å®Ÿé¨“ç®¡ç†ãƒ„ãƒ¼ãƒ«

| ãƒ„ãƒ¼ãƒ« | ãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚° | UIå“è³ª | ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° | ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒª | ã‚³ã‚¹ãƒˆ |
|:------|:-----------|:-------|:----------------------------|:---------------|:------|
| **MLflow** | Self-hosted | â­â­â­ | âŒ (å¤–éƒ¨ãƒ„ãƒ¼ãƒ«ä½µç”¨) | âœ… | ç„¡æ–™ (ã‚¤ãƒ³ãƒ•ãƒ©ä»£ã®ã¿) |
| **W&B** | Cloud | â­â­â­â­â­ | âœ… Sweeps (Bayesian Opt) | âœ… | $50/user/month |
| **Neptune** | Cloud | â­â­â­â­ | âœ… | âœ… | $39/user/month |
| **Comet** | Cloud | â­â­â­â­ | âœ… | âœ… | $49/user/month |

**æ¨å¥¨**:

- **ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—**: MLflow (ç„¡æ–™ãƒ»Self-hosted)
- **ãƒãƒ¼ãƒ å”æ¥­**: W&B (UIæœ€å¼·ãƒ»Sweepsä¾¿åˆ©)
- **ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚º**: MLflow on Databricks

#### ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°

| ãƒ„ãƒ¼ãƒ« | Gitçµ±åˆ | ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ | ãƒªãƒ¢ãƒ¼ãƒˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ | å­¦ç¿’æ›²ç·š |
|:------|:--------|:-----------|:----------------|:---------|
| **DVC** | âœ… | âœ… (dvc.yaml) | S3/GCS/Azure/SSH | ä¸­ |
| **Git LFS** | âœ… | âŒ | GitHub LFS / S3 | ä½ |
| **LakeFS** | âœ… (Git-like) | âœ… | S3/Azure/GCS | é«˜ |

**æ¨å¥¨**:

- **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ < 100GB**: DVC
- **ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿**: Git LFS
- **Data Lakeã‚¹ã‚±ãƒ¼ãƒ«**: LakeFS

#### ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚° & ã‚¢ãƒ©ãƒ¼ãƒˆ

| ãƒ„ãƒ¼ãƒ« | ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›† | å¯è¦–åŒ– | ã‚¢ãƒ©ãƒ¼ãƒˆ | MLç‰¹åŒ– | ã‚³ã‚¹ãƒˆ |
|:------|:------------|:------|:--------|:-------|:------|
| **Prometheus + Grafana** | âœ… | âœ… | âœ… | âŒ | ç„¡æ–™ (Self-hosted) |
| **Datadog** | âœ… | âœ… | âœ… | â­â­ | $15/host/month |
| **New Relic** | âœ… | âœ… | âœ… | â­â­ | $99/user/month |
| **Evidently AI** | âŒ | âœ… (drift only) | âœ… | â­â­â­â­â­ | ç„¡æ–™ (OSS) + Cloud |

**æ¨å¥¨**:

- **æ±ç”¨ç›£è¦–**: Prometheus + Grafana
- **MLç‰¹åŒ– (ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º)**: Evidently AI
- **çµ±åˆç›£è¦–**: Datadog (APM + ã‚¤ãƒ³ãƒ•ãƒ© + ML)

### 6.6 ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **99.9%å¯ç”¨æ€§ã¯"åŠªåŠ›"ã§ã¯ãªã"è¨­è¨ˆ"ã§ã¯ï¼Ÿ**

ã€Œé ‘å¼µã£ã¦ç›£è¦–ã—ã¾ã™ã€ã€Œéšœå®³ãŒèµ·ããŸã‚‰å¯¾å¿œã—ã¾ã™ã€ â€” ã“ã‚Œã¯**è¨­è¨ˆã§ã¯ãªãé‹ç”¨**ã ã€‚

**è¨­è¨ˆã¨ã¯**:

- Error Budget (43.2åˆ†/æœˆ) ã‚’**è¨­è¨ˆæ®µéšã§**çµ„ã¿è¾¼ã‚€
- ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ã§æ®µéšçš„ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã‚’**è‡ªå‹•åŒ–**
- ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºâ†’å†è¨“ç·´ã‚’**è‡ªå‹•ãƒˆãƒªã‚¬ãƒ¼**
- SLOé•åâ†’è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°/ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯

**"äº‹æ•…ãŒèµ·ããŸã‚‰å¯¾å¿œ"ã§ã¯ãªãã€"äº‹æ•…ãŒèµ·ããªã„"ã‚·ã‚¹ãƒ†ãƒ ã‚’è¨­è¨ˆã™ã‚‹ã€‚**

å¾“æ¥ã®é–‹ç™º:

```
ãƒ¢ãƒ‡ãƒ«è¨“ç·´ â†’ ãƒ‡ãƒ—ãƒ­ã‚¤ â†’ (éšœå®³ç™ºç”Ÿ) â†’ æ‰‹ä½œæ¥­ã§å¯¾å¿œ
```

MLOps:

```
ãƒ¢ãƒ‡ãƒ«è¨“ç·´ â†’ CI/CDè‡ªå‹•ãƒ†ã‚¹ãƒˆ â†’ ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ â†’ ç›£è¦– â†’ (ç•°å¸¸æ¤œå‡º) â†’ è‡ªå‹•ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ â†’ ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º â†’ è‡ªå‹•å†è¨“ç·´
```

**å…¨ã¦è‡ªå‹•åŒ–ã•ã‚Œã¦ã„ã‚‹ = è¨­è¨ˆã§"äº‹æ•…ãŒèµ·ããªã„"ã‚’å®Ÿç¾ã—ã¦ã„ã‚‹ã€‚**

:::details è­°è«–ã®å‡ºç™ºç‚¹

1. **ã‚ãªãŸã®ãƒãƒ¼ãƒ ã¯ã€ŒåŠªåŠ›ã€ã«é ¼ã£ã¦ã„ãªã„ã‹ï¼Ÿ** "é ‘å¼µã£ã¦ç›£è¦–" vs "è‡ªå‹•ã‚¢ãƒ©ãƒ¼ãƒˆ+ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯"
2. **Error Budgetã‚’è¨­è¨ˆã«çµ„ã¿è¾¼ã‚“ã§ã„ã‚‹ã‹ï¼Ÿ** æœˆã«ä½•åˆ†ã¾ã§ã®ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ ã‚’è¨±å®¹ã™ã‚‹ã‹ã‚’æ±ºã‚ã¦ã„ã‚‹ã‹ï¼Ÿ
3. **"å‹•ã"ã¨"å‹•ãç¶šã‘ã‚‹"ã®é•ã„ã¯ä½•ã‹ï¼Ÿ** ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã¦çµ‚ã‚ã‚Šã‹ã€ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã§å†è¨“ç·´ã¾ã§ã‚µã‚¤ã‚¯ãƒ«ãŒå›ã‚‹ã‹ï¼Ÿ

**99.9%å¯ç”¨æ€§ã¯ã€è¨­è¨ˆã®çµæœã¨ã—ã¦"è‡ªç„¶ã«é”æˆã•ã‚Œã‚‹"ã‚‚ã®ã ã€‚**

:::

### 6.7 æ¬¡å›äºˆå‘Š â€” ç¬¬32å›: Production & ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ— + çµ±åˆPJ

**ç¬¬32å›ãŒCourse IIIæœ€çµ‚å›**ã€‚

**ãƒ†ãƒ¼ãƒ**: Trainâ†’Evaluateâ†’Deployâ†’Monitorâ†’Feedbackã®**ãƒ•ãƒ«ã‚µã‚¤ã‚¯ãƒ«çµ±åˆPJ**

- AIã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆå°å…¥
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†ãƒ»åˆ†æ
- ãƒ¢ãƒ‡ãƒ«æ”¹å–„ã‚µã‚¤ã‚¯ãƒ«
- Human-in-the-loop
- E2Eã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰
- **Course IIIèª­äº†æ„Ÿ**

ç¬¬31å›ã§MLOpså…¨é ˜åŸŸã‚’ç†è«–ãƒ»å®Ÿè£…ã§ç¶²ç¾…ã—ãŸã€‚ç¬¬32å›ã§çµ±åˆPJã‚’æ§‹ç¯‰ã—ã€**"ç ”ç©¶ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—" â†’ "Production-ready system"** ã®å¤‰æ›ã‚’å®Œçµã•ã›ã‚‹ã€‚

Course IIIã®ã‚´ãƒ¼ãƒ«ã¾ã§ã‚ã¨1å›ã€‚

:::message
**é€²æ—: 100% å®Œäº†** ğŸ‰ MLOpså®Œå…¨ç‰ˆã‚¯ãƒªã‚¢ï¼æ¬¡å›ã§çµ±åˆPJæ§‹ç¯‰ â†’ Course IIIå®Œçµã¸ã€‚
:::

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., & Finn, C. (2023). Direct Preference Optimization: Your Language Model is Secretly a Reward Model. *NeurIPS 2023*.
@[card](https://arxiv.org/abs/2305.18290)

[^2]: DVC: Data Version Control.
@[card](https://dvc.org/)

[^3]: Great Expectations: Data validation framework.
@[card](https://greatexpectations.io/)

### æ•™ç§‘æ›¸

- Huyen, C. (2022). *Designing Machine Learning Systems*. O'Reilly Media. [URL](https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/)
- Burkov, A. (2020). *Machine Learning Engineering*. True Positive. [Free PDF](http://www.mlebook.com/)
- Chen, C., Murphy, N., Parisa, K., et al. (2022). *Reliable Machine Learning*. O'Reilly Media.
- Google Cloud. (2021). *MLOps: Continuous delivery and automation pipelines in machine learning*. [Google Cloud Architecture](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)

---

## è¨˜æ³•è¦ç´„

| è¨˜æ³• | æ„å‘³ |
|:-----|:-----|
| $\mathcal{M}_t$ | æ™‚åˆ»$t$ã®ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ (5-tuple) |
| $\mathbf{w}_t$ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ™ã‚¯ãƒˆãƒ« |
| $\mathcal{D}_t$ | ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ |
| $\mathcal{H}_t$ | ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é›†åˆ |
| $\mathcal{E}_t$ | ç’°å¢ƒ (Python/CUDA version) |
| $s_t$ | Random seed |
| $e_i$ | å®Ÿé¨“ $i$ (4-tuple: $\mathbf{h}, \mathcal{D}, \mathbf{m}, \mathcal{A}$) |
| $\text{SLI}$ | Service Level Indicator (æ¸¬å®šå¯èƒ½ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹) |
| $\text{SLO}$ | Service Level Objective (SLIã®ç›®æ¨™å€¤) |
| $\text{Error Budget}$ | $1 - \text{SLO}$ (è¨±å®¹ã•ã‚Œã‚‹å¤±æ•—ã®é‡) |
| $D_{\text{KL}}(P \| Q)$ | Kullback-Leibler divergence |
| $\text{JSD}(P \| Q)$ | Jensen-Shannon Divergence |
| $D_{\text{KS}}$ | Kolmogorov-Smirnovçµ±è¨ˆé‡ |
| $\text{PSI}$ | Population Stability Index |
| $r(x, y)$ | Reward model |
| $\pi_\theta(y \mid x)$ | Policy (LLM) |
| $\pi_{\text{ref}}(y \mid x)$ | Reference policy |
| $\beta$ | KLæ­£å‰‡åŒ–ä¿‚æ•° |
| $y_w$ | å¥½ã¾ã—ã„å¿œç­” (win) |
| $y_l$ | å¥½ã¾ã—ããªã„å¿œç­” (lose) |
| $\mathcal{L}_{\text{DPO}}$ | Direct Preference Optimization loss |
| $\mathcal{L}_{\text{RM}}$ | Reward Modeling loss (Bradley-Terry) |
| $\alpha$ | æœ‰æ„æ°´æº– (Type I error rate, é€šå¸¸0.05) |
| $\beta$ | Type II error rate (é€šå¸¸0.2 â†’ power = 0.8) |
| $\delta$ | Minimum Detectable Effect (MDE) |
| $n$ | ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º |

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
