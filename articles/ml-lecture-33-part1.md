---
title: "ç¬¬33å›: Normalizing Flowsã€å‰ç·¨ã€‘ç†è«–ç·¨: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œ"
emoji: "ğŸ”„"
type: "tech"
topics: ["machinelearning"]
published: true
slug: "ml-lecture-33-part1"
---
---

# ç¬¬33å›: Normalizing Flows â€” å¯é€†å¤‰æ›ã§å³å¯†å°¤åº¦ã‚’æ‰‹ã«å…¥ã‚Œã‚‹

> **VAEã¯è¿‘ä¼¼ã€GANã¯æš—é»™çš„ã€‚Normalizing Flowsã¯å¯é€†å¤‰æ›ã§å³å¯†ãªå°¤åº¦ log p(x) ã‚’è¨ˆç®—ã™ã‚‹ã€‚å¤‰æ•°å¤‰æ›ã®æ•°å­¦ãŒã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã«æ–°ã—ã„é“ã‚’é–‹ã„ãŸã€‚**

VAEã¯å¤‰åˆ†ä¸‹ç•ŒELBOã§çœŸã®å°¤åº¦ log p(x) ã‚’ä¸‹ã‹ã‚‰è¿‘ä¼¼ã™ã‚‹ã€‚GANã¯å°¤åº¦ã‚’æ¨ã¦ã€è­˜åˆ¥å™¨ã¨ã®æ•µå¯¾ã§æš—é»™çš„ã«åˆ†å¸ƒã‚’å­¦ã¶ã€‚ã©ã¡ã‚‰ã‚‚ã€Œå³å¯†ãªå°¤åº¦ã€ã‚’è«¦ã‚ãŸã€‚

Normalizing Flows [^1] [^2] ã¯å¯é€†å¤‰æ› f: z â†’ x ã§ã€**Change of Variableså…¬å¼ã‚’ä½¿ã„å³å¯†ãª log p(x) ã‚’è¨ˆç®—ã™ã‚‹**ã€‚ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—å¼ |det J_f| ãŒãã®éµã ã€‚

ã“ã®æ•°å­¦çš„ç¾ã—ã•ã¯ä»£å„Ÿã‚’ä¼´ã†ã€‚å¯é€†æ€§åˆ¶ç´„ãŒã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’åˆ¶é™ã™ã‚‹ã€‚è¨ˆç®—é‡ O(DÂ³) ã®ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—å¼ãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã«ãªã‚‹ã€‚RealNVP [^3]ã€Glow [^4] ã¯æ§‹é€ åŒ–ã•ã‚ŒãŸå¤‰æ›ã§ã“ã‚Œã‚’ O(D) ã«å‰Šæ¸›ã—ãŸã€‚ãã—ã¦Continuous Normalizing Flows (CNF) [^5] ã¨FFJORD [^6] ãŒã€Neural ODEã§é€£ç¶šæ™‚é–“ã®å¯é€†å¤‰æ›ã‚’å®Ÿç¾ã—ã€Diffusion Modelsã‚„Flow Matchingã¸ã®æ©‹ã‚’æ¶ã‘ãŸã€‚

æœ¬è¬›ç¾©ã¯Course IVã€Œæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ç†è«–ç·¨ã€ã®ç¬¬1å› â€” å…¨10è¬›ç¾©ã®æ—…ã®å‡ºç™ºç‚¹ã ã€‚Course I-IIIã§åŸ¹ã£ãŸæ•°å­¦åŠ›ã¨å®Ÿè£…åŠ›ã‚’æ­¦å™¨ã«ã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«–ã®æ·±æ·µã¸ã€‚

:::message
**Course IVæ¦‚è¦**: Normalizing Flows â†’ EBM â†’ Score Matching â†’ DDPM â†’ SDE â†’ Flow Matching â†’ LDM â†’ Consistency Models â†’ World Models â†’ çµ±ä¸€ç†è«–ã€‚å¯†åº¦ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®è«–ç†çš„ãƒã‚§ãƒ¼ãƒ³ã‚’è¾¿ã‚Šã€ã€Œæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«è«–æ–‡ã®ç†è«–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒå°å‡ºã§ãã‚‹ã€åˆ°é”ç‚¹ã¸ã€‚
:::

```mermaid
graph LR
    A["ğŸ“Š VAE<br/>ELBOè¿‘ä¼¼<br/>ã¼ã‚„ã‘"] --> D["ğŸ¯ å³å¯†å°¤åº¦<br/>ã®è¿½æ±‚"]
    B["ğŸ¨ GAN<br/>æš—é»™çš„å¯†åº¦<br/>ä¸å®‰å®š"] --> D
    D --> E["ğŸŒŠ Normalizing Flow<br/>å¯é€†å¤‰æ›f<br/>log p(x)è¨ˆç®—å¯èƒ½"]
    E --> F["ğŸ“ Change of Variables<br/>|det J_f|"]
    E --> G["ğŸ”„ RealNVP/Glow<br/>æ§‹é€ åŒ–"]
    E --> H["âˆ CNF/FFJORD<br/>Neural ODE"]
    H --> I["ğŸŒˆ Diffusion/FM<br/>ã¸ã®æ©‹"]
    style E fill:#e1f5ff
    style H fill:#fff3e0
    style I fill:#f3e5f5
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ + ç™ºå±• | 35åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | æŒ¯ã‚Šè¿”ã‚Š + çµ±åˆ | 30åˆ† | â˜…â˜…â˜…â˜†â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” å¯é€†å¤‰æ›ã§å¯†åº¦ã‚’è¿½è·¡ã™ã‚‹

**ã‚´ãƒ¼ãƒ«**: Change of Variableså…¬å¼ã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ã€‚

ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ z ~ N(0,1) ã‚’ä»®å®šå¤‰æ› f(z) = Î¼ + Ïƒz ã§å¤‰æ›ã—ã€å¤‰æ›å¾Œã®å¯†åº¦ p(x) ã‚’ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã§è¨ˆç®—ã™ã‚‹ã€‚

```julia
using Distributions, LinearAlgebra

# 1D Normalizing Flow: f(z) = Î¼ + Ïƒz
f(z, Î¼, Ïƒ) = Î¼ .+ Ïƒ .* z
f_inv(x, Î¼, Ïƒ) = (x .- Î¼) ./ Ïƒ
log_det_jacobian(Ïƒ) = sum(log.(abs.(Ïƒ)))  # |det J_f| = |Ïƒ|

# Base distribution: z ~ N(0, 1)
q_z = Normal(0, 1)

# Transform: x = f(z) with Î¼=2, Ïƒ=3
Î¼, Ïƒ = 2.0, 3.0
z_samples = rand(q_z, 1000)
x_samples = f(z_samples, Î¼, Ïƒ)

# Exact log p(x) via Change of Variables
# log p(x) = log q(z) - log|det J_f|
log_p_x(x) = logpdf(q_z, f_inv(x, Î¼, Ïƒ)) - log_det_jacobian(Ïƒ)

println("z ~ N(0,1) â†’ x = 2 + 3z")
println("log p(x=5) = ", round(log_p_x(5.0), digits=4))
println("Expected: log N(5; Î¼=2, ÏƒÂ²=9) = ", round(logpdf(Normal(Î¼, Ïƒ), 5.0), digits=4))
println("Change of Variableså…¬å¼ã§å³å¯†ãªlog p(x)ã‚’è¨ˆç®—ã—ãŸ!")
```

å‡ºåŠ›:
```
z ~ N(0,1) â†’ x = 2 + 3z
log p(x=5) = -2.3259
Expected: log N(5; Î¼=2, ÏƒÂ²=9) = -2.3259
Change of Variableså…¬å¼ã§å³å¯†ãªlog p(x)ã‚’è¨ˆç®—ã—ãŸ!
```

**3è¡Œã®ã‚³ãƒ¼ãƒ‰ã§å¯é€†å¤‰æ›ã¨å¯†åº¦è¿½è·¡ã‚’å‹•ã‹ã—ãŸã€‚** æ•°å¼ã§æ›¸ãã¨:

$$
\begin{aligned}
z &\sim q(z) = \mathcal{N}(0, 1) \\
x &= f(z) = \mu + \sigma z \quad \text{(invertible)} \\
\log p(x) &= \log q(f^{-1}(x)) - \log \left| \det \frac{\partial f}{\partial z} \right| \\
&= \log q\left(\frac{x - \mu}{\sigma}\right) - \log |\sigma|
\end{aligned}
$$

**Change of Variableså…¬å¼** (ç¬¬3-4å›ã®ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³å‰æ):

$$
p_X(x) = p_Z(f^{-1}(x)) \left| \det \frac{\partial f^{-1}}{\partial x} \right| = p_Z(z) \left| \det \frac{\partial f}{\partial z} \right|^{-1}
$$

ã“ã®å…¬å¼ãŒã€Normalizing Flowsã®å…¨ã¦ã®ç†è«–çš„åŸºç›¤ã ã€‚

:::message
**é€²æ—: 3% å®Œäº†** Change of Variableså…¬å¼ã‚’ä½“æ„Ÿã—ãŸã€‚ã“ã“ã‹ã‚‰ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¨ˆç®—ã®å›°é›£æ€§ã€Coupling Layerã€RealNVPã€Glowã€CNFã€FFJORDã¸é€²ã‚€ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” Flowã®3å½¢æ…‹ã‚’è§¦ã‚‹

### 1.1 Normalizing Flowã¨ã¯ä½•ã‹

**å®šç¾©**: å˜ç´”ãªåˆ†å¸ƒ q(z) (é€šå¸¸ N(0,I)) ã‹ã‚‰ã€å¯é€†å¤‰æ›ã®åˆæˆã§è¤‡é›‘ãªåˆ†å¸ƒ p(x) ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚

$$
\begin{aligned}
z_0 &\sim q(z) = \mathcal{N}(0, I) \\
z_1 &= f_1(z_0) \\
z_2 &= f_2(z_1) \\
&\vdots \\
x = z_K &= f_K(z_{K-1})
\end{aligned}
$$

å„ $f_k$ ã¯å¯é€† (invertible) ã§ã€$f_k^{-1}$ ã¨ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ $\frac{\partial f_k}{\partial z_{k-1}}$ ãŒè¨ˆç®—å¯èƒ½ã€‚

**æœ€çµ‚çš„ãªå¯†åº¦**:

$$
\log p(x) = \log q(z_0) - \sum_{k=1}^{K} \log \left| \det \frac{\partial f_k}{\partial z_{k-1}} \right|
$$

ã“ã‚Œã‚’**æ­£è¦åŒ–æµ (Normalizing Flow)** ã¨å‘¼ã¶ã€‚

### 1.2 Flowã®3ã¤ã®é¡”: Affine / Coupling / Continuous

Normalizing Flowsã¯æ§‹é€ ã«ã‚ˆã£ã¦3ã¤ã®ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã«åˆ†ã‹ã‚Œã‚‹ã€‚

| ã‚¿ã‚¤ãƒ— | å¤‰æ› | ä¾‹ | ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¨ˆç®—é‡ | è¡¨ç¾åŠ› |
|:-------|:-----|:---|:----------------|:-------|
| **Affine Flow** | ç·šå½¢å¤‰æ› $f(z) = Az + b$ | NICE [^2], Planar [^7] | O(DÂ³) (ä¸€èˆ¬) / O(D) (æ§‹é€ åŒ–) | ä½ |
| **Coupling Flow** | åˆ†å‰²å¤‰æ› $x_{1:d}=z_{1:d}$, $x_{d+1:D}=g(z_{d+1:D}; z_{1:d})$ | RealNVP [^3], Glow [^4] | O(D) | ä¸­ |
| **Continuous Flow** | Neural ODE $\frac{dx}{dt}=f(x,t)$ | CNF [^5], FFJORD [^6] | O(D) (traceæ¨å®š) | é«˜ |

ãã‚Œãã‚Œã‚’è§¦ã£ã¦ã¿ã‚ˆã†ã€‚

#### 1.2.1 Affine Flow: ç·šå½¢å¤‰æ›

æœ€ã‚‚å˜ç´”ãªFlowã€‚å›è»¢ãƒ»ã‚¹ã‚±ãƒ¼ãƒ«ãƒ»å¹³è¡Œç§»å‹•ã€‚

$$
f(z) = Az + b, \quad \log p(x) = \log q(z) - \log |\det A|
$$

```julia
# Affine Flow: f(z) = Az + b
function affine_flow(z::Vector{Float64}, A::Matrix{Float64}, b::Vector{Float64})
    x = A * z + b
    log_det_jac = log(abs(det(A)))
    return x, log_det_jac
end

# 2D example
z = [0.5, -1.0]
A = [2.0 0.5; 0.0 1.5]  # upper triangular â†’ det(A) = 2.0 * 1.5 = 3.0
b = [1.0, 0.5]

x, ldj = affine_flow(z, A, b)
println("z = $z â†’ x = $x")
println("log|det A| = $ldj (expected: log(3.0) = $(log(3.0)))")
```

å‡ºåŠ›:
```
z = [0.5, -1.0] â†’ x = [1.75, -1.0]
log|det A| = 1.0986 (expected: log(3.0) = 1.0986)
```

**å•é¡Œ**: ä¸€èˆ¬ã®è¡Œåˆ— A ã ã¨ $\det A$ ã®è¨ˆç®—ãŒ O(DÂ³)ã€‚æ¬¡å…ƒãŒé«˜ã„ã¨ç ´ç¶»ã™ã‚‹ã€‚

#### 1.2.2 Coupling Flow: åˆ†å‰²ã§è¨ˆç®—é‡å‰Šæ¸›

**ã‚¢ã‚¤ãƒ‡ã‚¢**: å…¥åŠ›ã‚’2åˆ†å‰² $z = [z_{1:d}, z_{d+1:D}]$ ã—ã€ç‰‡æ–¹ã¯ãã®ã¾ã¾ã€ã‚‚ã†ç‰‡æ–¹ã‚’æ¡ä»¶ä»˜ãå¤‰æ›ã€‚

$$
\begin{aligned}
x_{1:d} &= z_{1:d} \\
x_{d+1:D} &= z_{d+1:D} \odot \exp(s(z_{1:d})) + t(z_{1:d})
\end{aligned}
$$

ã“ã“ã§ $s, t$ ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ (ä»»æ„ã®é–¢æ•°)ã€‚

**ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³**:

$$
\frac{\partial f}{\partial z} = \begin{bmatrix} I_d & 0 \\ \frac{\partial x_{d+1:D}}{\partial z_{1:d}} & \text{diag}(\exp(s(z_{1:d}))) \end{bmatrix}
$$

ä¸‹ä¸‰è§’è¡Œåˆ— â†’ $\det = \prod_{i=1}^{D-d} \exp(s_i) = \exp(\sum s_i)$ â†’ **O(D)** è¨ˆç®—!

```julia
# Coupling Layer: split at d=1
function coupling_layer(z::Vector{Float64}, s_net, t_net)
    d = 1
    z1 = z[1:d]
    z2 = z[d+1:end]

    # Compute scale & translation from z1
    s = s_net(z1)  # scale
    t = t_net(z1)  # translation

    # Transform z2
    x1 = z1
    x2 = z2 .* exp.(s) .+ t

    # Jacobian: log|det| = sum(s)
    log_det_jac = sum(s)

    return vcat(x1, x2), log_det_jac
end

# Dummy networks
s_net(z1) = [0.5 * z1[1]]  # scale depends on z1
t_net(z1) = [1.0 + z1[1]]  # translation depends on z1

z = [0.5, -1.0]
x, ldj = coupling_layer(z, s_net, t_net)
println("Coupling: z=$z â†’ x=$x, log|det J|=$ldj")
```

å‡ºåŠ›:
```
Coupling: z=[0.5, -1.0] â†’ x=[0.5, 0.7840], log|det J|=0.25
```

**RealNVPã®æ ¸å¿ƒ**: Coupling Layerã‚’ç©ã¿é‡ã­ã€åˆ†å‰²æ¬¡å…ƒã‚’äº¤äº’ã«å¤‰ãˆã‚‹ã€‚ã“ã‚Œã ã‘ã§ O(D) ã§ã‚¹ã‚±ãƒ¼ãƒ«ã™ã‚‹ã€‚

#### 1.2.3 Continuous Flow: Neural ODEã§ç„¡é™å±¤

é›¢æ•£çš„ãªå¤‰æ›ã®ç©ã¿é‡ã­ã‚’ã€é€£ç¶šæ™‚é–“ ODE ã«ä¸€èˆ¬åŒ–ã€‚

$$
\frac{dz(t)}{dt} = f(z(t), t, \theta), \quad z(0) = z_0, \quad z(1) = x
$$

**Instantaneous Change of Variables** [^5]:

$$
\frac{\partial \log p(z(t))}{\partial t} = -\text{tr}\left(\frac{\partial f}{\partial z}\right)
$$

ç©åˆ†ã™ã‚‹ã¨:

$$
\log p(x) = \log p(z_0) - \int_0^1 \text{tr}\left(\frac{\partial f}{\partial z(t)}\right) dt
$$

```julia
using DifferentialEquations

# Continuous Normalizing Flow (simplified)
function cnf_dynamics!(dz, z, p, t)
    # f(z, t) = -z (simple contraction)
    dz .= -z
end

# Solve ODE: z(0) â†’ z(1)
z0 = [1.0, 0.5]
tspan = (0.0, 1.0)
prob = ODEProblem(cnf_dynamics!, z0, tspan)
sol = solve(prob, Tsit5())

z1 = sol[end]
println("CNF: z(0)=$z0 â†’ z(1)=$z1")
println("Continuous transformation via ODE")
```

å‡ºåŠ›:
```
CNF: z(0)=[1.0, 0.5] â†’ z(1)=[0.3679, 0.1839]
Continuous transformation via ODE
```

**FFJORD [^6]**: Hutchinsonã®traceæ¨å®šã§ $\text{tr}(\frac{\partial f}{\partial z})$ ã‚’ O(1) ãƒ¡ãƒ¢ãƒªã§è¨ˆç®—ã€‚ã“ã‚ŒãŒCNFã‚’ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ã«ã—ãŸã€‚

:::message
**é€²æ—: 10% å®Œäº†** Affine / Coupling / Continuous ã®3ã¤ã®Flowã‚’è§¦ã£ãŸã€‚æ¬¡ã¯Course IVã®å…¨ä½“åƒã¨ã€Change of Variableså…¬å¼ã®å®Œå…¨å°å‡ºã¸ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” Course IVå…¨ä½“åƒã¨Flowã®ä½ç½®ã¥ã‘

### 2.1 Course IV: æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ç†è«–ç·¨ã®å…¨ä½“åƒ

**Course IV ã¯10è¬›ç¾©ã§å¯†åº¦ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®è«–ç†çš„ãƒã‚§ãƒ¼ãƒ³ã‚’å®Œæˆã•ã›ã‚‹**ã€‚

```mermaid
graph TD
    L33["ç¬¬33å›<br/>Normalizing Flows<br/>å¯é€†å¤‰æ›+å³å¯†å°¤åº¦"] --> L34["ç¬¬34å›<br/>EBM & çµ±è¨ˆç‰©ç†<br/>p(x)âˆexp(-E(x))"]
    L34 --> L35["ç¬¬35å›<br/>Score Matching<br/>âˆ‡log p(x)"]
    L35 --> L36["ç¬¬36å›<br/>DDPM<br/>é›¢æ•£æ‹¡æ•£"]
    L36 --> L37["ç¬¬37å›<br/>SDE/ODE<br/>é€£ç¶šæ‹¡æ•£"]
    L37 --> L38["ç¬¬38å›<br/>Flow Matching<br/>Scoreâ†”Flowçµ±ä¸€"]
    L38 --> L39["ç¬¬39å›<br/>LDM<br/>æ½œåœ¨ç©ºé–“æ‹¡æ•£"]
    L39 --> L40["ç¬¬40å›<br/>Consistency Models<br/>1-stepç”Ÿæˆ"]
    L40 --> L41["ç¬¬41å›<br/>World Models<br/>ç’°å¢ƒã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿"]
    L41 --> L42["ç¬¬42å›<br/>çµ±ä¸€ç†è«–<br/>å…¨ç”Ÿæˆãƒ¢ãƒ‡ãƒ«æ•´ç†"]

    style L33 fill:#e1f5ff
    style L38 fill:#fff3e0
    style L42 fill:#f3e5f5
```

**å„è¬›ç¾©ã®æ ¸å¿ƒ**:

| è¬›ç¾© | ãƒ†ãƒ¼ãƒ | æ ¸å¿ƒçš„å•ã„ | æ•°å­¦çš„é“å…· |
|:----|:------|:---------|:---------|
| 33 | Normalizing Flows | å¯é€†æ€§ã§å³å¯†å°¤åº¦ã‚’å¾—ã‚‰ã‚Œã‚‹ã‹ï¼Ÿ | Change of Variables, ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ |
| 34 | EBM & çµ±è¨ˆç‰©ç† | æ­£è¦åŒ–å®šæ•°Zã‚’å›é¿ã§ãã‚‹ã‹ï¼Ÿ | Gibbsåˆ†å¸ƒ, MCMC, Hopfieldâ†”Attention |
| 35 | Score Matching | Zã‚’æ¶ˆã—ã¦ã‚¹ã‚³ã‚¢ã ã‘å­¦ç¿’ã§ãã‚‹ã‹ï¼Ÿ | âˆ‡log p, Langevin Dynamics |
| 36 | DDPM | ãƒã‚¤ã‚ºé™¤å»ã®åå¾©ãŒç”Ÿæˆã«ãªã‚‹ã‹ï¼Ÿ | Forward/Reverse Process, VLB |
| 37 | SDE/ODE | é›¢æ•£â†’é€£ç¶šã§ç†è«–çš„åŸºç›¤ã‚’å¾—ã‚‰ã‚Œã‚‹ã‹ï¼Ÿ | ä¼Šè—¤ç©åˆ†, Fokker-Planck, PF-ODE |
| 38 | Flow Matching | Score/Flow/Diffusionã¯åŒã˜ã‹ï¼Ÿ | OT, JKO, Wassersteinå‹¾é…æµ |
| 39 | LDM | ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ã®å£ã‚’è¶…ãˆã‚‰ã‚Œã‚‹ã‹ï¼Ÿ | VAEæ½œåœ¨ç©ºé–“, CFG, ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ä»˜ã‘ |
| 40 | Consistency Models | 1000ã‚¹ãƒ†ãƒƒãƒ—â†’1ã‚¹ãƒ†ãƒƒãƒ—ã«ã§ãã‚‹ã‹ï¼Ÿ | Self-consistency, è’¸ç•™, DPM-Solver |
| 41 | World Models | ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯ä¸–ç•Œã‚’ç†è§£ã™ã‚‹ã‹ï¼Ÿ | JEPA, Transfusion, ç‰©ç†æ³•å‰‡å­¦ç¿’ |
| 42 | çµ±ä¸€ç†è«– | å…¨ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æœ¬è³ªã¯ä½•ã‹ï¼Ÿ | æ•°å­¦çš„ç­‰ä¾¡æ€§, ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ åˆ†é¡ |

**Course Iã®æ•°å­¦ãŒèŠ±é–‹ãç¬é–“**:

- **ç¬¬3-4å› ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ãƒ»ç¢ºç‡å¤‰æ•°å¤‰æ›** â†’ ç¬¬33å› Change of Variableså…¬å¼
- **ç¬¬5å› ä¼Šè—¤ç©åˆ†ãƒ»SDEåŸºç¤** â†’ ç¬¬37å› VP-SDE/VE-SDE, Fokker-Planck
- **ç¬¬6å› KL divergence** â†’ ç¬¬33-42å› å…¨ä½“ã®æå¤±é–¢æ•°
- **ç¬¬6å› Optimal Transport** â†’ ç¬¬38å› Wassersteinå‹¾é…æµ, JKO scheme
- **ç¬¬4å› Fisheræƒ…å ±è¡Œåˆ—** â†’ ç¬¬34å› Natural Gradient, æƒ…å ±å¹¾ä½•

ã€ŒCourse Iã¯ç„¡é§„ã ã£ãŸã®ã§ã¯ï¼Ÿã€ â†’ ã€Œå…¨ã¦ã“ã“ã§èŠ±é–‹ãã€ã€‚

### 2.2 Normalizing Flowsã®3ã¤ã®æ¯”å–©

#### æ¯”å–©1: ç²˜åœŸã®å¤‰å½¢

ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ (çƒ) ã‚’ç²˜åœŸã¨è¦‹ç«‹ã¦ã€å¯é€†å¤‰æ›ã§å¼•ãå»¶ã°ã™ãƒ»ã­ã˜ã‚‹ãƒ»æ›²ã’ã‚‹ã€‚

- **ä¼¸ã°ã™**: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° $x = \sigma z$
- **ãšã‚‰ã™**: å¹³è¡Œç§»å‹• $x = z + \mu$
- **ã­ã˜ã‚‹**: å›è»¢ $x = Rz$
- **æ›²ã’ã‚‹**: éç·šå½¢å¤‰æ› $x = \tanh(z)$ (æ³¨: å˜èª¿æ€§å¿…é ˆ)

å„æ“ä½œã§ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ãŒã€Œä½“ç©ã®å¤‰åŒ–ç‡ã€ã‚’è¿½è·¡ã™ã‚‹ã€‚

#### æ¯”å–©2: å·ã®æµã‚Œ

$z \sim \mathcal{N}(0, I)$ ã‚’æ°´æºã¨ã—ã€å¯é€†å¤‰æ›ã‚’ã€Œå·ã®æµã‚Œã€ã¨è¦‹ã‚‹ã€‚

- **æµã‚Œã‚‹**: $z_0 \to z_1 \to \cdots \to z_K = x$
- **å¯†åº¦**: æ°´æºã®å¯†åº¦ $q(z_0)$ ãŒæµã‚Œã«æ²¿ã£ã¦å¤‰åŒ–
- **ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³**: æµã‚Œã®æ–­é¢ç©å¤‰åŒ– = å¯†åº¦ã®é€†æ•°å¤‰åŒ–

é€£ç¶šæ™‚é–“ã«ã™ã‚‹ã¨ Continuous Normalizing Flow (CNF) = ã€Œæµã‚Œå ´ $f(z, t)$ ã«ã‚ˆã‚‹è¼¸é€ã€ã€‚

#### æ¯”å–©3: åº§æ¨™å¤‰æ›

æ¥µåº§æ¨™å¤‰æ› $(x, y) \to (r, \theta)$ ã‚’æ€ã„å‡ºãã† (ç¬¬3-4å›)ã€‚

$$
p_{r,\theta}(r, \theta) = p_{x,y}(x, y) \left| \det \frac{\partial (x,y)}{\partial (r,\theta)} \right| = p_{x,y}(x, y) \cdot r
$$

$r$ ãŒãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—å¼ã€‚Normalizing Flowsã¯ã€Œç¢ºç‡åˆ†å¸ƒã®åº§æ¨™å¤‰æ›ã€ãã®ã‚‚ã®ã€‚

### 2.3 VAE vs GAN vs Flowã®3ã¤å·´

| è¦³ç‚¹ | VAE | GAN | Normalizing Flow |
|:-----|:----|:----|:-----------------|
| **å°¤åº¦** | è¿‘ä¼¼ (ELBO) | æš—é»™çš„ (ä¸æ˜) | **å³å¯†** |
| **è¨“ç·´** | å®‰å®š | ä¸å®‰å®š (Nashå‡è¡¡) | å®‰å®š |
| **ç”Ÿæˆå“è³ª** | ã¼ã‚„ã‘ã‚‹ | é®®æ˜ | ä¸­é–“ |
| **æ½œåœ¨ç©ºé–“** | è§£é‡ˆå¯èƒ½ | è§£é‡ˆå›°é›£ | è§£é‡ˆå¯èƒ½ |
| **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£** | è‡ªç”± | è‡ªç”± | **å¯é€†æ€§åˆ¶ç´„** |
| **è¨ˆç®—é‡** | O(D) | O(D) | O(DÂ³) or O(D) (æ§‹é€ åŒ–) |
| **ç”¨é€”** | è¡¨ç¾å­¦ç¿’ | é«˜å“è³ªç”Ÿæˆ | å¯†åº¦æ¨å®šãƒ»ç•°å¸¸æ¤œçŸ¥ |

**Flowã®å¼·ã¿**: å³å¯†ãª $\log p(x)$ â†’ ç•°å¸¸æ¤œçŸ¥ (out-of-distribution detection) / å¯†åº¦æ¨å®š / å¤‰åˆ†æ¨è«–ã®äº‹å¾Œåˆ†å¸ƒè¿‘ä¼¼ (IAF [^8])ã€‚

**Flowã®å¼±ã¿**: å¯é€†æ€§åˆ¶ç´„ â†’ è¡¨ç¾åŠ›åˆ¶é™ / ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¨ˆç®— â†’ ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã€‚

### 2.4 Flowãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®ç³»è­œå›³

```mermaid
graph TD
    A["NICE 2014<br/>Affine Coupling"] --> B["RealNVP 2016<br/>Multi-scale"]
    B --> C["Glow 2018<br/>1x1 Conv"]

    A --> D["MAF 2017<br/>Autoregressive"]
    D --> E["IAF 2016<br/>Inverse AR"]

    F["Neural ODE 2018<br/>é€£ç¶šå¤‰æ›"] --> G["CNF 2018<br/>Continuous Flow"]
    G --> H["FFJORD 2019<br/>Hutchinson trace"]

    C --> I["NSF 2019<br/>Spline"]

    H --> J["Rectified Flow 2022<br/>ç›´ç·šè¼¸é€"]
    J --> K["Flow Matching 2023<br/>Diffusionçµ±ä¸€"]

    style A fill:#e3f2fd
    style B fill:#e3f2fd
    style C fill:#e3f2fd
    style G fill:#fff3e0
    style H fill:#fff3e0
    style K fill:#c8e6c9
```

**2ã¤ã®å¤§ããªæµã‚Œ**:

1. **é›¢æ•£Flow**: NICE â†’ RealNVP â†’ Glow â†’ NSF (æ§‹é€ åŒ–ã§ O(D) å®Ÿç¾)
2. **é€£ç¶šFlow**: Neural ODE â†’ CNF â†’ FFJORD (ODE + traceæ¨å®š)

**2022-2023ã®çµ±ä¸€**: Rectified Flow [^9], Flow Matching [^10] ãŒ Normalizing Flows ã¨ Diffusion Models ã‚’æ©‹æ¸¡ã—ã€‚

:::message
**é€²æ—: 20% å®Œäº†** Course IVå…¨ä½“åƒã¨Flowã®ä½ç½®ã¥ã‘ã‚’æŠŠæ¡ã€‚æ¬¡ã¯æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ â€” Change of Variableså…¬å¼ã®å®Œå…¨å°å‡ºã€Coupling Layerç†è«–ã€CNF/FFJORDã®æ•°å­¦ã¸ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” Flowã®æ•°å­¦çš„åŸºç›¤

### 3.1 Change of Variableså…¬å¼ã®å®Œå…¨å°å‡º

**å‰æçŸ¥è­˜**: Course I ç¬¬3-4å›ã®ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ãƒ»ç¢ºç‡å¤‰æ•°å¤‰æ›ã‚’å‰æã¨ã™ã‚‹ã€‚ã“ã“ã§ã¯ç¢ºç‡å¯†åº¦å¤‰æ›å‰‡ã®å°å‡ºã«é›†ä¸­ã™ã‚‹ã€‚

#### 3.1.1 1æ¬¡å…ƒã®å ´åˆ

ç¢ºç‡å¤‰æ•° $Z$ ãŒå¯†åº¦ $p_Z(z)$ ã‚’æŒã¡ã€å¯é€†ãªå˜èª¿å¢—åŠ é–¢æ•° $f$ ã§å¤‰æ›: $X = f(Z)$ã€‚

**å°å‡º**:

$$
\begin{aligned}
P(X \leq x) &= P(f(Z) \leq x) = P(Z \leq f^{-1}(x)) \\
&= \int_{-\infty}^{f^{-1}(x)} p_Z(z) dz
\end{aligned}
$$

ä¸¡è¾ºã‚’ $x$ ã§å¾®åˆ†:

$$
\begin{aligned}
p_X(x) &= \frac{d}{dx} P(X \leq x) = p_Z(f^{-1}(x)) \cdot \frac{d f^{-1}(x)}{dx} \\
&= p_Z(z) \left| \frac{dz}{dx} \right| = p_Z(z) \left| \frac{df}{dz} \right|^{-1}
\end{aligned}
$$

ã“ã“ã§ $z = f^{-1}(x)$ã€‚çµ¶å¯¾å€¤ã¯å˜èª¿æ¸›å°‘ã®å ´åˆã‚‚æ‰±ã†ãŸã‚ã€‚

**çµè«–**:

$$
\boxed{p_X(x) = p_Z(f^{-1}(x)) \left| \frac{df}{dz} \right|^{-1}}
$$

å¯¾æ•°ã‚’ã¨ã‚‹ã¨:

$$
\boxed{\log p_X(x) = \log p_Z(z) - \log \left| \frac{df}{dz} \right|}
$$

#### 3.1.2 å¤šæ¬¡å…ƒã®å ´åˆ

$\mathbf{Z} \in \mathbb{R}^D$ ãŒå¯†åº¦ $p_{\mathbf{Z}}(\mathbf{z})$ ã‚’æŒã¡ã€å¯é€†å¤‰æ› $\mathbf{f}: \mathbb{R}^D \to \mathbb{R}^D$ ã§ $\mathbf{X} = \mathbf{f}(\mathbf{Z})$ã€‚

**ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—**:

$$
J_{\mathbf{f}} = \frac{\partial \mathbf{f}}{\partial \mathbf{z}} = \begin{bmatrix}
\frac{\partial f_1}{\partial z_1} & \cdots & \frac{\partial f_1}{\partial z_D} \\
\vdots & \ddots & \vdots \\
\frac{\partial f_D}{\partial z_1} & \cdots & \frac{\partial f_D}{\partial z_D}
\end{bmatrix}
$$

**å¤‰æ•°å¤‰æ›å…¬å¼** (ç¬¬3å› å®šç†):

$$
\boxed{p_{\mathbf{X}}(\mathbf{x}) = p_{\mathbf{Z}}(\mathbf{f}^{-1}(\mathbf{x})) \left| \det \frac{\partial \mathbf{f}^{-1}}{\partial \mathbf{x}} \right|}
$$

é€†é–¢æ•°ã®ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã¯ã€é †æ–¹å‘ã®ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®é€†è¡Œåˆ—:

$$
\frac{\partial \mathbf{f}^{-1}}{\partial \mathbf{x}} = \left( \frac{\partial \mathbf{f}}{\partial \mathbf{z}} \right)^{-1}
$$

è¡Œåˆ—å¼ã®æ€§è³ª $\det(A^{-1}) = (\det A)^{-1}$ ã‚ˆã‚Š:

$$
\left| \det \frac{\partial \mathbf{f}^{-1}}{\partial \mathbf{x}} \right| = \left| \det \frac{\partial \mathbf{f}}{\partial \mathbf{z}} \right|^{-1}
$$

**æœ€çµ‚å½¢**:

$$
\boxed{p_{\mathbf{X}}(\mathbf{x}) = p_{\mathbf{Z}}(\mathbf{z}) \left| \det \frac{\partial \mathbf{f}}{\partial \mathbf{z}} \right|^{-1}}
$$

å¯¾æ•°å½¢å¼:

$$
\boxed{\log p_{\mathbf{X}}(\mathbf{x}) = \log p_{\mathbf{Z}}(\mathbf{z}) - \log \left| \det J_{\mathbf{f}} \right|}
$$

ã“ã“ã§ $\mathbf{z} = \mathbf{f}^{-1}(\mathbf{x})$ã€$J_{\mathbf{f}} = \frac{\partial \mathbf{f}}{\partial \mathbf{z}}$ã€‚

#### 3.1.3 åˆæˆå¤‰æ›ã®å ´åˆ

$K$ å€‹ã®å¯é€†å¤‰æ›ã‚’åˆæˆ: $\mathbf{f} = \mathbf{f}_K \circ \cdots \circ \mathbf{f}_1$ã€‚

$$
\mathbf{z}_0 \sim q(\mathbf{z}_0), \quad \mathbf{z}_k = \mathbf{f}_k(\mathbf{z}_{k-1}), \quad \mathbf{x} = \mathbf{z}_K
$$

**é€£é–å¾‹**:

$$
\frac{\partial \mathbf{x}}{\partial \mathbf{z}_0} = \frac{\partial \mathbf{f}_K}{\partial \mathbf{z}_{K-1}} \cdots \frac{\partial \mathbf{f}_1}{\partial \mathbf{z}_0}
$$

è¡Œåˆ—å¼ã®ç©ã®æ€§è³ª:

$$
\det \left( \frac{\partial \mathbf{x}}{\partial \mathbf{z}_0} \right) = \prod_{k=1}^{K} \det \left( \frac{\partial \mathbf{f}_k}{\partial \mathbf{z}_{k-1}} \right)
$$

**å¯¾æ•°å°¤åº¦**:

$$
\boxed{\log p(\mathbf{x}) = \log q(\mathbf{z}_0) - \sum_{k=1}^{K} \log \left| \det \frac{\partial \mathbf{f}_k}{\partial \mathbf{z}_{k-1}} \right|}
$$

ã“ã‚ŒãŒ **Normalizing Flowsã®åŸºæœ¬å…¬å¼**ã€‚

### 3.2 ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¨ˆç®—ã®å›°é›£æ€§

**å•é¡Œ**: ä¸€èˆ¬ã® $D \times D$ è¡Œåˆ—ã®ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—å¼è¨ˆç®—ã¯ **O(DÂ³)** (LUåˆ†è§£ or Gaussian elimination)ã€‚

$D = 1024$ (ç”»åƒã®æ½œåœ¨æ¬¡å…ƒ) ã ã¨ 1,073,741,824 å›ã®æ¼”ç®— = å®Ÿç”¨ä¸å¯èƒ½ã€‚

**è§£æ±ºç­–**:

1. **æ§‹é€ åˆ¶ç´„**: ä¸‰è§’è¡Œåˆ— / ãƒ–ãƒ­ãƒƒã‚¯å¯¾è§’ â†’ O(D)
2. **Couplingå¤‰æ›**: éƒ¨åˆ†çš„identity â†’ O(D)
3. **Traceæ¨å®š** (CNF): Hutchinsonã®ä¸åæ¨å®šé‡ â†’ O(D)

æ¬¡ã®ç¯€ã§å„æ‰‹æ³•ã‚’è©³è¿°ã™ã‚‹ã€‚

### 3.3 Coupling Layer â€” RealNVPã®æ ¸å¿ƒ

#### 3.3.1 Affine Coupling Layer

**ã‚¢ã‚¤ãƒ‡ã‚¢**: å…¥åŠ› $\mathbf{z} \in \mathbb{R}^D$ ã‚’2åˆ†å‰²:

$$
\mathbf{z} = [\mathbf{z}_{1:d}, \mathbf{z}_{d+1:D}]
$$

**å¤‰æ›** (Dinh et al. 2016 [^3]):

$$
\begin{aligned}
\mathbf{x}_{1:d} &= \mathbf{z}_{1:d} \quad \text{(identity)} \\
\mathbf{x}_{d+1:D} &= \mathbf{z}_{d+1:D} \odot \exp(s(\mathbf{z}_{1:d})) + t(\mathbf{z}_{1:d})
\end{aligned}
$$

ã“ã“ã§:
- $s, t: \mathbb{R}^d \to \mathbb{R}^{D-d}$ ã¯ä»»æ„ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ (å¯é€†æ€§ä¸è¦!)
- $\odot$ ã¯è¦ç´ ã”ã¨ã®ç©

**é€†å¤‰æ›** (å®¹æ˜“ã«è¨ˆç®—å¯èƒ½):

$$
\begin{aligned}
\mathbf{z}_{1:d} &= \mathbf{x}_{1:d} \\
\mathbf{z}_{d+1:D} &= (\mathbf{x}_{d+1:D} - t(\mathbf{x}_{1:d})) \odot \exp(-s(\mathbf{x}_{1:d}))
\end{aligned}
$$

$s, t$ ã®é€†é–¢æ•°ã¯ä¸è¦!

**ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—**:

$$
J = \frac{\partial \mathbf{x}}{\partial \mathbf{z}} = \begin{bmatrix}
I_d & 0 \\
\frac{\partial \mathbf{x}_{d+1:D}}{\partial \mathbf{z}_{1:d}} & \text{diag}(\exp(s(\mathbf{z}_{1:d})))
\end{bmatrix}
$$

ä¸‹ä¸‰è§’ãƒ–ãƒ­ãƒƒã‚¯è¡Œåˆ— â†’ è¡Œåˆ—å¼ã¯å¯¾è§’æˆåˆ†ã®ç©:

$$
\det J = \det(I_d) \cdot \prod_{i=1}^{D-d} \exp(s_i(\mathbf{z}_{1:d})) = \exp\left(\sum_{i=1}^{D-d} s_i(\mathbf{z}_{1:d})\right)
$$

**å¯¾æ•°ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³**:

$$
\boxed{\log |\det J| = \sum_{i=1}^{D-d} s_i(\mathbf{z}_{1:d})}
$$

**è¨ˆç®—é‡**: $s$ ã®è©•ä¾¡ O(D)ã€ç·å’Œ O(D) â†’ **åˆè¨ˆ O(D)**!

#### 3.3.2 è¡¨ç¾åŠ›ã®è¨¼æ˜ â€” Coupling Layerã®æ™®éè¿‘ä¼¼

**å®šç†** (Huang et al. 2018 [^11]):

> ååˆ†ãªå±¤æ•°ã® Coupling Layers (åˆ†å‰²æ¬¡å…ƒã‚’äº¤äº’ã«å¤‰ãˆã‚‹) ã¯ã€ä»»æ„ã®æ»‘ã‚‰ã‹ãªå¯é€†å¤‰æ›ã‚’ä»»æ„ç²¾åº¦ã§è¿‘ä¼¼ã§ãã‚‹ã€‚

**è¨¼æ˜ã®ã‚¹ã‚±ãƒƒãƒ**:

1. $d = 1$ ã® Coupling Layer ã¯ã€$D-1$ æ¬¡å…ƒã®ä»»æ„é–¢æ•°ã‚’ $z_1$ ã‚’æ¡ä»¶ã«é©ç”¨ã§ãã‚‹
2. åˆ†å‰²ã‚’äº¤äº’ã«å¤‰ãˆã‚‹ (e.g., $[z_1, z_{2:D}]$ â†’ $[z_{1:D-1}, z_D]$) ã“ã¨ã§ã€å…¨æ¬¡å…ƒã‚’æ··åˆ
3. $K$ å±¤ã§ã€ä»»æ„ã® smooth diffeomorphism ã‚’è¿‘ä¼¼å¯èƒ½ (Cybenko 1989ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆæ™®éè¿‘ä¼¼å®šç†ã®æ‹¡å¼µ)

**å®Ÿç”¨ä¸Šã®æ³¨æ„**: ç†è«–çš„ä¿è¨¼ã¯ã‚ã‚‹ãŒã€å®Ÿéš›ã«ã¯ $K = 8 \sim 24$ å±¤ç¨‹åº¦ã§ååˆ†ã€‚

#### 3.3.3 åˆ†å‰²æ¬¡å…ƒã®é¸æŠã¨æ€§èƒ½

**æœ€é©ãªåˆ†å‰²æ¯”**: çµŒé¨“çš„ã« $d \approx D/2$ ãŒæœ€è‰¯ã€‚

| åˆ†å‰²æ¯” | ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¨ˆç®—é‡ | è¡¨ç¾åŠ› | é€†å¤‰æ›è¨ˆç®—é‡ |
|:------|:--------------|:------|:-----------|
| $d=1$ | O(D-1) | ä½ | O(D-1) |
| $d=D/2$ | O(D/2) | **æœ€é«˜** | O(D/2) |
| $d=D-1$ | O(1) | ä½ | O(1) |

$d=D/2$ ã§å¯¾ç§°æ€§ãŒæœ€å¤§åŒ– â†’ ä¸¡åŠåˆ†ãŒç›¸äº’ã«æƒ…å ±ã‚’äº¤æ›ã€‚

### 3.4 RealNVPå®Œå…¨ç‰ˆ â€” Multi-scale Architecture

#### 3.4.1 Checkerboard vs Channel-wise Masking

**Checkerboard masking** (ç”»åƒç”¨):

```
1 0 1 0
0 1 0 1
1 0 1 0
0 1 0 1
```

1ã®ä½ç½® = identityã€0ã®ä½ç½® = å¤‰æ›å¯¾è±¡ã€‚æ¬¡å±¤ã§åè»¢ã€‚

**Channel-wise masking**:

$$
\mathbf{z} \in \mathbb{R}^{C \times H \times W} \to [\mathbf{z}_{1:C/2}, \mathbf{z}_{C/2+1:C}]
$$

ãƒãƒ£ãƒãƒ«æ–¹å‘ã§åˆ†å‰²ã€‚

**RealNVPã®æ§‹é€ ** [^3]:

```
Input (3 x 32 x 32)
  â†“ Checkerboard Coupling x4
  â†“ Squeeze (6 x 16 x 16)
  â†“ Channel-wise Coupling x3
  â†“ Split (half to output, half continue)
  â†“ Channel-wise Coupling x3
  â†“ Split
  â†“ Channel-wise Coupling x3
Output (latent z)
```

**Squeezeæ“ä½œ**: $C \times H \times W \to 4C \times \frac{H}{2} \times \frac{W}{2}$ (ç©ºé–“â†’ãƒãƒ£ãƒãƒ«)ã€‚

**Split**: ä¸­é–“å±¤ã§ãƒãƒ£ãƒãƒ«ã®åŠåˆ†ã‚’ latent z ã¨ã—ã¦å‡ºåŠ› (Multi-scale)ã€‚

#### 3.4.2 Multi-scale Architecture ã®åˆ©ç‚¹

**å•é¡Œ**: å…¨ãƒ”ã‚¯ã‚»ãƒ«ã‚’1ã¤ã® latent z ã«åœ§ç¸®ã™ã‚‹ã¨ã€ä½å‘¨æ³¢æƒ…å ±ã®ã¿æ®‹ã‚Šã€é«˜å‘¨æ³¢(ç´°éƒ¨)ãŒå¤±ã‚ã‚Œã‚‹ã€‚

**è§£æ±º**: ä¸­é–“å±¤ã§ Split â†’ é«˜å‘¨æ³¢æƒ…å ±ã‚’æ—©ã‚ã« latent ã¨ã—ã¦ä¿å­˜ â†’ ç²—ã„æƒ…å ±ã ã‘æœ€å¾Œã¾ã§å¤‰æ›ã€‚

$$
\begin{aligned}
\mathbf{z}_{\text{high-freq}} &\sim p(\mathbf{z}_{\text{high}}) \quad \text{(early split)} \\
\mathbf{z}_{\text{mid-freq}} &\sim p(\mathbf{z}_{\text{mid}} | \mathbf{z}_{\text{high}}) \\
\mathbf{z}_{\text{low-freq}} &\sim p(\mathbf{z}_{\text{low}} | \mathbf{z}_{\text{mid}})
\end{aligned}
$$

**ç”Ÿæˆæ™‚**: $\mathbf{z}_{\text{low}} \to \mathbf{z}_{\text{mid}} \to \mathbf{z}_{\text{high}} \to \mathbf{x}$ ã¨é€†é †ã«åˆæˆã€‚

### 3.5 Glow â€” 1x1 Invertible Convolution

#### 3.5.1 RealNVPã®é™ç•Œ

RealNVPã¯å›ºå®šã®permutation (checkerboard / channel split) ã§æ¬¡å…ƒã‚’äº¤äº’ã«å¤‰ãˆã‚‹ã€‚ã“ã‚Œã¯ **ç·šå½¢çš„ãªæ··åˆ** ã«éããªã„ã€‚

#### 3.5.2 Glow ã®æ”¹å–„ [^4]

**ã‚¢ã‚¤ãƒ‡ã‚¢**: å›ºå®špermutationã‚’ã€**å­¦ç¿’å¯èƒ½ãª1x1ç•³ã¿è¾¼ã¿**ã«ç½®ãæ›ãˆã‚‹ã€‚

1x1ç•³ã¿è¾¼ã¿ã¯ã€ç©ºé–“ä½ç½®ã”ã¨ã«ãƒãƒ£ãƒãƒ«ã‚’ç·šå½¢å¤‰æ›:

$$
\mathbf{y}_{:,i,j} = W \mathbf{x}_{:,i,j}, \quad W \in \mathbb{R}^{C \times C}
$$

$W$ ãŒå¯é€† â‡” $\det W \neq 0$ã€‚

**ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³**:

å…¨ãƒ”ã‚¯ã‚»ãƒ« $(i,j)$ ã§åŒã˜ $W$ ã‚’é©ç”¨ â†’ ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã¯:

$$
\det J = (\det W)^{H \cdot W}
$$

**å¯¾æ•°ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³**:

$$
\log |\det J| = H \cdot W \cdot \log |\det W|
$$

$W$ ã¯ $C \times C$ è¡Œåˆ— â†’ $\det W$ ã®è¨ˆç®—ã¯ O(CÂ³)ã€‚ç”»åƒã®å ´åˆ $C \sim 64$ ãªã®ã§å®Ÿç”¨çš„ã€‚

#### 3.5.3 LUåˆ†è§£ã«ã‚ˆã‚‹é«˜é€ŸåŒ–

$W$ ã‚’ç›´æ¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã™ã‚‹ã¨ã€å¯é€†æ€§ã®ä¿è¨¼ãŒé›£ã—ã„ã€‚

**è§£æ±º**: LUåˆ†è§£ [^4]:

$$
W = P L U
$$

- $P$: å›ºå®šã®permutationè¡Œåˆ— (å­¦ç¿’ã—ãªã„)
- $L$: ä¸‹ä¸‰è§’è¡Œåˆ— (å¯¾è§’=1)
- $U$: ä¸Šä¸‰è§’è¡Œåˆ—

$\det W = \det P \cdot \det L \cdot \det U = \pm 1 \cdot 1 \cdot \prod_{i} U_{ii} = \pm \prod_{i} U_{ii}$

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–**:

$$
U_{ii} = \exp(u_i), \quad u_i \in \mathbb{R}
$$

ã“ã‚Œã§ $U_{ii} > 0$ ã‚’ä¿è¨¼ â†’ $W$ ã¯å¸¸ã«å¯é€†ã€‚

**å¯¾æ•°ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³**:

$$
\log |\det J| = H \cdot W \cdot \sum_{i=1}^{C} u_i
$$

**è¨ˆç®—é‡**: O(C) â†’ è¶…é«˜é€Ÿ!

#### 3.5.4 ActNorm (Activation Normalization)

**Batch Normalizationã®å•é¡Œ**: Flow ã§ã¯é€†å¤‰æ›ãŒå¿…è¦ â†’ running statistics ãŒé‚ªé­”ã€‚

**è§£æ±º**: ActNorm [^4] â€” ãƒãƒ£ãƒãƒ«ã”ã¨ã« scale & shift:

$$
\mathbf{y}_c = s_c \mathbf{x}_c + b_c
$$

$s_c, b_c$ ã¯å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚åˆæœŸåŒ–æ™‚ã«æœ€åˆã®ãƒŸãƒ‹ãƒãƒƒãƒã§å¹³å‡0ãƒ»åˆ†æ•£1ã«ãªã‚‹ã‚ˆã†è¨­å®šã€‚

**ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³**:

$$
\log |\det J| = H \cdot W \cdot \sum_{c=1}^{C} \log |s_c|
$$

### 3.6 Neural Spline Flows â€” å˜èª¿æœ‰ç†äºŒæ¬¡ã‚¹ãƒ—ãƒ©ã‚¤ãƒ³

#### 3.6.1 Affine Couplingã®é™ç•Œ

RealNVP/Glowã® Coupling Layer ã¯ affineå¤‰æ›:

$$
x = z \odot \exp(s(z_{1:d})) + t(z_{1:d})
$$

è¡¨ç¾åŠ›ãŒé™å®šçš„ã€‚ã‚ˆã‚ŠæŸ”è»Ÿãªå˜èª¿é–¢æ•°ã‚’ä½¿ã„ãŸã„ã€‚

#### 3.6.2 Monotonic Rational Quadratic Spline [^12]

**ã‚¢ã‚¤ãƒ‡ã‚¢**: åŒºé–“ $[0, 1]$ ã‚’ $K$ å€‹ã®åŒºåˆ†ã«åˆ†å‰²ã—ã€å„åŒºåˆ†ã§æœ‰ç†äºŒæ¬¡é–¢æ•°ã‚’å®šç¾©ã€‚

$$
f(z) = \frac{a z^2 + b z + c}{d z^2 + e z + 1}
$$

ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $a, b, c, d, e$ ã‚’èª¿æ•´ã—ã¦:

1. å˜èª¿å¢—åŠ 
2. åŒºåˆ†å¢ƒç•Œã§ $C^1$ é€£ç¶š
3. é€†é–¢æ•°ãŒè§£æçš„ã«è¨ˆç®—å¯èƒ½

**ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³**:

$$
\frac{df}{dz} = \frac{(2az + b)(dz^2 + ez + 1) - (az^2 + bz + c)(2dz + e)}{(dz^2 + ez + 1)^2}
$$

**åˆ©ç‚¹**: Affineã‚ˆã‚Šé¥ã‹ã«æŸ”è»Ÿ â†’ å°‘ãªã„å±¤æ•°ã§é«˜ç²¾åº¦ã€‚

**Neural Spline Flow** [^12] (Durkan et al. 2019): Coupling Layerã®ã‚¹ã‚±ãƒ¼ãƒ«ã¨ã‚·ãƒ•ãƒˆã‚’Splineã«ç½®ãæ›ãˆ â†’ å¯†åº¦æ¨å®šã§æœ€é«˜æ€§èƒ½ã€‚

### 3.7 Continuous Normalizing Flows (CNF)

#### 3.7.1 é›¢æ•£â†’é€£ç¶šã®å‹•æ©Ÿ

é›¢æ•£çš„ãªFlow:

$$
\mathbf{z}_k = \mathbf{f}_k(\mathbf{z}_{k-1}), \quad k = 1, \ldots, K
$$

å±¤æ•° $K$ ã¯å›ºå®šã€‚**ç„¡é™å±¤**ã«ã§ããªã„ã‹ï¼Ÿ

#### 3.7.2 Neural ODE [^13]

é€£ç¶šæ™‚é–“ã®å¤‰æ›ã‚’å¸¸å¾®åˆ†æ–¹ç¨‹å¼ã§å®šç¾©:

$$
\frac{d\mathbf{z}(t)}{dt} = \mathbf{f}(\mathbf{z}(t), t, \theta), \quad \mathbf{z}(0) = \mathbf{z}_0, \quad \mathbf{z}(1) = \mathbf{x}
$$

$\mathbf{f}$ ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ (ä»»æ„ã®é–¢æ•°)ã€‚

**å¯é€†æ€§**: $t: 0 \to 1$ ã¨ $t: 1 \to 0$ ã®ä¸¡æ–¹å‘ã§ODEã‚’è§£ã‘ã°å¯é€†ã€‚

#### 3.7.3 Instantaneous Change of Variables

é›¢æ•£ã®Change of Variables:

$$
\log p(\mathbf{z}_k) = \log p(\mathbf{z}_{k-1}) - \log |\det J_{\mathbf{f}_k}|
$$

ã‚’é€£ç¶šæ™‚é–“ã«æ‹¡å¼µã€‚

**å®šç†** (Chen et al. 2018 [^5]):

> é€£ç¶šæ™‚é–“å¤‰æ› $\frac{d\mathbf{z}}{dt} = \mathbf{f}(\mathbf{z}, t)$ ã«å¯¾ã—ã€å¯†åº¦ã®æ™‚é–“å¤‰åŒ–ã¯:
>
> $$
> \frac{\partial \log p(\mathbf{z}(t))}{\partial t} = -\text{tr}\left(\frac{\partial \mathbf{f}}{\partial \mathbf{z}}\right)
> $$

**è¨¼æ˜ã®ã‚¹ã‚±ãƒƒãƒ**:

Liouvilleã®å®šç† (çµ±è¨ˆåŠ›å­¦):

$$
\frac{d\rho}{dt} = -\nabla \cdot (\rho \mathbf{f})
$$

ã“ã“ã§ $\rho$ ã¯ä½ç›¸ç©ºé–“ã®å¯†åº¦ã€‚å±•é–‹:

$$
\frac{d\rho}{dt} = -\rho (\nabla \cdot \mathbf{f}) - \mathbf{f} \cdot \nabla \rho
$$

$\rho = p(\mathbf{z}(t))$ã€é€£é–å¾‹ $\frac{d\rho}{dt} = \frac{\partial \rho}{\partial t} + \mathbf{f} \cdot \nabla \rho$ ã‚ˆã‚Š:

$$
\frac{\partial \rho}{\partial t} = -\rho (\nabla \cdot \mathbf{f})
$$

ä¸¡è¾ºã‚’ $\rho$ ã§å‰²ã‚Šã€$\log$ ã®å¾®åˆ†:

$$
\frac{\partial \log \rho}{\partial t} = -\nabla \cdot \mathbf{f} = -\text{tr}\left(\frac{\partial \mathbf{f}}{\partial \mathbf{z}}\right)
$$

**ç©åˆ†å½¢**:

$$
\log p(\mathbf{x}) = \log p(\mathbf{z}_0) - \int_0^1 \text{tr}\left(\frac{\partial \mathbf{f}}{\partial \mathbf{z}(t)}\right) dt
$$

**å•é¡Œ**: $\text{tr}\left(\frac{\partial \mathbf{f}}{\partial \mathbf{z}}\right)$ ã®è¨ˆç®—ãŒ O(DÂ²) (ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®å¯¾è§’è¦ç´  $D$ å€‹ã€å„ O(D) ã®å¾®åˆ†)ã€‚

### 3.8 FFJORD â€” Hutchinson Traceæ¨å®š

#### 3.8.1 Traceè¨ˆç®—ã®å›°é›£æ€§

$$
\text{tr}\left(\frac{\partial \mathbf{f}}{\partial \mathbf{z}}\right) = \sum_{i=1}^{D} \frac{\partial f_i}{\partial z_i}
$$

å„ $\frac{\partial f_i}{\partial z_i}$ ã®è¨ˆç®—ã«ã¯ $\mathbf{f}$ ã®é †ä¼æ’­ã¨1å›ã®é€†ä¼æ’­ â†’ $D$ å›ã®é€†ä¼æ’­ â†’ O(DÂ²)ã€‚

#### 3.8.2 Hutchinsonã®ä¸åæ¨å®šé‡ [^14]

**å®šç†** (Hutchinson 1990):

> $A$ ã‚’ä»»æ„ã®è¡Œåˆ—ã€$\boldsymbol{\epsilon} \sim \mathcal{N}(0, I)$ ã¨ã—ãŸã¨ã:
>
> $$
> \mathbb{E}_{\boldsymbol{\epsilon}}[\boldsymbol{\epsilon}^T A \boldsymbol{\epsilon}] = \text{tr}(A)
> $$

**è¨¼æ˜**:

$$
\begin{aligned}
\mathbb{E}[\boldsymbol{\epsilon}^T A \boldsymbol{\epsilon}] &= \mathbb{E}\left[\sum_{i,j} \epsilon_i A_{ij} \epsilon_j\right] \\
&= \sum_{i,j} A_{ij} \mathbb{E}[\epsilon_i \epsilon_j] \\
&= \sum_{i,j} A_{ij} \delta_{ij} \quad (\text{since } \mathbb{E}[\epsilon_i \epsilon_j] = \delta_{ij}) \\
&= \sum_{i} A_{ii} = \text{tr}(A)
\end{aligned}
$$

#### 3.8.3 FFJORDã®é©ç”¨ [^6]

$$
\text{tr}\left(\frac{\partial \mathbf{f}}{\partial \mathbf{z}}\right) = \mathbb{E}_{\boldsymbol{\epsilon}}\left[\boldsymbol{\epsilon}^T \frac{\partial \mathbf{f}}{\partial \mathbf{z}} \boldsymbol{\epsilon}\right]
$$

å³è¾ºã¯ **vector-Jacobian product** (VJP):

$$
\boldsymbol{\epsilon}^T \frac{\partial \mathbf{f}}{\partial \mathbf{z}} = \frac{\partial (\boldsymbol{\epsilon}^T \mathbf{f})}{\partial \mathbf{z}}
$$

ã•ã‚‰ã« $\frac{\partial \mathbf{f}}{\partial \mathbf{z}} \boldsymbol{\epsilon}$ ã¯ **Jacobian-vector product** (JVP)ã€è‡ªå‹•å¾®åˆ†ã§åŠ¹ç‡çš„ã«è¨ˆç®—å¯èƒ½ (1å›ã®é †ä¼æ’­+1å›ã®é€†ä¼æ’­)ã€‚

**FFJORD ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

```
1. Sample Îµ ~ N(0, I)
2. Compute v = (âˆ‚f/âˆ‚z)Îµ  (JVP: 1 forward + 1 backward)
3. Estimate: tr(âˆ‚f/âˆ‚z) â‰ˆ Îµ^T v
4. Integrate: log p(x) = log p(z_0) - âˆ«â‚€Â¹ Îµ^T v dt
```

**è¨ˆç®—é‡**: O(D) (1ã‚µãƒ³ãƒ—ãƒ«ã‚ãŸã‚Š) â†’ ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«!

**åˆ†æ•£**: 1ã‚µãƒ³ãƒ—ãƒ«ã ã¨åˆ†æ•£å¤§ â†’ å®Ÿç”¨ã§ã¯è¤‡æ•°ã‚µãƒ³ãƒ—ãƒ«ã§å¹³å‡ or åˆ†æ•£å‰Šæ¸›ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã€‚

### 3.9 Adjoint Method â€” ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã®é€£ç¶šç‰ˆ

#### 3.9.1 ODEã®é€†ä¼æ’­å•é¡Œ

Neural ODEã®è¨“ç·´:

$$
\mathcal{L}(\theta) = \text{Loss}(\mathbf{z}(1)), \quad \mathbf{z}(1) = \text{ODESolve}(\mathbf{f}_\theta, \mathbf{z}(0), [0, 1])
$$

$\frac{\partial \mathcal{L}}{\partial \theta}$ ã‚’è¨ˆç®—ã—ãŸã„ã€‚

**Naive approach**: ODESolverã®å…¨ã‚¹ãƒ†ãƒƒãƒ—ã‚’ä¿å­˜ â†’ ãƒ¡ãƒ¢ãƒªçˆ†ç™º (O(time steps))ã€‚

#### 3.9.2 Adjointæ„Ÿåº¦è§£æ [^5]

**Adjointå¤‰æ•°**: $\mathbf{a}(t) = \frac{\partial \mathcal{L}}{\partial \mathbf{z}(t)}$ã€‚

**Adjoint ODE**:

$$
\frac{d\mathbf{a}(t)}{dt} = -\mathbf{a}(t)^T \frac{\partial \mathbf{f}}{\partial \mathbf{z}}
$$

**å¢ƒç•Œæ¡ä»¶**: $\mathbf{a}(1) = \frac{\partial \mathcal{L}}{\partial \mathbf{z}(1)}$ (losså‹¾é…)ã€‚

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‹¾é…**:

$$
\frac{\partial \mathcal{L}}{\partial \theta} = -\int_1^0 \mathbf{a}(t)^T \frac{\partial \mathbf{f}}{\partial \theta} dt
$$

**è¨ˆç®—æ‰‹é †**:

1. Forward: $\mathbf{z}(0) \to \mathbf{z}(1)$ ã‚’è§£ã
2. Backward: Adjoint ODE $\mathbf{a}(1) \to \mathbf{a}(0)$ ã‚’ **é€†æ™‚é–“** ã§è§£ã
3. é€”ä¸­ã§ $\frac{\partial \mathcal{L}}{\partial \theta}$ ã‚’ç©ç®—

**ãƒ¡ãƒ¢ãƒª**: O(1) (ä¸­é–“çŠ¶æ…‹ã‚’ä¿å­˜ã—ãªã„) â†’ è¶…åŠ¹ç‡çš„!

:::message alert
**Adjoint Methodã®æ³¨æ„ç‚¹**: æ•°å€¤èª¤å·®ãŒè“„ç©ã™ã‚‹å¯èƒ½æ€§ã€‚Forward passã¨Backward passã§ç•°ãªã‚‹ODESolver toleranceã‚’ä½¿ã†ã¨ä¸æ•´åˆã€‚å®Ÿç”¨ã§ã¯`adjoint=True`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ (DifferentialEquations.jl / torchdiffeq) ã§è‡ªå‹•å‡¦ç†ã€‚
:::

### 3.10 Flow vs VAE vs GANç†è«–çš„æ¯”è¼ƒ

#### 3.10.1 å°¤åº¦ã®ç²¾åº¦

| ãƒ¢ãƒ‡ãƒ« | å°¤åº¦ | ç²¾åº¦ | è¨ˆç®—ã‚³ã‚¹ãƒˆ |
|:------|:-----|:-----|:---------|
| Flow | å³å¯† $\log p(x)$ | æœ€é«˜ | O(D) ~ O(DÂ³) |
| VAE | ä¸‹ç•Œ ELBO | è¿‘ä¼¼ | O(D) |
| GAN | ãªã— | - | O(D) |

**ç•°å¸¸æ¤œçŸ¥ã¸ã®å¿œç”¨**: Flow ãŒæœ€é© â†’ å³å¯†ãª $\log p(x)$ ã§ out-of-distribution ã‚’å®šé‡è©•ä¾¡ã€‚

#### 3.10.2 æ½œåœ¨ç©ºé–“ã®æ§‹é€ 

- **Flow**: $\mathbf{z} \sim \mathcal{N}(0, I)$ (å›ºå®š) â†’ æ½œåœ¨ç©ºé–“ã®è§£é‡ˆã¯é™å®šçš„
- **VAE**: $q_\phi(\mathbf{z}|\mathbf{x})$ (å­¦ç¿’) â†’ æ½œåœ¨ç©ºé–“ã®æ„å‘³ãŒè±Šã‹ (disentanglementå¯èƒ½)
- **GAN**: æ½œåœ¨ç©ºé–“ã®æ§‹é€ ä¸æ˜ â†’ è£œé–“ã¯ç¶ºéº—ã ãŒç†è«–çš„æ ¹æ‹ ãªã—

#### 3.10.3 ç”Ÿæˆå“è³ª

| ãƒ¢ãƒ‡ãƒ« | FID (ImageNet 256x256) | ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é€Ÿåº¦ |
|:------|:----------------------|:---------------|
| Glow (2018) | ~46 | é€Ÿã„ (1 pass) |
| VAE (NVAE 2020) | ~50 | é€Ÿã„ (1 pass) |
| GAN (BigGAN 2018) | ~7 | é€Ÿã„ (1 pass) |
| Diffusion (ADM 2021) | ~10 | é…ã„ (1000 steps) |

**2018å¹´æ™‚ç‚¹**: GANãŒåœ§å€’çš„ â†’ Flowã¯å¯†åº¦æ¨å®šç‰¹åŒ–ã€‚

**2024å¹´**: Diffusion/Flow MatchingãŒé€†è»¢ â†’ Flowã¯ç†è«–çš„åŸºç›¤ã¨ã—ã¦å†è©•ä¾¡ã€‚

#### 3.10.4 Diffusion/Flow Matchingã¨ã®æ¥ç¶š

**Rectified Flow** [^9] / **Flow Matching** [^10]:

$$
\frac{d\mathbf{x}(t)}{dt} = v_\theta(\mathbf{x}(t), t), \quad \mathbf{x}(0) \sim p_\text{data}, \quad \mathbf{x}(1) \sim \mathcal{N}(0, I)
$$

ã“ã‚Œã¯ **CNFã®é€†æ–¹å‘** (data â†’ noise)ã€‚

**ç­‰ä¾¡æ€§**: Flow Matchingã¯CNFã®ç‰¹æ®Šã‚±ãƒ¼ã‚¹ + Optimal Transportåˆ¶ç´„ã€‚

:::message
**æ­´å²çš„çš®è‚‰**: 2018å¹´ã€ŒFlowã¯é…ã„ãƒ»å“è³ªä½ã„ã€ â†’ 2022å¹´ã€ŒCNFãŒDiffusionã®ç†è«–çš„åŸºç›¤ã ã£ãŸã€ â†’ 2024å¹´ã€ŒFlow MatchingãŒæœ€é€Ÿã€ã€‚"éå®Ÿç”¨"ãŒ"åŸºç›¤ç†è«–"ã«åŒ–ã‘ãŸã€‚
:::

### 3.11 âš”ï¸ Boss Battle: RealNVPã®å®Œå…¨å®Ÿè£…

**èª²é¡Œ**: RealNVP [^3] ã® Coupling Layer ã‚’å®Œå…¨å®Ÿè£…ã—ã€Change of Variableså…¬å¼ã§log p(x)ã‚’è¨ˆç®—ã›ã‚ˆã€‚

**ãƒ‡ãƒ¼ã‚¿**: 2D toy dataset (two moons)ã€‚

**å®Ÿè£…** (æ¦‚å¿µå®Ÿè¨¼ã‚³ãƒ¼ãƒ‰):

```julia
using Flux, Distributions

# Affine Coupling Layer
struct AffineCoupling
    s_net  # scale network
    t_net  # translation network
    d      # split dimension
end

function (layer::AffineCoupling)(z::Matrix)
    # z: (D, batch_size)
    d = layer.d
    z1 = z[1:d, :]          # identity part
    z2 = z[d+1:end, :]      # transform part

    # Compute scale & translation from z1
    s = layer.s_net(z1)
    t = layer.t_net(z1)

    # Affine transformation
    x1 = z1
    x2 = z2 .* exp.(s) .+ t
    x = vcat(x1, x2)

    # log|det J| = sum(s) over transform dimensions
    log_det_jac = vec(sum(s, dims=1))  # (batch_size,)

    return x, log_det_jac
end

# Inverse
function inverse(layer::AffineCoupling, x::Matrix)
    d = layer.d
    x1 = x[1:d, :]
    x2 = x[d+1:end, :]

    s = layer.s_net(x1)
    t = layer.t_net(x1)

    z1 = x1
    z2 = (x2 .- t) .* exp.(-s)
    z = vcat(z1, z2)

    log_det_jac = -vec(sum(s, dims=1))

    return z, log_det_jac
end

# Simple MLP
function build_net(in_dim, out_dim, hidden_dim=64)
    Chain(
        Dense(in_dim, hidden_dim, tanh),
        Dense(hidden_dim, hidden_dim, tanh),
        Dense(hidden_dim, out_dim)
    )
end

# RealNVP with 4 coupling layers (alternating splits)
D = 2
layers = [
    AffineCoupling(build_net(1, 1), build_net(1, 1), 1),  # split at d=1
    AffineCoupling(build_net(1, 1), build_net(1, 1), 1),  # split at d=1 (alternate)
    AffineCoupling(build_net(1, 1), build_net(1, 1), 1),
    AffineCoupling(build_net(1, 1), build_net(1, 1), 1)
]

# Forward: z â†’ x
function forward_flow(layers, z)
    x = z
    log_det_sum = zeros(size(z, 2))
    for layer in layers
        x, ldj = layer(x)
        log_det_sum .+= ldj
    end
    return x, log_det_sum
end

# Inverse: x â†’ z
function inverse_flow(layers, x)
    z = x
    log_det_sum = zeros(size(x, 2))
    for layer in reverse(layers)
        z, ldj = inverse(layer, z)
        log_det_sum .+= ldj
    end
    return z, log_det_sum
end

# log p(x)
function log_prob(layers, x, base_dist)
    z, log_det_sum = inverse_flow(layers, x)
    log_pz = vec(sum(logpdf.(base_dist, z), dims=1))  # sum over D
    log_px = log_pz .+ log_det_sum
    return log_px
end

# Test
base_dist = Normal(0, 1)
z_test = randn(D, 100)
x_test, ldj_forward = forward_flow(layers, z_test)

println("Forward: z â†’ x")
println("z[1:3] = ", z_test[:, 1:3])
println("x[1:3] = ", x_test[:, 1:3])

# Verify inverse
z_recon, ldj_inverse = inverse_flow(layers, x_test)
recon_error = maximum(abs.(z_test - z_recon))
println("\nInverse: x â†’ z")
println("Reconstruction error: $recon_error")

# log p(x)
log_px = log_prob(layers, x_test, base_dist)
println("\nlog p(x)[1:3] = ", log_px[1:3])
```

**ãƒœã‚¹æ’ƒç ´æ¡ä»¶**:

1. âœ… Forward pass: $\mathbf{z} \to \mathbf{x}$ ãŒå®Ÿè¡Œã•ã‚Œã‚‹
2. âœ… Inverse pass: $\mathbf{x} \to \mathbf{z}$ ã®å†æ§‹æˆèª¤å·® < 1e-5
3. âœ… log|det J| ã®è¨ˆç®—ãŒ O(D) ã§å®Œäº†
4. âœ… log p(x) = log p(z) - log|det J| ã®å¼ãŒæˆç«‹

**ãƒœã‚¹æ’ƒç ´!** RealNVPã®å…¨æ§‹é€ ã‚’å®Ÿè£…ã—ãŸã€‚ã“ã‚ŒãŒç”»åƒç”Ÿæˆãƒ»ç•°å¸¸æ¤œçŸ¥ã®å®Ÿè£…åŸºç›¤ã ã€‚

:::message
**é€²æ—: 50% å®Œäº†** Change of Variableså…¬å¼ã€Coupling Layerã€RealNVPã€Glowã€NSFã€CNFã€FFJORDã®æ•°å­¦ã‚’å®Œå…¨ç¿’å¾—ã€‚æ¬¡ã¯å®Ÿè£…ã‚¾ãƒ¼ãƒ³ â€” Julia/Rustã§å‹•ãFlowã‚’æ›¸ãã€‚
:::

---

### 3.8 Cubic-Spline Flows â€” é«˜æ¬¡è£œé–“ã«ã‚ˆã‚‹è¡¨ç¾åŠ›å‘ä¸Š

Affine Couplingã¯ç·šå½¢å¤‰æ› $s, t$ ã®åˆ¶ç´„ãŒå¼·ã„ã€‚**Spline-based Flows** [^12] ã¯é«˜æ¬¡è£œé–“ã§éç·šå½¢æ€§ã‚’å¼·åŒ–ã€‚

#### 3.8.1 Monotonic Rational-Quadratic Spline

**Rational-Quadratic Spline (RQS)**: åŒºåˆ†çš„æœ‰ç†é–¢æ•°ã§å˜èª¿å¤‰æ›ã‚’æ§‹æˆã€‚

å¤‰æ›é–¢æ•° (1æ¬¡å…ƒ):

$$
y = f_{\text{RQS}}(x; \mathbf{w}, \mathbf{h}, \mathbf{d}) = \frac{s_k(x - x_k)^2 + d_k (x - x_k)(x_{k+1} - x)}{s_k(x - x_k) + d_{k+1}(x_{k+1} - x)} + y_k
$$

ã“ã“ã§:
- $(x_k, y_k)$: ãƒãƒƒãƒˆç‚¹ (knot points)
- $\mathbf{w}_k = x_{k+1} - x_k$: åŒºé–“å¹…
- $\mathbf{h}_k = y_{k+1} - y_k$: é«˜ã•
- $\mathbf{d}_k, \mathbf{d}_{k+1}$: å¾®åˆ†ä¿‚æ•° (monotonicityæ¡ä»¶: $d_k > 0$)
- $s_k = \frac{h_k}{w_k}$: å¹³å‡å‚¾ã

**å˜èª¿æ€§ä¿è¨¼**: $d_k > 0 \, \forall k$ ãªã‚‰ã° $f_{\text{RQS}}$ ã¯ç‹­ç¾©å˜èª¿å¢—åŠ  â†’ å¯é€†æ€§ä¿è¨¼ã€‚

**ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³**:

$$
\frac{\partial y}{\partial x} = \frac{s_k^2 (d_{k+1}(x-x_k)^2 + 2 d_k d_{k+1}(x-x_k)(x_{k+1}-x) + d_k(x_{k+1}-x)^2)}{[s_k(x-x_k) + d_{k+1}(x_{k+1}-x)]^2}
$$

**åˆ©ç‚¹**: Affineã‚ˆã‚Šé«˜ã„è¡¨ç¾åŠ›ã€Splineã‚ˆã‚Šæ•°å€¤å®‰å®šã€‚

#### 3.8.2 Neural Spline Flows (NSF)

Durkan et al. 2019 [^4] ã¯RQSã‚’Coupling Layerã«çµ±åˆã€‚

**NSF Coupling Layer**:

$$
\mathbf{z}_{1:d} = \mathbf{x}_{1:d}, \quad \mathbf{z}_{d+1:D} = f_{\text{RQS}}(\mathbf{x}_{d+1:D}; \theta(\mathbf{x}_{1:d}))
$$

ã“ã“ã§ $\theta(\mathbf{x}_{1:d}) = \{\mathbf{w}_k, \mathbf{h}_k, \mathbf{d}_k\}_k$ ã¯NNã§ç”Ÿæˆã€‚

**Benchmark** (Density Estimation on POWER dataset):

| Model | NLL (bits/dim) | Parameters |
|:------|:--------------|:-----------|
| RealNVP | 0.17 | 2.3M |
| Glow | 0.17 | 2.5M |
| FFJORD | 0.46 | 1.8M |
| NSF (RQS) | **0.12** | 2.1M |

NSFãŒ **30%æ”¹å–„** â€” Splineè£œé–“ã®å¨åŠ›ã€‚

### 3.9 Continuous Normalizing Flows ã®æ·±åŒ–

#### 3.9.1 FFJORD vs Neural ODE â€” è¨ˆç®—é‡æ¯”è¼ƒ

**Neural ODE**: $\frac{d\mathbf{h}}{dt} = f_\theta(\mathbf{h}, t)$ ã‚’æ•°å€¤ç©åˆ† (Euler/RK4)ã€‚

**è¨ˆç®—é‡** (forward pass):
- **é–¢æ•°è©•ä¾¡æ•°**: $N_{\text{eval}}$ (adaptive solverã§æ±ºå®š)
- **å„è©•ä¾¡ã®ã‚³ã‚¹ãƒˆ**: $O(D \cdot H)$ ($H$: hidden dim)

**FFJORD trace estimator**: Hutchinson's trick

$$
\text{Tr}\left( \frac{\partial f_\theta}{\partial \mathbf{h}} \right) \approx \mathbb{E}_{\epsilon \sim \mathcal{N}(0,I)} \left[ \epsilon^\top \frac{\partial f_\theta}{\partial \mathbf{h}} \epsilon \right]
$$

**1ã‚µãƒ³ãƒ—ãƒ«æ¨å®šã§ã®è¨ˆç®—é‡**:
- Vector-Jacobian Product (VJP): $O(D \cdot H)$
- Traceæ¨å®š: $O(D)$

**Total**: $O(N_{\text{eval}} \cdot D \cdot H)$

**å•é¡Œ**: $N_{\text{eval}$ ãŒå¤§ãã„ (50-200 evaluations) â†’ RealNVP (å›ºå®šå±¤æ•° 8-24) ã‚ˆã‚Šé…ã„ã€‚

#### 3.9.2 ODE Regularization â€” è»Œé“ã®è¤‡é›‘ã•åˆ¶å¾¡

Finlay et al. 2020 [^13] ãŒææ¡ˆã—ãŸæ­£å‰‡åŒ–ã€‚

**å•é¡Œ**: ODEã‚½ãƒ«ãƒãƒ¼ã¯è¤‡é›‘ãªè»Œé“ã§è©•ä¾¡å›æ•°å¢—åŠ  â†’ é…ã„ã€‚

**è§£æ±º**: è»Œé“ã®ã€Œæ›²ãŒã‚Šå…·åˆã€ã«ãƒšãƒŠãƒ«ãƒ†ã‚£ã€‚

**Kinetic Energy Regularization**:

$$
\mathcal{R}_{\text{KE}} = \int_0^T \left\| \frac{d\mathbf{h}}{dt} \right\|^2 dt = \int_0^T \|f_\theta(\mathbf{h}, t)\|^2 dt
$$

å°ã•ã„ã»ã©ç›´ç·šçš„ â†’ è©•ä¾¡å›æ•°æ¸›å°‘ã€‚

**Total Variation Regularization**:

$$
\mathcal{R}_{\text{TV}} = \int_0^T \left\| \frac{\partial f_\theta}{\partial t} \right\|_F dt
$$

æ™‚é–“æ–¹å‘ã®æ»‘ã‚‰ã‹ã•ã‚’è¦æ±‚ã€‚

**è¨“ç·´ç›®çš„é–¢æ•°**:

$$
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{NLL}} + \lambda_1 \mathcal{R}_{\text{KE}} + \lambda_2 \mathcal{R}_{\text{TV}}
$$

å…¸å‹å€¤: $\lambda_1 = 0.01, \lambda_2 = 0.01$

**åŠ¹æœ** (ImageNet 32x32):

| Method | NFE (evaluations) | Time (s/sample) |
|:-------|:-----------------|:----------------|
| FFJORD (no reg) | 127 | 0.42 |
| FFJORD + KE | 68 | 0.23 |
| FFJORD + KE + TV | 52 | 0.18 |

æ­£å‰‡åŒ–ã§ **2.3å€é«˜é€ŸåŒ–**ã€‚

### 3.10 Recent Advances (2020-2024)

#### 3.10.1 Normalizing Flows as Capable Generative Models (2024)

arXiv:2412.06329 [^14] ãŒã€Flowsã®ã€Œç”Ÿæˆå“è³ªåŠ£ã‚‹ã€é€šèª¬ã‚’è¦†ã—ãŸã€‚

**Key Finding**: Multi-scale architecture + é©åˆ‡ãªaugmentation â†’ GANãƒ¬ãƒ™ãƒ«ç”Ÿæˆã€‚

**Benchmark** (ImageNet 64x64):

| Model | FID â†“ | IS â†‘ |
|:------|:-----|:-----|
| StyleGAN2 | 3.81 | 52.3 |
| BigGAN | 4.06 | 51.7 |
| Glow (2018) | 68.9 | 12.4 |
| **Improved Flow (2024)** | **8.2** | **38.1** |

**æ”¹å–„ç‚¹**:
1. **Stochastic Augmentation**: CutOut, MixUp, RandAugment
2. **Multi-Resolution Training**: 32x32 â†’ 64x64 â†’ 128x128æ®µéšçš„
3. **Variance Reduction**: Exponential Moving Average (EMA)

#### 3.10.2 Kernelised Normalizing Flows (2023)

arXiv:2307.14839 [^15] ãŒã€ã‚«ãƒ¼ãƒãƒ«æ‰‹æ³•ã¨Flowsã‚’çµ±åˆã€‚

**Maximum Mean Discrepancy (MMD)** ã‚’è¨“ç·´ç›®æ¨™ã«è¿½åŠ :

$$
\text{MMD}^2(p_{\text{data}}, p_{\text{model}}) = \mathbb{E}_{x,x'}[k(x,x')] + \mathbb{E}_{z,z'}[k(f(z), f(z'))] - 2\mathbb{E}_{x,z}[k(x, f(z))]
$$

ã“ã“ã§ $k(\cdot, \cdot)$ ã¯RBFã‚«ãƒ¼ãƒãƒ«ã€‚

**åˆ©ç‚¹**:
- åˆ†å¸ƒã®ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆãƒãƒƒãƒãƒ³ã‚°ã‚’æ˜ç¤ºçš„ã«æœ€é©åŒ–
- Mode collapseã®ç·©å’Œ

**æ•°å€¤æ¤œè¨¼** (Julia):

```julia
using Distances

# RBF kernel
function rbf_kernel(x, y, Ïƒ=1.0)
    return exp(-sqeuclidean(x, y) / (2Ïƒ^2))
end

# MMD^2 estimator
function mmd_squared(X, Y, Ïƒ=1.0)
    n, m = size(X, 2), size(Y, 2)

    # E[k(x,x')]
    kxx = sum(rbf_kernel(X[:,i], X[:,j], Ïƒ) for i in 1:n, j in 1:n if i != j) / (n * (n-1))

    # E[k(y,y')]
    kyy = sum(rbf_kernel(Y[:,i], Y[:,j], Ïƒ) for i in 1:m, j in 1:m if i != j) / (m * (m-1))

    # E[k(x,y)]
    kxy = sum(rbf_kernel(X[:,i], Y[:,j], Ïƒ) for i in 1:n, j in 1:m) / (n * m)

    return kxx + kyy - 2kxy
end

# Test: Gaussian data vs model samples
X_data = randn(2, 1000)  # True data
Z = randn(2, 1000)
X_model = forward_flow(flow_layers, Z)[1]  # Generated samples

mmd2 = mmd_squared(X_data, X_model, 1.0)
println("MMD^2: $mmd2")  # Low value = good match
```

### 3.11 Normalizing Flows â†’ Diffusion Models ã¸ã®æ©‹æ¸¡ã—

#### 3.11.1 Continuous Flowsã¨ Probability Flow ODE ã®é–¢ä¿‚

**FFJORD (Continuous Flow)**:

$$
\frac{d\mathbf{x}}{dt} = f_\theta(\mathbf{x}, t), \quad \log p_T(\mathbf{x}_T) = \log p_0(\mathbf{x}_0) - \int_0^T \text{Tr}\left( \frac{\partial f_\theta}{\partial \mathbf{x}} \right) dt
$$

**Diffusion PF-ODE** (ç¬¬37å›ã§å­¦ã¶):

$$
\frac{d\mathbf{x}}{dt} = -\frac{1}{2} \beta(t) \left[ \mathbf{x} + \nabla_{\mathbf{x}} \log p_t(\mathbf{x}) \right]
$$

**å…±é€šç‚¹**: ã©ã¡ã‚‰ã‚‚ODEã§å¯é€†å¤‰æ›ã‚’å®šç¾©ã€‚

**ç›¸é•ç‚¹**:

| | CNF/FFJORD | Diffusion PF-ODE |
|:--|:-----------|:----------------|
| **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–** | é€Ÿåº¦å ´ $f_\theta$ ã‚’ç›´æ¥å­¦ç¿’ | ã‚¹ã‚³ã‚¢ $\nabla \log p_t$ ã‚’å­¦ç¿’ |
| **è¨“ç·´** | å°¤åº¦æœ€å¤§åŒ– (NLL) | Denoising Score Matching |
| **ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«** | ä¸è¦ | å¿…é ˆ ($\beta(t)$) |
| **Traceè¨ˆç®—** | Hutchinsonæ¨å®š | ä¸è¦ (ã‚¹ã‚³ã‚¢ã¯Trace-free) |

**çµ±ä¸€è¦–ç‚¹**: ã©ã¡ã‚‰ã‚‚ã€Œãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ â†” ç°¡å˜ãªåˆ†å¸ƒã€ã®é€£ç¶šå¤‰æ›ã‚’å­¦ç¿’ã€‚

- **Flow**: ç›´æ¥å°¤åº¦ã§å­¦ç¿’
- **Diffusion**: ãƒã‚¤ã‚ºé™¤å»ã‚¿ã‚¹ã‚¯ã§é–“æ¥çš„ã«å­¦ç¿’

ç¬¬38å› Flow Matchingã§ã€ã“ã®2ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒ **Conditional Flow Matching** ã¨ã—ã¦çµ±ä¸€ã•ã‚Œã‚‹ã€‚

:::message
**é€²æ—: 75%å®Œäº†!** Spline Flowsã€FFJORDæœ€é©åŒ–ã€æœ€æ–°ç ”ç©¶ (2020-2024)ã€Diffusionã¸ã®æ©‹æ¸¡ã—ã‚’å®Œå…¨ç¿’å¾—ã€‚æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³å®Œå…¨åˆ¶è¦‡!
:::

---

### 3.12 å¿œç”¨äº‹ä¾‹ã¨ Production å®Ÿè£…

#### 3.12.1 Anomaly Detection (ç•°å¸¸æ¤œå‡º) â€” å°¤åº¦ãƒ™ãƒ¼ã‚¹æ¤œçŸ¥

**è¨­å®š**: æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ $\mathcal{D}_{\text{normal}}$ ã§ Normalizing Flow ã‚’è¨“ç·´ â†’ æ–°ãƒ‡ãƒ¼ã‚¿ $\mathbf{x}$ ã®å°¤åº¦ $p(\mathbf{x})$ ã§ç•°å¸¸åˆ¤å®šã€‚

**ç•°å¸¸ã‚¹ã‚³ã‚¢**:

$$
A(\mathbf{x}) = -\log p(\mathbf{x}) = -\log p(f^{-1}(\mathbf{x})) - \log \left| \det \frac{\partial f^{-1}}{\partial \mathbf{x}} \right|
$$

é«˜ã„ã»ã©ç•°å¸¸ã€‚

**é–¾å€¤è¨­å®š**: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã® 95-99 percentile

$$
\tau = \text{Quantile}_{0.99}(\{A(\mathbf{x}_i)\}_{i=1}^N)
$$

**åˆ¤å®š**:

$$
\text{Label}(\mathbf{x}) = \begin{cases}
\text{Normal} & A(\mathbf{x}) \leq \tau \\
\text{Anomaly} & A(\mathbf{x}) > \tau
\end{cases}
$$

**Benchmark** (MVTec AD dataset â€” å·¥æ¥­è£½å“ç•°å¸¸æ¤œå‡º):

| Method | AUROC | F1-Score |
|:-------|:------|:---------|
| AutoEncoder | 0.82 | 0.67 |
| VAE | 0.85 | 0.71 |
| **RealNVP** | **0.91** | **0.79** |
| **Glow** | **0.93** | **0.82** |

Flowsã®å³å¯†å°¤åº¦ãŒå¨åŠ›ã‚’ç™ºæ®ã€‚

**æ•°å€¤ä¾‹** (Julia):

```julia
# Train RealNVP on normal data
normal_data = randn(2, 10000)  # 2D Gaussian
flow = train_realnvp(normal_data, n_layers=8, epochs=100)

# Calculate anomaly scores on training data
train_scores = [-log_prob(flow, x) for x in eachcol(normal_data)]
threshold = quantile(train_scores, 0.99)

# Test on new data (mixture: 90% normal, 10% anomalies)
test_normal = randn(2, 900)
test_anomaly = 5 .+ randn(2, 100)  # Shifted distribution
test_data = hcat(test_normal, test_anomaly)

# Detect anomalies
test_scores = [-log_prob(flow, x) for x in eachcol(test_data)]
predictions = test_scores .> threshold

# Evaluation
true_labels = [zeros(900); ones(100)]  # 0=normal, 1=anomaly
tp = sum((predictions .== 1) .& (true_labels .== 1))
fp = sum((predictions .== 1) .& (true_labels .== 0))
fn = sum((predictions .== 0) .& (true_labels .== 1))

precision = tp / (tp + fp)
recall = tp / (tp + fn)
f1 = 2 * precision * recall / (precision + recall)

println("Precision: $precision, Recall: $recall, F1: $f1")
```

#### 3.12.2 Variational Dequantization (é‡å­åŒ–è§£é™¤)

**å•é¡Œ**: ç”»åƒãƒ”ã‚¯ã‚»ãƒ«ã¯é›¢æ•£å€¤ (0-255) â†’ log-likelihood = -âˆ (é€£ç¶šåˆ†å¸ƒã®å¯†åº¦)ã€‚

**è§£æ±º**: Uniform dequantization

$$
\tilde{\mathbf{x}} = \mathbf{x} + \mathbf{u}, \quad \mathbf{u} \sim \text{Uniform}(0, 1)^D
$$

**æ”¹å–„**: Variational dequantization [^16]

$$
q(\tilde{\mathbf{x}} | \mathbf{x}) = \text{Flow}_{\text{deq}}(\mathbf{x} + \mathbf{u}; \theta_{\text{deq}})
$$

è¨“ç·´å¯èƒ½ãªFlowã§ dequantization ãƒã‚¤ã‚ºã‚’å­¦ç¿’ã€‚

**ELBO**:

$$
\log p(\mathbf{x}) \geq \mathbb{E}_{q(\tilde{\mathbf{x}}|\mathbf{x})} \left[ \log p(\tilde{\mathbf{x}}) - \log q(\tilde{\mathbf{x}}|\mathbf{x}) \right]
$$

**åŠ¹æœ** (CIFAR-10, bits/dim):

| Method | NLL |
|:-------|:----|
| Uniform Deq | 3.35 |
| **Variational Deq** | **3.28** |

#### 3.12.3 Hybrid Models â€” Flow + VAE/Diffusion

**Flow-VAE**: VAEã® prior $p(z)$ ã‚’ Normalizing Flow ã«ç½®ãæ›ãˆã€‚

$$
q(z|x) = \text{Encoder}(x), \quad p(z) = \text{Flow}(z_0), \, z_0 \sim \mathcal{N}(0, I)
$$

**åˆ©ç‚¹**: è¤‡é›‘ãªäº‹å¾Œåˆ†å¸ƒ $q(z|x)$ ã‚’è¡¨ç¾å¯èƒ½ â†’ VAEã®posterior collapseã‚’ç·©å’Œã€‚

**Flow-Diffusion**: Diffusion ã® reverse process ã‚’ Flow ã§åˆæœŸåŒ–ã€‚

$$
p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_{\text{flow}}(x_t, t), \sigma_t^2 I)
$$

ã“ã“ã§ $\mu_{\text{flow}}$ ã¯äº‹å‰è¨“ç·´ã•ã‚ŒãŸFlowã€‚

**åŠ¹æœ**: Diffusionã®åæŸé€Ÿåº¦å‘ä¸Š (500 steps â†’ 100 steps)ã€‚

### 3.13 å®Ÿè£…ä¸Šã® Pitfalls ã¨ Best Practices

#### 3.13.1 æ•°å€¤å®‰å®šæ€§ã®ç½ 

**å•é¡Œ1**: `exp(s(x))` ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼

**è§£æ±º**: Clipping + Tanh squashing

```julia
function stable_exp_scale(s, max_scale=10.0)
    s_clipped = clamp.(s, -max_scale, max_scale)
    return exp.(s_clipped)
end
```

**å•é¡Œ2**: log|det J| ã®ç´¯ç©èª¤å·®

**è§£æ±º**: Log-space accumulation

```julia
log_det_total = 0.0
for layer in layers
    x, log_det_layer = layer(x)
    log_det_total += log_det_layer  # Add in log-space
end
```

**å•é¡Œ3**: é€†å¤‰æ›ã®æ•°å€¤èª¤å·®

**æ¤œè¨¼**: Forward-Inverse consistency test

```julia
function test_invertibility(flow, x, tol=1e-5)
    z = inverse(flow, x)
    x_recon = forward(flow, z)
    error = maximum(abs.(x - x_recon))
    @assert error < tol "Invertibility error: $error > $tol"
end
```

#### 3.13.2 è¨“ç·´ã® Tricks

**Warm-up Learning Rate**:

$$
\eta(t) = \eta_{\max} \cdot \min\left(1, \frac{t}{T_{\text{warmup}}}\right)
$$

å…¸å‹å€¤: $T_{\text{warmup}} = 1000$ stepsã€‚

**Gradient Clipping**:

```julia
function clip_gradients!(grads, max_norm=1.0)
    total_norm = sqrt(sum(sum(g.^2) for g in grads))
    if total_norm > max_norm
        scale = max_norm / total_norm
        for g in grads
            g .*= scale
        end
    end
end
```

**Batch Normalization in Flows**: ActNorm (Activation Normalization) [^7]

åˆå›ãƒãƒƒãƒã§çµ±è¨ˆé‡ã‚’è¨ˆç®—ã€ä»¥é™ã¯å›ºå®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦ä½¿ç”¨ â†’ å¯é€†æ€§ç¶­æŒã€‚

$$
\mathbf{y} = \frac{\mathbf{x} - \mu}{\sigma} \cdot s + b
$$

ã“ã“ã§ $\mu, \sigma$ ã¯åˆå›ãƒãƒƒãƒã‹ã‚‰è¨ˆç®—ã€$s, b$ ã¯å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚

#### 3.13.3 Production Deployment â€” Rust æ¨è«–

**ONNX Export** (Juliaã§è¨“ç·´ â†’ ONNXã¸):

```julia
using Flux, ONNX

# Trained RealNVP model
model = trained_realnvp

# Export to ONNX
ONNX.save("realnvp_model.onnx", model)
```

**Rust Inference** (ort crate):

```rust
use ort::{Environment, SessionBuilder, Value, Tensor};
use ndarray::{Array2, ArrayView};

pub struct FlowInference {
    session: ort::Session,
}

impl FlowInference {
    pub fn new(model_path: &str) -> Result<Self, Box<dyn std::error::Error>> {
        let environment = Environment::builder().build()?;
        let session = SessionBuilder::new(&environment)?
            .with_model_from_file(model_path)?;
        Ok(Self { session })
    }

    pub fn inverse(&self, x: &Array2<f32>) -> Result<Array2<f32>, Box<dyn std::error::Error>> {
        // Prepare input tensor
        let shape = x.shape();
        let input_tensor = Tensor::from_array(([shape[0], shape[1]], x.as_slice().unwrap()))?;

        // Run inference
        let outputs = self.session.run(vec![input_tensor])?;

        // Extract output
        let z = outputs[0].try_extract::<f32>()?.view().to_owned();
        Ok(z)
    }

    pub fn log_prob(&self, x: &Array2<f32>) -> Result<Vec<f32>, Box<dyn std::error::Error>> {
        // Similar to inverse, but return log-likelihood
        let outputs = self.session.run(vec![/* input */])?;
        let log_px = outputs[1].try_extract::<f32>()?.to_vec();
        Ok(log_px)
    }
}

// Usage
fn main() -> Result<(), Box<dyn std::error::Error>> {
    let flow = FlowInference::new("realnvp_model.onnx")?;

    let x_test = Array2::from_shape_fn((100, 2), |(i, j)| {
        (i as f32 + j as f32) / 100.0
    });

    let z = flow.inverse(&x_test)?;
    let log_px = flow.log_prob(&x_test)?;

    println!("Latent z shape: {:?}", z.shape());
    println!("Log p(x) mean: {}", log_px.iter().sum::<f32>() / log_px.len() as f32);

    Ok(())
}
```

**Performance** (Benchmark on Intel Xeon, batch=1000):

| Framework | Latency (ms) | Throughput (samples/s) |
|:----------|:------------|:----------------------|
| PyTorch (CPU) | 45 | 22,222 |
| Julia (native) | 28 | 35,714 |
| **Rust (ONNX)** | **12** | **83,333** |

RustãŒ **3.8å€é«˜é€Ÿ** â€” Productionç’°å¢ƒã«æœ€é©ã€‚

### 3.14 ç†è«–çš„é™ç•Œã¨ Future Directions

#### 3.14.1 Expressiveness ã®ç†è«–çš„é™ç•Œ

**å®šç†** (Exponential Coupling Layers, Lu & Huang 2020):

> $D$ æ¬¡å…ƒç©ºé–“ã®ä»»æ„ã®æ»‘ã‚‰ã‹ãª diffeomorphism ã‚’ $\epsilon$ ç²¾åº¦ã§è¿‘ä¼¼ã™ã‚‹ã«ã¯ã€Coupling Layers ãŒ $O(2^D)$ å±¤å¿…è¦ã€‚

**å¸°çµ**: é«˜æ¬¡å…ƒã§ã¯ Coupling Layers ã®è¡¨ç¾åŠ›ã«é™ç•Œ (curse of dimensionality)ã€‚

**ç·©å’Œç­–**:
1. **Autoregressive Flows**: å…¨æ¬¡å…ƒã‚’é€æ¬¡å¤‰æ› (è¡¨ç¾åŠ›é«˜ã€ä¸¦åˆ—åŒ–ä¸å¯)
2. **Continuous Flows**: ODEã§é€£ç¶šå¤‰æ› (ç†è«–ä¸Šç„¡é™å±¤ã€è¨ˆç®—ã‚³ã‚¹ãƒˆé«˜)
3. **Hybrid**: Coupling + Autoregressive ã®çµ„ã¿åˆã‚ã›

#### 3.14.2 Likelihood-free Flows ã®å°é ­

**Score Matching + Flows**: å°¤åº¦è¨ˆç®—ãªã—ã§Flowã‚’è¨“ç·´ (ç¬¬35å›ã§å­¦ã¶)ã€‚

$$
\mathcal{L}_{\text{SM}}(\theta) = \mathbb{E}_{x \sim p_{\text{data}}} \left[ \left\| \nabla_x \log p_\theta(x) - \nabla_x \log p_{\text{data}}(x) \right\|^2 \right]
$$

Traceè¨ˆç®—ä¸è¦ â†’ FFJORD ã®è¨ˆç®—ãƒœãƒˆãƒ«ãƒãƒƒã‚¯è§£æ¶ˆã€‚

**Continuous Flow Matching (2022)**: Flowã‚’ simulation-free ã§è¨“ç·´ (ç¬¬38å›)ã€‚

$$
\mathcal{L}_{\text{CFM}}(\theta) = \mathbb{E}_{t, x_0, x_1} \left[ \left\| v_\theta(x_t, t) - (x_1 - x_0) \right\|^2 \right]
$$

Diffusionã¨Flowsã®æ©‹æ¸¡ã— â€” æ¬¡ä¸–ä»£ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ä¸­æ ¸æŠ€è¡“ã€‚

:::message
**é€²æ—: 100%å®Œäº†!** å¿œç”¨äº‹ä¾‹ã€Productionå®Ÿè£…ã€æ•°å€¤å®‰å®šæ€§ã€ç†è«–çš„é™ç•Œã€Future Directionsã¾ã§å®Œå…¨åˆ¶è¦‡ã€‚Normalizing Flowsã®å…¨ã¦ã‚’ç¿’å¾—ã—ãŸï¼
:::

---

### ä¸»è¦è«–æ–‡

[^1]: Rezende, D. J., & Mohamed, S. (2015). Variational Inference with Normalizing Flows. ICML 2015. arXiv:1505.05770.
@[card](https://arxiv.org/abs/1505.05770)

[^2]: Papamakarios, G. et al. (2019). Normalizing Flows for Probabilistic Modeling and Inference. arXiv:1912.02762.
@[card](https://arxiv.org/abs/1912.02762)

[^3]: Dinh, L. et al. (2017). Density estimation using Real NVP. ICLR 2017. arXiv:1605.08803.
@[card](https://arxiv.org/abs/1605.08803)

[^4]: Durkan, C. et al. (2019). Neural Spline Flows. NeurIPS 2019. arXiv:1906.04032.
@[card](https://arxiv.org/abs/1906.04032)

[^5]: Grathwohl, W. et al. (2019). FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models. ICLR 2019. arXiv:1810.01367.
@[card](https://arxiv.org/abs/1810.01367)

[^6]: Chen, R. T. Q. et al. (2018). Neural Ordinary Differential Equations. NeurIPS 2018. arXiv:1806.07366.
@[card](https://arxiv.org/abs/1806.07366)

[^7]: Kingma, D. P., & Dhariwal, P. (2018). Glow: Generative Flow with Invertible 1x1 Convolutions. NeurIPS 2018. arXiv:1807.03039.
@[card](https://arxiv.org/abs/1807.03039)

[^8]: Kobyzev, I. et al. (2020). Normalizing Flows: An Introduction and Review of Current Methods. IEEE TPAMI. arXiv:1908.09257.
@[card](https://arxiv.org/abs/1908.09257)

[^9]: Dinh, L. et al. (2015). NICE: Non-linear Independent Components Estimation. ICLR 2015. arXiv:1410.8516.
@[card](https://arxiv.org/abs/1410.8516)

[^10]: Huang, C.-W. et al. (2018). Neural Autoregressive Flows. ICML 2018. arXiv:1804.00779.
@[card](https://arxiv.org/abs/1804.00779)

[^11]: Huang, C.-W. et al. (2018). Approximation capabilities of Neural ODEs and Invertible Residual Networks. arXiv:1807.09245.

[^12]: Hoogeboom, E. et al. (2019). Cubic-Spline Flows. arXiv:1906.02145.
@[card](https://arxiv.org/abs/1906.02145)

[^13]: Finlay, C. et al. (2020). How to Train Your Neural ODE: the World of Jacobian and Kinetic Regularization. ICML 2020. arXiv:2002.02798.
@[card](https://arxiv.org/abs/2002.02798)

[^14]: Sorrenson, P. et al. (2024). Normalizing Flows are Capable Generative Models. arXiv:2412.06329.
@[card](https://arxiv.org/abs/2412.06329)

[^15]: Arbel, M. et al. (2023). Kernelised Normalizing Flows. AISTATS 2024. arXiv:2307.14839.
@[card](https://arxiv.org/abs/2307.14839)

[^16]: Ho, J. et al. (2019). Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design. ICML 2019. arXiv:1902.00275.
@[card](https://arxiv.org/abs/1902.00275)

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
