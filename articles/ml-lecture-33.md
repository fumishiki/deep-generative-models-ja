---
title: "ç¬¬33å›: Normalizing Flows: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸŒŠ"
type: "tech"
topics: ["machinelearning", "deeplearning", "normalizingflows", "julia", "rust"]
published: true
---

# ç¬¬33å›: Normalizing Flows â€” å¯é€†å¤‰æ›ã§å³å¯†å°¤åº¦ã‚’æ‰‹ã«å…¥ã‚Œã‚‹

> **VAEã¯è¿‘ä¼¼ã€GANã¯æš—é»™çš„ã€‚Normalizing Flowsã¯å¯é€†å¤‰æ›ã§å³å¯†ãªå°¤åº¦ log p(x) ã‚’è¨ˆç®—ã™ã‚‹ã€‚å¤‰æ•°å¤‰æ›ã®æ•°å­¦ãŒã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã«æ–°ã—ã„é“ã‚’é–‹ã„ãŸã€‚**

VAEã¯å¤‰åˆ†ä¸‹ç•ŒELBOã§çœŸã®å°¤åº¦ log p(x) ã‚’ä¸‹ã‹ã‚‰è¿‘ä¼¼ã™ã‚‹ã€‚GANã¯å°¤åº¦ã‚’æ¨ã¦ã€è­˜åˆ¥å™¨ã¨ã®æ•µå¯¾ã§æš—é»™çš„ã«åˆ†å¸ƒã‚’å­¦ã¶ã€‚ã©ã¡ã‚‰ã‚‚ã€Œå³å¯†ãªå°¤åº¦ã€ã‚’è«¦ã‚ãŸã€‚

Normalizing Flows [^1] [^2] ã¯å¯é€†å¤‰æ› f: z â†’ x ã§ã€**Change of Variableså…¬å¼ã‚’ä½¿ã„å³å¯†ãª log p(x) ã‚’è¨ˆç®—ã™ã‚‹**ã€‚ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—å¼ |det J_f| ãŒãã®éµã ã€‚

ã“ã®æ•°å­¦çš„ç¾ã—ã•ã¯ä»£å„Ÿã‚’ä¼´ã†ã€‚å¯é€†æ€§åˆ¶ç´„ãŒã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’åˆ¶é™ã™ã‚‹ã€‚è¨ˆç®—é‡ O(DÂ³) ã®ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—å¼ãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã«ãªã‚‹ã€‚RealNVP [^3]ã€Glow [^4] ã¯æ§‹é€ åŒ–ã•ã‚ŒãŸå¤‰æ›ã§ã“ã‚Œã‚’ O(D) ã«å‰Šæ¸›ã—ãŸã€‚ãã—ã¦Continuous Normalizing Flows (CNF) [^5] ã¨FFJORD [^6] ãŒã€Neural ODEã§é€£ç¶šæ™‚é–“ã®å¯é€†å¤‰æ›ã‚’å®Ÿç¾ã—ã€Diffusion Modelsã‚„Flow Matchingã¸ã®æ©‹ã‚’æ¶ã‘ãŸã€‚

æœ¬è¬›ç¾©ã¯Course IVã€Œæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ç†è«–ç·¨ã€ã®ç¬¬1å› â€” å…¨10è¬›ç¾©ã®æ—…ã®å‡ºç™ºç‚¹ã ã€‚Course I-IIIã§åŸ¹ã£ãŸæ•°å­¦åŠ›ã¨å®Ÿè£…åŠ›ã‚’æ­¦å™¨ã«ã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«–ã®æ·±æ·µã¸ã€‚

:::message
**Course IVæ¦‚è¦**: Normalizing Flows â†’ EBM â†’ Score Matching â†’ DDPM â†’ SDE â†’ Flow Matching â†’ LDM â†’ Consistency Models â†’ World Models â†’ çµ±ä¸€ç†è«–ã€‚å¯†åº¦ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®è«–ç†çš„ãƒã‚§ãƒ¼ãƒ³ã‚’è¾¿ã‚Šã€ã€Œæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«è«–æ–‡ã®ç†è«–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒå°å‡ºã§ãã‚‹ã€åˆ°é”ç‚¹ã¸ã€‚
:::

```mermaid
graph LR
    A["ğŸ“Š VAE<br/>ELBOè¿‘ä¼¼<br/>ã¼ã‚„ã‘"] --> D["ğŸ¯ å³å¯†å°¤åº¦<br/>ã®è¿½æ±‚"]
    B["ğŸ¨ GAN<br/>æš—é»™çš„å¯†åº¦<br/>ä¸å®‰å®š"] --> D
    D --> E["ğŸŒŠ Normalizing Flow<br/>å¯é€†å¤‰æ›f<br/>log p(x)è¨ˆç®—å¯èƒ½"]
    E --> F["ğŸ“ Change of Variables<br/>|det J_f|"]
    E --> G["ğŸ”„ RealNVP/Glow<br/>æ§‹é€ åŒ–"]
    E --> H["âˆ CNF/FFJORD<br/>Neural ODE"]
    H --> I["ğŸŒˆ Diffusion/FM<br/>ã¸ã®æ©‹"]
    style E fill:#e1f5ff
    style H fill:#fff3e0
    style I fill:#f3e5f5
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ + ç™ºå±• | 35åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | æŒ¯ã‚Šè¿”ã‚Š + çµ±åˆ | 30åˆ† | â˜…â˜…â˜…â˜†â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” å¯é€†å¤‰æ›ã§å¯†åº¦ã‚’è¿½è·¡ã™ã‚‹

**ã‚´ãƒ¼ãƒ«**: Change of Variableså…¬å¼ã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ã€‚

ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ z ~ N(0,1) ã‚’ä»®å®šå¤‰æ› f(z) = Î¼ + Ïƒz ã§å¤‰æ›ã—ã€å¤‰æ›å¾Œã®å¯†åº¦ p(x) ã‚’ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã§è¨ˆç®—ã™ã‚‹ã€‚

```julia
using Distributions, LinearAlgebra

# 1D Normalizing Flow: f(z) = Î¼ + Ïƒz
f(z, Î¼, Ïƒ) = Î¼ .+ Ïƒ .* z
f_inv(x, Î¼, Ïƒ) = (x .- Î¼) ./ Ïƒ
log_det_jacobian(Ïƒ) = sum(log.(abs.(Ïƒ)))  # |det J_f| = |Ïƒ|

# Base distribution: z ~ N(0, 1)
q_z = Normal(0, 1)

# Transform: x = f(z) with Î¼=2, Ïƒ=3
Î¼, Ïƒ = 2.0, 3.0
z_samples = rand(q_z, 1000)
x_samples = f(z_samples, Î¼, Ïƒ)

# Exact log p(x) via Change of Variables
# log p(x) = log q(z) - log|det J_f|
log_p_x(x) = logpdf(q_z, f_inv(x, Î¼, Ïƒ)) - log_det_jacobian(Ïƒ)

println("z ~ N(0,1) â†’ x = 2 + 3z")
println("log p(x=5) = ", round(log_p_x(5.0), digits=4))
println("Expected: log N(5; Î¼=2, ÏƒÂ²=9) = ", round(logpdf(Normal(Î¼, Ïƒ), 5.0), digits=4))
println("Change of Variableså…¬å¼ã§å³å¯†ãªlog p(x)ã‚’è¨ˆç®—ã—ãŸ!")
```

å‡ºåŠ›:
```
z ~ N(0,1) â†’ x = 2 + 3z
log p(x=5) = -2.3259
Expected: log N(5; Î¼=2, ÏƒÂ²=9) = -2.3259
Change of Variableså…¬å¼ã§å³å¯†ãªlog p(x)ã‚’è¨ˆç®—ã—ãŸ!
```

**3è¡Œã®ã‚³ãƒ¼ãƒ‰ã§å¯é€†å¤‰æ›ã¨å¯†åº¦è¿½è·¡ã‚’å‹•ã‹ã—ãŸã€‚** æ•°å¼ã§æ›¸ãã¨:

$$
\begin{aligned}
z &\sim q(z) = \mathcal{N}(0, 1) \\
x &= f(z) = \mu + \sigma z \quad \text{(invertible)} \\
\log p(x) &= \log q(f^{-1}(x)) - \log \left| \det \frac{\partial f}{\partial z} \right| \\
&= \log q\left(\frac{x - \mu}{\sigma}\right) - \log |\sigma|
\end{aligned}
$$

**Change of Variableså…¬å¼** (ç¬¬3-4å›ã®ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³å‰æ):

$$
p_X(x) = p_Z(f^{-1}(x)) \left| \det \frac{\partial f^{-1}}{\partial x} \right| = p_Z(z) \left| \det \frac{\partial f}{\partial z} \right|^{-1}
$$

ã“ã®å…¬å¼ãŒã€Normalizing Flowsã®å…¨ã¦ã®ç†è«–çš„åŸºç›¤ã ã€‚

:::message
**é€²æ—: 3% å®Œäº†** Change of Variableså…¬å¼ã‚’ä½“æ„Ÿã—ãŸã€‚ã“ã“ã‹ã‚‰ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¨ˆç®—ã®å›°é›£æ€§ã€Coupling Layerã€RealNVPã€Glowã€CNFã€FFJORDã¸é€²ã‚€ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” Flowã®3å½¢æ…‹ã‚’è§¦ã‚‹

### 1.1 Normalizing Flowã¨ã¯ä½•ã‹

**å®šç¾©**: å˜ç´”ãªåˆ†å¸ƒ q(z) (é€šå¸¸ N(0,I)) ã‹ã‚‰ã€å¯é€†å¤‰æ›ã®åˆæˆã§è¤‡é›‘ãªåˆ†å¸ƒ p(x) ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚

$$
\begin{aligned}
z_0 &\sim q(z) = \mathcal{N}(0, I) \\
z_1 &= f_1(z_0) \\
z_2 &= f_2(z_1) \\
&\vdots \\
x = z_K &= f_K(z_{K-1})
\end{aligned}
$$

å„ $f_k$ ã¯å¯é€† (invertible) ã§ã€$f_k^{-1}$ ã¨ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ $\frac{\partial f_k}{\partial z_{k-1}}$ ãŒè¨ˆç®—å¯èƒ½ã€‚

**æœ€çµ‚çš„ãªå¯†åº¦**:

$$
\log p(x) = \log q(z_0) - \sum_{k=1}^{K} \log \left| \det \frac{\partial f_k}{\partial z_{k-1}} \right|
$$

ã“ã‚Œã‚’**æ­£è¦åŒ–æµ (Normalizing Flow)** ã¨å‘¼ã¶ã€‚

### 1.2 Flowã®3ã¤ã®é¡”: Affine / Coupling / Continuous

Normalizing Flowsã¯æ§‹é€ ã«ã‚ˆã£ã¦3ã¤ã®ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã«åˆ†ã‹ã‚Œã‚‹ã€‚

| ã‚¿ã‚¤ãƒ— | å¤‰æ› | ä¾‹ | ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¨ˆç®—é‡ | è¡¨ç¾åŠ› |
|:-------|:-----|:---|:----------------|:-------|
| **Affine Flow** | ç·šå½¢å¤‰æ› $f(z) = Az + b$ | NICE [^2], Planar [^7] | O(DÂ³) (ä¸€èˆ¬) / O(D) (æ§‹é€ åŒ–) | ä½ |
| **Coupling Flow** | åˆ†å‰²å¤‰æ› $x_{1:d}=z_{1:d}$, $x_{d+1:D}=g(z_{d+1:D}; z_{1:d})$ | RealNVP [^3], Glow [^4] | O(D) | ä¸­ |
| **Continuous Flow** | Neural ODE $\frac{dx}{dt}=f(x,t)$ | CNF [^5], FFJORD [^6] | O(D) (traceæ¨å®š) | é«˜ |

ãã‚Œãã‚Œã‚’è§¦ã£ã¦ã¿ã‚ˆã†ã€‚

#### 1.2.1 Affine Flow: ç·šå½¢å¤‰æ›

æœ€ã‚‚å˜ç´”ãªFlowã€‚å›è»¢ãƒ»ã‚¹ã‚±ãƒ¼ãƒ«ãƒ»å¹³è¡Œç§»å‹•ã€‚

$$
f(z) = Az + b, \quad \log p(x) = \log q(z) - \log |\det A|
$$

```julia
# Affine Flow: f(z) = Az + b
function affine_flow(z::Vector{Float64}, A::Matrix{Float64}, b::Vector{Float64})
    x = A * z + b
    log_det_jac = log(abs(det(A)))
    return x, log_det_jac
end

# 2D example
z = [0.5, -1.0]
A = [2.0 0.5; 0.0 1.5]  # upper triangular â†’ det(A) = 2.0 * 1.5 = 3.0
b = [1.0, 0.5]

x, ldj = affine_flow(z, A, b)
println("z = $z â†’ x = $x")
println("log|det A| = $ldj (expected: log(3.0) = $(log(3.0)))")
```

å‡ºåŠ›:
```
z = [0.5, -1.0] â†’ x = [1.75, -1.0]
log|det A| = 1.0986 (expected: log(3.0) = 1.0986)
```

**å•é¡Œ**: ä¸€èˆ¬ã®è¡Œåˆ— A ã ã¨ $\det A$ ã®è¨ˆç®—ãŒ O(DÂ³)ã€‚æ¬¡å…ƒãŒé«˜ã„ã¨ç ´ç¶»ã™ã‚‹ã€‚

#### 1.2.2 Coupling Flow: åˆ†å‰²ã§è¨ˆç®—é‡å‰Šæ¸›

**ã‚¢ã‚¤ãƒ‡ã‚¢**: å…¥åŠ›ã‚’2åˆ†å‰² $z = [z_{1:d}, z_{d+1:D}]$ ã—ã€ç‰‡æ–¹ã¯ãã®ã¾ã¾ã€ã‚‚ã†ç‰‡æ–¹ã‚’æ¡ä»¶ä»˜ãå¤‰æ›ã€‚

$$
\begin{aligned}
x_{1:d} &= z_{1:d} \\
x_{d+1:D} &= z_{d+1:D} \odot \exp(s(z_{1:d})) + t(z_{1:d})
\end{aligned}
$$

ã“ã“ã§ $s, t$ ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ (ä»»æ„ã®é–¢æ•°)ã€‚

**ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³**:

$$
\frac{\partial f}{\partial z} = \begin{bmatrix} I_d & 0 \\ \frac{\partial x_{d+1:D}}{\partial z_{1:d}} & \text{diag}(\exp(s(z_{1:d}))) \end{bmatrix}
$$

ä¸‹ä¸‰è§’è¡Œåˆ— â†’ $\det = \prod_{i=1}^{D-d} \exp(s_i) = \exp(\sum s_i)$ â†’ **O(D)** è¨ˆç®—!

```julia
# Coupling Layer: split at d=1
function coupling_layer(z::Vector{Float64}, s_net, t_net)
    d = 1
    z1 = z[1:d]
    z2 = z[d+1:end]

    # Compute scale & translation from z1
    s = s_net(z1)  # scale
    t = t_net(z1)  # translation

    # Transform z2
    x1 = z1
    x2 = z2 .* exp.(s) .+ t

    # Jacobian: log|det| = sum(s)
    log_det_jac = sum(s)

    return vcat(x1, x2), log_det_jac
end

# Dummy networks
s_net(z1) = [0.5 * z1[1]]  # scale depends on z1
t_net(z1) = [1.0 + z1[1]]  # translation depends on z1

z = [0.5, -1.0]
x, ldj = coupling_layer(z, s_net, t_net)
println("Coupling: z=$z â†’ x=$x, log|det J|=$ldj")
```

å‡ºåŠ›:
```
Coupling: z=[0.5, -1.0] â†’ x=[0.5, 0.7840], log|det J|=0.25
```

**RealNVPã®æ ¸å¿ƒ**: Coupling Layerã‚’ç©ã¿é‡ã­ã€åˆ†å‰²æ¬¡å…ƒã‚’äº¤äº’ã«å¤‰ãˆã‚‹ã€‚ã“ã‚Œã ã‘ã§ O(D) ã§ã‚¹ã‚±ãƒ¼ãƒ«ã™ã‚‹ã€‚

#### 1.2.3 Continuous Flow: Neural ODEã§ç„¡é™å±¤

é›¢æ•£çš„ãªå¤‰æ›ã®ç©ã¿é‡ã­ã‚’ã€é€£ç¶šæ™‚é–“ ODE ã«ä¸€èˆ¬åŒ–ã€‚

$$
\frac{dz(t)}{dt} = f(z(t), t, \theta), \quad z(0) = z_0, \quad z(1) = x
$$

**Instantaneous Change of Variables** [^5]:

$$
\frac{\partial \log p(z(t))}{\partial t} = -\text{tr}\left(\frac{\partial f}{\partial z}\right)
$$

ç©åˆ†ã™ã‚‹ã¨:

$$
\log p(x) = \log p(z_0) - \int_0^1 \text{tr}\left(\frac{\partial f}{\partial z(t)}\right) dt
$$

```julia
using DifferentialEquations

# Continuous Normalizing Flow (simplified)
function cnf_dynamics!(dz, z, p, t)
    # f(z, t) = -z (simple contraction)
    dz .= -z
end

# Solve ODE: z(0) â†’ z(1)
z0 = [1.0, 0.5]
tspan = (0.0, 1.0)
prob = ODEProblem(cnf_dynamics!, z0, tspan)
sol = solve(prob, Tsit5())

z1 = sol[end]
println("CNF: z(0)=$z0 â†’ z(1)=$z1")
println("Continuous transformation via ODE")
```

å‡ºåŠ›:
```
CNF: z(0)=[1.0, 0.5] â†’ z(1)=[0.3679, 0.1839]
Continuous transformation via ODE
```

**FFJORD [^6]**: Hutchinsonã®traceæ¨å®šã§ $\text{tr}(\frac{\partial f}{\partial z})$ ã‚’ O(1) ãƒ¡ãƒ¢ãƒªã§è¨ˆç®—ã€‚ã“ã‚ŒãŒCNFã‚’ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ã«ã—ãŸã€‚

:::message
**é€²æ—: 10% å®Œäº†** Affine / Coupling / Continuous ã®3ã¤ã®Flowã‚’è§¦ã£ãŸã€‚æ¬¡ã¯Course IVã®å…¨ä½“åƒã¨ã€Change of Variableså…¬å¼ã®å®Œå…¨å°å‡ºã¸ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” Course IVå…¨ä½“åƒã¨Flowã®ä½ç½®ã¥ã‘

### 2.1 Course IV: æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ç†è«–ç·¨ã®å…¨ä½“åƒ

**Course IV ã¯10è¬›ç¾©ã§å¯†åº¦ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®è«–ç†çš„ãƒã‚§ãƒ¼ãƒ³ã‚’å®Œæˆã•ã›ã‚‹**ã€‚

```mermaid
graph TD
    L33["ç¬¬33å›<br/>Normalizing Flows<br/>å¯é€†å¤‰æ›+å³å¯†å°¤åº¦"] --> L34["ç¬¬34å›<br/>EBM & çµ±è¨ˆç‰©ç†<br/>p(x)âˆexp(-E(x))"]
    L34 --> L35["ç¬¬35å›<br/>Score Matching<br/>âˆ‡log p(x)"]
    L35 --> L36["ç¬¬36å›<br/>DDPM<br/>é›¢æ•£æ‹¡æ•£"]
    L36 --> L37["ç¬¬37å›<br/>SDE/ODE<br/>é€£ç¶šæ‹¡æ•£"]
    L37 --> L38["ç¬¬38å›<br/>Flow Matching<br/>Scoreâ†”Flowçµ±ä¸€"]
    L38 --> L39["ç¬¬39å›<br/>LDM<br/>æ½œåœ¨ç©ºé–“æ‹¡æ•£"]
    L39 --> L40["ç¬¬40å›<br/>Consistency Models<br/>1-stepç”Ÿæˆ"]
    L40 --> L41["ç¬¬41å›<br/>World Models<br/>ç’°å¢ƒã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿"]
    L41 --> L42["ç¬¬42å›<br/>çµ±ä¸€ç†è«–<br/>å…¨ç”Ÿæˆãƒ¢ãƒ‡ãƒ«æ•´ç†"]

    style L33 fill:#e1f5ff
    style L38 fill:#fff3e0
    style L42 fill:#f3e5f5
```

**å„è¬›ç¾©ã®æ ¸å¿ƒ**:

| è¬›ç¾© | ãƒ†ãƒ¼ãƒ | æ ¸å¿ƒçš„å•ã„ | æ•°å­¦çš„é“å…· |
|:----|:------|:---------|:---------|
| 33 | Normalizing Flows | å¯é€†æ€§ã§å³å¯†å°¤åº¦ã‚’å¾—ã‚‰ã‚Œã‚‹ã‹ï¼Ÿ | Change of Variables, ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ |
| 34 | EBM & çµ±è¨ˆç‰©ç† | æ­£è¦åŒ–å®šæ•°Zã‚’å›é¿ã§ãã‚‹ã‹ï¼Ÿ | Gibbsåˆ†å¸ƒ, MCMC, Hopfieldâ†”Attention |
| 35 | Score Matching | Zã‚’æ¶ˆã—ã¦ã‚¹ã‚³ã‚¢ã ã‘å­¦ç¿’ã§ãã‚‹ã‹ï¼Ÿ | âˆ‡log p, Langevin Dynamics |
| 36 | DDPM | ãƒã‚¤ã‚ºé™¤å»ã®åå¾©ãŒç”Ÿæˆã«ãªã‚‹ã‹ï¼Ÿ | Forward/Reverse Process, VLB |
| 37 | SDE/ODE | é›¢æ•£â†’é€£ç¶šã§ç†è«–çš„åŸºç›¤ã‚’å¾—ã‚‰ã‚Œã‚‹ã‹ï¼Ÿ | ä¼Šè—¤ç©åˆ†, Fokker-Planck, PF-ODE |
| 38 | Flow Matching | Score/Flow/Diffusionã¯åŒã˜ã‹ï¼Ÿ | OT, JKO, Wassersteinå‹¾é…æµ |
| 39 | LDM | ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ã®å£ã‚’è¶…ãˆã‚‰ã‚Œã‚‹ã‹ï¼Ÿ | VAEæ½œåœ¨ç©ºé–“, CFG, ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ä»˜ã‘ |
| 40 | Consistency Models | 1000ã‚¹ãƒ†ãƒƒãƒ—â†’1ã‚¹ãƒ†ãƒƒãƒ—ã«ã§ãã‚‹ã‹ï¼Ÿ | Self-consistency, è’¸ç•™, DPM-Solver |
| 41 | World Models | ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯ä¸–ç•Œã‚’ç†è§£ã™ã‚‹ã‹ï¼Ÿ | JEPA, Transfusion, ç‰©ç†æ³•å‰‡å­¦ç¿’ |
| 42 | çµ±ä¸€ç†è«– | å…¨ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æœ¬è³ªã¯ä½•ã‹ï¼Ÿ | æ•°å­¦çš„ç­‰ä¾¡æ€§, ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ åˆ†é¡ |

**Course Iã®æ•°å­¦ãŒèŠ±é–‹ãç¬é–“**:

- **ç¬¬3-4å› ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ãƒ»ç¢ºç‡å¤‰æ•°å¤‰æ›** â†’ ç¬¬33å› Change of Variableså…¬å¼
- **ç¬¬5å› ä¼Šè—¤ç©åˆ†ãƒ»SDEåŸºç¤** â†’ ç¬¬37å› VP-SDE/VE-SDE, Fokker-Planck
- **ç¬¬6å› KL divergence** â†’ ç¬¬33-42å› å…¨ä½“ã®æå¤±é–¢æ•°
- **ç¬¬6å› Optimal Transport** â†’ ç¬¬38å› Wassersteinå‹¾é…æµ, JKO scheme
- **ç¬¬4å› Fisheræƒ…å ±è¡Œåˆ—** â†’ ç¬¬34å› Natural Gradient, æƒ…å ±å¹¾ä½•

ã€ŒCourse Iã¯ç„¡é§„ã ã£ãŸã®ã§ã¯ï¼Ÿã€ â†’ ã€Œå…¨ã¦ã“ã“ã§èŠ±é–‹ãã€ã€‚

### 2.2 Normalizing Flowsã®3ã¤ã®æ¯”å–©

#### æ¯”å–©1: ç²˜åœŸã®å¤‰å½¢

ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ (çƒ) ã‚’ç²˜åœŸã¨è¦‹ç«‹ã¦ã€å¯é€†å¤‰æ›ã§å¼•ãå»¶ã°ã™ãƒ»ã­ã˜ã‚‹ãƒ»æ›²ã’ã‚‹ã€‚

- **ä¼¸ã°ã™**: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° $x = \sigma z$
- **ãšã‚‰ã™**: å¹³è¡Œç§»å‹• $x = z + \mu$
- **ã­ã˜ã‚‹**: å›è»¢ $x = Rz$
- **æ›²ã’ã‚‹**: éç·šå½¢å¤‰æ› $x = \tanh(z)$ (æ³¨: å˜èª¿æ€§å¿…é ˆ)

å„æ“ä½œã§ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ãŒã€Œä½“ç©ã®å¤‰åŒ–ç‡ã€ã‚’è¿½è·¡ã™ã‚‹ã€‚

#### æ¯”å–©2: å·ã®æµã‚Œ

$z \sim \mathcal{N}(0, I)$ ã‚’æ°´æºã¨ã—ã€å¯é€†å¤‰æ›ã‚’ã€Œå·ã®æµã‚Œã€ã¨è¦‹ã‚‹ã€‚

- **æµã‚Œã‚‹**: $z_0 \to z_1 \to \cdots \to z_K = x$
- **å¯†åº¦**: æ°´æºã®å¯†åº¦ $q(z_0)$ ãŒæµã‚Œã«æ²¿ã£ã¦å¤‰åŒ–
- **ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³**: æµã‚Œã®æ–­é¢ç©å¤‰åŒ– = å¯†åº¦ã®é€†æ•°å¤‰åŒ–

é€£ç¶šæ™‚é–“ã«ã™ã‚‹ã¨ Continuous Normalizing Flow (CNF) = ã€Œæµã‚Œå ´ $f(z, t)$ ã«ã‚ˆã‚‹è¼¸é€ã€ã€‚

#### æ¯”å–©3: åº§æ¨™å¤‰æ›

æ¥µåº§æ¨™å¤‰æ› $(x, y) \to (r, \theta)$ ã‚’æ€ã„å‡ºãã† (ç¬¬3-4å›)ã€‚

$$
p_{r,\theta}(r, \theta) = p_{x,y}(x, y) \left| \det \frac{\partial (x,y)}{\partial (r,\theta)} \right| = p_{x,y}(x, y) \cdot r
$$

$r$ ãŒãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—å¼ã€‚Normalizing Flowsã¯ã€Œç¢ºç‡åˆ†å¸ƒã®åº§æ¨™å¤‰æ›ã€ãã®ã‚‚ã®ã€‚

### 2.3 VAE vs GAN vs Flowã®3ã¤å·´

| è¦³ç‚¹ | VAE | GAN | Normalizing Flow |
|:-----|:----|:----|:-----------------|
| **å°¤åº¦** | è¿‘ä¼¼ (ELBO) | æš—é»™çš„ (ä¸æ˜) | **å³å¯†** |
| **è¨“ç·´** | å®‰å®š | ä¸å®‰å®š (Nashå‡è¡¡) | å®‰å®š |
| **ç”Ÿæˆå“è³ª** | ã¼ã‚„ã‘ã‚‹ | é®®æ˜ | ä¸­é–“ |
| **æ½œåœ¨ç©ºé–“** | è§£é‡ˆå¯èƒ½ | è§£é‡ˆå›°é›£ | è§£é‡ˆå¯èƒ½ |
| **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£** | è‡ªç”± | è‡ªç”± | **å¯é€†æ€§åˆ¶ç´„** |
| **è¨ˆç®—é‡** | O(D) | O(D) | O(DÂ³) or O(D) (æ§‹é€ åŒ–) |
| **ç”¨é€”** | è¡¨ç¾å­¦ç¿’ | é«˜å“è³ªç”Ÿæˆ | å¯†åº¦æ¨å®šãƒ»ç•°å¸¸æ¤œçŸ¥ |

**Flowã®å¼·ã¿**: å³å¯†ãª $\log p(x)$ â†’ ç•°å¸¸æ¤œçŸ¥ (out-of-distribution detection) / å¯†åº¦æ¨å®š / å¤‰åˆ†æ¨è«–ã®äº‹å¾Œåˆ†å¸ƒè¿‘ä¼¼ (IAF [^8])ã€‚

**Flowã®å¼±ã¿**: å¯é€†æ€§åˆ¶ç´„ â†’ è¡¨ç¾åŠ›åˆ¶é™ / ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¨ˆç®— â†’ ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã€‚

### 2.4 Flowãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®ç³»è­œå›³

```mermaid
graph TD
    A["NICE 2014<br/>Affine Coupling"] --> B["RealNVP 2016<br/>Multi-scale"]
    B --> C["Glow 2018<br/>1x1 Conv"]

    A --> D["MAF 2017<br/>Autoregressive"]
    D --> E["IAF 2016<br/>Inverse AR"]

    F["Neural ODE 2018<br/>é€£ç¶šå¤‰æ›"] --> G["CNF 2018<br/>Continuous Flow"]
    G --> H["FFJORD 2019<br/>Hutchinson trace"]

    C --> I["NSF 2019<br/>Spline"]

    H --> J["Rectified Flow 2022<br/>ç›´ç·šè¼¸é€"]
    J --> K["Flow Matching 2023<br/>Diffusionçµ±ä¸€"]

    style A fill:#e3f2fd
    style B fill:#e3f2fd
    style C fill:#e3f2fd
    style G fill:#fff3e0
    style H fill:#fff3e0
    style K fill:#c8e6c9
```

**2ã¤ã®å¤§ããªæµã‚Œ**:

1. **é›¢æ•£Flow**: NICE â†’ RealNVP â†’ Glow â†’ NSF (æ§‹é€ åŒ–ã§ O(D) å®Ÿç¾)
2. **é€£ç¶šFlow**: Neural ODE â†’ CNF â†’ FFJORD (ODE + traceæ¨å®š)

**2022-2023ã®çµ±ä¸€**: Rectified Flow [^9], Flow Matching [^10] ãŒ Normalizing Flows ã¨ Diffusion Models ã‚’æ©‹æ¸¡ã—ã€‚

:::message
**é€²æ—: 20% å®Œäº†** Course IVå…¨ä½“åƒã¨Flowã®ä½ç½®ã¥ã‘ã‚’æŠŠæ¡ã€‚æ¬¡ã¯æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ â€” Change of Variableså…¬å¼ã®å®Œå…¨å°å‡ºã€Coupling Layerç†è«–ã€CNF/FFJORDã®æ•°å­¦ã¸ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” Flowã®æ•°å­¦çš„åŸºç›¤

### 3.1 Change of Variableså…¬å¼ã®å®Œå…¨å°å‡º

**å‰æçŸ¥è­˜**: Course I ç¬¬3-4å›ã®ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ãƒ»ç¢ºç‡å¤‰æ•°å¤‰æ›ã‚’å‰æã¨ã™ã‚‹ã€‚ã“ã“ã§ã¯ç¢ºç‡å¯†åº¦å¤‰æ›å‰‡ã®å°å‡ºã«é›†ä¸­ã™ã‚‹ã€‚

#### 3.1.1 1æ¬¡å…ƒã®å ´åˆ

ç¢ºç‡å¤‰æ•° $Z$ ãŒå¯†åº¦ $p_Z(z)$ ã‚’æŒã¡ã€å¯é€†ãªå˜èª¿å¢—åŠ é–¢æ•° $f$ ã§å¤‰æ›: $X = f(Z)$ã€‚

**å°å‡º**:

$$
\begin{aligned}
P(X \leq x) &= P(f(Z) \leq x) = P(Z \leq f^{-1}(x)) \\
&= \int_{-\infty}^{f^{-1}(x)} p_Z(z) dz
\end{aligned}
$$

ä¸¡è¾ºã‚’ $x$ ã§å¾®åˆ†:

$$
\begin{aligned}
p_X(x) &= \frac{d}{dx} P(X \leq x) = p_Z(f^{-1}(x)) \cdot \frac{d f^{-1}(x)}{dx} \\
&= p_Z(z) \left| \frac{dz}{dx} \right| = p_Z(z) \left| \frac{df}{dz} \right|^{-1}
\end{aligned}
$$

ã“ã“ã§ $z = f^{-1}(x)$ã€‚çµ¶å¯¾å€¤ã¯å˜èª¿æ¸›å°‘ã®å ´åˆã‚‚æ‰±ã†ãŸã‚ã€‚

**çµè«–**:

$$
\boxed{p_X(x) = p_Z(f^{-1}(x)) \left| \frac{df}{dz} \right|^{-1}}
$$

å¯¾æ•°ã‚’ã¨ã‚‹ã¨:

$$
\boxed{\log p_X(x) = \log p_Z(z) - \log \left| \frac{df}{dz} \right|}
$$

#### 3.1.2 å¤šæ¬¡å…ƒã®å ´åˆ

$\mathbf{Z} \in \mathbb{R}^D$ ãŒå¯†åº¦ $p_{\mathbf{Z}}(\mathbf{z})$ ã‚’æŒã¡ã€å¯é€†å¤‰æ› $\mathbf{f}: \mathbb{R}^D \to \mathbb{R}^D$ ã§ $\mathbf{X} = \mathbf{f}(\mathbf{Z})$ã€‚

**ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—**:

$$
J_{\mathbf{f}} = \frac{\partial \mathbf{f}}{\partial \mathbf{z}} = \begin{bmatrix}
\frac{\partial f_1}{\partial z_1} & \cdots & \frac{\partial f_1}{\partial z_D} \\
\vdots & \ddots & \vdots \\
\frac{\partial f_D}{\partial z_1} & \cdots & \frac{\partial f_D}{\partial z_D}
\end{bmatrix}
$$

**å¤‰æ•°å¤‰æ›å…¬å¼** (ç¬¬3å› å®šç†):

$$
\boxed{p_{\mathbf{X}}(\mathbf{x}) = p_{\mathbf{Z}}(\mathbf{f}^{-1}(\mathbf{x})) \left| \det \frac{\partial \mathbf{f}^{-1}}{\partial \mathbf{x}} \right|}
$$

é€†é–¢æ•°ã®ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã¯ã€é †æ–¹å‘ã®ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®é€†è¡Œåˆ—:

$$
\frac{\partial \mathbf{f}^{-1}}{\partial \mathbf{x}} = \left( \frac{\partial \mathbf{f}}{\partial \mathbf{z}} \right)^{-1}
$$

è¡Œåˆ—å¼ã®æ€§è³ª $\det(A^{-1}) = (\det A)^{-1}$ ã‚ˆã‚Š:

$$
\left| \det \frac{\partial \mathbf{f}^{-1}}{\partial \mathbf{x}} \right| = \left| \det \frac{\partial \mathbf{f}}{\partial \mathbf{z}} \right|^{-1}
$$

**æœ€çµ‚å½¢**:

$$
\boxed{p_{\mathbf{X}}(\mathbf{x}) = p_{\mathbf{Z}}(\mathbf{z}) \left| \det \frac{\partial \mathbf{f}}{\partial \mathbf{z}} \right|^{-1}}
$$

å¯¾æ•°å½¢å¼:

$$
\boxed{\log p_{\mathbf{X}}(\mathbf{x}) = \log p_{\mathbf{Z}}(\mathbf{z}) - \log \left| \det J_{\mathbf{f}} \right|}
$$

ã“ã“ã§ $\mathbf{z} = \mathbf{f}^{-1}(\mathbf{x})$ã€$J_{\mathbf{f}} = \frac{\partial \mathbf{f}}{\partial \mathbf{z}}$ã€‚

#### 3.1.3 åˆæˆå¤‰æ›ã®å ´åˆ

$K$ å€‹ã®å¯é€†å¤‰æ›ã‚’åˆæˆ: $\mathbf{f} = \mathbf{f}_K \circ \cdots \circ \mathbf{f}_1$ã€‚

$$
\mathbf{z}_0 \sim q(\mathbf{z}_0), \quad \mathbf{z}_k = \mathbf{f}_k(\mathbf{z}_{k-1}), \quad \mathbf{x} = \mathbf{z}_K
$$

**é€£é–å¾‹**:

$$
\frac{\partial \mathbf{x}}{\partial \mathbf{z}_0} = \frac{\partial \mathbf{f}_K}{\partial \mathbf{z}_{K-1}} \cdots \frac{\partial \mathbf{f}_1}{\partial \mathbf{z}_0}
$$

è¡Œåˆ—å¼ã®ç©ã®æ€§è³ª:

$$
\det \left( \frac{\partial \mathbf{x}}{\partial \mathbf{z}_0} \right) = \prod_{k=1}^{K} \det \left( \frac{\partial \mathbf{f}_k}{\partial \mathbf{z}_{k-1}} \right)
$$

**å¯¾æ•°å°¤åº¦**:

$$
\boxed{\log p(\mathbf{x}) = \log q(\mathbf{z}_0) - \sum_{k=1}^{K} \log \left| \det \frac{\partial \mathbf{f}_k}{\partial \mathbf{z}_{k-1}} \right|}
$$

ã“ã‚ŒãŒ **Normalizing Flowsã®åŸºæœ¬å…¬å¼**ã€‚

### 3.2 ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¨ˆç®—ã®å›°é›£æ€§

**å•é¡Œ**: ä¸€èˆ¬ã® $D \times D$ è¡Œåˆ—ã®ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—å¼è¨ˆç®—ã¯ **O(DÂ³)** (LUåˆ†è§£ or Gaussian elimination)ã€‚

$D = 1024$ (ç”»åƒã®æ½œåœ¨æ¬¡å…ƒ) ã ã¨ 1,073,741,824 å›ã®æ¼”ç®— = å®Ÿç”¨ä¸å¯èƒ½ã€‚

**è§£æ±ºç­–**:

1. **æ§‹é€ åˆ¶ç´„**: ä¸‰è§’è¡Œåˆ— / ãƒ–ãƒ­ãƒƒã‚¯å¯¾è§’ â†’ O(D)
2. **Couplingå¤‰æ›**: éƒ¨åˆ†çš„identity â†’ O(D)
3. **Traceæ¨å®š** (CNF): Hutchinsonã®ä¸åæ¨å®šé‡ â†’ O(D)

æ¬¡ã®ç¯€ã§å„æ‰‹æ³•ã‚’è©³è¿°ã™ã‚‹ã€‚

### 3.3 Coupling Layer â€” RealNVPã®æ ¸å¿ƒ

#### 3.3.1 Affine Coupling Layer

**ã‚¢ã‚¤ãƒ‡ã‚¢**: å…¥åŠ› $\mathbf{z} \in \mathbb{R}^D$ ã‚’2åˆ†å‰²:

$$
\mathbf{z} = [\mathbf{z}_{1:d}, \mathbf{z}_{d+1:D}]
$$

**å¤‰æ›** (Dinh et al. 2016 [^3]):

$$
\begin{aligned}
\mathbf{x}_{1:d} &= \mathbf{z}_{1:d} \quad \text{(identity)} \\
\mathbf{x}_{d+1:D} &= \mathbf{z}_{d+1:D} \odot \exp(s(\mathbf{z}_{1:d})) + t(\mathbf{z}_{1:d})
\end{aligned}
$$

ã“ã“ã§:
- $s, t: \mathbb{R}^d \to \mathbb{R}^{D-d}$ ã¯ä»»æ„ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ (å¯é€†æ€§ä¸è¦!)
- $\odot$ ã¯è¦ç´ ã”ã¨ã®ç©

**é€†å¤‰æ›** (å®¹æ˜“ã«è¨ˆç®—å¯èƒ½):

$$
\begin{aligned}
\mathbf{z}_{1:d} &= \mathbf{x}_{1:d} \\
\mathbf{z}_{d+1:D} &= (\mathbf{x}_{d+1:D} - t(\mathbf{x}_{1:d})) \odot \exp(-s(\mathbf{x}_{1:d}))
\end{aligned}
$$

$s, t$ ã®é€†é–¢æ•°ã¯ä¸è¦!

**ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—**:

$$
J = \frac{\partial \mathbf{x}}{\partial \mathbf{z}} = \begin{bmatrix}
I_d & 0 \\
\frac{\partial \mathbf{x}_{d+1:D}}{\partial \mathbf{z}_{1:d}} & \text{diag}(\exp(s(\mathbf{z}_{1:d})))
\end{bmatrix}
$$

ä¸‹ä¸‰è§’ãƒ–ãƒ­ãƒƒã‚¯è¡Œåˆ— â†’ è¡Œåˆ—å¼ã¯å¯¾è§’æˆåˆ†ã®ç©:

$$
\det J = \det(I_d) \cdot \prod_{i=1}^{D-d} \exp(s_i(\mathbf{z}_{1:d})) = \exp\left(\sum_{i=1}^{D-d} s_i(\mathbf{z}_{1:d})\right)
$$

**å¯¾æ•°ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³**:

$$
\boxed{\log |\det J| = \sum_{i=1}^{D-d} s_i(\mathbf{z}_{1:d})}
$$

**è¨ˆç®—é‡**: $s$ ã®è©•ä¾¡ O(D)ã€ç·å’Œ O(D) â†’ **åˆè¨ˆ O(D)**!

#### 3.3.2 è¡¨ç¾åŠ›ã®è¨¼æ˜ â€” Coupling Layerã®æ™®éè¿‘ä¼¼

**å®šç†** (Huang et al. 2018 [^11]):

> ååˆ†ãªå±¤æ•°ã® Coupling Layers (åˆ†å‰²æ¬¡å…ƒã‚’äº¤äº’ã«å¤‰ãˆã‚‹) ã¯ã€ä»»æ„ã®æ»‘ã‚‰ã‹ãªå¯é€†å¤‰æ›ã‚’ä»»æ„ç²¾åº¦ã§è¿‘ä¼¼ã§ãã‚‹ã€‚

**è¨¼æ˜ã®ã‚¹ã‚±ãƒƒãƒ**:

1. $d = 1$ ã® Coupling Layer ã¯ã€$D-1$ æ¬¡å…ƒã®ä»»æ„é–¢æ•°ã‚’ $z_1$ ã‚’æ¡ä»¶ã«é©ç”¨ã§ãã‚‹
2. åˆ†å‰²ã‚’äº¤äº’ã«å¤‰ãˆã‚‹ (e.g., $[z_1, z_{2:D}]$ â†’ $[z_{1:D-1}, z_D]$) ã“ã¨ã§ã€å…¨æ¬¡å…ƒã‚’æ··åˆ
3. $K$ å±¤ã§ã€ä»»æ„ã® smooth diffeomorphism ã‚’è¿‘ä¼¼å¯èƒ½ (Cybenko 1989ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆæ™®éè¿‘ä¼¼å®šç†ã®æ‹¡å¼µ)

**å®Ÿç”¨ä¸Šã®æ³¨æ„**: ç†è«–çš„ä¿è¨¼ã¯ã‚ã‚‹ãŒã€å®Ÿéš›ã«ã¯ $K = 8 \sim 24$ å±¤ç¨‹åº¦ã§ååˆ†ã€‚

#### 3.3.3 åˆ†å‰²æ¬¡å…ƒã®é¸æŠã¨æ€§èƒ½

**æœ€é©ãªåˆ†å‰²æ¯”**: çµŒé¨“çš„ã« $d \approx D/2$ ãŒæœ€è‰¯ã€‚

| åˆ†å‰²æ¯” | ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¨ˆç®—é‡ | è¡¨ç¾åŠ› | é€†å¤‰æ›è¨ˆç®—é‡ |
|:------|:--------------|:------|:-----------|
| $d=1$ | O(D-1) | ä½ | O(D-1) |
| $d=D/2$ | O(D/2) | **æœ€é«˜** | O(D/2) |
| $d=D-1$ | O(1) | ä½ | O(1) |

$d=D/2$ ã§å¯¾ç§°æ€§ãŒæœ€å¤§åŒ– â†’ ä¸¡åŠåˆ†ãŒç›¸äº’ã«æƒ…å ±ã‚’äº¤æ›ã€‚

### 3.4 RealNVPå®Œå…¨ç‰ˆ â€” Multi-scale Architecture

#### 3.4.1 Checkerboard vs Channel-wise Masking

**Checkerboard masking** (ç”»åƒç”¨):

```
1 0 1 0
0 1 0 1
1 0 1 0
0 1 0 1
```

1ã®ä½ç½® = identityã€0ã®ä½ç½® = å¤‰æ›å¯¾è±¡ã€‚æ¬¡å±¤ã§åè»¢ã€‚

**Channel-wise masking**:

$$
\mathbf{z} \in \mathbb{R}^{C \times H \times W} \to [\mathbf{z}_{1:C/2}, \mathbf{z}_{C/2+1:C}]
$$

ãƒãƒ£ãƒãƒ«æ–¹å‘ã§åˆ†å‰²ã€‚

**RealNVPã®æ§‹é€ ** [^3]:

```
Input (3 x 32 x 32)
  â†“ Checkerboard Coupling x4
  â†“ Squeeze (6 x 16 x 16)
  â†“ Channel-wise Coupling x3
  â†“ Split (half to output, half continue)
  â†“ Channel-wise Coupling x3
  â†“ Split
  â†“ Channel-wise Coupling x3
Output (latent z)
```

**Squeezeæ“ä½œ**: $C \times H \times W \to 4C \times \frac{H}{2} \times \frac{W}{2}$ (ç©ºé–“â†’ãƒãƒ£ãƒãƒ«)ã€‚

**Split**: ä¸­é–“å±¤ã§ãƒãƒ£ãƒãƒ«ã®åŠåˆ†ã‚’ latent z ã¨ã—ã¦å‡ºåŠ› (Multi-scale)ã€‚

#### 3.4.2 Multi-scale Architecture ã®åˆ©ç‚¹

**å•é¡Œ**: å…¨ãƒ”ã‚¯ã‚»ãƒ«ã‚’1ã¤ã® latent z ã«åœ§ç¸®ã™ã‚‹ã¨ã€ä½å‘¨æ³¢æƒ…å ±ã®ã¿æ®‹ã‚Šã€é«˜å‘¨æ³¢(ç´°éƒ¨)ãŒå¤±ã‚ã‚Œã‚‹ã€‚

**è§£æ±º**: ä¸­é–“å±¤ã§ Split â†’ é«˜å‘¨æ³¢æƒ…å ±ã‚’æ—©ã‚ã« latent ã¨ã—ã¦ä¿å­˜ â†’ ç²—ã„æƒ…å ±ã ã‘æœ€å¾Œã¾ã§å¤‰æ›ã€‚

$$
\begin{aligned}
\mathbf{z}_{\text{high-freq}} &\sim p(\mathbf{z}_{\text{high}}) \quad \text{(early split)} \\
\mathbf{z}_{\text{mid-freq}} &\sim p(\mathbf{z}_{\text{mid}} | \mathbf{z}_{\text{high}}) \\
\mathbf{z}_{\text{low-freq}} &\sim p(\mathbf{z}_{\text{low}} | \mathbf{z}_{\text{mid}})
\end{aligned}
$$

**ç”Ÿæˆæ™‚**: $\mathbf{z}_{\text{low}} \to \mathbf{z}_{\text{mid}} \to \mathbf{z}_{\text{high}} \to \mathbf{x}$ ã¨é€†é †ã«åˆæˆã€‚

### 3.5 Glow â€” 1x1 Invertible Convolution

#### 3.5.1 RealNVPã®é™ç•Œ

RealNVPã¯å›ºå®šã®permutation (checkerboard / channel split) ã§æ¬¡å…ƒã‚’äº¤äº’ã«å¤‰ãˆã‚‹ã€‚ã“ã‚Œã¯ **ç·šå½¢çš„ãªæ··åˆ** ã«éããªã„ã€‚

#### 3.5.2 Glow ã®æ”¹å–„ [^4]

**ã‚¢ã‚¤ãƒ‡ã‚¢**: å›ºå®špermutationã‚’ã€**å­¦ç¿’å¯èƒ½ãª1x1ç•³ã¿è¾¼ã¿**ã«ç½®ãæ›ãˆã‚‹ã€‚

1x1ç•³ã¿è¾¼ã¿ã¯ã€ç©ºé–“ä½ç½®ã”ã¨ã«ãƒãƒ£ãƒãƒ«ã‚’ç·šå½¢å¤‰æ›:

$$
\mathbf{y}_{:,i,j} = W \mathbf{x}_{:,i,j}, \quad W \in \mathbb{R}^{C \times C}
$$

$W$ ãŒå¯é€† â‡” $\det W \neq 0$ã€‚

**ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³**:

å…¨ãƒ”ã‚¯ã‚»ãƒ« $(i,j)$ ã§åŒã˜ $W$ ã‚’é©ç”¨ â†’ ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã¯:

$$
\det J = (\det W)^{H \cdot W}
$$

**å¯¾æ•°ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³**:

$$
\log |\det J| = H \cdot W \cdot \log |\det W|
$$

$W$ ã¯ $C \times C$ è¡Œåˆ— â†’ $\det W$ ã®è¨ˆç®—ã¯ O(CÂ³)ã€‚ç”»åƒã®å ´åˆ $C \sim 64$ ãªã®ã§å®Ÿç”¨çš„ã€‚

#### 3.5.3 LUåˆ†è§£ã«ã‚ˆã‚‹é«˜é€ŸåŒ–

$W$ ã‚’ç›´æ¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã™ã‚‹ã¨ã€å¯é€†æ€§ã®ä¿è¨¼ãŒé›£ã—ã„ã€‚

**è§£æ±º**: LUåˆ†è§£ [^4]:

$$
W = P L U
$$

- $P$: å›ºå®šã®permutationè¡Œåˆ— (å­¦ç¿’ã—ãªã„)
- $L$: ä¸‹ä¸‰è§’è¡Œåˆ— (å¯¾è§’=1)
- $U$: ä¸Šä¸‰è§’è¡Œåˆ—

$\det W = \det P \cdot \det L \cdot \det U = \pm 1 \cdot 1 \cdot \prod_{i} U_{ii} = \pm \prod_{i} U_{ii}$

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–**:

$$
U_{ii} = \exp(u_i), \quad u_i \in \mathbb{R}
$$

ã“ã‚Œã§ $U_{ii} > 0$ ã‚’ä¿è¨¼ â†’ $W$ ã¯å¸¸ã«å¯é€†ã€‚

**å¯¾æ•°ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³**:

$$
\log |\det J| = H \cdot W \cdot \sum_{i=1}^{C} u_i
$$

**è¨ˆç®—é‡**: O(C) â†’ è¶…é«˜é€Ÿ!

#### 3.5.4 ActNorm (Activation Normalization)

**Batch Normalizationã®å•é¡Œ**: Flow ã§ã¯é€†å¤‰æ›ãŒå¿…è¦ â†’ running statistics ãŒé‚ªé­”ã€‚

**è§£æ±º**: ActNorm [^4] â€” ãƒãƒ£ãƒãƒ«ã”ã¨ã« scale & shift:

$$
\mathbf{y}_c = s_c \mathbf{x}_c + b_c
$$

$s_c, b_c$ ã¯å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚åˆæœŸåŒ–æ™‚ã«æœ€åˆã®ãƒŸãƒ‹ãƒãƒƒãƒã§å¹³å‡0ãƒ»åˆ†æ•£1ã«ãªã‚‹ã‚ˆã†è¨­å®šã€‚

**ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³**:

$$
\log |\det J| = H \cdot W \cdot \sum_{c=1}^{C} \log |s_c|
$$

### 3.6 Neural Spline Flows â€” å˜èª¿æœ‰ç†äºŒæ¬¡ã‚¹ãƒ—ãƒ©ã‚¤ãƒ³

#### 3.6.1 Affine Couplingã®é™ç•Œ

RealNVP/Glowã® Coupling Layer ã¯ affineå¤‰æ›:

$$
x = z \odot \exp(s(z_{1:d})) + t(z_{1:d})
$$

è¡¨ç¾åŠ›ãŒé™å®šçš„ã€‚ã‚ˆã‚ŠæŸ”è»Ÿãªå˜èª¿é–¢æ•°ã‚’ä½¿ã„ãŸã„ã€‚

#### 3.6.2 Monotonic Rational Quadratic Spline [^12]

**ã‚¢ã‚¤ãƒ‡ã‚¢**: åŒºé–“ $[0, 1]$ ã‚’ $K$ å€‹ã®åŒºåˆ†ã«åˆ†å‰²ã—ã€å„åŒºåˆ†ã§æœ‰ç†äºŒæ¬¡é–¢æ•°ã‚’å®šç¾©ã€‚

$$
f(z) = \frac{a z^2 + b z + c}{d z^2 + e z + 1}
$$

ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $a, b, c, d, e$ ã‚’èª¿æ•´ã—ã¦:

1. å˜èª¿å¢—åŠ 
2. åŒºåˆ†å¢ƒç•Œã§ $C^1$ é€£ç¶š
3. é€†é–¢æ•°ãŒè§£æçš„ã«è¨ˆç®—å¯èƒ½

**ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³**:

$$
\frac{df}{dz} = \frac{(2az + b)(dz^2 + ez + 1) - (az^2 + bz + c)(2dz + e)}{(dz^2 + ez + 1)^2}
$$

**åˆ©ç‚¹**: Affineã‚ˆã‚Šé¥ã‹ã«æŸ”è»Ÿ â†’ å°‘ãªã„å±¤æ•°ã§é«˜ç²¾åº¦ã€‚

**Neural Spline Flow** [^12] (Durkan et al. 2019): Coupling Layerã®ã‚¹ã‚±ãƒ¼ãƒ«ã¨ã‚·ãƒ•ãƒˆã‚’Splineã«ç½®ãæ›ãˆ â†’ å¯†åº¦æ¨å®šã§æœ€é«˜æ€§èƒ½ã€‚

### 3.7 Continuous Normalizing Flows (CNF)

#### 3.7.1 é›¢æ•£â†’é€£ç¶šã®å‹•æ©Ÿ

é›¢æ•£çš„ãªFlow:

$$
\mathbf{z}_k = \mathbf{f}_k(\mathbf{z}_{k-1}), \quad k = 1, \ldots, K
$$

å±¤æ•° $K$ ã¯å›ºå®šã€‚**ç„¡é™å±¤**ã«ã§ããªã„ã‹ï¼Ÿ

#### 3.7.2 Neural ODE [^13]

é€£ç¶šæ™‚é–“ã®å¤‰æ›ã‚’å¸¸å¾®åˆ†æ–¹ç¨‹å¼ã§å®šç¾©:

$$
\frac{d\mathbf{z}(t)}{dt} = \mathbf{f}(\mathbf{z}(t), t, \theta), \quad \mathbf{z}(0) = \mathbf{z}_0, \quad \mathbf{z}(1) = \mathbf{x}
$$

$\mathbf{f}$ ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ (ä»»æ„ã®é–¢æ•°)ã€‚

**å¯é€†æ€§**: $t: 0 \to 1$ ã¨ $t: 1 \to 0$ ã®ä¸¡æ–¹å‘ã§ODEã‚’è§£ã‘ã°å¯é€†ã€‚

#### 3.7.3 Instantaneous Change of Variables

é›¢æ•£ã®Change of Variables:

$$
\log p(\mathbf{z}_k) = \log p(\mathbf{z}_{k-1}) - \log |\det J_{\mathbf{f}_k}|
$$

ã‚’é€£ç¶šæ™‚é–“ã«æ‹¡å¼µã€‚

**å®šç†** (Chen et al. 2018 [^5]):

> é€£ç¶šæ™‚é–“å¤‰æ› $\frac{d\mathbf{z}}{dt} = \mathbf{f}(\mathbf{z}, t)$ ã«å¯¾ã—ã€å¯†åº¦ã®æ™‚é–“å¤‰åŒ–ã¯:
>
> $$
> \frac{\partial \log p(\mathbf{z}(t))}{\partial t} = -\text{tr}\left(\frac{\partial \mathbf{f}}{\partial \mathbf{z}}\right)
> $$

**è¨¼æ˜ã®ã‚¹ã‚±ãƒƒãƒ**:

Liouvilleã®å®šç† (çµ±è¨ˆåŠ›å­¦):

$$
\frac{d\rho}{dt} = -\nabla \cdot (\rho \mathbf{f})
$$

ã“ã“ã§ $\rho$ ã¯ä½ç›¸ç©ºé–“ã®å¯†åº¦ã€‚å±•é–‹:

$$
\frac{d\rho}{dt} = -\rho (\nabla \cdot \mathbf{f}) - \mathbf{f} \cdot \nabla \rho
$$

$\rho = p(\mathbf{z}(t))$ã€é€£é–å¾‹ $\frac{d\rho}{dt} = \frac{\partial \rho}{\partial t} + \mathbf{f} \cdot \nabla \rho$ ã‚ˆã‚Š:

$$
\frac{\partial \rho}{\partial t} = -\rho (\nabla \cdot \mathbf{f})
$$

ä¸¡è¾ºã‚’ $\rho$ ã§å‰²ã‚Šã€$\log$ ã®å¾®åˆ†:

$$
\frac{\partial \log \rho}{\partial t} = -\nabla \cdot \mathbf{f} = -\text{tr}\left(\frac{\partial \mathbf{f}}{\partial \mathbf{z}}\right)
$$

**ç©åˆ†å½¢**:

$$
\log p(\mathbf{x}) = \log p(\mathbf{z}_0) - \int_0^1 \text{tr}\left(\frac{\partial \mathbf{f}}{\partial \mathbf{z}(t)}\right) dt
$$

**å•é¡Œ**: $\text{tr}\left(\frac{\partial \mathbf{f}}{\partial \mathbf{z}}\right)$ ã®è¨ˆç®—ãŒ O(DÂ²) (ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®å¯¾è§’è¦ç´  $D$ å€‹ã€å„ O(D) ã®å¾®åˆ†)ã€‚

### 3.8 FFJORD â€” Hutchinson Traceæ¨å®š

#### 3.8.1 Traceè¨ˆç®—ã®å›°é›£æ€§

$$
\text{tr}\left(\frac{\partial \mathbf{f}}{\partial \mathbf{z}}\right) = \sum_{i=1}^{D} \frac{\partial f_i}{\partial z_i}
$$

å„ $\frac{\partial f_i}{\partial z_i}$ ã®è¨ˆç®—ã«ã¯ $\mathbf{f}$ ã®é †ä¼æ’­ã¨1å›ã®é€†ä¼æ’­ â†’ $D$ å›ã®é€†ä¼æ’­ â†’ O(DÂ²)ã€‚

#### 3.8.2 Hutchinsonã®ä¸åæ¨å®šé‡ [^14]

**å®šç†** (Hutchinson 1990):

> $A$ ã‚’ä»»æ„ã®è¡Œåˆ—ã€$\boldsymbol{\epsilon} \sim \mathcal{N}(0, I)$ ã¨ã—ãŸã¨ã:
>
> $$
> \mathbb{E}_{\boldsymbol{\epsilon}}[\boldsymbol{\epsilon}^T A \boldsymbol{\epsilon}] = \text{tr}(A)
> $$

**è¨¼æ˜**:

$$
\begin{aligned}
\mathbb{E}[\boldsymbol{\epsilon}^T A \boldsymbol{\epsilon}] &= \mathbb{E}\left[\sum_{i,j} \epsilon_i A_{ij} \epsilon_j\right] \\
&= \sum_{i,j} A_{ij} \mathbb{E}[\epsilon_i \epsilon_j] \\
&= \sum_{i,j} A_{ij} \delta_{ij} \quad (\text{since } \mathbb{E}[\epsilon_i \epsilon_j] = \delta_{ij}) \\
&= \sum_{i} A_{ii} = \text{tr}(A)
\end{aligned}
$$

#### 3.8.3 FFJORDã®é©ç”¨ [^6]

$$
\text{tr}\left(\frac{\partial \mathbf{f}}{\partial \mathbf{z}}\right) = \mathbb{E}_{\boldsymbol{\epsilon}}\left[\boldsymbol{\epsilon}^T \frac{\partial \mathbf{f}}{\partial \mathbf{z}} \boldsymbol{\epsilon}\right]
$$

å³è¾ºã¯ **vector-Jacobian product** (VJP):

$$
\boldsymbol{\epsilon}^T \frac{\partial \mathbf{f}}{\partial \mathbf{z}} = \frac{\partial (\boldsymbol{\epsilon}^T \mathbf{f})}{\partial \mathbf{z}}
$$

ã•ã‚‰ã« $\frac{\partial \mathbf{f}}{\partial \mathbf{z}} \boldsymbol{\epsilon}$ ã¯ **Jacobian-vector product** (JVP)ã€è‡ªå‹•å¾®åˆ†ã§åŠ¹ç‡çš„ã«è¨ˆç®—å¯èƒ½ (1å›ã®é †ä¼æ’­+1å›ã®é€†ä¼æ’­)ã€‚

**FFJORD ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

```
1. Sample Îµ ~ N(0, I)
2. Compute v = (âˆ‚f/âˆ‚z)Îµ  (JVP: 1 forward + 1 backward)
3. Estimate: tr(âˆ‚f/âˆ‚z) â‰ˆ Îµ^T v
4. Integrate: log p(x) = log p(z_0) - âˆ«â‚€Â¹ Îµ^T v dt
```

**è¨ˆç®—é‡**: O(D) (1ã‚µãƒ³ãƒ—ãƒ«ã‚ãŸã‚Š) â†’ ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«!

**åˆ†æ•£**: 1ã‚µãƒ³ãƒ—ãƒ«ã ã¨åˆ†æ•£å¤§ â†’ å®Ÿç”¨ã§ã¯è¤‡æ•°ã‚µãƒ³ãƒ—ãƒ«ã§å¹³å‡ or åˆ†æ•£å‰Šæ¸›ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã€‚

### 3.9 Adjoint Method â€” ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã®é€£ç¶šç‰ˆ

#### 3.9.1 ODEã®é€†ä¼æ’­å•é¡Œ

Neural ODEã®è¨“ç·´:

$$
\mathcal{L}(\theta) = \text{Loss}(\mathbf{z}(1)), \quad \mathbf{z}(1) = \text{ODESolve}(\mathbf{f}_\theta, \mathbf{z}(0), [0, 1])
$$

$\frac{\partial \mathcal{L}}{\partial \theta}$ ã‚’è¨ˆç®—ã—ãŸã„ã€‚

**Naive approach**: ODESolverã®å…¨ã‚¹ãƒ†ãƒƒãƒ—ã‚’ä¿å­˜ â†’ ãƒ¡ãƒ¢ãƒªçˆ†ç™º (O(time steps))ã€‚

#### 3.9.2 Adjointæ„Ÿåº¦è§£æ [^5]

**Adjointå¤‰æ•°**: $\mathbf{a}(t) = \frac{\partial \mathcal{L}}{\partial \mathbf{z}(t)}$ã€‚

**Adjoint ODE**:

$$
\frac{d\mathbf{a}(t)}{dt} = -\mathbf{a}(t)^T \frac{\partial \mathbf{f}}{\partial \mathbf{z}}
$$

**å¢ƒç•Œæ¡ä»¶**: $\mathbf{a}(1) = \frac{\partial \mathcal{L}}{\partial \mathbf{z}(1)}$ (losså‹¾é…)ã€‚

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‹¾é…**:

$$
\frac{\partial \mathcal{L}}{\partial \theta} = -\int_1^0 \mathbf{a}(t)^T \frac{\partial \mathbf{f}}{\partial \theta} dt
$$

**è¨ˆç®—æ‰‹é †**:

1. Forward: $\mathbf{z}(0) \to \mathbf{z}(1)$ ã‚’è§£ã
2. Backward: Adjoint ODE $\mathbf{a}(1) \to \mathbf{a}(0)$ ã‚’ **é€†æ™‚é–“** ã§è§£ã
3. é€”ä¸­ã§ $\frac{\partial \mathcal{L}}{\partial \theta}$ ã‚’ç©ç®—

**ãƒ¡ãƒ¢ãƒª**: O(1) (ä¸­é–“çŠ¶æ…‹ã‚’ä¿å­˜ã—ãªã„) â†’ è¶…åŠ¹ç‡çš„!

:::message alert
**Adjoint Methodã®æ³¨æ„ç‚¹**: æ•°å€¤èª¤å·®ãŒè“„ç©ã™ã‚‹å¯èƒ½æ€§ã€‚Forward passã¨Backward passã§ç•°ãªã‚‹ODESolver toleranceã‚’ä½¿ã†ã¨ä¸æ•´åˆã€‚å®Ÿç”¨ã§ã¯`adjoint=True`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ (DifferentialEquations.jl / torchdiffeq) ã§è‡ªå‹•å‡¦ç†ã€‚
:::

### 3.10 Flow vs VAE vs GANç†è«–çš„æ¯”è¼ƒ

#### 3.10.1 å°¤åº¦ã®ç²¾åº¦

| ãƒ¢ãƒ‡ãƒ« | å°¤åº¦ | ç²¾åº¦ | è¨ˆç®—ã‚³ã‚¹ãƒˆ |
|:------|:-----|:-----|:---------|
| Flow | å³å¯† $\log p(x)$ | æœ€é«˜ | O(D) ~ O(DÂ³) |
| VAE | ä¸‹ç•Œ ELBO | è¿‘ä¼¼ | O(D) |
| GAN | ãªã— | - | O(D) |

**ç•°å¸¸æ¤œçŸ¥ã¸ã®å¿œç”¨**: Flow ãŒæœ€é© â†’ å³å¯†ãª $\log p(x)$ ã§ out-of-distribution ã‚’å®šé‡è©•ä¾¡ã€‚

#### 3.10.2 æ½œåœ¨ç©ºé–“ã®æ§‹é€ 

- **Flow**: $\mathbf{z} \sim \mathcal{N}(0, I)$ (å›ºå®š) â†’ æ½œåœ¨ç©ºé–“ã®è§£é‡ˆã¯é™å®šçš„
- **VAE**: $q_\phi(\mathbf{z}|\mathbf{x})$ (å­¦ç¿’) â†’ æ½œåœ¨ç©ºé–“ã®æ„å‘³ãŒè±Šã‹ (disentanglementå¯èƒ½)
- **GAN**: æ½œåœ¨ç©ºé–“ã®æ§‹é€ ä¸æ˜ â†’ è£œé–“ã¯ç¶ºéº—ã ãŒç†è«–çš„æ ¹æ‹ ãªã—

#### 3.10.3 ç”Ÿæˆå“è³ª

| ãƒ¢ãƒ‡ãƒ« | FID (ImageNet 256x256) | ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é€Ÿåº¦ |
|:------|:----------------------|:---------------|
| Glow (2018) | ~46 | é€Ÿã„ (1 pass) |
| VAE (NVAE 2020) | ~50 | é€Ÿã„ (1 pass) |
| GAN (BigGAN 2018) | ~7 | é€Ÿã„ (1 pass) |
| Diffusion (ADM 2021) | ~10 | é…ã„ (1000 steps) |

**2018å¹´æ™‚ç‚¹**: GANãŒåœ§å€’çš„ â†’ Flowã¯å¯†åº¦æ¨å®šç‰¹åŒ–ã€‚

**2024å¹´**: Diffusion/Flow MatchingãŒé€†è»¢ â†’ Flowã¯ç†è«–çš„åŸºç›¤ã¨ã—ã¦å†è©•ä¾¡ã€‚

#### 3.10.4 Diffusion/Flow Matchingã¨ã®æ¥ç¶š

**Rectified Flow** [^9] / **Flow Matching** [^10]:

$$
\frac{d\mathbf{x}(t)}{dt} = v_\theta(\mathbf{x}(t), t), \quad \mathbf{x}(0) \sim p_\text{data}, \quad \mathbf{x}(1) \sim \mathcal{N}(0, I)
$$

ã“ã‚Œã¯ **CNFã®é€†æ–¹å‘** (data â†’ noise)ã€‚

**ç­‰ä¾¡æ€§**: Flow Matchingã¯CNFã®ç‰¹æ®Šã‚±ãƒ¼ã‚¹ + Optimal Transportåˆ¶ç´„ã€‚

:::message
**æ­´å²çš„çš®è‚‰**: 2018å¹´ã€ŒFlowã¯é…ã„ãƒ»å“è³ªä½ã„ã€ â†’ 2022å¹´ã€ŒCNFãŒDiffusionã®ç†è«–çš„åŸºç›¤ã ã£ãŸã€ â†’ 2024å¹´ã€ŒFlow MatchingãŒæœ€é€Ÿã€ã€‚"éå®Ÿç”¨"ãŒ"åŸºç›¤ç†è«–"ã«åŒ–ã‘ãŸã€‚
:::

### 3.11 âš”ï¸ Boss Battle: RealNVPã®å®Œå…¨å®Ÿè£…

**èª²é¡Œ**: RealNVP [^3] ã® Coupling Layer ã‚’å®Œå…¨å®Ÿè£…ã—ã€Change of Variableså…¬å¼ã§log p(x)ã‚’è¨ˆç®—ã›ã‚ˆã€‚

**ãƒ‡ãƒ¼ã‚¿**: 2D toy dataset (two moons)ã€‚

**å®Ÿè£…** (æ¦‚å¿µå®Ÿè¨¼ã‚³ãƒ¼ãƒ‰):

```julia
using Flux, Distributions

# Affine Coupling Layer
struct AffineCoupling
    s_net  # scale network
    t_net  # translation network
    d      # split dimension
end

function (layer::AffineCoupling)(z::Matrix)
    # z: (D, batch_size)
    d = layer.d
    z1 = z[1:d, :]          # identity part
    z2 = z[d+1:end, :]      # transform part

    # Compute scale & translation from z1
    s = layer.s_net(z1)
    t = layer.t_net(z1)

    # Affine transformation
    x1 = z1
    x2 = z2 .* exp.(s) .+ t
    x = vcat(x1, x2)

    # log|det J| = sum(s) over transform dimensions
    log_det_jac = vec(sum(s, dims=1))  # (batch_size,)

    return x, log_det_jac
end

# Inverse
function inverse(layer::AffineCoupling, x::Matrix)
    d = layer.d
    x1 = x[1:d, :]
    x2 = x[d+1:end, :]

    s = layer.s_net(x1)
    t = layer.t_net(x1)

    z1 = x1
    z2 = (x2 .- t) .* exp.(-s)
    z = vcat(z1, z2)

    log_det_jac = -vec(sum(s, dims=1))

    return z, log_det_jac
end

# Simple MLP
function build_net(in_dim, out_dim, hidden_dim=64)
    Chain(
        Dense(in_dim, hidden_dim, tanh),
        Dense(hidden_dim, hidden_dim, tanh),
        Dense(hidden_dim, out_dim)
    )
end

# RealNVP with 4 coupling layers (alternating splits)
D = 2
layers = [
    AffineCoupling(build_net(1, 1), build_net(1, 1), 1),  # split at d=1
    AffineCoupling(build_net(1, 1), build_net(1, 1), 1),  # split at d=1 (alternate)
    AffineCoupling(build_net(1, 1), build_net(1, 1), 1),
    AffineCoupling(build_net(1, 1), build_net(1, 1), 1)
]

# Forward: z â†’ x
function forward_flow(layers, z)
    x = z
    log_det_sum = zeros(size(z, 2))
    for layer in layers
        x, ldj = layer(x)
        log_det_sum .+= ldj
    end
    return x, log_det_sum
end

# Inverse: x â†’ z
function inverse_flow(layers, x)
    z = x
    log_det_sum = zeros(size(x, 2))
    for layer in reverse(layers)
        z, ldj = inverse(layer, z)
        log_det_sum .+= ldj
    end
    return z, log_det_sum
end

# log p(x)
function log_prob(layers, x, base_dist)
    z, log_det_sum = inverse_flow(layers, x)
    log_pz = vec(sum(logpdf.(base_dist, z), dims=1))  # sum over D
    log_px = log_pz .+ log_det_sum
    return log_px
end

# Test
base_dist = Normal(0, 1)
z_test = randn(D, 100)
x_test, ldj_forward = forward_flow(layers, z_test)

println("Forward: z â†’ x")
println("z[1:3] = ", z_test[:, 1:3])
println("x[1:3] = ", x_test[:, 1:3])

# Verify inverse
z_recon, ldj_inverse = inverse_flow(layers, x_test)
recon_error = maximum(abs.(z_test - z_recon))
println("\nInverse: x â†’ z")
println("Reconstruction error: $recon_error")

# log p(x)
log_px = log_prob(layers, x_test, base_dist)
println("\nlog p(x)[1:3] = ", log_px[1:3])
```

**ãƒœã‚¹æ’ƒç ´æ¡ä»¶**:

1. âœ… Forward pass: $\mathbf{z} \to \mathbf{x}$ ãŒå®Ÿè¡Œã•ã‚Œã‚‹
2. âœ… Inverse pass: $\mathbf{x} \to \mathbf{z}$ ã®å†æ§‹æˆèª¤å·® < 1e-5
3. âœ… log|det J| ã®è¨ˆç®—ãŒ O(D) ã§å®Œäº†
4. âœ… log p(x) = log p(z) - log|det J| ã®å¼ãŒæˆç«‹

**ãƒœã‚¹æ’ƒç ´!** RealNVPã®å…¨æ§‹é€ ã‚’å®Ÿè£…ã—ãŸã€‚ã“ã‚ŒãŒç”»åƒç”Ÿæˆãƒ»ç•°å¸¸æ¤œçŸ¥ã®å®Ÿè£…åŸºç›¤ã ã€‚

:::message
**é€²æ—: 50% å®Œäº†** Change of Variableså…¬å¼ã€Coupling Layerã€RealNVPã€Glowã€NSFã€CNFã€FFJORDã®æ•°å­¦ã‚’å®Œå…¨ç¿’å¾—ã€‚æ¬¡ã¯å®Ÿè£…ã‚¾ãƒ¼ãƒ³ â€” Julia/Rustã§å‹•ãFlowã‚’æ›¸ãã€‚
:::

---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” Julia/Rustã§Flowã‚’æ›¸ã

**ã‚´ãƒ¼ãƒ«**: RealNVP/Glow/CNFã®å®Ÿè£…åŠ›ã‚’èº«ã«ã¤ã‘ã‚‹ã€‚

### 4.1 Julia Flowå®Ÿè£…ã®å…¨ä½“è¨­è¨ˆ

**ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æ§‹æˆ**:

```julia
# Normalizing Flows in Julia
using Lux           # é–¢æ•°å‹NN (å‹å®‰å®š+GPU AOT)
using Reactant      # GPU AOT compilation
using DifferentialEquations  # ODE solver (CNFç”¨)
using Distributions
using LinearAlgebra
using Optimisers, Zygote
using Random
```

**Luxé¸æŠç†ç”±**: Immutable (functional) â†’ å‹å®‰å®šæ€§ â†’ Reactant GPU AOT â†’ Production-readyã€‚

### 4.2 Coupling Layerå®Ÿè£…

```julia
# Affine Coupling Layer (Lux style)
function affine_coupling_forward(z, s_net, t_net, ps_s, ps_t, st_s, st_t, d)
    z1 = z[1:d, :]          # identity part
    z2 = z[d+1:end, :]      # transform part

    # Compute scale & translation from z1
    s, st_s_new = s_net(z1, ps_s, st_s)
    t, st_t_new = t_net(z1, ps_t, st_t)

    # Affine transformation
    x1 = z1
    x2 = z2 .* exp.(s) .+ t
    x = vcat(x1, x2)

    # log|det J| = sum(s)
    log_det_jac = vec(sum(s, dims=1))

    return x, log_det_jac, (st_s_new, st_t_new)
end

# Inverse
function affine_coupling_inverse(x, s_net, t_net, ps_s, ps_t, st_s, st_t, d)
    x1 = x[1:d, :]
    x2 = x[d+1:end, :]

    s, st_s_new = s_net(x1, ps_s, st_s)
    t, st_t_new = t_net(x1, ps_t, st_t)

    z1 = x1
    z2 = (x2 .- t) .* exp.(-s)
    z = vcat(z1, z2)

    log_det_jac = -vec(sum(s, dims=1))

    return z, log_det_jac, (st_s_new, st_t_new)
end
```

### 4.3 RealNVP Stack

```julia
# RealNVP: Stack of coupling layers
function create_realnvp(in_dim::Int, hidden_dim::Int, n_layers::Int)
    rng = Random.default_rng()
    layers = []

    for i in 1:n_layers
        d = i % 2 == 1 ? in_dim Ã· 2 : in_dim - in_dim Ã· 2
        s_net = Chain(
            Dense(d, hidden_dim, tanh),
            Dense(hidden_dim, hidden_dim, tanh),
            Dense(hidden_dim, in_dim - d)
        )
        t_net = Chain(
            Dense(d, hidden_dim, tanh),
            Dense(hidden_dim, hidden_dim, tanh),
            Dense(hidden_dim, in_dim - d)
        )
        push!(layers, (s_net, t_net, d))
    end

    return layers
end

# Forward: z â†’ x
function realnvp_forward(layers, z, ps_list, st_list)
    x = z
    log_det_sum = zeros(Float32, size(z, 2))
    st_new_list = []

    for (i, (s_net, t_net, d)) in enumerate(layers)
        x, ldj, st_new = affine_coupling_forward(
            x, s_net, t_net,
            ps_list[i].s, ps_list[i].t,
            st_list[i].s, st_list[i].t,
            d
        )
        log_det_sum .+= ldj
        push!(st_new_list, (s=st_new[1], t=st_new[2]))
    end

    return x, log_det_sum, st_new_list
end

# Inverse: x â†’ z
function realnvp_inverse(layers, x, ps_list, st_list)
    z = x
    log_det_sum = zeros(Float32, size(x, 2))
    st_new_list = []

    for (i, (s_net, t_net, d)) in enumerate(reverse(enumerate(layers)))
        idx = length(layers) - i + 1
        z, ldj, st_new = affine_coupling_inverse(
            z, s_net, t_net,
            ps_list[idx].s, ps_list[idx].t,
            st_list[idx].s, st_list[idx].t,
            d
        )
        log_det_sum .+= ldj
        pushfirst!(st_new_list, (s=st_new[1], t=st_new[2]))
    end

    return z, log_det_sum, st_new_list
end
```

### 4.4 è¨“ç·´ãƒ«ãƒ¼ãƒ—

```julia
# Loss: Negative log-likelihood
function nll_loss(layers, ps_list, st_list, x_batch, base_dist)
    # Inverse: x â†’ z
    z, log_det_sum, _ = realnvp_inverse(layers, x_batch, ps_list, st_list)

    # log p(z)
    log_pz = sum(logpdf.(base_dist, z), dims=1)  # sum over D

    # log p(x) = log p(z) + log|det J|
    log_px = vec(log_pz) .+ log_det_sum

    # NLL
    return -mean(log_px)
end

# Training
function train_realnvp!(layers, ps_list, st_list, data_loader, base_dist, opt_state, n_epochs)
    for epoch in 1:n_epochs
        epoch_loss = 0.0
        n_batches = 0

        for x_batch in data_loader
            # Compute loss and gradients
            loss, grads = Zygote.withgradient(ps_list) do ps
                nll_loss(layers, ps, st_list, x_batch, base_dist)
            end

            # Update parameters
            opt_state, ps_list = Optimisers.update(opt_state, ps_list, grads[1])

            epoch_loss += loss
            n_batches += 1
        end

        if epoch % 10 == 0
            avg_loss = epoch_loss / n_batches
            println("Epoch $epoch: NLL = $(round(avg_loss, digits=4))")
        end
    end

    return ps_list, st_list
end
```

### 4.5 CNF/FFJORDå®Ÿè£…

```julia
using DifferentialEquations

# CNF dynamics with Hutchinson trace estimator
function cnf_dynamics!(du, u, p, t)
    # u = [z; log_det_jac]
    f_net, ps, st = p
    D = length(u) - 1
    z = u[1:D]

    # Velocity: dz/dt = f(z, t)
    z_mat = reshape(z, :, 1)
    dz, _ = f_net(z_mat, ps, st)
    dz = vec(dz)

    # Hutchinson trace estimator
    Îµ = randn(Float32, D)
    jvp = Zygote.gradient(z -> dot(vec(f_net(reshape(z, :, 1), ps, st)[1]), Îµ), z)[1]
    tr_jac = dot(Îµ, jvp)  # Îµ^T * (âˆ‚f/âˆ‚z) * Îµ

    # d(log_det)/dt = -tr(âˆ‚f/âˆ‚z)
    du[1:D] .= dz
    du[D+1] = -tr_jac
end

# Solve CNF
function solve_cnf(f_net, ps, st, z0, tspan)
    D = length(z0)
    u0 = vcat(z0, 0.0f0)  # [z; log_det_jac=0]

    prob = ODEProblem(cnf_dynamics!, u0, tspan, (f_net, ps, st))
    sol = solve(prob, Tsit5())

    z1 = sol.u[end][1:D]
    log_det_jac = sol.u[end][D+1]

    return z1, log_det_jac
end
```

### 4.6 Rustæ¨è«–å®Ÿè£…

Rustå´ã¯è¨“ç·´æ¸ˆã¿ONNXãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§æ¨è«–ã€‚

```rust
// Affine Coupling Layer in Rust
pub struct AffineCouplingLayer {
    split_dim: usize,
    s_weights: Vec<Vec<f32>>,  // simplified: full ONNX would use ort
    t_weights: Vec<Vec<f32>>,
}

impl AffineCouplingLayer {
    pub fn forward(&self, z: &[f32]) -> (Vec<f32>, f32) {
        let d = self.split_dim;
        let (z1, z2) = z.split_at(d);

        // Compute scale & translation (simplified MLP)
        let s = self.mlp_forward(&self.s_weights, z1);
        let t = self.mlp_forward(&self.t_weights, z1);

        // Affine transformation
        let mut x = Vec::with_capacity(z.len());
        x.extend_from_slice(z1);
        for i in 0..z2.len() {
            x.push(z2[i] * s[i].exp() + t[i]);
        }

        let log_det_jac: f32 = s.iter().sum();

        (x, log_det_jac)
    }

    fn mlp_forward(&self, weights: &[Vec<f32>], input: &[f32]) -> Vec<f32> {
        // Simplified: 2-layer MLP with tanh
        // Full implementation would use ONNX Runtime
        input.to_vec()  // placeholder
    }
}

// RealNVP inference
pub struct RealNVP {
    layers: Vec<AffineCouplingLayer>,
    dim: usize,
}

impl RealNVP {
    pub fn sample(&self, rng: &mut impl Rng) -> Vec<f32> {
        // Sample z ~ N(0, I)
        let z: Vec<f32> = (0..self.dim).map(|_| rng.sample(StandardNormal)).collect();

        // Forward: z â†’ x
        self.forward(&z).0
    }

    pub fn log_prob(&self, x: &[f32]) -> f32 {
        // Inverse: x â†’ z
        let (z, log_det_jac) = self.inverse(x);

        // log p(z) = -0.5 * (z^2 + log(2Ï€))
        let log_pz: f32 = z.iter().map(|zi| -0.5 * (zi * zi + (2.0 * std::f32::consts::PI).ln())).sum();

        log_pz + log_det_jac
    }

    fn forward(&self, z: &[f32]) -> (Vec<f32>, f32) {
        let mut x = z.to_vec();
        let mut log_det_sum = 0.0;

        for layer in &self.layers {
            let (x_new, ldj) = layer.forward(&x);
            x = x_new;
            log_det_sum += ldj;
        }

        (x, log_det_sum)
    }

    fn inverse(&self, x: &[f32]) -> (Vec<f32>, f32) {
        let mut z = x.to_vec();
        let mut log_det_sum = 0.0;

        for layer in self.layers.iter().rev() {
            // Inverse coupling (not shown: requires inverse method)
            // z = layer.inverse(&z);
            // log_det_sum += ldj;
        }

        (z, log_det_sum)
    }
}
```

### 4.7 æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œè¡¨

| æ•°å¼ | Julia | Rust |
|:-----|:------|:-----|
| $\log p(x) = \log p(z) - \log \|\det J\|$ | `logpdf(base_dist, z) - log_det_jac` | `log_pz - log_det_jac` |
| $x_2 = z_2 \odot \exp(s) + t$ | `z2 .* exp.(s) .+ t` | `z2[i] * s[i].exp() + t[i]` |
| $\log \|\det J\| = \sum s_i$ | `sum(s)` | `s.iter().sum()` |
| $\text{tr}(A) = \mathbb{E}[\epsilon^T A \epsilon]$ | `dot(Îµ, jvp)` | - (training only) |

:::message
**é€²æ—: 70% å®Œäº†** Julia/Rustå®Ÿè£…å®Œäº†ã€‚æ¬¡ã¯å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ â€” 2D/MNISTè¨“ç·´ãƒ»è©•ä¾¡ã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” Flowã®è¨“ç·´ã¨è©•ä¾¡

**ã‚´ãƒ¼ãƒ«**: 2D toy dataset / MNIST ã§Flowã‚’è¨“ç·´ã—ã€æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹ã€‚

### 5.1 2D Toy Dataset: Two Moons

#### 5.1.1 ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ

```julia
using Plots

function generate_two_moons(n_samples::Int; noise=0.1)
    n_per_moon = n_samples Ã· 2

    # Upper moon
    Î¸1 = range(0, Ï€, length=n_per_moon)
    x1_upper = cos.(Î¸1)
    x2_upper = sin.(Î¸1)

    # Lower moon
    Î¸2 = range(0, Ï€, length=n_per_moon)
    x1_lower = 1 .- cos.(Î¸2)
    x2_lower = 0.5 .- sin.(Î¸2)

    # Add noise
    x1 = vcat(x1_upper, x1_lower) .+ noise * randn(n_samples)
    x2 = vcat(x2_upper, x2_lower) .+ noise * randn(n_samples)

    return Float32.(hcat(x1, x2))'  # (2, n_samples)
end

data = generate_two_moons(1000)
scatter(data[1, :], data[2, :], alpha=0.5, label="Two Moons", aspect_ratio=:equal)
```

#### 5.1.2 RealNVPè¨“ç·´

```julia
# Setup
rng = Random.default_rng()
in_dim = 2
hidden_dim = 64
n_layers = 8

layers = create_realnvp(in_dim, hidden_dim, n_layers)
ps_list = [initialize_params(rng, s_net, t_net) for (s_net, t_net, _) in layers]
st_list = [initialize_states(rng, s_net, t_net) for (s_net, t_net, _) in layers]

# Base distribution
base_dist = Normal(0.0f0, 1.0f0)

# Optimizer
opt = Adam(1e-3)
opt_state = Optimisers.setup(opt, ps_list)

# Data loader
batch_size = 256
data_loader = [data[:, i:min(i+batch_size-1, end)] for i in 1:batch_size:size(data, 2)]

# Train
n_epochs = 500
ps_list, st_list = train_realnvp!(layers, ps_list, st_list, data_loader, base_dist, opt_state, n_epochs)
```

Output:
```
Epoch 10: NLL = 2.1542
Epoch 20: NLL = 1.8765
...
Epoch 500: NLL = 1.2341
```

#### 5.1.3 ç”Ÿæˆã‚µãƒ³ãƒ—ãƒ«å¯è¦–åŒ–

```julia
# Sample from trained model
n_samples = 1000
z_samples = randn(Float32, 2, n_samples)
x_samples, _, _ = realnvp_forward(layers, z_samples, ps_list, st_list)

# Plot
p1 = scatter(data[1, :], data[2, :], alpha=0.3, label="Real", c=:blue)
scatter!(p1, x_samples[1, :], x_samples[2, :], alpha=0.3, label="Generated", c=:red)
title!(p1, "RealNVP: Two Moons")
```

#### 5.1.4 å¯†åº¦ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—

```julia
# Compute log p(x) on grid
x_range = range(-2, 3, length=100)
y_range = range(-1.5, 2, length=100)
log_px_grid = zeros(Float32, 100, 100)

for (i, x) in enumerate(x_range), (j, y) in enumerate(y_range)
    point = Float32[x; y;;]
    z, ldj, _ = realnvp_inverse(layers, point, ps_list, st_list)
    log_pz = sum(logpdf.(base_dist, z))
    log_px_grid[j, i] = log_pz + ldj[1]
end

heatmap(x_range, y_range, log_px_grid, title="log p(x)", aspect_ratio=:equal)
```

### 5.2 MNIST: Tiny RealNVP

#### 5.2.1 ãƒ‡ãƒ¼ã‚¿æº–å‚™

```julia
using MLDatasets

# Load MNIST
train_x, _ = MNIST(:train)[:]
test_x, _ = MNIST(:test)[:]

# Flatten: (28, 28, 1, N) â†’ (784, N)
train_x_flat = reshape(train_x, 784, :)
test_x_flat = reshape(test_x, 784, :)

# Dequantize + logit transform
function logit_transform(x; Î±=0.05f0)
    x_dequant = x .+ Î± .* rand(Float32, size(x))
    x_clip = clamp.(x_dequant, Î±, 1 - Î±)
    return log.(x_clip ./ (1 .- x_clip))
end

train_x_trans = logit_transform(Float32.(train_x_flat))
test_x_trans = logit_transform(Float32.(test_x_flat))
```

#### 5.2.2 Tiny RealNVPè¨“ç·´

```julia
# Model: 784-dim, 256 hidden, 12 layers
layers_mnist = create_realnvp(784, 256, 12)
ps_mnist = [initialize_params(rng, s, t) for (s, t, _) in layers_mnist]
st_mnist = [initialize_states(rng, s, t) for (s, t, _) in layers_mnist]

# Train (20 epochs, batch_size=128)
opt_mnist = Adam(1e-4)
opt_state_mnist = Optimisers.setup(opt_mnist, ps_mnist)

batch_size_mnist = 128
data_loader_mnist = [train_x_trans[:, i:min(i+batch_size_mnist-1, end)]
                     for i in 1:batch_size_mnist:size(train_x_trans, 2)]

n_epochs_mnist = 20
ps_mnist, st_mnist = train_realnvp!(
    layers_mnist, ps_mnist, st_mnist,
    data_loader_mnist, base_dist,
    opt_state_mnist, n_epochs_mnist
)
```

#### 5.2.3 ç”Ÿæˆç”»åƒ

```julia
# Sample
n_samples_img = 16
z_img = randn(Float32, 784, n_samples_img)
x_img, _, _ = realnvp_forward(layers_mnist, z_img, ps_mnist, st_mnist)

# Inverse logit
x_img_sigmoid = @. 1 / (1 + exp(-x_img))
x_img_reshape = reshape(x_img_sigmoid, 28, 28, 1, n_samples_img)

# Plot 4x4 grid
plot([Gray.(x_img_reshape[:, :, 1, i]) for i in 1:16]..., layout=(4, 4), size=(400, 400))
```

### 5.3 è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

#### 5.3.1 ç†è«–ãƒã‚§ãƒƒã‚¯

:::details **Q1: Change of Variableså…¬å¼**

> $X = f(Z)$, $f$ å¯é€†ã€‚$p_X(x)$ ã‚’ $p_Z$ ã¨ $f$ ã§è¡¨ã›ã€‚

**è§£ç­”**: $p_X(x) = p_Z(f^{-1}(x)) \left| \det \frac{\partial f^{-1}}{\partial x} \right| = p_Z(z) \left| \det \frac{\partial f}{\partial z} \right|^{-1}$
:::

:::details **Q2: Coupling Layerãƒ¤ã‚³ãƒ“ã‚¢ãƒ³**

> $x_{1:d} = z_{1:d}$, $x_{d+1:D} = z_{d+1:D} \odot \exp(s(z_{1:d})) + t(z_{1:d})$ã€‚$\log |\det J|$ = ?

**è§£ç­”**: $\log |\det J| = \sum_{i=1}^{D-d} s_i(z_{1:d})$ (ä¸‹ä¸‰è§’ãƒ–ãƒ­ãƒƒã‚¯è¡Œåˆ—ã®å¯¾è§’æˆåˆ†ã®ç©)
:::

:::details **Q3: CNFå¯†åº¦å¤‰åŒ–**

> $\frac{dz}{dt} = f(z, t)$ã€‚$\frac{\partial \log p(z(t))}{\partial t}$ = ?

**è§£ç­”**: $\frac{\partial \log p(z(t))}{\partial t} = -\text{tr}\left(\frac{\partial f}{\partial z}\right)$ (Liouvilleã®å®šç†)
:::

:::details **Q4: Hutchinson trace**

> $\text{tr}(A)$ ã‚’æœŸå¾…å€¤ã§ã€‚

**è§£ç­”**: $\text{tr}(A) = \mathbb{E}_{\epsilon \sim \mathcal{N}(0,I)}[\epsilon^T A \epsilon]$
:::

:::details **Q5: Flow vs VAE vs GANå°¤åº¦**

**è§£ç­”**:
- Flow: å³å¯† $\log p(x) = \log p(z) - \log |\det J|$
- VAE: è¿‘ä¼¼ ELBO $\leq \log p(x)$
- GAN: ä¸æ˜ (æš—é»™çš„)
:::

#### 5.3.2 å®Ÿè£…ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] Forward: $z \to x$ å®Ÿè¡Œ
- [ ] Inverse: $x \to z$ å†æ§‹æˆèª¤å·® < 1e-5
- [ ] $\log |\det J|$ ãŒ O(D)
- [ ] $\log p(x)$ æ•°å€¤çš„ã«æ­£ã—ã„
- [ ] è¨“ç·´ã§NLLæ¸›å°‘
- [ ] ç”Ÿæˆã‚µãƒ³ãƒ—ãƒ«ãŒåˆ†å¸ƒã«è¿‘ã„
- [ ] å¯†åº¦ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ãŒãƒ‡ãƒ¼ã‚¿ã¨ä¸€è‡´

:::message
**é€²æ—: 85% å®Œäº†** 2D/MNISTå®Ÿé¨“å®Œäº†ã€‚è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆçµ‚äº†ã€‚æ¬¡ã¯ç™ºå±•ã‚¾ãƒ¼ãƒ³ â€” Flow Matching/JKO scheme/æœ€æ–°ç ”ç©¶ã€‚
:::

---

## Zone 6: ğŸ“ æŒ¯ã‚Šè¿”ã‚Š + çµ±åˆã‚¾ãƒ¼ãƒ³ï¼ˆ30minï¼‰

:::message
**Zone 6ã®ç›®çš„**: Flowã¨Diffusionã®çµ±ä¸€ç†è«–ã§ã‚ã‚‹**Flow Matching**ã‚’ç†è§£ã—ã€JKOã‚¹ã‚­ãƒ¼ãƒ ã®æ•°ç†åŸºç›¤ã‚’å­¦ã¶ã€‚2024-2026ã®æœ€æ–°ç ”ç©¶å‹•å‘ã‚’æŠŠæ¡ã—ã€Normalizing Flowã®æœªæ¥ã‚’å±•æœ›ã™ã‚‹ã€‚
:::

### 6.1 Flow Matching: Flowã¨Diffusionã®çµ±ä¸€

#### 6.1.1 Flow Matchingã®å‹•æ©Ÿ

**å•é¡Œ**: CNF/FFJORDã¯å¼·åŠ›ã ãŒã€ä»¥ä¸‹ã®èª²é¡ŒãŒã‚ã‚‹:

1. **å°¤åº¦è¨ˆç®—ã‚³ã‚¹ãƒˆ**: Hutchinson trace estimatorã¯åˆ†æ•£ãŒå¤§ããä¸å®‰å®š
2. **ODEã‚½ãƒ«ãƒãƒ¼ã®é…ã•**: æ¨è«–æ™‚ã«RK45ãªã©å¤šæ®µæ³•ãŒå¿…è¦
3. **è¨“ç·´ã®ä¸å®‰å®šæ€§**: $\text{tr}(\partial f/\partial z)$ ã®å­¦ç¿’ãŒé›£ã—ã„

**è§£æ±ºç­–**: Flow Matchingã¯ã€Œãƒ™ã‚¯ãƒˆãƒ«å ´ $v_t(x)$ ã‚’**ç›´æ¥å›å¸°**ã€ã™ã‚‹æ–°ã—ã„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€‚

#### 6.1.2 Flow Matchingå®šå¼åŒ–

**å®šç¾©**: ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p_1(x)$ ã¨ãƒã‚¤ã‚ºåˆ†å¸ƒ $p_0(z)$ ã‚’çµã¶**ç¢ºç‡ãƒ‘ã‚¹** $p_t(x)$ ã‚’è€ƒãˆã‚‹ã€‚

$$
p_t(x) = \int p_t(x|x_1) p_1(x_1) dx_1
$$

ã“ã“ã§ $p_t(x|x_1)$ ã¯**æ¡ä»¶ä»˜ãç¢ºç‡ãƒ‘ã‚¹**(ä¾‹: Gaussianãƒ–ãƒ©ãƒ¼):

$$
p_t(x|x_1) = \mathcal{N}(x; (1-t)x_1 + t \mu, \sigma_t^2 I)
$$

**ç›®æ¨™**: ã“ã® $p_t(x)$ ã‚’ç”Ÿæˆã™ã‚‹**ãƒ™ã‚¯ãƒˆãƒ«å ´** $v_t(x)$ ã‚’å­¦ç¿’ã™ã‚‹:

$$
\frac{dx}{dt} = v_t(x), \quad x(0) \sim p_0, \quad x(1) \sim p_1
$$

#### 6.1.3 Conditional Flow Matching (CFM) æå¤±

**ç›´æ¥å­¦ç¿’ã¯å›°é›£**: $p_t(x)$ ã¯é™°çš„ã«ã—ã‹å®šç¾©ã•ã‚Œã¦ã„ãªã„ã€‚

**è§£æ±º**: **æ¡ä»¶ä»˜ããƒ™ã‚¯ãƒˆãƒ«å ´** $u_t(x|x_1)$ ã‚’ä½¿ã†:

$$
u_t(x|x_1) = \frac{d}{dt} \mathbb{E}_{p_t(x|x_1)}[x] = \frac{t x_1 + (1-t)\mu - x}{\sigma_t^2}
$$

**CFMæå¤±**:

$$
\mathcal{L}_{\text{CFM}}(\theta) = \mathbb{E}_{t \sim U[0,1], x_1 \sim p_1, x \sim p_t(\cdot|x_1)} \left[ \| v_t(x; \theta) - u_t(x|x_1) \|^2 \right]
$$

**é‡è¦æ€§è³ª**: ã“ã®æå¤±ã‚’æœ€å°åŒ–ã™ã‚‹ã¨ $v_t(x) \to \nabla \log p_t(x)$ (ã‚¹ã‚³ã‚¢é–¢æ•°) ã«åæŸã™ã‚‹!

#### 6.1.4 Flow Matching vs CNF vs Diffusion

| æ‰‹æ³• | ãƒ™ã‚¯ãƒˆãƒ«å ´ | æå¤± | å°¤åº¦ | æ¨è«–é€Ÿåº¦ |
|------|------------|------|------|----------|
| **CNF** | $f(z,t)$ (Neural ODE) | NLL + trace(Jacobian) | å³å¯† | é…ã„ (ODE) |
| **FFJORD** | $f(z,t)$ | NLL + Hutchinson | å³å¯† | é…ã„ (ODE) |
| **Flow Matching** | $v_t(x)$ | MSEå›å¸° $\|\|v_t - u_t\|\|^2$ | ä¸è¦ | é€Ÿã„ (1-stepå¯) |
| **DDPM** | $\epsilon_\theta(x_t, t)$ | MSEå›å¸° $\|\|\epsilon - \epsilon_\theta\|\|^2$ | ä¸è¦ | é€Ÿã„ (å°‘ã‚¹ãƒ†ãƒƒãƒ—) |

**çµè«–**: Flow Matchingã¯CNFã®ã€Œå°¤åº¦è¨ˆç®—ã‚’æ¨ã¦ã¦å›å¸°ã«ç‰¹åŒ–ã€ã—ãŸã‚‚ã®ã€‚Diffusionã¨æ•°å­¦çš„ã«ç­‰ä¾¡[^8]ã€‚

#### 6.1.5 Flow Matchingå®Ÿè£… (Julia/Lux)

```julia
# Conditional Flow Matching training
using Lux, Random, Optimisers, Zygote

# Vector field network
vnet = Chain(
    Dense(2 => 64, relu),
    Dense(64 => 128, relu),
    Dense(128 => 64, relu),
    Dense(64 => 2)  # Output: velocity field
)

ps, st = Lux.setup(Xoshiro(42), vnet)

# CFM loss
function cfm_loss(ps, st, x1_batch)
    t = rand(Float32, 1, size(x1_batch, 2))  # Uniform t âˆˆ [0,1]
    Î¼ = zeros(Float32, 2, size(x1_batch, 2))  # Prior mean
    Ïƒ_t = 0.1f0 .* (1.0f0 .- t)  # Noise schedule

    # Sample x_t from conditional path
    Îµ = randn(Float32, size(x1_batch))
    x_t = (1.0f0 .- t) .* x1_batch .+ t .* Î¼ .+ Ïƒ_t .* Îµ

    # Target conditional velocity
    u_t = (x1_batch .- x_t) ./ (Ïƒ_t.^2 .+ 1f-6)

    # Predict velocity
    v_t, st_new = vnet(x_t, ps, st)

    # MSE loss
    loss = mean((v_t .- u_t).^2)
    return loss, st_new
end

# Training loop
opt = Adam(1f-3)
opt_state = Optimisers.setup(opt, ps)

for epoch in 1:1000
    x1_batch = sample_data(256)  # Your data sampler

    (loss, st), back = Zygote.pullback(ps -> cfm_loss(ps, st, x1_batch), ps)
    grads = back((one(loss), nothing))[1]

    opt_state, ps = Optimisers.update(opt_state, ps, grads)

    if epoch % 100 == 0
        println("Epoch $epoch: Loss = $(loss)")
    end
end

# Sampling via ODE solve (Euler method)
function sample_flow_matching(vnet, ps, st, n_samples, n_steps=100)
    x = randn(Float32, 2, n_samples)  # Start from N(0,I)
    dt = 1.0f0 / n_steps

    for step in 1:n_steps
        t = step * dt
        v, _ = vnet(x, ps, st)
        x = x .+ dt .* v  # Euler step
    end

    return x
end

samples = sample_flow_matching(vnet, ps, st, 1000)
```

**ãƒã‚¤ãƒ³ãƒˆ**:
- **æå¤±é–¢æ•°ã¯å˜ç´”ãªå›å¸°**: $\|\|v_t - u_t\|\|^2$ ã®ã¿
- **å°¤åº¦è¨ˆç®—ãªã—**: traceã‚‚ä¸è¦
- **ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¯é«˜é€Ÿ**: å°‘ãªã„ã‚¹ãƒ†ãƒƒãƒ—æ•°ã§OK (10-50ã‚¹ãƒ†ãƒƒãƒ—)
- **Diffusionã¨ç­‰ä¾¡**: DDPMã® $\epsilon_\theta$ ã‚’ãƒ™ã‚¯ãƒˆãƒ«å ´ $v_t$ ã«å¤‰æ›ã—ãŸã ã‘

### 6.2 JKOã‚¹ã‚­ãƒ¼ãƒ : Wassersteinå‹¾é…æµã®è¦–ç‚¹

#### 6.2.1 JKOã‚¹ã‚­ãƒ¼ãƒ ã¨ã¯

**Jordan-Kinderlehrer-Otto (JKO) ã‚¹ã‚­ãƒ¼ãƒ **ã¯ã€ç¢ºç‡åˆ†å¸ƒã®æ™‚é–“ç™ºå±•ã‚’**Wassersteinè·é›¢ã®æœ€æ€¥é™ä¸‹**ã¨ã—ã¦å®šå¼åŒ–ã™ã‚‹æ çµ„ã¿[^9]ã€‚

**å•é¡Œè¨­å®š**: ã‚¨ãƒãƒ«ã‚®ãƒ¼æ±é–¢æ•° $\mathcal{F}[p]$ ã‚’æŒã¤åˆ†å¸ƒ $p_t$ ã®å‹¾é…æµ:

$$
\frac{\partial p_t}{\partial t} = -\nabla \cdot (p_t \nabla \frac{\delta \mathcal{F}}{\delta p})
$$

ã“ã‚Œã¯**Fokker-Planckæ–¹ç¨‹å¼**ã¨å‘¼ã°ã‚Œã‚‹ã€‚

#### 6.2.2 JKOã‚¹ã‚­ãƒ¼ãƒ ã®é›¢æ•£åŒ–

**JKOã‚¹ã‚­ãƒ¼ãƒ ã®å®šç¾©**: æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ— $\tau$ ã§ä»¥ä¸‹ã‚’ç¹°ã‚Šè¿”ã™:

$$
p_{k+1} = \arg\min_{p} \left\{ \mathcal{F}[p] + \frac{1}{2\tau} W_2^2(p, p_k) \right\}
$$

ã“ã“ã§ $W_2(p, q)$ ã¯**2-Wassersteinè·é›¢**:

$$
W_2^2(p, q) = \inf_{\pi \in \Pi(p,q)} \int \|x - y\|^2 d\pi(x,y)
$$

**è§£é‡ˆ**: $p_{k+1}$ ã¯ã€Œã‚¨ãƒãƒ«ã‚®ãƒ¼ $\mathcal{F}$ ã‚’æ¸›ã‚‰ã—ã¤ã¤ã€$p_k$ ã‹ã‚‰é ã–ã‹ã‚Šã™ããªã„ã€ã¨ã„ã†åˆ¶ç´„æœ€é©åŒ–ã®è§£ã€‚

#### 6.2.3 Normalizing Flowã¨JKOã®é–¢ä¿‚

**ç™ºè¦‹**: Normalizing Flowã®å­¦ç¿’ã¯**é›¢æ•£JKOã‚¹ã‚­ãƒ¼ãƒ **ã¨è¦‹ãªã›ã‚‹[^10]!

**å¯¾å¿œé–¢ä¿‚**:

| JKOã‚¹ã‚­ãƒ¼ãƒ  | Normalizing Flow |
|-------------|-------------------|
| ã‚¨ãƒãƒ«ã‚®ãƒ¼ $\mathcal{F}[p]$ | NLL $-\log p(x)$ |
| Wassersteinè·é›¢ $W_2(p, q)$ | Flowå¤‰æ›ã®æ­£å‰‡åŒ– |
| æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ— $\tau$ | å­¦ç¿’ç‡ $\eta$ |
| å‹¾é…æµ $\frac{\partial p}{\partial t}$ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–° $\frac{d\theta}{dt}$ |

**è¨¼æ˜ã®ã‚¹ã‚±ãƒƒãƒ**:

1. Flowã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã‚’å°‘ã—å‹•ã‹ã™: $\theta \to \theta + \Delta\theta$
2. ã“ã‚Œã¯åˆ†å¸ƒ $p_\theta(x)$ ã‚’å¤‰åŒ–ã•ã›ã‚‹: $p_\theta \to p_{\theta + \Delta\theta}$
3. ã“ã®å¤‰åŒ–é‡ã¯ $W_2$ è·é›¢ã§æ¸¬ã‚Œã‚‹
4. NLLã‚’æ¸›ã‚‰ã™æ–¹å‘ã« $\theta$ ã‚’å‹•ã‹ã™ã¨ã€JKOã‚¹ã‚­ãƒ¼ãƒ ã®æ›´æ–°å¼ã¨ä¸€è‡´

**çµè«–**: Normalizing Flowã®è¨“ç·´ã¯ã€ŒWassersteinç©ºé–“ä¸Šã®å‹¾é…é™ä¸‹æ³•ã€ã§ã‚ã‚‹ã€‚

#### 6.2.4 å®Ÿç”¨çš„æ„ç¾©

**1. åæŸä¿è¨¼**: JKOç†è«–ã«ã‚ˆã‚Šã€Flowã®è¨“ç·´ãŒã€Œã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’å˜èª¿æ¸›å°‘ã•ã›ã‚‹ã€ã“ã¨ãŒä¿è¨¼ã•ã‚Œã‚‹ã€‚

**2. æœ€é©è¼¸é€ã¨ã®æ¥ç¶š**: Optimal Transportç†è«–ãŒFlowã®è¨­è¨ˆã«ä½¿ãˆã‚‹:
   - **Monge-AmpÃ¨reæ–¹ç¨‹å¼**: æœ€é©è¼¸é€ã®è§£ã¯å‡¸é–¢æ•° $\phi$ ã®å‹¾é… $\nabla \phi$
   - **Brenierå®šç†**: æœ€é©è¼¸é€å†™åƒã¯ä¸€æ„ã«å­˜åœ¨
   - **Coupling Layerã®æ­£å½“åŒ–**: $x = T(z)$ ã¯æœ€é©è¼¸é€å†™åƒã®é›¢æ•£è¿‘ä¼¼

**3. Flowã¨Diffusionã®çµ±ä¸€**: ä¸¡è€…ã¨ã‚‚ã€ŒWassersteinå‹¾é…æµã®é›¢æ•£åŒ–ã€ã¨ã—ã¦ç†è§£ã§ãã‚‹:
   - **Flow**: æ±ºå®šè«–çš„ãªçµŒè·¯ (ODEã‚½ãƒ«ãƒãƒ¼)
   - **Diffusion**: ç¢ºç‡çš„ãªçµŒè·¯ (SDEã‚½ãƒ«ãƒãƒ¼)

### 6.3 æœ€æ–°ç ”ç©¶å‹•å‘ (2024-2026)

#### 6.3.1 Flow Matching ã®ç™ºå±•

**Stochastic Interpolants (2023-2024)**[^11]:
- Flow Matchingã‚’SDEã«æ‹¡å¼µ
- Diffusionã¨Flowã®ä¸­é–“çš„ãªæ‰‹æ³•
- æ¨è«–æ™‚ã«ãƒã‚¤ã‚ºæ³¨å…¥ã§å¤šæ§˜æ€§å‘ä¸Š

**Rectified Flow (2024)**[^12]:
- ã€Œæ›²ãŒã£ãŸFlowã€ã‚’ã€Œç›´ç·šçš„ãªFlowã€ã«ä¿®æ­£
- 1-stepã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒå¯èƒ½ã«
- Distillationæ‰‹æ³•ã¨ã—ã¦æ³¨ç›®

**Policy Flow (2024)**:
- å¼·åŒ–å­¦ç¿’ã¨Flowã®èåˆ
- æ–¹ç­– $\pi(a|s)$ ã‚’Flowã§ãƒ¢ãƒ‡ãƒ«åŒ–
- é€£ç¶šè¡Œå‹•ç©ºé–“ã®åŠ¹ç‡çš„æ¢ç´¢

#### 6.3.2 é«˜é€ŸåŒ–ãƒ»åŠ¹ç‡åŒ–

**Consistency Models (2023)**[^13]:
- Diffusionã®è’¸ç•™ã«ã‚ˆã‚Š1-stepã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿç¾
- Flowã«ã‚‚ConsistencyåŸç†ã‚’é©ç”¨å¯èƒ½
- æ¨è«–é€Ÿåº¦100å€ä»¥ä¸Šã®é«˜é€ŸåŒ–

**Latent Diffusion/Flow (2024)**:
- ç”»åƒã‚’æ½œåœ¨ç©ºé–“ $z$ ã«åœ§ç¸®ã—ã¦ã‹ã‚‰Flow/Diffusion
- Stable Diffusion 3.0ã¯Flow Matchingãƒ™ãƒ¼ã‚¹
- è¨ˆç®—é‡ã‚’1/10ä»¥ä¸‹ã«å‰Šæ¸›

**Continuous Normalizing Flows with Adjoint (2024)**:
- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®æ”¹å–„ (O(1) ãƒ¡ãƒ¢ãƒª)
- ã‚ˆã‚Šæ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å­¦ç¿’ãŒå¯èƒ½
- Physics-Informed CNFã¸ã®å¿œç”¨

#### 6.3.3 å¿œç”¨åˆ†é‡ã®æ‹¡å¤§

**1. ã‚¿ãƒ³ãƒ‘ã‚¯è³ªæ§‹é€ äºˆæ¸¬**:
- AlphaFold3 (2024) ã¯Flow-based
- åŸå­åº§æ¨™ã®åŒæ™‚åˆ†å¸ƒã‚’å­¦ç¿’
- Diffusion/Flowãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰

**2. åˆ†å­ç”Ÿæˆ**:
- SE(3)-equivariant Flow
- å›è»¢ãƒ»ä¸¦é€²ä¸å¤‰æ€§ã‚’æŒã¤Flow
- è–¬å‰¤å€™è£œã®è‡ªå‹•è¨­è¨ˆ

**3. æ™‚ç³»åˆ—äºˆæ¸¬**:
- Temporal Normalizing Flow
- ä¸è¦å‰‡ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚ç³»åˆ—ã®å‡¦ç†
- Neural ODE + Flowã®èåˆ

**4. å› æœæ¨è«–**:
- Causal Normalizing Flow
- ä»‹å…¥åˆ†å¸ƒ $p(y|do(x))$ ã®å­¦ç¿’
- åäº‹å®Ÿæ¨è«–ã¸ã®å¿œç”¨

#### 6.3.4 ç†è«–çš„é€²å±•

**Universal Approximation of Flows (2024)**:
- Coupling Layer ã®ç†è«–çš„ä¿è¨¼å¼·åŒ–
- æœ‰é™å¹…ã§ã‚‚ universal approximation å¯èƒ½
- å¿…è¦å±¤æ•°ã®ä¸Šç•Œå°å‡º

**Flow Matching = Diffusion ã®å³å¯†è¨¼æ˜ (2024)**:
- CFMæå¤±ã¨DDPMæå¤±ãŒæœ¬è³ªçš„ã«åŒä¸€
- ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla \log p_t$ ã¸ã®åæŸä¿è¨¼
- åæŸé€Ÿåº¦ã®è§£æ

**Wasserstein Gradient Flow ã®é›¢æ•£åŒ–èª¤å·® (2025)**:
- JKOã‚¹ã‚­ãƒ¼ãƒ ã®æ•°å€¤è§£æ
- æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ— $\tau$ ã«å¯¾ã™ã‚‹èª¤å·® $O(\tau^2)$ ã®è¨¼æ˜
- é©å¿œçš„ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºã®è¨­è¨ˆæŒ‡é‡

:::message alert
**Zone 6 å®Œäº†**: Flow Matchingã®æ•°ç†ã€JKOã‚¹ã‚­ãƒ¼ãƒ ã€2024-2026æœ€æ–°ç ”ç©¶ã‚’ç¶²ç¾…ã€‚æ¬¡ã¯**æŒ¯ã‚Šè¿”ã‚Šçµ±åˆ**ã§å…¨ä½“ã‚’ã¾ã¨ã‚ã‚‹ã€‚
:::

---

## Zone 6: ğŸ“ æŒ¯ã‚Šè¿”ã‚Š + çµ±åˆã‚¾ãƒ¼ãƒ³ â€” FAQ & Next Steps (30min)

### 7.1 æœ¬è¬›ç¾©ã§é”æˆã—ãŸã“ã¨

**æ•°å­¦çš„ç†è§£ (Zone 3)**:

âœ… **Change of Variableså…¬å¼ã®å®Œå…¨å°å‡º**
- 1æ¬¡å…ƒ â†’ å¤šæ¬¡å…ƒ â†’ åˆæˆå¤‰æ›
- ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—å¼ã®æ„å‘³: ä½“ç©è¦ç´ ã®å¤‰åŒ–ç‡
- $\log p(x) = \log p(z) - \log |\det J_f|$ ã®å³å¯†ãªè¨¼æ˜

âœ… **Coupling Layerã®ç†è«–**
- ä¸‰è§’è¡Œåˆ—æ§‹é€ ã§ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¨ˆç®—ã‚’ O(DÂ³) â†’ O(D) ã«å‰Šæ¸›
- Affine Coupling Layer (RealNVP)
- Multi-scale architecture

âœ… **Glowã®é©æ–°**
- Actnorm (Batch Normã®å¯é€†ç‰ˆ)
- 1Ã—1 Invertible Convolution
- LUåˆ†è§£ã«ã‚ˆã‚‹ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¨ˆç®—ã®åŠ¹ç‡åŒ–

âœ… **Continuous Normalizing Flows**
- Instantaneous Change of Variables: $\frac{\partial \log p(z(t))}{\partial t} = -\text{tr}(\frac{\partial f}{\partial z})$
- Neural ODE: é›¢æ•£å±¤ â†’ é€£ç¶šæ™‚é–“ODE
- Adjoint Method: ãƒ¡ãƒ¢ãƒª O(1) ã®é€†ä¼æ’­

âœ… **FFJORD**
- Hutchinson traceæ¨å®š: O(DÂ²) â†’ O(D)
- Vector-Jacobian Product (VJP) ã«ã‚ˆã‚‹åŠ¹ç‡çš„è¨ˆç®—
- $\text{tr}(A) = \mathbb{E}[\mathbf{v}^\top A \mathbf{v}]$

**å®Ÿè£…åŠ› (Zone 4-5)**:

âœ… **Julia + Lux.jl ã§ã®RealNVPå®Œå…¨å®Ÿè£…**
- Affine Coupling Layer
- å¤šå±¤Flow modelã®æ§‹ç¯‰
- è¨“ç·´ãƒ«ãƒ¼ãƒ— (negative log likelihoodæœ€å°åŒ–)
- 2D Moons dataset ã§ã®å®Ÿé¨“

âœ… **CNF/FFJORDã®æ§‹é€ ç†è§£**
- DifferentialEquations.jl + ODE solver
- Hutchinson trace estimatorå®Ÿè£…
- Neural ODE dynamics

âœ… **å®Ÿé¨“ã«ã‚ˆã‚‹æ¤œè¨¼**
- å¯†åº¦æ¨å®šç²¾åº¦: Flow vs VAEæ¯”è¼ƒ (å³å¯†å°¤åº¦ vs ELBO)
- Out-of-Distributionæ¤œçŸ¥: 95%+ ç²¾åº¦
- ç”Ÿæˆå“è³ªã®è©•ä¾¡

**ç†è«–çš„å±•æœ› (Zone 2, 6)**:

âœ… **Course IVå…¨ä½“åƒã®æŠŠæ¡**
- NF â†’ EBM â†’ Score â†’ DDPM â†’ SDE â†’ Flow Matching â†’ LDM â†’ Consistency â†’ World Models â†’ çµ±ä¸€ç†è«–
- 10è¬›ç¾©ã®è«–ç†çš„ãƒã‚§ãƒ¼ãƒ³

âœ… **VAE/GAN/Flowã®3ã¤å·´**
- å°¤åº¦: è¿‘ä¼¼ (VAE) / æš—é»™çš„ (GAN) / **å³å¯† (Flow)**
- è¨“ç·´å®‰å®šæ€§ãƒ»ç”Ÿæˆå“è³ªãƒ»ç”¨é€”ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

âœ… **Flow Matchingã¸ã®æ©‹æ¸¡ã—**
- Probability Flow ODE (PF-ODE)
- Rectified Flow: ç›´ç·šè¼¸é€
- Optimal Transportè¦–ç‚¹ã§ã®çµ±ä¸€
- æœ€æ–°ç ”ç©¶: TarFlow, Stable Diffusion 3, Flux.1

**åˆ°é”ãƒ¬ãƒ™ãƒ«**:

- **åˆç´š â†’ ä¸­ç´šçªç ´**: Change of Variablesã®æ•°å­¦ã‚’å®Œå…¨ç†è§£
- **å®Ÿè£…åŠ›**: Lux.jlã§å‹•ãFlowã‚’è‡ªåŠ›ã§æ›¸ã‘ã‚‹
- **ç†è«–çš„æ´å¯Ÿ**: Flowã®é™ç•Œã¨Flow Matchingã¸ã®é€²åŒ–ã‚’ç†è§£
- **æ¬¡ã¸ã®æº–å‚™**: ç¬¬37-38å› (SDE/ODE, Flow Matching) ã¸ã®åœŸå°å®Œæˆ

### 7.2 ã‚ˆãã‚ã‚‹è³ªå• (FAQ)

#### Q1: Normalizing Flowsã€çµå±€å®Ÿå‹™ã§ä½¿ã‚ã‚Œã¦ã„ã‚‹ã®ï¼Ÿ

**A**: **2026å¹´ç¾åœ¨ã€å¾©æ´»ã—ã¤ã¤ã‚ã‚‹** (Flow MatchingçµŒç”±)ã€‚

**ç”¨é€”åˆ¥ã®ç¾çŠ¶**:

| ç”¨é€” | ä¸»æµæ‰‹æ³• | Flowã®å½¹å‰² | å®Ÿä¾‹ |
|:-----|:--------|:----------|:-----|
| **ç”»åƒç”Ÿæˆ (å“è³ªé‡è¦–)** | Diffusion | Flow Matchingã¨ã—ã¦å¾©æ´» | Stable Diffusion 3, Flux.1 |
| **ç”»åƒç”Ÿæˆ (é€Ÿåº¦é‡è¦–)** | GAN / Consistency | Rectified FlowãŒç«¶åˆ | 10-50 stepsç”Ÿæˆ |
| **å¯†åº¦æ¨å®š** | **Normalizing Flow** | ä»–æ‰‹æ³•ã§ã¯ä¸å¯èƒ½ | é‡‘èãƒªã‚¹ã‚¯ã€ç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ |
| **ç•°å¸¸æ¤œçŸ¥ (OOD)** | **Normalizing Flow** | å³å¯†ãª $\log p(x)$ ãŒå¿…é ˆ | è£½é€ æ¥­ã€åŒ»ç™‚ç”»åƒ |
| **å¤‰åˆ†æ¨è«–** | IAF (Flow) + VAE | äº‹å¾Œåˆ†å¸ƒè¿‘ä¼¼ | ãƒ™ã‚¤ã‚ºæ·±å±¤å­¦ç¿’ |
| **æ½œåœ¨ç©ºé–“æ­£å‰‡åŒ–** | Flow + VAE / Flow + Diffusion | è¡¨ç¾å­¦ç¿’å¼·åŒ– | disentangled representation |

**æ­´å²çš„æ¨ç§»**:
- **2016-2019**: RealNVP, Glowå…¨ç›› â€” ã€Œæ¬¡ä¸–ä»£ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã€ã¨ã—ã¦æ³¨ç›®
- **2020-2022**: DDPM, Stable Diffusionã®å°é ­ â€” Flowã¯ä¸€æ™‚ä¸‹ç«
- **2023-2026**: Flow Matchingç™»å ´ â€” ç†è«–ã¨å®Ÿè£…ã®èåˆã§**å¾©æ´»**

**çµè«–**: ç”Ÿæˆå“è³ªã§ã¯Diffusionã«ä¸€åº¦æ•—åŒ— â†’ Flow Matchingã§æ•°å­¦çš„åŸºç›¤ã‚’ä¿ã¡ã¤ã¤å®Ÿç”¨æ€§ã‚’å–ã‚Šæˆ»ã—ãŸã€‚

#### Q2: RealNVP vs Glow vs FFJORDã€ã©ã‚Œã‚’é¸ã¶ã¹ãï¼Ÿ

| è¦³ç‚¹ | RealNVP | Glow | FFJORD/CNF |
|:-----|:--------|:-----|:-----------|
| **å®Ÿè£…é›£æ˜“åº¦** | â˜…â˜†â˜† (æœ€ã‚‚ç°¡å˜) | â˜…â˜…â˜† (1Ã—1 Convè¤‡é›‘) | â˜…â˜…â˜… (ODE solverå¿…è¦) |
| **è¨“ç·´é€Ÿåº¦** | é€Ÿã„ | é€Ÿã„ | é…ã„ (ODEç©åˆ†) |
| **æ¨è«–é€Ÿåº¦** | æœ€é€Ÿ (~5ms/100 samples) | é€Ÿã„ (~10ms) | é…ã„ (~50ms) |
| **è¡¨ç¾åŠ›** | ä¸­ (Couplingåˆ¶ç´„) | é«˜ (1Ã—1 Conv) | **æœ€é«˜** (åˆ¶ç´„ãªã—) |
| **ãƒ¡ãƒ¢ãƒª** | O(KÂ·D) | O(KÂ·D) | O(1) (Adjoint) |
| **ç”¨é€”** | ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã€OODæ¤œçŸ¥ | é«˜å“è³ªç”Ÿæˆ | ç ”ç©¶ã€è¤‡é›‘åˆ†å¸ƒ |

**æ¨å¥¨ãƒ•ãƒ­ãƒ¼**:
1. **ã¾ãšRealNVP** â†’ ã‚·ãƒ³ãƒ—ãƒ«ã€å®Ÿè£…100è¡Œã€ãƒ‡ãƒãƒƒã‚°å®¹æ˜“
2. **ä¸è¶³ãªã‚‰Glow** â†’ 1Ã—1 Convã§è¡¨ç¾åŠ›å‘ä¸Šã€multi-scale
3. **ã•ã‚‰ã«å¿…è¦ãªã‚‰FFJORD** â†’ åˆ¶ç´„ãªã—ã€Flow Matchingã¸ã®æ‹¡å¼µå®¹æ˜“

**å®Ÿå‹™**: 95%ã®ã‚±ãƒ¼ã‚¹ã¯RealNVPã§ååˆ†ã€‚ç ”ç©¶ãƒ»PoC ãªã‚‰FFJORDã€‚

#### Q3: ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—å¼ã€æœ¬å½“ã« O(D) ã§æ¸ˆã‚€ã®ï¼Ÿ

**A**: **Coupling Layerã«é™ã‚Šã€ã¯ã„**ã€‚

**è¨ˆç®—é‡ã®å†…è¨³**:

| æ‰‹æ³• | ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³æ§‹é€  | $\det$ è¨ˆç®—é‡ | ç†ç”± |
|:-----|:-------------|:-------------|:-----|
| **ä¸€èˆ¬ã®å¯é€†è¡Œåˆ—** | å¯†è¡Œåˆ— | O(DÂ³) | LUåˆ†è§£ or å›ºæœ‰å€¤è¨ˆç®— |
| **ä¸‰è§’è¡Œåˆ—** | ä¸Š/ä¸‹ä¸‰è§’ | O(D) | å¯¾è§’è¦ç´ ã®ç© |
| **Coupling Layer** | ä¸‹ä¸‰è§’ãƒ–ãƒ­ãƒƒã‚¯ | O(D) | $\det = \det(I) \cdot \det(\text{diag}(\exp(s)))$ |
| **FFJORD (Hutchinson)** | traceæ¨å®š | O(D) | VJP 1å› (ç¢ºç‡çš„ã€åˆ†æ•£ã‚ã‚Š) |
| **Glow 1Ã—1 Conv** | CÃ—Cè¡Œåˆ— | O(CÂ³) | Cã¯å›ºå®š (â‰¤512)ã€ç”»åƒã‚µã‚¤ã‚ºéä¾å­˜ |

**æ³¨æ„ç‚¹**:
- Coupling Layerã¯**è§£æçš„** â†’ å³å¯†ã«O(D)ã€åˆ†æ•£ãªã—
- FFJORDã¯**ç¢ºç‡çš„æ¨å®š** â†’ æœŸå¾…å€¤ã¯O(D)ã€åˆ†æ•£ã‚ã‚Š (è¤‡æ•°ã‚µãƒ³ãƒ—ãƒ«ã§ç²¾åº¦å‘ä¸Šå¯èƒ½)
- Glow 1Ã—1 Convã¯ç”»åƒã®**ãƒãƒ£ãƒãƒ«æ•°Cã®ã¿**ã«ä¾å­˜ â†’ é«˜è§£åƒåº¦ã§ã‚‚O(CÂ³)

**çµè«–**: Coupling Layerã®ã€Œä¸‰è§’è¡Œåˆ—åŒ–ã€ãŒã€Flowã®å®Ÿç”¨åŒ–ã‚’å¯èƒ½ã«ã—ãŸå¤©æ‰çš„ã‚¢ã‚¤ãƒ‡ã‚¢ã€‚

#### Q4: CNFã¨Diffusionã®ODEã€ä½•ãŒé•ã†ã®ï¼Ÿ

**A**: è¨“ç·´æ–¹æ³•ã¨ç›®çš„ãŒç•°ãªã‚‹ãŒã€**æ•°å­¦çš„ã«ã¯åŒã˜æ çµ„ã¿** (ODE-based transport)ã€‚

| è¦³ç‚¹ | CNF (Normalizing Flow) | Diffusion (PF-ODE) |
|:-----|:----------------------|:------------------|
| **ç›®çš„** | ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p(x)$ ã‚’ç›´æ¥ãƒ¢ãƒ‡ãƒ«åŒ– | ãƒã‚¤ã‚ºé™¤å»éç¨‹ $p_t(x)$ ã‚’ãƒ¢ãƒ‡ãƒ«åŒ– |
| **è¨“ç·´** | æœ€å°¤æ¨å®š $\max \log p(x)$ | ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚° or ãƒã‚¤ã‚ºäºˆæ¸¬ $\epsilon_\theta$ |
| **ODEå½¢å¼** | $\frac{dz}{dt} = f(z, t)$ (ä»»æ„) | $\frac{dx}{dt} = f - \frac{1}{2} g^2 \nabla \log p_t$ (ã‚¹ã‚³ã‚¢ä¾å­˜) |
| **å°¤åº¦è¨ˆç®—** | å³å¯† (traceç©åˆ†) | å›°é›£ (å¤‰åˆ†ä¸‹ç•Œã®ã¿) |
| **ç”Ÿæˆå“è³ª** | ä¸­ç¨‹åº¦ | **SOTA** (ImageNet, SD) |
| **ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°** | 1-pass ODE | 10-1000 steps |
| **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£** | Couplingåˆ¶ç´„ (å¾“æ¥) | U-Net/Transformer (è‡ªç”±) |

**Flow Matchingã®æ´å¯Ÿ**:
- ã“ã®2ã¤ã¯**åŒã˜ODE frameworkã®ç•°ãªã‚‹è¨“ç·´æ–¹æ³•**
- CNF: ãƒ™ã‚¯ãƒˆãƒ«å ´ $f$ ã‚’ç›´æ¥å­¦ç¿’
- Diffusion (PF-ODE): ã‚¹ã‚³ã‚¢ $\nabla \log p_t$ ã‚’å­¦ç¿’ â†’ $f$ ã‚’å°å‡º
- Flow Matching: ä¸¡è€…ã‚’çµ±ä¸€ â€” æ¡ä»¶ä»˜ããƒ•ãƒ­ãƒ¼ $v_t(x_t | x_0)$ ã‚’å­¦ç¿’

**ç¬¬38å›ã§å®Œå…¨çµ±ä¸€** â€” Benamou-Brenierå…¬å¼ã€Wassersteinå‹¾é…æµã§å…¨ã¦ãŒç¹‹ãŒã‚‹ã€‚

#### Q5: Flowã®ã€Œå¯é€†æ€§ã€ã€çµå±€ä½•ãŒå¬‰ã—ã„ã®ï¼Ÿ

**A**: 3ã¤ã®æœ¬è³ªçš„åˆ©ç‚¹ã€‚

**1. å³å¯†ãª $\log p(x)$ è¨ˆç®—**
- VAE: ELBO (ä¸‹ç•Œ) â†’ çœŸã®å°¤åº¦ã¯ä¸æ˜
- GAN: å°¤åº¦è¨ˆç®—ä¸å¯ â†’ å¯†åº¦æ¨å®šä¸å¯èƒ½
- **Flow**: Change of Variables ã§å³å¯† â†’ ç•°å¸¸æ¤œçŸ¥ã€ãƒ¢ãƒ‡ãƒ«é¸æŠã€ãƒ™ã‚¤ã‚ºæ¨è«–ã§å¿…é ˆ

**2. åŒæ–¹å‘å¤‰æ›**
- ãƒ‡ãƒ¼ã‚¿ç©ºé–“ $x$ â†” æ½œåœ¨ç©ºé–“ $z$ ã®å¯é€†ãƒãƒƒãƒ”ãƒ³ã‚°
- **é †æ–¹å‘** ($z \to x$): ç”Ÿæˆ (ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°)
- **é€†æ–¹å‘** ($x \to z$): ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° (è¡¨ç¾å­¦ç¿’)
- ç”¨é€”: æ½œåœ¨ç©ºé–“ã§ã®è£œé–“ã€å±æ€§ç·¨é›†ã€ã‚¹ã‚¿ã‚¤ãƒ«è»¢ç§»

**3. è¨“ç·´ã®å®‰å®šæ€§**
- æœ€å°¤æ¨å®š (MLE) â†’ æ˜ç¢ºãªç›®çš„é–¢æ•°
- æ•µå¯¾çš„è¨“ç·´ä¸è¦ (GANã®ã‚ˆã†ãª mode collapse / ä¸å®‰å®šæ€§ãŒãªã„)
- åæŸæ€§ã®ç†è«–ä¿è¨¼

**ä»£å„Ÿ**:
- ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åˆ¶ç´„ (Coupling Layerã¯å…¥åŠ›ã®åŠåˆ†ã‚’ã‚³ãƒ”ãƒ¼ â†’ æƒ…å ±ãƒœãƒˆãƒ«ãƒãƒƒã‚¯)
- ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¨ˆç®—ã‚³ã‚¹ãƒˆ (Coupling/CNFã§ O(D) ã ãŒã€ä¾ç„¶ã¨ã—ã¦è¨ˆç®—å¿…è¦)

**Flow Matchingã®å†è§£é‡ˆ**:
- ã€Œå¯é€†æ€§ã€ã¯ç”Ÿæˆæ™‚ã®**çµŒè·¯ã®æ€§è³ª** (æ±ºå®šè«–çš„ODE)
- ã€Œå¯é€†æ€§ã€ã¯ãƒ¢ãƒ‡ãƒ«ã®**æ§‹é€ åˆ¶ç´„ã§ã¯ãªã„** (éå¯é€†ãƒ™ã‚¯ãƒˆãƒ«å ´ã‚’å­¦ç¿’å¯èƒ½)
- ODEã§ç©åˆ†ã™ã‚Œã°æ±ºå®šè«–çš„çµŒè·¯ â†’ å®Ÿè³ªçš„ã«ã€Œå¯é€†ã€

#### Q6: Course Iã®ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã€çµå±€ã“ã“ã§ä½•ã«ä½¿ã£ãŸï¼Ÿ

**A**: **å…¨ã¦ã®ç†è«–çš„åŸºç›¤**ã€‚

**å…·ä½“çš„ãªå¯¾å¿œ**:

| Course I (ç¬¬3-5å›) | æœ¬è¬›ç¾©ã§ã®ä½¿ç”¨ç®‡æ‰€ |
|:------------------|:----------------|
| **ç¬¬3å› æ¥µåº§æ¨™å¤‰æ›** | Zone 2.3 ã€Œåº§æ¨™å¤‰æ›ã€ã®æ¯”å–© â€” $p_{r,\theta} = p_{x,y} \cdot r$ |
| **ç¬¬4å› ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—** | Zone 3.1.2 å¤šæ¬¡å…ƒChange of Variables â€” $J_f = \frac{\partial \mathbf{f}}{\partial \mathbf{z}}$ |
| **ç¬¬4å› $\det$ ã®æ€§è³ª** | Zone 3.1.3 åˆæˆå¤‰æ› â€” $\det(AB) = \det(A) \det(B)$ |
| **ç¬¬4å› ç¢ºç‡å¤‰æ•°å¤‰æ›** | Zone 3.1 å®Œå…¨å°å‡º â€” $p_X(x) = p_Z(z) | \det J_f |^{-1}$ |
| **ç¬¬5å› ä¼Šè—¤ç©åˆ†ãƒ»SDE** | Zone 3.4.2 Instantaneous Change of Variables |
| **ç¬¬5å› å¸¸å¾®åˆ†æ–¹ç¨‹å¼** | Zone 4.2 CNF/FFJORDå®Ÿè£… (DifferentialEquations.jl) |

**ã€Œãªãœã‚ã‚“ãªæŠ½è±¡çš„ãªæ•°å­¦ã‚’...ã€ã®ç­”ãˆ**:
- Normalizing Flowsã®å³å¯†ãªå°å‡ºã«**ä¸å¯æ¬ **
- ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ãªã—ã§ã¯ $\log p(x)$ ã®è¨ˆç®—ä¸å¯èƒ½
- ç¬¬37-38å›ã§ã•ã‚‰ã«æ·±åŒ– (Fokker-Planckæ–¹ç¨‹å¼ã€JKOã‚¹ã‚­ãƒ¼ãƒ )

**æ¨å¥¨**: Course I ç¬¬3-5å›ã‚’å¾©ç¿’ã™ã‚‹ã¨ã€æœ¬è¬›ç¾©ãŒ**2å€ç†è§£ã§ãã‚‹**ã€‚ç‰¹ã«ç¬¬4å›ã€Œãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã¨ç¢ºç‡å¤‰æ•°å¤‰æ›ã€ã¯å¿…ä¿®ã€‚

#### Q7: ã€ŒFlow Matchingã§å¯é€†æ€§ä¸è¦ã€ãªã‚‰ã€ã‚‚ã†Flowã˜ã‚ƒãªã„ã®ã§ã¯ï¼Ÿ

**A**: **ç”¨èªã®å†å®šç¾©ãŒèµ·ãã¦ã„ã‚‹**ã€‚ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚·ãƒ•ãƒˆã®éæ¸¡æœŸã€‚

**ä¼çµ±çš„å®šç¾© (2014-2019)**:
- Normalizing Flow = å¯é€†å¤‰æ› $f_1, \ldots, f_K$ ã®åˆæˆ
- å¯é€†æ€§ = Flowã®**æœ¬è³ª** (Change of Variableså…¬å¼ã®å‰æ)
- $f^{-1}$ ãŒè¨ˆç®—å¯èƒ½ = å¿…é ˆæ¡ä»¶

**æ–°ã—ã„å®šç¾© (2022-)**:
- Flow = ãƒ™ã‚¯ãƒˆãƒ«å ´ $v_t(x)$ ã«ã‚ˆã‚‹**è¼¸é€ (transport)**
- ODE $\frac{dx}{dt} = v_t(x)$ ã§çµŒè·¯ã‚’å®šç¾©
- å¯é€†æ€§ = æ±ºå®šè«–çš„ODEã®**æ€§è³ª** (ãƒ¢ãƒ‡ãƒ«åˆ¶ç´„ã§ã¯ãªã„)

**çµ±ä¸€çš„è¦–ç‚¹ (Optimal Transport)**:
- ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p_0$ ã‹ã‚‰ãƒã‚¤ã‚ºåˆ†å¸ƒ $p_1$ ã¸ã®**æ¸¬åº¦ã®è¼¸é€**
- çµŒè·¯ = æ¸¬åº¦ã®æ™‚é–“ç™ºå±• (Continuity Equation)
- Wassersteinè·é›¢ã‚’æœ€å°åŒ– â† ç¬¬38å›ã§è©³èª¬

**è¨€è‘‰ã®æ•´ç†**:

| ç”¨èª | æ„å‘³ | æ–‡è„ˆ |
|:-----|:-----|:-----|
| **Normalizing Flow (ç‹­ç¾©)** | å¯é€†å¤‰æ›ã®åˆæˆ (RealNVP, Glow) | 2014-2019 |
| **Continuous Normalizing Flow** | Neural ODE-based Flow | 2018- |
| **Flow Matching** | ãƒ™ã‚¯ãƒˆãƒ«å ´å­¦ç¿’ (éå¯é€†OK) | 2022- |
| **Flow (åºƒç¾©)** | ODE-based transport å…¨èˆ¬ | ç¾åœ¨ã®çµ±ä¸€çš„ç†è§£ |

**çµè«–**:
- ã€ŒNormalizing Flowã€ã¨ã€ŒFlow Matchingã€ã¯**æ­´å²çš„ã«ã¯åˆ¥æ–‡è„ˆ**
- æ•°å­¦çš„ã«ã¯åŒã˜æ çµ„ã¿ (ODE-based transport)
- ç¬¬38å›ã§**å®Œå…¨çµ±ä¸€** â€” Optimal Transportè¦–ç‚¹ã§å…¨ã¦ãŒç¹‹ãŒã‚‹

**æ¯”å–©**: ã€ŒFlowã€ã¯ã€Œå·ã®æµã‚Œã€ã€‚å¾“æ¥ã¯ã€Œå¯é€†ãªæ°´è·¯ã€ã®ã¿æ‰±ã£ãŸã€‚Flow Matchingã¯ã€Œä»»æ„ã®ãƒ™ã‚¯ãƒˆãƒ«å ´ã«ã‚ˆã‚‹è¼¸é€ã€ã«ä¸€èˆ¬åŒ–ã€‚æœ¬è³ªã¯ã€Œæµã‚Œ (transport)ã€ãã®ã‚‚ã®ã€‚

#### Q8: å®Ÿè£…ã§æœ€ã‚‚è‹¦åŠ´ã™ã‚‹ãƒã‚¤ãƒ³ãƒˆã¯ï¼Ÿ

**A**: **3ã¤ã®è½ã¨ã—ç©´**ã€‚

**1. æ•°å€¤ä¸å®‰å®šæ€§**
- **å•é¡Œ**: $\exp(s)$ ãŒå¤§ãã™ãã‚‹ â†’ ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼
- **è§£æ±º**: $s$ ã‚’ `tanh` ã§ã‚¯ãƒªãƒƒãƒ— (Glowã®å®Ÿè£…)
  ```julia
  s = tanh(s_net(z1))  # [-1, 1] ã«åˆ¶é™
  ```

**2. é€†å¤‰æ›ã®æ¤œè¨¼**
- **å•é¡Œ**: $f^{-1}(f(z)) \neq z$ (å†æ§‹æˆèª¤å·®)
- **è§£æ±º**: ãƒ†ã‚¹ãƒˆã§æ¤œè¨¼
  ```julia
  z_recon = inverse(model, forward(model, z))
  @assert maximum(abs.(z - z_recon)) < 1e-5
  ```

**3. ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¨ˆç®—ã®ãƒã‚°**
- **å•é¡Œ**: $\log |\det J|$ ã®ç¬¦å·ãƒŸã‚¹ã€æ¬¡å…ƒé›†ç´„ãƒŸã‚¹
- **è§£æ±º**: å˜ç´”ãªã‚±ãƒ¼ã‚¹ (Affineå¤‰æ›) ã§æ‰‹è¨ˆç®—ã¨æ¯”è¼ƒ
  ```julia
  # Affine: f(z) = 2z + 1 â†’ log|det J| = log(2)
  @test log_det_jacobian â‰ˆ log(2.0)
  ```

**ãƒ‡ãƒãƒƒã‚°ã®ã‚³ãƒ„**:
- 1D â†’ 2D â†’ é«˜æ¬¡å…ƒã®é †ã§å®Ÿè£…
- å„å±¤ã®å‡ºåŠ›ã‚’å¯è¦–åŒ–
- RealNVPã‹ã‚‰å§‹ã‚ã€Glowã¯å¾Œå›ã—

#### Q9: Flowã‚’ä½¿ã£ãŸç•°å¸¸æ¤œçŸ¥ã€ã©ã†å®Ÿè£…ã™ã‚‹ï¼Ÿ

**A**: **3ã‚¹ãƒ†ãƒƒãƒ—**ã€‚

**Step 1: æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´**
```julia
# Normal data only
X_normal = load_normal_data()

# Train RealNVP
model = RealNVP(D, 6, 64)
ps, st = train_realnvp(model, X_normal; n_epochs=100)
```

**Step 2: é–¾å€¤è¨­å®š (Validation Set)**
```julia
# Compute log p(x) on validation set
log_p_val = eval_log_p(model, ps, st, X_val)

# Set threshold at 95th percentile
threshold = quantile(log_p_val, 0.05)  # Lower 5% = anomaly
```

**Step 3: æ¨è«–æ™‚ã®ç•°å¸¸åˆ¤å®š**
```julia
function is_anomaly(model, ps, st, x_test, threshold)
    log_p = eval_log_p(model, ps, st, x_test)
    return log_p < threshold
end

# Test
anomaly_flags = is_anomaly(model, ps, st, X_test, threshold)
```

**å®Ÿä¾‹ (Zone 5.4)**:
- 2D Moons (æ­£å¸¸) vs Uniform noise (ç•°å¸¸)
- Accuracy: 95-98%
- VAEã®ELBOã§ã¯é–¾å€¤è¨­å®šãŒå›°é›£ (Gapä¸æ˜)

**ç”£æ¥­å¿œç”¨**:
- è£½é€ æ¥­: ä¸è‰¯å“æ¤œçŸ¥
- åŒ»ç™‚: ç¨€ãªç–¾æ‚£ã®æ¤œå‡º
- ã‚µã‚¤ãƒãƒ¼ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£: ç•°å¸¸é€šä¿¡æ¤œçŸ¥

#### Q10: æ¬¡ã«å­¦ã¶ã¹ãã“ã¨ã¯ï¼Ÿ

**A**: **Course IV ã®è«–ç†çš„ãƒã‚§ãƒ¼ãƒ³ã‚’è¾¿ã‚‹**ã€‚

**æ¨å¥¨å­¦ç¿’é †**:

1. **ç¬¬34å› (EBM)** â€” æ­£è¦åŒ–å®šæ•° $Z$ ã®å›é¿
   - ãªãœ $p(x) = \frac{1}{Z} e^{-E(x)}$ ã‹ï¼Ÿ
   - Hopfield Network â†” Transformer Attention
   - Contrastive Divergence

2. **ç¬¬35å› (Score Matching)** â€” $\nabla \log p(x)$ ã®ã¿å­¦ç¿’
   - $Z$ ãŒæ¶ˆãˆã‚‹æ•°å­¦
   - Denoising Score Matching
   - Langevin MCMC

3. **ç¬¬37å› (SDE/ODE)** â€” é€£ç¶šæ‹¡æ•£ã®æ•°å­¦
   - VP-SDE, VE-SDE
   - ä¼Šè—¤ç©åˆ†ã€Fokker-Planckæ–¹ç¨‹å¼
   - **Probability Flow ODE** (Diffusion â†” Flowæ¥ç¶š)

4. **ç¬¬38å› (Flow Matching)** â€” **æœ€é‡è¦**
   - Optimal Transport
   - JKO scheme (Wassersteinå‹¾é…æµ)
   - **Flowã¨Diffusionã®æ•°å­¦çš„ç­‰ä¾¡æ€§ã®è¨¼æ˜**
   - Rectified Flowå®Ÿè£…

5. **ç¬¬36å› (DDPM)** â€” ãƒã‚¤ã‚ºé™¤å»ã®åå¾©
   - Forward/Reverse Markové€£é–
   - å¤‰åˆ†ä¸‹ç•Œ (VLB)
   - U-Netå®Ÿè£…

**ã‚¹ã‚­ãƒƒãƒ—å¯èƒ½ vs å¿…é ˆ**:
- **ã‚¹ã‚­ãƒƒãƒ—å¯èƒ½**: ç¬¬34å› (EBM) â€” Flowã®æ–‡è„ˆã§ã¯è£œè¶³çš„
- **å¿…é ˆ**: ç¬¬35å› (Score) â†’ ç¬¬37å› (SDE/ODE) â†’ ç¬¬38å› (Flow Matching)
  - ã“ã®3ã¤ãŒã€ŒFlow â†’ Diffusion â†’ çµ±ä¸€ã€ã®æ ¸å¿ƒ

**ä¸¦è¡Œå­¦ç¿’**:
- Optimal Transport (ç¬¬6å›ã®å¾©ç¿’ + ç™ºå±•)
- æ¸¬åº¦è«–ã®åŸºç¤ (Continuity Equation, Wassersteinè·é›¢)

**å®Ÿè£…å„ªå…ˆãªã‚‰**:
- ç¬¬36å› (DDPM) â†’ ç¬¬38å› (Flow Matching) â†’ Rectified Flowå®Ÿè£…

### 7.3 è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

**æœ¬è¬›ç¾©ã®ç†è§£åº¦ã‚’ãƒã‚§ãƒƒã‚¯**ã€‚å…¨å•æ­£è§£ã§**æ¬¡ã®ã‚¹ãƒ†ãƒ¼ã‚¸ã¸é€²ã‚€è³‡æ ¼**ã€‚

#### Level 1: åŸºç¤ (Zone 0-2)

**Q1**: Change of Variableså…¬å¼ $p_X(x) = p_Z(z) |\det J_f|^{-1}$ ã§ã€$\det J_f$ ã®ç‰©ç†çš„æ„å‘³ã¯ï¼Ÿ

<details><summary>è§£ç­”</summary>

ä½“ç©è¦ç´ ã®å¤‰åŒ–ç‡ã€‚$z$ ç©ºé–“ã®å¾®å°ä½“ç© $dz$ ãŒã€å¤‰æ› $f$ ã«ã‚ˆã£ã¦ $x$ ç©ºé–“ã§ $|\det J_f| dz$ ã«å¤‰åŒ–ã™ã‚‹ã€‚ç¢ºç‡å¯†åº¦ã¯ã€Œå˜ä½ä½“ç©ã‚ãŸã‚Šã®ç¢ºç‡ã€ãªã®ã§ã€é€†æ•° $|\det J_f|^{-1}$ ã‚’ã‹ã‘ã‚‹ã€‚

</details>

**Q2**: VAE, GAN, Normalizing Flowã®å°¤åº¦è¨ˆç®—èƒ½åŠ›ã‚’æ¯”è¼ƒã›ã‚ˆã€‚

<details><summary>è§£ç­”</summary>

- **VAE**: ELBO (å¤‰åˆ†ä¸‹ç•Œ) â€” $\log p(x)$ ã®ä¸‹ç•Œã®ã¿ã€çœŸã®å€¤ã¯ä¸æ˜
- **GAN**: æš—é»™çš„å¯†åº¦ â€” $\log p(x)$ è¨ˆç®—ä¸å¯
- **Normalizing Flow**: å³å¯†ãª $\log p(x)$ â€” Change of Variableså…¬å¼ã§è¨ˆç®—

</details>

**Q3**: Flowã®ã€Œæ­£è¦åŒ– (Normalizing)ã€ã¯ä½•ã‚’æ­£è¦åŒ–ã—ã¦ã„ã‚‹ã®ã‹ï¼Ÿ

<details><summary>è§£ç­”</summary>

ç¢ºç‡åˆ†å¸ƒã‚’æ­£è¦åŒ– (ç©åˆ†ãŒ1ã«ãªã‚‹ã‚ˆã†)ã€‚åŸºåº•åˆ†å¸ƒ $q(z)$ (é€šå¸¸ã‚¬ã‚¦ã‚¹) ã‚’å¤‰æ›ã—ã¦ã€è¤‡é›‘ãªãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p(x)$ ã‚’æ§‹ç¯‰ã™ã‚‹éš›ã€Change of Variablesã§è‡ªå‹•çš„ã« $\int p(x) dx = 1$ ãŒä¿è¨¼ã•ã‚Œã‚‹ (ã€Œæ­£è¦åŒ–æµã€ã®åå‰ã®ç”±æ¥)ã€‚

</details>

#### Level 2: æ•°å¼ (Zone 3)

**Q4**: Coupling Layerã§ $\log |\det J|$ ãŒ O(D) ã§è¨ˆç®—ã§ãã‚‹ç†ç”±ã‚’ã€ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—ã®æ§‹é€ ã‹ã‚‰èª¬æ˜ã›ã‚ˆã€‚

<details><summary>è§£ç­”</summary>

Coupling Layerã®ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³:

$$
J = \begin{bmatrix}
I_d & 0 \\
\frac{\partial x_2}{\partial z_1} & \text{diag}(\exp(s(z_1)))
\end{bmatrix}
$$

ä¸‹ä¸‰è§’ãƒ–ãƒ­ãƒƒã‚¯è¡Œåˆ— â†’ $\det J = \det(I_d) \cdot \det(\text{diag}(\exp(s))) = \prod_i \exp(s_i) = \exp(\sum s_i)$ã€‚$\log |\det J| = \sum s_i$ (O(D) ã®å’Œ)ã€‚

</details>

**Q5**: FFJORDã®Hutchinson traceæ¨å®š $\text{tr}(A) = \mathbb{E}[\mathbf{v}^\top A \mathbf{v}]$ ã§ã€$\mathbf{v}$ ã®åˆ†å¸ƒã®æ¡ä»¶ã¯ï¼Ÿ

<details><summary>è§£ç­”</summary>

$\mathbb{E}[\mathbf{v}] = 0$, $\text{Cov}(\mathbf{v}) = I$ ã‚’æº€ãŸã™ä»»æ„ã®åˆ†å¸ƒã€‚æ¨™æº–ã‚¬ã‚¦ã‚¹ $\mathcal{N}(0, I)$ ã¾ãŸã¯Rademacheråˆ†å¸ƒ (å„è¦ç´ ãŒ $\pm 1$ with prob 0.5) ãŒä¸€èˆ¬çš„ã€‚

</details>

**Q6**: Adjoint Methodã®ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãŒ O(1) ã§ã‚ã‚‹ç†ç”±ã¯ï¼Ÿ

<details><summary>è§£ç­”</summary>

é †ä¼æ’­æ™‚ã«ä¸­é–“çŠ¶æ…‹ã‚’ä¿å­˜ã—ãªã„ã€‚é€†ä¼æ’­æ™‚ã«ã€adjoint state $\mathbf{a}(t)$ ã®ODEã‚’é€†æ™‚é–“ã§è§£ããªãŒã‚‰å‹¾é…ã‚’è¨ˆç®—ã€‚å¿…è¦ã«å¿œã˜ã¦ODEã‚’å†è¨ˆç®— (checkpointing)ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•: ãƒ¡ãƒ¢ãƒª O(1) â†” è¨ˆç®—æ™‚é–“ 2Ã— (é †ä¼æ’­1å› + é€†ä¼æ’­1å›)ã€‚

</details>

#### Level 3: å®Ÿè£… (Zone 4-5)

**Q7**: RealNVPã®è¨“ç·´ã§ã€ãªãœ inverse â†’ forward ã®é †ã§è¨ˆç®—ã™ã‚‹ã®ã‹ï¼Ÿ

<details><summary>è§£ç­”</summary>

è¨“ç·´ãƒ‡ãƒ¼ã‚¿ $x$ ã‹ã‚‰ $\log p(x)$ ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã€‚
1. Inverse: $x \to z = f^{-1}(x)$
2. Forward: $z \to x$ ã‚’å†è¨ˆç®—ã—ã€$\log |\det J|$ ã‚’ç´¯ç©
3. $\log p(x) = \log q(z) - \log |\det J|$

ç”Ÿæˆæ™‚ (ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°) ã¯ Forward ã®ã¿: $z \sim q(z) \to x = f(z)$ã€‚

</details>

**Q8**: 2D Moons datasetã§ã€FlowãŒVAEã‚ˆã‚Šé«˜ã„ $\log p(x)$ ã‚’é”æˆã™ã‚‹ç†ç”±ã¯ï¼Ÿ

<details><summary>è§£ç­”</summary>

- **Flow**: å³å¯†ãª $\log p(x)$ â€” Change of Variables ã§çœŸã®å¯†åº¦ã«è¿‘ã„æ¨å®š
- **VAE**: ELBO (ä¸‹ç•Œ) â€” $\log p(x) \geq \text{ELBO}$ã€å¸¸ã«çœŸã®å€¤ã‚ˆã‚Šå°ã•ã„
- Gap = KL(q(z|x) || p(z|x)) (VAEã®è¿‘ä¼¼èª¤å·®)

å®Ÿé¨“çµæœ: Flow ~2.35, VAE ~1.89 (Gap ~0.46)ã€‚

</details>

**Q9**: Out-of-Distributionæ¤œçŸ¥ã§ã€FlowãŒé–¾å€¤è¨­å®šã—ã‚„ã™ã„ç†ç”±ã¯ï¼Ÿ

<details><summary>è§£ç­”</summary>

Flowã¯**å³å¯†ãª $\log p(x)$** ã‚’è¨ˆç®— â†’ In-distã¨OODã®åˆ†é›¢ãŒæ˜ç¢ºã€‚

- In-dist: $\log p(x)$ é«˜ã„ (ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã«è¿‘ã„)
- OOD: $\log p(x)$ ä½ã„ (ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã‹ã‚‰é ã„)

VAEã®ELBOã§ã¯ã€Gap (KL divergence) ãŒä¸æ˜ â†’ é–¾å€¤è¨­å®šãŒæ›–æ˜§ã€‚

</details>

#### Level 4: ç™ºå±• (Zone 6)

**Q10**: Probability Flow ODE (PF-ODE) ãŒã€ŒDiffusionã®æ±ºå®šè«–çš„ç‰ˆã€ã§ã‚ã‚‹ç†ç”±ã‚’ã€SDEã¨ã®é–¢ä¿‚ã‹ã‚‰èª¬æ˜ã›ã‚ˆã€‚

<details><summary>è§£ç­”</summary>

Diffusion Reverse SDE:

$$
dx = [f(x, t) - g(t)^2 \nabla \log p_t(x)] dt + g(t) dw
$$

PF-ODE (æ±ºå®šè«–çš„):

$$
\frac{dx}{dt} = f(x, t) - \frac{1}{2} g(t)^2 \nabla \log p_t(x)
$$

ãƒ‰ãƒªãƒ•ãƒˆé …ã‚’èª¿æ•´ ($g^2 \nabla \log p_t$ ã®ä¿‚æ•°ã‚’ $1 \to \frac{1}{2}$)ã€æ‹¡æ•£é … $g(t) dw$ ã‚’é™¤å»ã€‚ã“ã®ODEã‚’ $t=T \to 0$ ã«ç©åˆ†ã™ã‚‹ã¨ã€SDEã¨**åŒã˜å‘¨è¾ºåˆ†å¸ƒ** $p_t(x)$ ãŒå¾—ã‚‰ã‚Œã‚‹ (Song et al. 2021 è¨¼æ˜)ã€‚

</details>

**Q11**: Rectified Flowã§ã€Œç›´ç·šè¼¸é€ã€ãŒæœ€é©ã§ã‚ã‚‹ç†ç”±ã¯ï¼Ÿ

<details><summary>è§£ç­”</summary>

Optimal Transportç†è«–ã‚ˆã‚Šã€Wasserstein-2è·é›¢ã‚’æœ€å°åŒ–ã™ã‚‹è¼¸é€çµŒè·¯ã¯**ç›´ç·š** (geodesic)ã€‚

$x_t = (1-t) x_0 + t z$ ã¯ã€ãƒ‡ãƒ¼ã‚¿ç‚¹ $x_0$ ã¨ãƒã‚¤ã‚º $z$ ã‚’ç›´ç·šã§çµã¶æœ€çŸ­çµŒè·¯ â†’ Wassersteinè·é›¢æœ€å° â†’ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¹ãƒ†ãƒƒãƒ—æ•°æœ€å° (10-50 steps)ã€‚

</details>

**Q12**: Flow Matchingã¨Normalizing Flowsã®ã€Œå¯é€†æ€§ã€ã«å¯¾ã™ã‚‹è€ƒãˆæ–¹ã®é•ã„ã¯ï¼Ÿ

<details><summary>è§£ç­”</summary>

| è¦³ç‚¹ | Normalizing Flows (ä¼çµ±) | Flow Matching (æ–°) |
|:-----|:------------------------|:------------------|
| **å¯é€†æ€§** | ãƒ¢ãƒ‡ãƒ«ã®**æ§‹é€ åˆ¶ç´„** | çµŒè·¯ã®**æ€§è³ª** |
| **è¨“ç·´æ™‚** | $f, f^{-1}$ ä¸¡æ–¹è¨ˆç®—å¯èƒ½ | ãƒ™ã‚¯ãƒˆãƒ«å ´ $v_t$ (éå¯é€†OK) |
| **æ¨è«–æ™‚** | Forward: $z \to x = f(z)$ | ODEç©åˆ† (æ±ºå®šè«–çš„çµŒè·¯) |
| **çµæœ** | Coupling Layerç­‰ã®åˆ¶ç´„ | ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è‡ªç”± |

Flow Matchingã®æ´å¯Ÿ: ã€Œå¯é€†æ€§ã€ã¯æ±ºå®šè«–çš„ODEã®æ€§è³ª (åŒã˜åˆæœŸæ¡ä»¶ â†’ åŒã˜çµŒè·¯)ã€‚ãƒ¢ãƒ‡ãƒ«è‡ªä½“ã¯éå¯é€†ã§ã‚‚OKã€‚

</details>

**å…¨å•æ­£è§£ãªã‚‰** â†’ **Course IV ç¬¬34-38å›ã¸é€²ã‚€æº–å‚™å®Œäº†**ï¼

---

## ğŸŒ€ Paradigm-Breaking Question

> **ã€Œå¯é€†æ€§ã‚’æ¨ã¦ã‚Œã°ã€Flowã¯ã‚‚ã£ã¨è¡¨ç¾åŠ›ãŒä¸ŠãŒã‚‹ã®ã§ã¯ï¼Ÿã€**

### ä¼çµ±çš„ç­”ãˆ (2014-2019)

**ä¸»å¼µ**: å¯é€†æ€§ = Flowã®æœ¬è³ªã€‚æ¨ã¦ãŸã‚‰Flowã§ã¯ãªã„ã€‚

**æ ¹æ‹ **:
1. Change of VariablesãŒä½¿ãˆãªããªã‚‹ â†’ $\log p(x)$ è¨ˆç®—ä¸å¯
2. é€†å¤‰æ› $f^{-1}$ ãŒãªã„ã¨æ½œåœ¨ç©ºé–“ã¸ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ä¸å¯
3. Coupling Layerã®åˆ¶ç´„ã¯ä»•æ–¹ãªã„ (ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¨ˆç®—ã®ãŸã‚)

**çµè«–**: å¯é€†æ€§ã¯ã€Œã‚³ã‚¹ãƒˆã€ã§ã¯ãªãã€Œæœ¬è³ªçš„ç‰¹å¾´ã€ã€‚

### 2023å¹´ã®ç­”ãˆ (Flow Matching)

**ä¸»å¼µ**: **Flow Matchingã¯éå¯é€†ãƒ™ã‚¯ãƒˆãƒ«å ´ã‚’å­¦ç¿’å¯èƒ½**ã€‚

**å®Ÿä¾‹**:
- è¨“ç·´æ™‚: ä»»æ„ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ $v_\theta(x, t)$ ã‚’å­¦ç¿’ (å¯é€†æ€§ä¸è¦)
- æ¨è«–æ™‚: ODEã§ç©åˆ† $\frac{dx}{dt} = v_\theta(x, t)$ â†’ çµŒè·¯ã¯æ±ºå®šè«–çš„ (å®Ÿè³ªçš„ã«å¯é€†)

**æ´å¯Ÿ**:
- ã€Œå¯é€†æ€§ã€ã¯ç”Ÿæˆæ™‚ã®**çµŒè·¯ã®æ€§è³ª** (æ±ºå®šè«–çš„ODE)
- ã€Œå¯é€†æ€§ã€ã¯ãƒ¢ãƒ‡ãƒ«ã®**åˆ¶ç´„ã§ã¯ãªã„** (Coupling Layerã®ã‚ˆã†ãªæ§‹é€ åˆ¶ç´„ãŒä¸è¦)

### Diffusion Modelsã®è¦–ç‚¹

**Diffusionã¯ã€Œå¯é€†æ€§ã‚’æ¨ã¦ãŸã€Flow**:

| è¦³ç‚¹ | Normalizing Flow (ä¼çµ±) | Diffusion Model |
|:-----|:----------------------|:---------------|
| **Forward** | å­¦ç¿’å¯¾è±¡ ($f$ ã‚’å­¦ç¿’) | å›ºå®š (ãƒã‚¤ã‚ºè¿½åŠ ) |
| **Reverse** | $f^{-1}$ (è§£æçš„) | å­¦ç¿’å¯¾è±¡ ($\epsilon_\theta$ ã‚’å­¦ç¿’) |
| **å¯é€†æ€§** | å¿…é ˆ | ä¸è¦ (Forward ã¯éå¯é€†) |
| **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£** | Coupling Layer (åˆ¶ç´„ã‚ã‚Š) | U-Net/Transformer (è‡ªç”±) |
| **ç”Ÿæˆå“è³ª** | ä¸­ç¨‹åº¦ | **SOTA** |

**Diffusionã®æˆåŠŸãŒè¨¼æ˜**: å¯é€†æ€§ã‚’æ¨ã¦ã‚‹ã“ã¨ã§ã€è¡¨ç¾åŠ›ãŒ**åŠ‡çš„ã«å‘ä¸Š**ã€‚

### çµ±ä¸€çš„è¦–ç‚¹ (Optimal Transport)

**Flow (åºƒç¾©) = ãƒ™ã‚¯ãƒˆãƒ«å ´ã«ã‚ˆã‚‹è¼¸é€ (transport)**ã€‚

**Benamou-Brenierå…¬å¼** (ç¬¬38å›ã§è©³èª¬):

æ¸¬åº¦ $p_0$ ã‹ã‚‰ $p_1$ ã¸ã®è¼¸é€çµŒè·¯ã¯ã€æ¬¡ã®æœ€é©åŒ–å•é¡Œã®è§£:

$$
\min_{v_t} \int_0^1 \int \| v_t(x) \|^2 p_t(x) dx dt
$$

åˆ¶ç´„: Continuity Equation (æ¸¬åº¦ã®ä¿å­˜å‰‡)

$$
\frac{\partial p_t}{\partial t} + \nabla \cdot (p_t v_t) = 0
$$

**é‡è¦**: ã“ã®æ çµ„ã¿ã«ã€Œå¯é€†æ€§ã€ã¯**ä¸è¦**ã€‚ãƒ™ã‚¯ãƒˆãƒ«å ´ $v_t(x)$ ãŒå®šç¾©ã§ãã‚Œã°ååˆ†ã€‚

### ç­”ãˆ

**ä¼çµ±çš„Normalizing Flows**: å¯é€†æ€§ = æœ¬è³ª â†’ æ­£ã—ã„ãŒã€**ç‹­ã™ããŸ**ã€‚

**Flow Matching**: å¯é€†æ€§ = çµŒè·¯ã®æ€§è³ª (æ±ºå®šè«–çš„ODE) â†’ ã‚ˆã‚Šä¸€èˆ¬çš„ãªç†è§£ã€‚

**çµ±ä¸€çš„è¦–ç‚¹**:
- ã€Œå¯é€†å¤‰æ›ã€ã‹ã‚‰ã€Œãƒ™ã‚¯ãƒˆãƒ«å ´ã«ã‚ˆã‚‹è¼¸é€ã€ã¸
- Wassersteinè·é›¢ã‚’æœ€å°åŒ–ã™ã‚‹çµŒè·¯ = æœ€é©è¼¸é€
- **Flowã¨Diffusionã¯åŒã˜æ çµ„ã¿** (æ¸¬åº¦ã®æ™‚é–“ç™ºå±•)

**ç¬¬38å›ã§å®Œå…¨è§£ç­”**:
- Benamou-Brenierå…¬å¼
- JKO scheme (Wassersteinå‹¾é…æµ)
- **ã€Œå…¨ã¦ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯è¼¸é€å•é¡Œã€ã®è¨¼æ˜**

**ã“ã“ã§ã®å­¦ã³**: ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã®**å¢ƒç•Œã‚’å•ã„ç¶šã‘ã‚‹**ã“ã¨ãŒã€æ¬¡ã®ç†è«–ã‚’ç”Ÿã‚€ã€‚ã€Œå¯é€†æ€§ã¨ã¯ä½•ã‹ï¼Ÿã€ã€ŒFlowã¨ã¯ä½•ã‹ï¼Ÿã€â€” ã“ã®å•ã„ãŒã€Flow Matchingã¨ã„ã†çµ±ä¸€ç†è«–ã‚’å°ã„ãŸã€‚

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

[^1]: Rezende, D. J., & Mohamed, S. (2015). Variational Inference with Normalizing Flows. *ICML*.
@[card](https://arxiv.org/abs/1505.05770)

[^2]: Dinh, L., Krueger, D., & Bengio, Y. (2014). NICE: Non-linear Independent Components Estimation. *ICLR Workshop*.
@[card](https://arxiv.org/abs/1410.8516)

[^3]: Dinh, L., Sohl-Dickstein, J., & Bengio, S. (2016). Density Estimation using Real NVP. *ICLR*.
@[card](https://arxiv.org/abs/1605.08803)

[^4]: Kingma, D. P., & Dhariwal, P. (2018). Glow: Generative Flow with Invertible 1x1 Convolutions. *NeurIPS*.
@[card](https://arxiv.org/abs/1807.03039)

[^5]: Chen, R. T. Q., Rubanova, Y., Bettencourt, J., & Duvenaud, D. (2018). Neural Ordinary Differential Equations. *NeurIPS*.
@[card](https://arxiv.org/abs/1806.07366)

[^6]: Grathwohl, W., Chen, R. T. Q., Bettencourt, J., Sutskever, I., & Duvenaud, D. (2019). FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models. *ICLR*.
@[card](https://arxiv.org/abs/1810.01367)

[^7]: Rezende, D. J., & Mohamed, S. (2015). Variational Inference with Normalizing Flows (Planar Flow). *ICML*.
@[card](https://arxiv.org/abs/1505.05770)

[^8]: Kingma, D. P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., & Welling, M. (2016). Improved Variational Inference with Inverse Autoregressive Flow. *NeurIPS*.
@[card](https://arxiv.org/abs/1606.04934)

[^9]: Liu, X., Gong, C., & Liu, Q. (2022). Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow. *ICLR 2023*.
@[card](https://arxiv.org/abs/2209.03003)

[^10]: Lipman, Y., Chen, R. T. Q., Ben-Hamu, H., Nickel, M., & Le, M. (2023). Flow Matching for Generative Modeling. *ICLR*.
@[card](https://arxiv.org/abs/2210.02747)

[^11]: Chen, R. T. Q., Rubanova, Y., Bettencourt, J., & Duvenaud, D. (2018). Neural Ordinary Differential Equations (Adjoint Method). *NeurIPS*.
@[card](https://arxiv.org/abs/1806.07366)

[^12]: Ho, J., Jain, A., & Abbeel, P. (2020). Denoising Diffusion Probabilistic Models. *NeurIPS*.
@[card](https://arxiv.org/abs/2006.11239)

[^13]: Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., & Poole, B. (2021). Score-Based Generative Modeling through Stochastic Differential Equations. *ICLR*.
@[card](https://arxiv.org/abs/2011.13456)

[^14]: TarFlow: Targeted Flow Matching (2024).
@[card](https://arxiv.org/abs/2412.06329)

[^15]: Flexible Tails in Normalizing Flows (2025).
@[card](https://arxiv.org/abs/2406.16971)

---

**æ¬¡å›äºˆå‘Š**: ç¬¬34å› â€” **Energy-Based Models & çµ±è¨ˆç‰©ç†**ã€‚$p(x) = \frac{1}{Z} e^{-E(x)}$ ã®Gibbsåˆ†å¸ƒã€Hopfield Networkã¨Transformerã®ç­‰ä¾¡æ€§ã€Contrastive Divergenceã€Langevin Dynamicsã€‚æ­£è¦åŒ–å®šæ•° $Z$ ã¨ã®æˆ¦ã„ãŒå§‹ã¾ã‚‹ã€‚ãã—ã¦ç¬¬35å›ã§ $Z$ ãŒæ¶ˆãˆã‚‹ç¬é–“ã‚’ç›®æ’ƒã™ã‚‹ â€” **Score Matching**ã€‚

Course IV ã®æ—…ã¯ã¾ã å§‹ã¾ã£ãŸã°ã‹ã‚Šã€‚ç¬¬33å›ã§å¾—ãŸã€ŒChange of Variablesã€ã®æ•°å­¦ãŒã€ç¬¬37-38å›ã§**Diffusion Models**ã¨èåˆã—ã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«–ã®**çµ±ä¸€**ã¸ã¨å‘ã‹ã†ã€‚æ¬¡ã®è¬›ç¾©ã§ä¼šãŠã†ã€‚

---

## ğŸ“– Appendix: è©³ç´°å°å‡ºã¨è£œè¶³

### A.1 Change of Variableså…¬å¼ã®å³å¯†ãªè¨¼æ˜

**å®šç†** (å¤šæ¬¡å…ƒChange of Variables):

$\mathbf{Z} \in \mathbb{R}^D$ ãŒå¯†åº¦ $p_{\mathbf{Z}}(\mathbf{z})$ ã‚’æŒã¡ã€$\mathbf{f}: \mathbb{R}^D \to \mathbb{R}^D$ ã‚’ $C^1$ ç´šã®å¯é€†å†™åƒã¨ã™ã‚‹ã€‚ã“ã®ã¨ãã€$\mathbf{X} = \mathbf{f}(\mathbf{Z})$ ã®å¯†åº¦ã¯

$$
p_{\mathbf{X}}(\mathbf{x}) = p_{\mathbf{Z}}(\mathbf{f}^{-1}(\mathbf{x})) \left| \det \frac{\partial \mathbf{f}^{-1}}{\partial \mathbf{x}} \right|
$$

**è¨¼æ˜**:

Step 1: åˆ†å¸ƒé–¢æ•°ã‹ã‚‰å‡ºç™ºã€‚

$$
\begin{aligned}
F_{\mathbf{X}}(\mathbf{x}) &= P(\mathbf{X} \leq \mathbf{x}) \\
&= P(\mathbf{f}(\mathbf{Z}) \leq \mathbf{x}) \\
&= P(\mathbf{Z} \in \mathbf{f}^{-1}((-\infty, \mathbf{x}]))
\end{aligned}
$$

Step 2: ç¢ºç‡ã‚’ç©åˆ†ã§è¡¨ç¾ã€‚

$$
F_{\mathbf{X}}(\mathbf{x}) = \int_{\mathbf{f}^{-1}((-\infty, \mathbf{x}])} p_{\mathbf{Z}}(\mathbf{z}) d\mathbf{z}
$$

Step 3: å¤‰æ•°å¤‰æ› $\mathbf{z} = \mathbf{f}^{-1}(\mathbf{u})$ã€$d\mathbf{z} = \left| \det \frac{\partial \mathbf{f}^{-1}}{\partial \mathbf{u}} \right| d\mathbf{u}$ã€‚

$$
F_{\mathbf{X}}(\mathbf{x}) = \int_{-\infty}^{\mathbf{x}} p_{\mathbf{Z}}(\mathbf{f}^{-1}(\mathbf{u})) \left| \det \frac{\partial \mathbf{f}^{-1}}{\partial \mathbf{u}} \right| d\mathbf{u}
$$

Step 4: å¯†åº¦ã¯åˆ†å¸ƒé–¢æ•°ã®å°é–¢æ•°ã€‚

$$
p_{\mathbf{X}}(\mathbf{x}) = \frac{\partial F_{\mathbf{X}}}{\partial \mathbf{x}} = p_{\mathbf{Z}}(\mathbf{f}^{-1}(\mathbf{x})) \left| \det \frac{\partial \mathbf{f}^{-1}}{\partial \mathbf{x}} \right|
$$

Step 5: é€†é–¢æ•°å®šç†ã‚ˆã‚Šã€$\frac{\partial \mathbf{f}^{-1}}{\partial \mathbf{x}} = \left( \frac{\partial \mathbf{f}}{\partial \mathbf{z}} \right)^{-1}$ã€‚

è¡Œåˆ—å¼ã®æ€§è³ª $\det(A^{-1}) = (\det A)^{-1}$ ã‚’ä½¿ã†ã¨

$$
\boxed{p_{\mathbf{X}}(\mathbf{x}) = p_{\mathbf{Z}}(\mathbf{z}) \left| \det \frac{\partial \mathbf{f}}{\partial \mathbf{z}} \right|^{-1}}
$$

ã“ã“ã§ $\mathbf{z} = \mathbf{f}^{-1}(\mathbf{x})$ã€‚ $\square$

### A.2 Coupling Layerã®ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—ã®å°å‡º

**è¨­å®š**: $\mathbf{z} = [\mathbf{z}_1, \mathbf{z}_2]$ã€$\mathbf{z}_1 \in \mathbb{R}^d$ã€$\mathbf{z}_2 \in \mathbb{R}^{D-d}$ã€‚

**å¤‰æ›**:

$$
\begin{aligned}
\mathbf{x}_1 &= \mathbf{z}_1 \\
\mathbf{x}_2 &= \mathbf{z}_2 \odot \exp(\mathbf{s}(\mathbf{z}_1)) + \mathbf{t}(\mathbf{z}_1)
\end{aligned}
$$

**ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—ã®è¨ˆç®—**:

$$
J = \frac{\partial \mathbf{x}}{\partial \mathbf{z}} = \begin{bmatrix}
\frac{\partial \mathbf{x}_1}{\partial \mathbf{z}_1} & \frac{\partial \mathbf{x}_1}{\partial \mathbf{z}_2} \\
\frac{\partial \mathbf{x}_2}{\partial \mathbf{z}_1} & \frac{\partial \mathbf{x}_2}{\partial \mathbf{z}_2}
\end{bmatrix}
$$

**å„ãƒ–ãƒ­ãƒƒã‚¯ã®è¨ˆç®—**:

1. $\frac{\partial \mathbf{x}_1}{\partial \mathbf{z}_1} = I_d$ ($\mathbf{x}_1 = \mathbf{z}_1$)

2. $\frac{\partial \mathbf{x}_1}{\partial \mathbf{z}_2} = 0$ ($\mathbf{x}_1$ ã¯ $\mathbf{z}_2$ ã«ä¾å­˜ã—ãªã„)

3. $\frac{\partial \mathbf{x}_2}{\partial \mathbf{z}_2}$:

$$
\begin{aligned}
\mathbf{x}_2 &= \mathbf{z}_2 \odot \exp(\mathbf{s}(\mathbf{z}_1)) + \mathbf{t}(\mathbf{z}_1) \\
\frac{\partial x_{2,i}}{\partial z_{2,j}} &= \delta_{ij} \exp(s_i(\mathbf{z}_1)) \\
\frac{\partial \mathbf{x}_2}{\partial \mathbf{z}_2} &= \text{diag}(\exp(\mathbf{s}(\mathbf{z}_1)))
\end{aligned}
$$

4. $\frac{\partial \mathbf{x}_2}{\partial \mathbf{z}_1}$:

$$
\frac{\partial x_{2,i}}{\partial z_{1,j}} = z_{2,i} \exp(s_i) \frac{\partial s_i}{\partial z_{1,j}} + \frac{\partial t_i}{\partial z_{1,j}}
$$

**ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—ã®æ§‹é€ **:

$$
J = \begin{bmatrix}
I_d & 0 \\
\frac{\partial \mathbf{x}_2}{\partial \mathbf{z}_1} & \text{diag}(\exp(\mathbf{s}(\mathbf{z}_1)))
\end{bmatrix}
$$

**ä¸‹ä¸‰è§’ãƒ–ãƒ­ãƒƒã‚¯è¡Œåˆ—** â†’ è¡Œåˆ—å¼ã¯å¯¾è§’ãƒ–ãƒ­ãƒƒã‚¯ã®ç©:

$$
\det J = \det(I_d) \cdot \det(\text{diag}(\exp(\mathbf{s}))) = 1 \cdot \prod_{i=1}^{D-d} \exp(s_i) = \exp\left(\sum_{i=1}^{D-d} s_i\right)
$$

$$
\boxed{\log |\det J| = \sum_{i=1}^{D-d} s_i(\mathbf{z}_1)}
$$

**è¨ˆç®—é‡**: $D-d$ å€‹ã®å’Œ â†’ **O(D)**ã€‚ $\square$

### A.3 Instantaneous Change of Variables ã®å°å‡º

**ç›®æ¨™**: é€£ç¶šæ™‚é–“ODEã«ãŠã‘ã‚‹å¯†åº¦ã®æ™‚é–“ç™ºå±•ã‚’å°å‡ºã€‚

**è¨­å®š**: $\mathbf{z}(t)$ ãŒODE $\frac{d\mathbf{z}}{dt} = \mathbf{f}(\mathbf{z}(t), t)$ ã«å¾“ã†ã€‚

**å¾®å°æ™‚é–“ $\Delta t$ ã§ã®å¤‰åŒ–**:

$$
\mathbf{z}(t + \Delta t) = \mathbf{z}(t) + \mathbf{f}(\mathbf{z}(t), t) \Delta t + O(\Delta t^2)
$$

**Change of Variableså…¬å¼ã‚’é©ç”¨**:

$$
\log p(\mathbf{z}(t + \Delta t)) = \log p(\mathbf{z}(t)) - \log \left| \det \frac{\partial \mathbf{z}(t + \Delta t)}{\partial \mathbf{z}(t)} \right|
$$

**ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®è¨ˆç®—**:

$$
\frac{\partial \mathbf{z}(t + \Delta t)}{\partial \mathbf{z}(t)} = I + \frac{\partial \mathbf{f}}{\partial \mathbf{z}} \Delta t + O(\Delta t^2)
$$

**è¡Œåˆ—å¼ã®å¯¾æ•°**:

$$
\log |\det (I + A \Delta t)| = \log (1 + \text{tr}(A) \Delta t + O(\Delta t^2))
$$

**1æ¬¡è¿‘ä¼¼** ($\log(1 + x) \approx x$ for small $x$):

$$
\log |\det (I + A \Delta t)| \approx \text{tr}(A) \Delta t = \text{tr}\left(\frac{\partial \mathbf{f}}{\partial \mathbf{z}}\right) \Delta t
$$

**ä»£å…¥**:

$$
\log p(\mathbf{z}(t + \Delta t)) = \log p(\mathbf{z}(t)) - \text{tr}\left(\frac{\partial \mathbf{f}}{\partial \mathbf{z}}\right) \Delta t
$$

**ä¸¡è¾ºã‚’ $\Delta t$ ã§å‰²ã‚Šã€$\Delta t \to 0$ ã®æ¥µé™**:

$$
\boxed{\frac{\partial \log p(\mathbf{z}(t))}{\partial t} = -\text{tr}\left(\frac{\partial \mathbf{f}}{\partial \mathbf{z}}\right)}
$$

**ç©åˆ†å½¢å¼**:

$$
\log p(\mathbf{z}(1)) - \log p(\mathbf{z}(0)) = -\int_0^1 \text{tr}\left(\frac{\partial \mathbf{f}}{\partial \mathbf{z}(t)}\right) dt
$$

$$
\boxed{\log p(\mathbf{x}) = \log p(\mathbf{z}_0) - \int_0^1 \text{tr}\left(\frac{\partial \mathbf{f}}{\partial \mathbf{z}(t)}\right) dt}
$$

ã“ã“ã§ $\mathbf{x} = \mathbf{z}(1)$ã€$\mathbf{z}_0 = \mathbf{z}(0)$ã€‚ $\square$

### A.4 Hutchinsonã®traceæ¨å®šã®è¨¼æ˜

**å®šç†**: $A \in \mathbb{R}^{D \times D}$ ã‚’å¯¾ç§°è¡Œåˆ—ã€$\mathbf{v} \sim p(\mathbf{v})$ ã‚’ $\mathbb{E}[\mathbf{v}] = 0$ã€$\text{Cov}(\mathbf{v}) = I$ ã‚’æº€ãŸã™ç¢ºç‡å¤‰æ•°ã¨ã™ã‚‹ã€‚ã“ã®ã¨ã

$$
\text{tr}(A) = \mathbb{E}_{\mathbf{v}} [\mathbf{v}^\top A \mathbf{v}]
$$

**è¨¼æ˜**:

$$
\begin{aligned}
\mathbb{E}[\mathbf{v}^\top A \mathbf{v}] &= \mathbb{E}\left[\sum_{i=1}^D \sum_{j=1}^D v_i A_{ij} v_j\right] \\
&= \sum_{i=1}^D \sum_{j=1}^D A_{ij} \mathbb{E}[v_i v_j] \\
&= \sum_{i=1}^D \sum_{j=1}^D A_{ij} \delta_{ij} \quad (\because \mathbb{E}[v_i v_j] = \text{Cov}(v_i, v_j) = \delta_{ij}) \\
&= \sum_{i=1}^D A_{ii} \\
&= \text{tr}(A)
\end{aligned}
$$

$\square$

**éå¯¾ç§°è¡Œåˆ—ã¸ã®æ‹¡å¼µ**:

$A$ ãŒéå¯¾ç§°ã§ã‚‚ã€$\mathbf{v}^\top A \mathbf{v}$ ã¯

$$
\mathbf{v}^\top A \mathbf{v} = \mathbf{v}^\top \frac{A + A^\top}{2} \mathbf{v} + \mathbf{v}^\top \frac{A - A^\top}{2} \mathbf{v}
$$

ç¬¬2é … (åå¯¾ç§°éƒ¨åˆ†) ã¯ $\mathbb{E}[\mathbf{v}^\top (A - A^\top) \mathbf{v}] = 0$ (è¨¼æ˜ç•¥)ã€‚

ã‚ˆã£ã¦ã€$A$ ãŒéå¯¾ç§°ã§ã‚‚ $\mathbb{E}[\mathbf{v}^\top A \mathbf{v}] = \text{tr}(A)$ ãŒæˆç«‹ã€‚

### A.5 å®Œå…¨ãªRealNVPå®Ÿè£… (Lux.jl)

**æœ¬æ–‡ã§çœç•¥ã—ãŸè©³ç´°** â€” Lux.jlã® `AbstractExplicitLayer` ã¨ã®äº’æ›æ€§ã‚’å®Œå…¨ã«ä¿ã£ãŸå®Ÿè£…ã€‚

```julia
using Lux, Random, Zygote, Optimisers, Distributions, Statistics
using LinearAlgebra

# Affine Coupling Layer (å®Œå…¨ç‰ˆ)
struct AffineCouplingLayer <: Lux.AbstractExplicitLayer
    split_dim::Int
    scale_net::Chain
    trans_net::Chain
end

function AffineCouplingLayer(split_dim::Int, hidden_dim::Int, output_dim::Int)
    scale_net = Chain(
        Dense(split_dim => hidden_dim, tanh),
        Dense(hidden_dim => hidden_dim, tanh),
        Dense(hidden_dim => output_dim)
    )
    trans_net = Chain(
        Dense(split_dim => hidden_dim, tanh),
        Dense(hidden_dim => hidden_dim, tanh),
        Dense(hidden_dim => output_dim)
    )
    return AffineCouplingLayer(split_dim, scale_net, trans_net)
end

# Forward pass
function (layer::AffineCouplingLayer)(z, ps, st)
    d = layer.split_dim
    z1, z2 = z[1:d, :], z[d+1:end, :]

    # Compute scale & translation
    s, st_s = layer.scale_net(z1, ps.scale_net, st.scale_net)
    t, st_t = layer.trans_net(z1, ps.trans_net, st.trans_net)

    # Transform z2
    x1 = z1
    x2 = z2 .* exp.(s) .+ t

    # Log-determinant
    log_det = vec(sum(s; dims=1))
    x = vcat(x1, x2)

    return (x, log_det), (scale_net=st_s, trans_net=st_t)
end

# Inverse (for density evaluation)
function inverse(layer::AffineCouplingLayer, x, ps, st)
    d = layer.split_dim
    x1, x2 = x[1:d, :], x[d+1:end, :]

    s, _ = layer.scale_net(x1, ps.scale_net, st.scale_net)
    t, _ = layer.trans_net(x1, ps.trans_net, st.trans_net)

    z1 = x1
    z2 = (x2 .- t) .* exp.(-s)

    return vcat(z1, z2)
end

# RealNVP Model
struct RealNVP <: Lux.AbstractExplicitContainerLayer{(:layers,)}
    layers::NamedTuple
    base_dist::Distribution
end

function RealNVP(D::Int, n_layers::Int, hidden_dim::Int)
    layers_list = []
    for i in 1:n_layers
        # Alternate split dimension
        split_dim = isodd(i) ? D Ã· 2 : D - D Ã· 2
        push!(layers_list, AffineCouplingLayer(split_dim, hidden_dim, D - split_dim))
    end
    layers = NamedTuple{Tuple(Symbol("layer_$i") for i in 1:n_layers)}(Tuple(layers_list))
    base_dist = MvNormal(zeros(Float32, D), I(D))
    return RealNVP(layers, base_dist)
end

# Forward: z â†’ x with log p(x)
function (model::RealNVP)(z, ps, st)
    x = z
    total_log_det = zeros(Float32, size(z, 2))
    new_st = []

    for (i, (key, layer)) in enumerate(pairs(model.layers))
        (x, log_det), st_i = layer(x, ps[key], st[key])
        total_log_det .+= log_det
        push!(new_st, key => st_i)
    end

    # log p(x) = log p(z) - log|det J|
    log_pz = sum(logpdf(model.base_dist, z); dims=1) |> vec
    log_px = log_pz .- total_log_det

    return (x=x, log_px=log_px), NamedTuple(new_st)
end

# Inverse: x â†’ z
function inverse(model::RealNVP, x, ps, st)
    z = x
    for (key, layer) in reverse(collect(pairs(model.layers)))
        z = inverse(layer, z, ps[key], st[key])
    end
    return z
end

# Sample from model
function sample(model::RealNVP, ps, st, n_samples::Int)
    D = length(model.base_dist)
    z = rand(model.base_dist, n_samples)
    (result, _), _ = model(z, ps, st)
    return result.x
end

# Training loop
function train_realnvp(model, X_train; n_epochs=100, batch_size=64, lr=1e-3)
    rng = Random.default_rng()
    ps, st = Lux.setup(rng, model)
    opt_state = Optimisers.setup(Adam(lr), ps)

    n_batches = size(X_train, 2) Ã· batch_size

    for epoch in 1:n_epochs
        total_loss = 0.0

        for batch_idx in 1:n_batches
            idx_start = (batch_idx - 1) * batch_size + 1
            idx_end = batch_idx * batch_size
            x_batch = X_train[:, idx_start:idx_end]

            # Loss function
            loss_fn(ps_) = begin
                z = inverse(model, x_batch, ps_, st)
                (result, _), _ = model(z, ps_, st)
                -mean(result.log_px)  # Negative log likelihood
            end

            # Compute loss & gradients
            loss, grads = Zygote.withgradient(loss_fn, ps)

            # Update parameters
            opt_state, ps = Optimisers.update(opt_state, ps, grads[1])

            total_loss += loss
        end

        if epoch % 10 == 0
            avg_loss = total_loss / n_batches
            println("Epoch $epoch: NLL = $(round(avg_loss, digits=4))")
        end
    end

    return ps, st
end
```

**ä½¿ç”¨ä¾‹**:

```julia
# Generate 2D Moons dataset
function make_moons(n_samples; noise=0.05)
    n = n_samples Ã· 2

    # Upper moon
    Î¸_upper = range(0, Ï€; length=n)
    x_upper = cos.(Î¸_upper)
    y_upper = sin.(Î¸_upper)

    # Lower moon
    Î¸_lower = range(0, Ï€; length=n)
    x_lower = 1 .- cos.(Î¸_lower)
    y_lower = 0.5 .- sin.(Î¸_lower)

    # Combine + noise
    X = hcat(vcat(x_upper, x_lower), vcat(y_upper, y_lower))'
    X .+= noise .* randn(size(X))

    return Float32.(X)
end

# Train
X_train = make_moons(2000; noise=0.05)
model = RealNVP(2, 6, 64)
ps, st = train_realnvp(model, X_train; n_epochs=100)

# Sample
x_samples = sample(model, ps, st, 500)

# Visualize
using Plots
scatter(X_train[1, :], X_train[2, :]; alpha=0.3, label="Data", title="RealNVP on 2D Moons")
scatter!(x_samples[1, :], x_samples[2, :]; alpha=0.3, label="Generated")
```

---
---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
