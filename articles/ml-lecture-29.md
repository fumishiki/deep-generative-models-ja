---
title: "ç¬¬29å›: RAG (æ¤œç´¢å¢—å¼·ç”Ÿæˆ): 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸ”"
type: "tech"
topics: ["machinelearning", "rag", "vectordatabase", "julia", "rust"]
published: true
---

# ç¬¬29å›: RAG (æ¤œç´¢å¢—å¼·ç”Ÿæˆ) â€” ãƒ¢ãƒ‡ãƒ«ã®çŸ¥è­˜ã‚’å¤–éƒ¨çŸ¥è­˜ã§æ‹¡å¼µã™ã‚‹

> **LLMã®çŸ¥è­˜ã¯å­¦ç¿’æ™‚ç‚¹ã§å›ºå®šã•ã‚Œã‚‹ã€‚ã ãŒä¸–ç•Œã¯å¤‰ã‚ã‚Šç¶šã‘ã‚‹ã€‚RAGã¯å¤–éƒ¨çŸ¥è­˜æºã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§å‚ç…§ã—ã€æœ€æ–°ãƒ»æ­£ç¢ºãƒ»æ–‡è„ˆã«ç‰¹åŒ–ã—ãŸå¿œç­”ã‚’ç”Ÿæˆã™ã‚‹ã€‚**

ç¬¬28å›ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’å­¦ã‚“ã ã€‚ã ãŒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã ã‘ã§ã¯**LLMã®çŸ¥è­˜ã®é™ç•Œ**ã‚’è¶…ãˆã‚‰ã‚Œãªã„ã€‚å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«ãªã„æƒ…å ±ã€æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€ä¼æ¥­å›ºæœ‰ã®çŸ¥è­˜ã«ã¯ã‚¢ã‚¯ã‚»ã‚¹ã§ããªã„ã€‚

RAG (Retrieval-Augmented Generation) [^1] ã¯ã“ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ã€‚**å¤–éƒ¨çŸ¥è­˜æºã‹ã‚‰é–¢é€£æ–‡æ›¸ã‚’æ¤œç´¢ã—ã€ãã‚Œã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ç”Ÿæˆã«åˆ©ç”¨**ã™ã‚‹ã“ã¨ã§ã€LLMã®çŸ¥è­˜ã‚’å‹•çš„ã«æ‹¡å¼µã™ã‚‹ã€‚

æœ¬è¬›ç¾©ã§ã¯ã€RAGã®åŸºç¤ç†è«–ã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«DBå®Ÿè£…ã€Agentic RAGã€è©•ä¾¡æ‰‹æ³•ã¾ã§ã€å®Ÿè£…ã‚’å«ã‚ã¦å®Œå…¨ç¿’å¾—ã™ã‚‹ã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2024-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚
:::

```mermaid
graph LR
    Q["ğŸ“ Query"] --> R["ğŸ” Retriever"]
    R --> DB["ğŸ“š Knowledge Base<br/>(Vector DB)"]
    DB --> C["ğŸ“„ Context"]
    C --> G["ğŸ¤– Generator<br/>(LLM)"]
    Q --> G
    G --> A["âœ¨ Answer"]
    style R fill:#e3f2fd
    style DB fill:#fff3e0
    style G fill:#c8e6c9
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | ç™ºå±•ã‚¾ãƒ¼ãƒ³ | 20åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 7 | æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å‹•ã‹ã™

**ã‚´ãƒ¼ãƒ«**: RAGã®å¨åŠ›ã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ã€‚

æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªRAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³: BM25æ¤œç´¢ + LLMç”Ÿæˆã‚’3è¡Œã§å‹•ã‹ã™ã€‚

```julia
using LinearAlgebra, Statistics

# Simplified RAG pipeline (BM25 retrieval + generation)

# Knowledge base (documents)
documents = [
    "Paris is the capital of France. It is known for the Eiffel Tower.",
    "Tokyo is the capital of Japan. It has a population of 14 million.",
    "Berlin is the capital of Germany. The Berlin Wall fell in 1989.",
    "London is the capital of England. Big Ben is a famous landmark.",
]

# Query
query = "What is the capital of France?"

# Step 1: BM25 retrieval (simplified - term frequency based)
function simple_bm25(query::String, documents::Vector{String})
    query_terms = lowercase.(split(query))
    scores = zeros(length(documents))

    for (i, doc) in enumerate(documents)
        doc_terms = lowercase.(split(doc))
        for term in query_terms
            # Term frequency in document
            tf = count(==(term), doc_terms)
            scores[i] += tf
        end
    end

    # Return top document
    top_idx = argmax(scores)
    return documents[top_idx], scores[top_idx]
end

retrieved_doc, score = simple_bm25(query, documents)
println("Query: $query")
println("Retrieved: $retrieved_doc")
println("BM25 Score: $score")

# Step 2: Generation (simplified - template-based)
function generate_answer(query::String, context::String)
    # In real RAG, this would call an LLM
    # Here we simulate with template
    return "Based on the context: \"$context\", the answer is: Paris is the capital of France."
end

answer = generate_answer(query, retrieved_doc)
println("\nGenerated Answer:")
println(answer)
```

å‡ºåŠ›:
```
Query: What is the capital of France?
Retrieved: Paris is the capital of France. It is known for the Eiffel Tower.
BM25 Score: 4.0

Generated Answer:
Based on the context: "Paris is the capital of France. It is known for the Eiffel Tower.", the answer is: Paris is the capital of France.
```

**3è¡Œã§å¤–éƒ¨çŸ¥è­˜ã‚’æ¤œç´¢ã—ã€å¿œç­”ã‚’ç”Ÿæˆã—ãŸã€‚**

- **Without RAG**: LLMã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®çŸ¥è­˜ã®ã¿ã«ä¾å­˜
- **With RAG**: å¤–éƒ¨çŸ¥è­˜ã‚’æ¤œç´¢ â†’ æœ€æ–°ãƒ»æ­£ç¢ºãƒ»æ–‡è„ˆç‰¹åŒ–ã®å¿œç­”

ã“ã®èƒŒå¾Œã«ã‚ã‚‹ç†è«–:

$$
\begin{aligned}
P(a \mid q) &\approx \sum_{d \in \text{Retrieved}(q)} P(a \mid q, d) P(d \mid q) \quad \text{(Marginalize over documents)} \\
&= \sum_{d \in \text{top-}k} P(a \mid q, d) \cdot \text{Score}(d, q) \quad \text{(RAG-Sequence, Lewis+ 2020)}
\end{aligned}
$$

ã“ã“ã§:
- $q$: ã‚¯ã‚¨ãƒª
- $d$: æ¤œç´¢ã•ã‚ŒãŸæ–‡æ›¸
- $a$: ç”Ÿæˆã•ã‚ŒãŸå¿œç­”
- $\text{Retrieved}(q)$: ã‚¯ã‚¨ãƒª $q$ ã«å¯¾ã™ã‚‹æ¤œç´¢çµæœ

RAGã¯**æ¤œç´¢ã¨ç”Ÿæˆã‚’çµ±åˆ**ã—ã€LLMã®çŸ¥è­˜ã‚’å‹•çš„ã«æ‹¡å¼µã™ã‚‹ã€‚

:::message
**é€²æ—: 3% å®Œäº†** RAGã®å¨åŠ›ã‚’ä½“æ„Ÿã—ãŸã€‚ã“ã“ã‹ã‚‰æ¤œç´¢æˆ¦ç•¥ãƒ»ãƒ™ã‚¯ãƒˆãƒ«DBãƒ»Agentic RAGã‚’å®Œå…¨ç¿’å¾—ã™ã‚‹ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” RAGã®4ã¤ã®æ§‹æˆè¦ç´ 

### 1.1 RAGã®åŸºæœ¬ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

Lewis et al. (2020) [^1] ãŒæå”±ã—ãŸå…ƒç¥–RAGã¯ä»¥ä¸‹ã®3ã‚¹ãƒ†ãƒƒãƒ—:

1. **Retrieval**: ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹æ–‡æ›¸ã‚’Top-kæ¤œç´¢
2. **Augmentation**: æ¤œç´¢çµæœã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«çµ±åˆ
3. **Generation**: æ‹¡å¼µã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§LLMãŒå¿œç­”ç”Ÿæˆ

```mermaid
sequenceDiagram
    participant User
    participant Retriever
    participant VectorDB
    participant LLM

    User->>Retriever: Query
    Retriever->>VectorDB: Embed & Search
    VectorDB-->>Retriever: Top-k Docs
    Retriever->>LLM: Query + Context
    LLM-->>User: Generated Answer
```

### 1.2 RAG vs Fine-tuning vs Prompting

| æ‰‹æ³• | çŸ¥è­˜æ›´æ–° | ã‚³ã‚¹ãƒˆ | ç²¾åº¦ | é©ç”¨å ´é¢ |
|:-----|:--------|:------|:-----|:---------|
| **Prompting** | ä¸å¯ | ä½ | ä¸­ | æ±ç”¨ã‚¿ã‚¹ã‚¯ |
| **Fine-tuning** | å†å­¦ç¿’å¿…è¦ | é«˜ | é«˜ | ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ– |
| **RAG** | ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ  | ä¸­ | é«˜ | å‹•çš„çŸ¥è­˜ãƒ»æœ€æ–°æƒ…å ± |

**RAGã®åˆ©ç‚¹**:
- çŸ¥è­˜æ›´æ–°ãŒãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ï¼ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¿½åŠ ã®ã¿ï¼‰
- å‡ºå…¸ã‚’æ˜ç¤ºå¯èƒ½ï¼ˆHallucinationæŠ‘åˆ¶ï¼‰
- Fine-tuningã‚ˆã‚Šä½ã‚³ã‚¹ãƒˆ

**RAGã®æ¬ ç‚¹**:
- æ¤œç´¢å“è³ªã«ä¾å­˜
- ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·å¢—åŠ ï¼ˆæ¤œç´¢ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ï¼‰
- é•·æ–‡æ›¸ã®å‡¦ç†ãŒå›°é›£ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·åˆ¶é™ï¼‰

### 1.3 RAGã®é€²åŒ–: Naive â†’ Agentic

```mermaid
graph TD
    N["Naive RAG<br/>(2020)"] --> A["Advanced RAG<br/>(2021-2022)"]
    A --> M["Modular RAG<br/>(2023)"]
    M --> AG["Agentic RAG<br/>(2024)"]

    N -.å›ºå®šæ¤œç´¢.-> N2["Query â†’ Retrieve â†’ Generate"]
    A -.Rerankingè¿½åŠ .-> A2["Query â†’ Retrieve â†’ Rerank â†’ Generate"]
    M -.ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–.-> M2["Pre-Retrieval + Retrieval + Post-Retrieval"]
    AG -.è‡ªå¾‹åˆ¶å¾¡.-> AG2["Self-RAG / CRAG / Adaptive-RAG"]

    style AG fill:#c8e6c9
```

**Naive RAG** (2020):
- å˜ç´”ãªæ¤œç´¢ â†’ ç”Ÿæˆ
- å›ºå®šãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
- æ¤œç´¢ç²¾åº¦ãŒä½ã„

**Advanced RAG** (2021-2022):
- Pre-Retrieval: Query Rewriting, Expansion
- Post-Retrieval: Reranking, Filtering
- æ¤œç´¢ç²¾åº¦å‘ä¸Š

**Modular RAG** (2023):
- ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åˆ†é›¢ï¼ˆæ¤œç´¢ãƒ»Rerankingãƒ»ç”Ÿæˆï¼‰
- ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½

**Agentic RAG** (2024) [^4]:
- **Self-RAG** [^2]: åçœãƒˆãƒ¼ã‚¯ãƒ³ã§æ¤œç´¢ãƒ»ç”Ÿæˆã‚’è‡ªå·±åˆ¶å¾¡
- **CRAG** [^3]: æ¤œç´¢çµæœã®æ­£ç¢ºæ€§è©•ä¾¡ + çŸ¥è­˜è£œæ­£
- **Adaptive-RAG**: ã‚¯ã‚¨ãƒªè¤‡é›‘åº¦ã«å¿œã˜ãŸæ¤œç´¢æˆ¦ç•¥è‡ªå‹•é¸æŠ

### 1.4 RAGã®4ã¤ã®ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ

| ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ | å½¹å‰² | æŠ€è¡“ |
|:-------------|:-----|:-----|
| **Embedding** | ãƒ†ã‚­ã‚¹ãƒˆâ†’ãƒ™ã‚¯ãƒˆãƒ«å¤‰æ› | Sentence-BERT, E5, BGE |
| **Vector DB** | ãƒ™ã‚¯ãƒˆãƒ«ä¿å­˜ãƒ»æ¤œç´¢ | FAISS, Qdrant, Milvus |
| **Retrieval** | é–¢é€£æ–‡æ›¸æ¤œç´¢ | BM25, Dense, Hybrid |
| **Reranking** | æ¤œç´¢çµæœã®å†é †ä½ä»˜ã‘ | Cross-Encoder, ColBERT |

### 1.5 RAGé©ç”¨ä¾‹

#### 1.5.1 ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆ

**ã‚·ãƒŠãƒªã‚ª**: è£½å“ãƒãƒ‹ãƒ¥ã‚¢ãƒ«10,000ãƒšãƒ¼ã‚¸ã‹ã‚‰è³ªå•ã«å›ç­”

```
Query: "How do I reset the device?"
Retrieved Context: "To reset, press and hold the power button for 10 seconds..."
Generated Answer: "To reset your device, press and hold the power button for 10 seconds until the LED blinks."
```

**ãƒ¡ãƒªãƒƒãƒˆ**: æœ€æ–°ãƒãƒ‹ãƒ¥ã‚¢ãƒ«å‚ç…§ã€å‡ºå…¸æ˜ç¤ºã§ä¿¡é ¼æ€§å‘ä¸Š

#### 1.5.2 æ³•å‹™ãƒ»ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹

**ã‚·ãƒŠãƒªã‚ª**: æ³•ä»¤ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰é–¢é€£æ¡æ–‡ã‚’æ¤œç´¢

```
Query: "What are GDPR requirements for data retention?"
Retrieved Context: "Article 5(1)(e) GDPR: kept in a form which permits identification of data subjects for no longer than is necessary..."
Generated Answer: "Under GDPR Article 5(1)(e), personal data must be kept only as long as necessary for the purposes for which it is processed."
```

**ãƒ¡ãƒªãƒƒãƒˆ**: æ­£ç¢ºãªæ³•ä»¤å¼•ç”¨ã€æœ€æ–°æ”¹æ­£ã«è‡ªå‹•å¯¾å¿œ

#### 1.5.3 ç¤¾å†…ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹

**ã‚·ãƒŠãƒªã‚ª**: Slack/Notion/Confluenceã‹ã‚‰ç¤¾å†…æƒ…å ±æ¤œç´¢

```
Query: "What is the procedure for expense reimbursement?"
Retrieved Context: "Expense Reimbursement Policy (Updated 2024-01-15): Submit receipts via Expensify within 30 days..."
Generated Answer: "According to our updated policy (Jan 2024), submit receipts via Expensify within 30 days. Approvals take 3-5 business days."
```

**ãƒ¡ãƒªãƒƒãƒˆ**: åˆ†æ•£çŸ¥è­˜ã®çµ±åˆã€å¸¸ã«æœ€æ–°æƒ…å ±

:::message
**é€²æ—: 10% å®Œäº†** RAGã®å…¨ä½“åƒã‚’æŠŠæ¡ã—ãŸã€‚ã“ã“ã‹ã‚‰æ•°å¼ä¿®è¡Œã§æ¤œç´¢ãƒ»Embeddingãƒ»è©•ä¾¡ã®ç†è«–ã‚’å®Œå…¨æ§‹ç¯‰ã™ã‚‹ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœRAGãŒå¿…é ˆãªã®ã‹

### 2.1 æœ¬ã‚·ãƒªãƒ¼ã‚ºã«ãŠã‘ã‚‹ä½ç½®ã¥ã‘

```mermaid
graph TD
    C3["Course III<br/>ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç¤¾ä¼šå®Ÿè£…"]
    C3 --> L28["ç¬¬28å›<br/>ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"]
    L28 --> L29["ç¬¬29å›<br/>ğŸ”RAG<br/>(ä»Šå›)"]
    L29 --> L30["ç¬¬30å›<br/>ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"]
    L30 --> L31["ç¬¬31å›<br/>MLOps"]
    style L29 fill:#c8e6c9
```

**Course IIIã®å¤–éƒ¨çŸ¥è­˜çµ±åˆç·¨ã€‚** ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ(ç¬¬28å›)ã§æŒ‡ç¤ºã‚’æœ€é©åŒ–ã—ã€RAG(æœ¬è¬›ç¾©)ã§å¤–éƒ¨çŸ¥è­˜ã‚’çµ±åˆã—ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ(ç¬¬30å›)ã§è‡ªå¾‹è¡Œå‹•ã‚’å®Ÿç¾ã™ã‚‹ã€‚

### 2.2 RAGãŒå¿…é ˆã®3ã¤ã®ç†ç”±

#### 2.2.1 çŸ¥è­˜ã®é®®åº¦å•é¡Œ

**LLMã®çŸ¥è­˜ã‚«ãƒƒãƒˆã‚ªãƒ•**: GPT-4ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¯2023å¹´9æœˆã¾ã§ â†’ 2024å¹´ä»¥é™ã®æƒ…å ±ã¯çŸ¥ã‚‰ãªã„

| è³ªå• | LLMå˜ä½“ | RAG |
|:-----|:--------|:----|
| 2024å¹´ã®å¤§çµ±é ˜é¸æŒ™çµæœã¯ï¼Ÿ | âŒ çŸ¥è­˜ã‚«ãƒƒãƒˆã‚ªãƒ•å‰ãªã®ã§ç­”ãˆã‚‰ã‚Œãªã„ | âœ… æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢ â†’ æ­£ç¢ºå›ç­” |
| ä»Šæ—¥ã®ç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆã¯ï¼Ÿ | âŒ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿ãªã— | âœ… APIã‹ã‚‰å–å¾— â†’ æ­£ç¢ºå›ç­” |
| ç¤¾å†…ã®æœ€æ–°è¦å®šã¯ï¼Ÿ | âŒ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œãªã„ | âœ… ç¤¾å†…DBã‹ã‚‰æ¤œç´¢ â†’ æ­£ç¢ºå›ç­” |

#### 2.2.2 Hallucination (å¹»è¦š) ã®æŠ‘åˆ¶

LLMã¯çŸ¥ã‚‰ãªã„ã“ã¨ã‚’**è‡ªä¿¡æº€ã€…ã«æé€ **ã™ã‚‹ã€‚

**Without RAG**:
```
User: "What is the capital of Atlantis?"
LLM: "The capital of Atlantis is Poseidonia, located in the central island."
```
ï¼ˆæ¶ç©ºã®éƒ½å¸‚ã«ã¤ã„ã¦å…·ä½“çš„ã«å›ç­” â€” å®Œå…¨ãªHallucinationï¼‰

**With RAG**:
```
User: "What is the capital of Atlantis?"
Retriever: [æ¤œç´¢çµæœãªã—]
LLM: "I couldn't find information about Atlantis in the knowledge base. Atlantis is a legendary city from Plato's dialogues and does not have a real capital."
```

#### 2.2.3 ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒ»ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹

**Fine-tuningã®å•é¡Œ**: ä¼æ¥­å›ºæœ‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¢ãƒ‡ãƒ«ã«å­¦ç¿’ã•ã›ã‚‹ â†’ ãƒ‡ãƒ¼ã‚¿æµå‡ºãƒªã‚¹ã‚¯

**RAGã®åˆ©ç‚¹**:
- ãƒ‡ãƒ¼ã‚¿ã¯ãƒ­ãƒ¼ã‚«ãƒ«DBã«ä¿å­˜ï¼ˆãƒ¢ãƒ‡ãƒ«ã«å«ã¾ã‚Œãªã„ï¼‰
- ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡å¯èƒ½ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼æ¨©é™ã«å¿œã˜ãŸæ¤œç´¢ï¼‰
- ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ãŒå®¹æ˜“ï¼ˆDBã‹ã‚‰å‰Šé™¤ã™ã‚‹ã ã‘ï¼‰

### 2.3 æœ¬è¬›ç¾©ã§å­¦ã¶ã“ã¨

| ãƒˆãƒ”ãƒƒã‚¯ | è¡Œæ•° | é›£æ˜“åº¦ | å®Ÿè£… |
|:--------|:-----|:-------|:-----|
| **Zone 3.1** Embeddingç†è«– | 300 | â˜…â˜…â˜… | Sentence-BERTå®Ÿè£… |
| **Zone 3.2** BM25å®Œå…¨ç‰ˆ | 250 | â˜…â˜…â˜…â˜… | IDF/TFè¨ˆç®—ãƒ»ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ |
| **Zone 3.3** Dense Retrieval | 300 | â˜…â˜…â˜…â˜… | Bi-Encoderå®Ÿè£… |
| **Zone 3.4** Hybrid Search | 250 | â˜…â˜…â˜…â˜… | BM25+Denseèåˆãƒ»RRF |
| **Zone 3.5** Reranking | 300 | â˜…â˜…â˜…â˜…â˜… | Cross-Encoder/ColBERT |
| **Zone 3.6** Agentic RAG | 350 | â˜…â˜…â˜…â˜…â˜… | Self-RAG/CRAG/Adaptive |
| **Zone 4** ğŸ¦€Rust Vector DB | 600 | â˜…â˜…â˜…â˜… | HNSW/Qdrantçµ±åˆ |
| **Zone 4** âš¡Juliaæ¤œç´¢ | 400 | â˜…â˜…â˜…â˜… | BM25/Embedding/Rerank |
| **Zone 4** ğŸ”®Elixir RAGã‚µãƒ¼ãƒ“ãƒ³ã‚° | 300 | â˜…â˜…â˜…â˜… | åˆ†æ•£æ¤œç´¢ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚° |
| **Zone 5** RAGè©•ä¾¡ | 300 | â˜…â˜…â˜… | RAGAS/Faithfulness |

### 2.4 å­¦ç¿’æˆ¦ç•¥ â€” 3ã¤ã®ãƒ•ã‚§ãƒ¼ã‚º

```mermaid
graph LR
    P1["ğŸ“– Phase 1<br/>ç†è«–ç¿’å¾—<br/>(Zone 3)"] --> P2["ğŸ’» Phase 2<br/>å®Ÿè£…<br/>(Zone 4)"]
    P2 --> P3["ğŸ”¬ Phase 3<br/>è©•ä¾¡<br/>(Zone 5)"]
    P1 -.BM25/Dense/Hybrid.-> P2
    P2 -.Rust/Julia/Elixir.-> P3
    P3 -.RAGASè©•ä¾¡.-> P1
```

**æ¨å¥¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ1é€±é–“ï¼‰**:

| Day | å†…å®¹ | æ™‚é–“ |
|:----|:-----|:-----|
| Day 1 | Zone 0-2 + Zone 3.1-3.2 (Embedding/BM25) | 2h |
| Day 2 | Zone 3.3-3.4 (Dense/Hybrid) | 2h |
| Day 3 | Zone 3.5-3.6 (Reranking/Agentic) | 2h |
| Day 4 | Zone 4 Rust Vector DBå®Ÿè£… | 3h |
| Day 5 | Zone 4 Juliaæ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ | 2h |
| Day 6 | Zone 4 Elixir RAGã‚µãƒ¼ãƒ“ãƒ³ã‚° | 2h |
| Day 7 | Zone 5-7 (è©•ä¾¡/å®Ÿé¨“/å¾©ç¿’) | 2h |

:::details ãƒˆãƒ­ã‚¤ã®æœ¨é¦¬: 3è¨€èªRAGãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯
æœ¬è¬›ç¾©ã§ã¯**Rust + Julia + Elixir**ã§RAGã‚’å®Ÿè£…:

- **ğŸ¦€ Rust**: ãƒ™ã‚¯ãƒˆãƒ«DB (HNSWå®Ÿè£…, Qdrantçµ±åˆ)
- **âš¡ Julia**: æ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ (BM25, Embedding, Reranking)
- **ğŸ”® Elixir**: åˆ†æ•£RAGã‚µãƒ¼ãƒ“ãƒ³ã‚° (GenServer, ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°, ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°)

ç¬¬28å›ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã¨ã€æœ¬è¬›ç¾©ã®RAGã‚’çµ„ã¿åˆã‚ã›ã‚Œã°ã€**Production-readyãªRAGã‚·ã‚¹ãƒ†ãƒ **ãŒæ§‹ç¯‰ã§ãã‚‹ã€‚
:::

:::message
**é€²æ—: 20% å®Œäº†** RAGã®å…¨ä½“åƒã¨å¿…è¦æ€§ã‚’ç†è§£ã—ãŸã€‚ã“ã“ã‹ã‚‰60åˆ†ã®æ•°å¼ä¿®è¡Œã«å…¥ã‚‹ â€” Embeddingç†è«–ã‹ã‚‰Agentic RAGã¾ã§å®Œå…¨å°å‡ºã™ã‚‹ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” RAGç†è«–ã®å®Œå…¨æ§‹ç¯‰

### 3.1 Embeddingç†è«– â€” ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã«åŸ‹ã‚è¾¼ã‚€

#### 3.1.1 Embeddingã®å®šç¾©

**Embedding**: é«˜æ¬¡å…ƒã®é›¢æ•£ã‚·ãƒ³ãƒœãƒ«ï¼ˆå˜èªãƒ»æ–‡ï¼‰ã‚’ä½æ¬¡å…ƒã®é€£ç¶šãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã«å†™åƒ

$$
f: \mathcal{V} \to \mathbb{R}^d
$$

ã“ã“ã§:
- $\mathcal{V}$: èªå½™ç©ºé–“ï¼ˆé›¢æ•£ï¼‰
- $\mathbb{R}^d$: Embeddingç©ºé–“ï¼ˆé€£ç¶šã€$d \approx 384\text{-}1536$ï¼‰

**Distributional Hypothesis** (Harris 1954):

> *"You shall know a word by the company it keeps"*

å˜èªã®æ„å‘³ã¯æ–‡è„ˆã«ã‚ˆã£ã¦æ±ºã¾ã‚‹ â†’ é¡ä¼¼æ–‡è„ˆã®å˜èªã¯é¡ä¼¼Embeddingã‚’æŒã¤ã€‚

#### 3.1.2 Word Embeddings (Word2Vec, GloVe)

**Word2Vec** (Mikolov+ 2013):

$$
\max_{\theta} \sum_{t=1}^T \sum_{-c \leq j \leq c, j \neq 0} \log P(w_{t+j} \mid w_t; \theta)
$$

ã“ã“ã§:
- $w_t$: ä¸­å¿ƒèª
- $w_{t+j}$: æ–‡è„ˆèª
- $c$: ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º

**GloVe** (Pennington+ 2014):

$$
\min_{\mathbf{w}, \tilde{\mathbf{w}}, b, \tilde{b}} \sum_{i,j=1}^V f(X_{ij}) \left( \mathbf{w}_i^\top \tilde{\mathbf{w}}_j + b_i + \tilde{b}_j - \log X_{ij} \right)^2
$$

ã“ã“ã§:
- $X_{ij}$: å˜èª $i$ ã¨ $j$ ã®å…±èµ·å›æ•°
- $\mathbf{w}_i, \tilde{\mathbf{w}}_j$: Embedding
- $f(X_{ij})$: é‡ã¿é–¢æ•°ï¼ˆé »å‡ºèªã‚’æŠ‘åˆ¶ï¼‰

#### 3.1.3 Sentence Embeddings (BERT, Sentence-BERT)

**BERT** (Devlin+ 2019):

æ–‡å…¨ä½“ã®Embedding: $[CLS]$ ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ™ã‚¯ãƒˆãƒ«

$$
\mathbf{h}_{\text{[CLS]}} = \text{Encoder}(\text{[CLS]}, w_1, \ldots, w_n)
$$

**å•é¡Œ**: BERTã¯æ–‡ãƒšã‚¢ã‚’jointã«å‡¦ç† â†’ $n$ æ–‡ã®é¡ä¼¼åº¦è¨ˆç®—ã« $O(n^2)$ ã®æ¨è«–ãŒå¿…è¦

**Sentence-BERT** (Reimers & Gurevych 2019):

Siamese Network ã§ç‹¬ç«‹ã«Encode:

$$
\begin{aligned}
\mathbf{u} &= \text{BERT}(s_1) \quad \text{(sentence 1)} \\
\mathbf{v} &= \text{BERT}(s_2) \quad \text{(sentence 2)} \\
\text{sim}(s_1, s_2) &= \cos(\mathbf{u}, \mathbf{v}) = \frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{u}\| \|\mathbf{v}\|}
\end{aligned}
$$

**å­¦ç¿’**: Contrastive Loss or Triplet Loss

$$
\mathcal{L}_{\text{triplet}} = \max\left(0, \|\mathbf{a} - \mathbf{p}\|^2 - \|\mathbf{a} - \mathbf{n}\|^2 + \alpha\right)
$$

ã“ã“ã§:
- $\mathbf{a}$: anchor (åŸºæº–æ–‡)
- $\mathbf{p}$: positive (é¡ä¼¼æ–‡)
- $\mathbf{n}$: negative (éé¡ä¼¼æ–‡)
- $\alpha$: margin

#### 3.1.4 Contrastive Learning (SimCLR, InfoNCE)

**InfoNCE Loss** (Oord+ 2018):

$$
\mathcal{L} = -\log \frac{\exp(\text{sim}(\mathbf{z}_i, \mathbf{z}_j) / \tau)}{\sum_{k=1}^{2N} \mathbb{1}_{k \neq i} \exp(\text{sim}(\mathbf{z}_i, \mathbf{z}_k) / \tau)}
$$

ã“ã“ã§:
- $\mathbf{z}_i, \mathbf{z}_j$: positive pair
- $\tau$: temperature
- $N$: ãƒãƒƒãƒã‚µã‚¤ã‚º

**ç›´æ„Ÿ**: positive pairã®é¡ä¼¼åº¦ã‚’æœ€å¤§åŒ–ã€negative pairsã¨ã®é¡ä¼¼åº¦ã‚’æœ€å°åŒ–

#### 3.1.5 Embedding Qualityè©•ä¾¡

**STS (Semantic Textual Similarity) Benchmark**:

$$
\text{Spearman}(\{\text{sim}_{\text{pred}}\}, \{\text{sim}_{\text{human}}\})
$$

äººé–“ã®é¡ä¼¼åº¦è©•ä¾¡ã¨äºˆæ¸¬é¡ä¼¼åº¦ã®Spearmanç›¸é–¢ã€‚

**MTEB (Massive Text Embedding Benchmark)** (2022):

56ã‚¿ã‚¹ã‚¯ã§Embeddingå“è³ªã‚’ç·åˆè©•ä¾¡ï¼ˆRetrieval, Classification, Clustering, STSç­‰ï¼‰

### 3.2 BM25 (Best Matching 25) â€” ã‚¹ãƒ‘ãƒ¼ã‚¹æ¤œç´¢ã®ç‹é“

#### 3.2.1 BM25ã®å®šç¾©

**BM25** (Robertson & Zaragoza 2009):

$$
\text{BM25}(D, Q) = \sum_{i=1}^n \text{IDF}(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot \left(1 - b + b \cdot \frac{|D|}{\text{avgdl}}\right)}
$$

ã“ã“ã§:
- $D$: æ–‡æ›¸
- $Q = \{q_1, \ldots, q_n\}$: ã‚¯ã‚¨ãƒªã®å˜èªé›†åˆ
- $f(q_i, D)$: æ–‡æ›¸ $D$ ã«ãŠã‘ã‚‹å˜èª $q_i$ ã®å‡ºç¾é »åº¦ (TF)
- $|D|$: æ–‡æ›¸ $D$ ã®é•·ã•ï¼ˆå˜èªæ•°ï¼‰
- $\text{avgdl}$: ã‚³ãƒ¼ãƒ‘ã‚¹ã®å¹³å‡æ–‡æ›¸é•·
- $k_1, b$: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé€šå¸¸ $k_1=1.2, b=0.75$ï¼‰

**IDF (Inverse Document Frequency)**:

$$
\text{IDF}(q_i) = \log \frac{N - n(q_i) + 0.5}{n(q_i) + 0.5}
$$

ã“ã“ã§:
- $N$: ã‚³ãƒ¼ãƒ‘ã‚¹ã®ç·æ–‡æ›¸æ•°
- $n(q_i)$: å˜èª $q_i$ ã‚’å«ã‚€æ–‡æ›¸æ•°

#### 3.2.2 BM25ã®ç›´æ„Ÿ

**TF (Term Frequency) éƒ¨åˆ†**:

$$
\frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot \left(1 - b + b \cdot \frac{|D|}{\text{avgdl}}\right)}
$$

- $f(q_i, D) \uparrow$ â†’ ã‚¹ã‚³ã‚¢ $\uparrow$ ï¼ˆå˜èªãŒé »å‡º â†’ é–¢é€£æ€§é«˜ï¼‰
- ã ãŒ $f(q_i, D) \to \infty$ ã§ã‚‚ $\to k_1 + 1$ ï¼ˆé£½å’Œï¼‰
- $|D| \uparrow$ â†’ åˆ†æ¯ $\uparrow$ â†’ ã‚¹ã‚³ã‚¢ $\downarrow$ ï¼ˆé•·æ–‡æ›¸ã‚’æ­£è¦åŒ–ï¼‰

**IDFéƒ¨åˆ†**:

$$
\text{IDF}(q_i) = \log \frac{N - n(q_i) + 0.5}{n(q_i) + 0.5}
$$

- $n(q_i) \downarrow$ â†’ IDF $\uparrow$ ï¼ˆãƒ¬ã‚¢å˜èª â†’ é‡è¦ï¼‰
- $n(q_i) \uparrow$ â†’ IDF $\downarrow$ ï¼ˆé »å‡ºå˜èª â†’ é‡è¦åº¦ä½ï¼‰

#### 3.2.3 BM25ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´

**$k_1$**: TFã®é£½å’Œåº¦ã‚’åˆ¶å¾¡

- $k_1 = 0$: TFã‚’ç„¡è¦–ï¼ˆIDF onlyï¼‰
- $k_1 \to \infty$: TFã®é£½å’Œãªã—ï¼ˆç”Ÿã®TFï¼‰
- æ¨å¥¨: $k_1 \in [1.2, 2.0]$

**$b$**: æ–‡æ›¸é•·æ­£è¦åŒ–ã®å¼·åº¦

- $b = 0$: æ­£è¦åŒ–ãªã—ï¼ˆçŸ­æ–‡æ›¸ã¨é•·æ–‡æ›¸ã‚’åŒç­‰ã«æ‰±ã†ï¼‰
- $b = 1$: å®Œå…¨æ­£è¦åŒ–ï¼ˆé•·æ–‡æ›¸ã‚’å³ã—ããƒšãƒŠãƒ«ãƒ†ã‚£ï¼‰
- æ¨å¥¨: $b \in [0.75, 0.85]$

#### 3.2.4 æ•°å€¤æ¤œè¨¼: BM25è¨ˆç®—

```julia
# BM25 calculation example
function bm25_score(query_terms::Vector{String}, doc_terms::Vector{String},
                    doc_freq::Dict{String, Int}, n_docs::Int, avg_doc_len::Float64,
                    k1::Float64=1.2, b::Float64=0.75)
    score = 0.0
    doc_len = length(doc_terms)

    for term in query_terms
        # TF: term frequency in document
        tf = count(==(term), doc_terms)

        # DF: number of documents containing term
        df = get(doc_freq, term, 0)

        # IDF
        idf = log((n_docs - df + 0.5) / (df + 0.5))

        # BM25 formula
        numerator = tf * (k1 + 1)
        denominator = tf + k1 * (1 - b + b * (doc_len / avg_doc_len))

        score += idf * (numerator / denominator)
    end

    return score
end

# Example
query = ["capital", "france"]
doc1 = ["paris", "is", "the", "capital", "of", "france"]
doc2 = ["london", "is", "the", "capital", "of", "england"]
doc_freq = Dict("capital" => 2, "france" => 1, "paris" => 1, "london" => 1, "england" => 1)
n_docs = 2
avg_doc_len = 6.0

score1 = bm25_score(query, doc1, doc_freq, n_docs, avg_doc_len)
score2 = bm25_score(query, doc2, doc_freq, n_docs, avg_doc_len)

println("BM25 Score (Doc1): $(round(score1, digits=3))")
println("BM25 Score (Doc2): $(round(score2, digits=3))")
```

### 3.3 Dense Retrieval â€” Neural Embeddingç©ºé–“ã§ã®æ¤œç´¢

#### 3.3.1 Bi-Encoder Architecture

**Bi-Encoder**: ã‚¯ã‚¨ãƒªã¨æ–‡æ›¸ã‚’ç‹¬ç«‹ã«Encode

$$
\begin{aligned}
\mathbf{q} &= f_Q(\text{Query}; \theta_Q) \quad \in \mathbb{R}^d \\
\mathbf{d} &= f_D(\text{Document}; \theta_D) \quad \in \mathbb{R}^d \\
\text{sim}(Q, D) &= \mathbf{q}^\top \mathbf{d} = \cos(\mathbf{q}, \mathbf{d}) \cdot \|\mathbf{q}\| \cdot \|\mathbf{d}\|
\end{aligned}
$$

é€šå¸¸ $\|\mathbf{q}\| = \|\mathbf{d}\| = 1$ ã«æ­£è¦åŒ– â†’ $\text{sim} = \cos(\mathbf{q}, \mathbf{d})$

**åˆ©ç‚¹**:
- æ–‡æ›¸ã‚’ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§Encodeå¯èƒ½ â†’ Vector DBã«ä¿å­˜
- ã‚¯ã‚¨ãƒªæ™‚ã¯ $\mathbf{q}$ ã®ã¿Encode â†’ é«˜é€Ÿ

**å­¦ç¿’**: In-batch Negatives (InfoNCE)

$$
\mathcal{L} = -\log \frac{\exp(\mathbf{q}^\top \mathbf{d}^+ / \tau)}{\exp(\mathbf{q}^\top \mathbf{d}^+ / \tau) + \sum_{i=1}^{B-1} \exp(\mathbf{q}^\top \mathbf{d}_i^- / \tau)}
$$

ã“ã“ã§:
- $\mathbf{d}^+$: positive document
- $\mathbf{d}_i^-$: negative documents (åŒä¸€ãƒãƒƒãƒå†…ã®ä»–ã®æ–‡æ›¸)
- $B$: ãƒãƒƒãƒã‚µã‚¤ã‚º

#### 3.3.2 Dense Passage Retrieval (DPR)

**DPR** (Karpukhin+ 2020):

$$
\text{sim}(q, d) = \mathbf{E}_Q(q)^\top \mathbf{E}_D(d)
$$

$\mathbf{E}_Q, \mathbf{E}_D$: BERT-based encoders

**Hard Negative Mining**:

ãƒ©ãƒ³ãƒ€ãƒ ãªnegativeã§ã¯ãªãã€**BM25ã§Top-kã ãŒGold labelã§ãªã„ã‚‚ã®**ã‚’negativeã¨ã—ã¦ä½¿ç”¨ â†’ å­¦ç¿’åŠ¹ç‡å‘ä¸Š

$$
\mathcal{L} = -\log \frac{\exp(\mathbf{q}^\top \mathbf{d}^+)}{\exp(\mathbf{q}^\top \mathbf{d}^+) + \sum_{d^- \in \text{HardNeg}} \exp(\mathbf{q}^\top \mathbf{d}^-)}
$$

#### 3.3.3 Approximate Nearest Neighbor (ANN) Search

**å•é¡Œ**: $N$ æ–‡æ›¸ã‹ã‚‰ Top-k ã‚’æ¢ã™ã®ã« $O(Nd)$ ã®è¨ˆç®— â†’ $N=10^9$ ã§éç¾å®Ÿçš„

**è§£æ±º**: Approximate Nearest Neighbor (ANN)

| æ‰‹æ³• | åŸç† | è¨ˆç®—é‡ | ç²¾åº¦ |
|:-----|:-----|:-------|:-----|
| **HNSW** | éšå±¤ã‚°ãƒ©ãƒ• | $O(\log N)$ | é«˜ |
| **IVF** | ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° | $O(\sqrt{N})$ | ä¸­ |
| **Product Quantization** | ãƒ™ã‚¯ãƒˆãƒ«é‡å­åŒ– | $O(N/m)$ | ä½ |

**HNSW (Hierarchical Navigable Small World)**:

éšå±¤çš„ãªã‚°ãƒ©ãƒ•æ§‹é€ ã§è¿‘å‚æ¢ç´¢ã‚’é«˜é€ŸåŒ–ã€‚

$$
\begin{aligned}
&\text{Layer 0 (densest): å…¨ãƒãƒ¼ãƒ‰} \\
&\text{Layer 1: ã‚µãƒ–ã‚µãƒ³ãƒ—ãƒ«} \\
&\text{Layer } L\text{: ç²—ã„ã‚°ãƒ©ãƒ•} \\
&\text{Search: Layer } L \to 0 \text{ ã«é™ã‚ŠãªãŒã‚‰è¿‘å‚æ¢ç´¢}
\end{aligned}
$$

**è¨ˆç®—é‡**: $O(\log N)$ (å¹³å‡)ã€ç²¾åº¦: 95-99%

### 3.4 Hybrid Retrieval â€” Sparse + Dense ã®çµ±åˆ

#### 3.4.1 Hybrid Search ã®å‹•æ©Ÿ

**BM25 (Sparse)ã®å¼·ã¿**:
- ãƒ¬ã‚¢å˜èªãƒ»å›ºæœ‰åè©ã«å¼·ã„
- å®Œå…¨ä¸€è‡´ã«å¼·ã„
- é«˜é€Ÿ

**Dense (Neural)ã®å¼·ã¿**:
- æ„å‘³çš„é¡ä¼¼æ€§ã«å¼·ã„
- è¨€ã„æ›ãˆãƒ»åŒç¾©èªã«å¼·ã„
- å¤šè¨€èªå¯¾å¿œ

**ä¸¡è€…ã¯ç›¸è£œçš„** â†’ çµ±åˆã™ã‚‹ã¨ç²¾åº¦å‘ä¸Š

#### 3.4.2 Reciprocal Rank Fusion (RRF)

**RRF** (Cormack+ 2009):

BM25ã¨Denseã®æ¤œç´¢çµæœã‚’çµ±åˆã€‚

$$
\text{RRF}(d) = \sum_{r \in \{r_{\text{BM25}}, r_{\text{Dense}}\}} \frac{1}{k + \text{rank}_r(d)}
$$

ã“ã“ã§:
- $\text{rank}_r(d)$: æ¤œç´¢æ‰‹æ³• $r$ ã«ãŠã‘ã‚‹æ–‡æ›¸ $d$ ã®ãƒ©ãƒ³ã‚¯
- $k$: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé€šå¸¸ $k=60$ï¼‰

**ç›´æ„Ÿ**: ä¸¡æ–¹ã§ä¸Šä½ã«ãƒ©ãƒ³ã‚¯ã•ã‚ŒãŸæ–‡æ›¸ãŒé«˜ã‚¹ã‚³ã‚¢

**ä¾‹**:

| Document | BM25 Rank | Dense Rank | RRF Score |
|:---------|:----------|:-----------|:----------|
| Doc A | 1 | 3 | $\frac{1}{60+1} + \frac{1}{60+3} = 0.032$ |
| Doc B | 2 | 1 | $\frac{1}{60+2} + \frac{1}{60+1} = 0.032$ |
| Doc C | 3 | 2 | $\frac{1}{60+3} + \frac{1}{60+2} = 0.032$ |

#### 3.4.3 Weighted Fusion

**Weighted Sum**:

$$
\text{Score}(d) = \alpha \cdot \text{Score}_{\text{BM25}}(d) + (1 - \alpha) \cdot \text{Score}_{\text{Dense}}(d)
$$

$\alpha$: BM25ã¨Denseã®é‡ã¿ï¼ˆé€šå¸¸ $\alpha \in [0.3, 0.7]$ï¼‰

**å•é¡Œ**: ã‚¹ã‚³ã‚¢ã®ã‚¹ã‚±ãƒ¼ãƒ«ãŒç•°ãªã‚‹ â†’ æ­£è¦åŒ–ãŒå¿…è¦

**Min-Maxæ­£è¦åŒ–**:

$$
\text{Score}_{\text{norm}}(d) = \frac{\text{Score}(d) - \min_i \text{Score}(d_i)}{\max_i \text{Score}(d_i) - \min_i \text{Score}(d_i)}
$$

### 3.5 Reranking â€” æ¤œç´¢çµæœã®ç²¾åº¦å‘ä¸Š

#### 3.5.1 Cross-Encoder

**Bi-Encoder vs Cross-Encoder**:

| | Bi-Encoder | Cross-Encoder |
|:--|:-----------|:--------------|
| **Input** | Query, Document ã‚’ç‹¬ç«‹ã«Encode | $[\text{CLS}] Q [\text{SEP}] D [\text{SEP}]$ ã‚’ä¸€ç·’ã«Encode |
| **Interaction** | ãªã—ï¼ˆãƒ‰ãƒƒãƒˆç©ã®ã¿ï¼‰ | ã‚ã‚Šï¼ˆAttentionå±¤ã§ç›¸äº’ä½œç”¨ï¼‰ |
| **ç²¾åº¦** | ä¸­ | é«˜ |
| **é€Ÿåº¦** | é€Ÿï¼ˆãƒ™ã‚¯ãƒˆãƒ«DBæ´»ç”¨ï¼‰ | é…ï¼ˆå„ãƒšã‚¢ã§æ¨è«–å¿…è¦ï¼‰ |

**Cross-Encoder Score**:

$$
\text{Score}(Q, D) = \sigma(\mathbf{W} \cdot \text{BERT}([Q; D])_{\text{[CLS]}})
$$

$\sigma$: sigmoid

**ä½¿ã„åˆ†ã‘**:
1. **Retrieval**: Bi-Encoder ã§ Top-100 ã‚’å–å¾—ï¼ˆé«˜é€Ÿï¼‰
2. **Reranking**: Cross-Encoder ã§ Top-100 ã‚’ Top-10 ã«çµã‚Šè¾¼ã¿ï¼ˆé«˜ç²¾åº¦ï¼‰

#### 3.5.2 ColBERT (Late Interaction)

**ColBERT** (Khattab & Zaharia 2020):

Bi-Encoderã®é€Ÿåº¦ + Cross-Encoderã®ç²¾åº¦ã‚’ä¸¡ç«‹ã€‚

**ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**:

$$
\begin{aligned}
\mathbf{E}_Q &= \text{BERT}(Q) \quad \in \mathbb{R}^{|Q| \times d} \quad \text{(token-level embeddings)} \\
\mathbf{E}_D &= \text{BERT}(D) \quad \in \mathbb{R}^{|D| \times d} \\
\text{Score}(Q, D) &= \sum_{i=1}^{|Q|} \max_{j=1}^{|D|} \mathbf{E}_Q[i] \cdot \mathbf{E}_D[j]^\top
\end{aligned}
$$

**MaxSim**: å„ã‚¯ã‚¨ãƒªãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã—ã€æœ€ã‚‚é¡ä¼¼ã™ã‚‹æ–‡æ›¸ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¦‹ã¤ã‘ã¦ã‚¹ã‚³ã‚¢åŒ–

**åˆ©ç‚¹**:
- æ–‡æ›¸ã‚’ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§Encodeå¯èƒ½ï¼ˆBi-EncoderåŒæ§˜ï¼‰
- Token-levelã®ç›¸äº’ä½œç”¨ï¼ˆCross-Encoderçš„ï¼‰
- é€Ÿåº¦: Bi-Encoderã®2-3å€é…ã„ãŒã€Cross-Encoderã®10å€é€Ÿ

### 3.6 Agentic RAG â€” è‡ªå¾‹çš„æ¤œç´¢åˆ¶å¾¡

#### 3.6.1 Self-RAG (Self-Reflective RAG)

**Self-RAG** (Asai+ 2024) [^2]:

LLMãŒ**åçœãƒˆãƒ¼ã‚¯ãƒ³**ã‚’ç”Ÿæˆã—ã€æ¤œç´¢ãƒ»ç”Ÿæˆã‚’è‡ªå·±åˆ¶å¾¡ã€‚

**åçœãƒˆãƒ¼ã‚¯ãƒ³ã®ç¨®é¡**:

| ãƒˆãƒ¼ã‚¯ãƒ³ | æ„å‘³ | ä¾‹ |
|:--------|:-----|:---|
| **[Retrieval]** | æ¤œç´¢ãŒå¿…è¦ã‹ | Yes/No |
| **[IsRel]** | æ¤œç´¢çµæœãŒé–¢é€£ã—ã¦ã„ã‚‹ã‹ | Relevant/Irrelevant |
| **[IsSup]** | ç”ŸæˆãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æ”¯æŒã•ã‚Œã¦ã„ã‚‹ã‹ | Fully/Partially/No |
| **[IsUse]** | ç”ŸæˆãŒã‚¯ã‚¨ãƒªã«æœ‰ç”¨ã‹ | 5/4/3/2/1 |

**ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼**:

```
1. Query â†’ LLM generates [Retrieval] token
2. If [Retrieval]=Yes â†’ Retrieve documents
3. LLM generates answer + [IsRel], [IsSup], [IsUse] tokens
4. If [IsSup]=No â†’ Re-retrieve or generate from memory
5. Return best answer based on reflection scores
```

**å­¦ç¿’**:

$$
\mathcal{L} = \mathcal{L}_{\text{LM}} + \lambda \mathcal{L}_{\text{Reflection}}
$$

åçœãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦å­¦ç¿’ã€‚

#### 3.6.2 CRAG (Corrective RAG)

**CRAG** (Yan+ 2024) [^3]:

æ¤œç´¢çµæœã®**æ­£ç¢ºæ€§ã‚’è©•ä¾¡**ã—ã€ä¸æ­£ç¢ºãªã‚‰è£œæ­£ã€‚

**ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼**:

```
1. Query â†’ Retrieve top-k documents
2. Evaluator: Score each document â†’ {Correct, Ambiguous, Incorrect}
3. If all Correct â†’ Generate
4. If some Ambiguous â†’ Re-retrieve with query refinement
5. If Incorrect â†’ Use web search to augment knowledge
6. Generate answer from corrected context
```

**Evaluator**:

è»½é‡LM (T5-baseç­‰) ã§æ–‡æ›¸ã®æ­£ç¢ºæ€§ã‚’ã‚¹ã‚³ã‚¢åŒ–:

$$
p_{\text{correct}} = \sigma(\mathbf{W} \cdot \text{Encoder}(Q, D))
$$

**Knowledge Refinement**:

ä¸æ­£ç¢ºãªæ–‡æ›¸ã‹ã‚‰é–¢é€£éƒ¨åˆ†ã®ã¿æŠ½å‡ºï¼ˆæ–‡å˜ä½ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰ã€‚

#### 3.6.3 Adaptive-RAG

**Adaptive-RAG** (Jeong+ 2024):

ã‚¯ã‚¨ãƒªã®**è¤‡é›‘åº¦ã«å¿œã˜ã¦æ¤œç´¢æˆ¦ç•¥ã‚’å‹•çš„é¸æŠ**ã€‚

**æˆ¦ç•¥**:

| ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ— | æˆ¦ç•¥ | ä¾‹ |
|:-----------|:-----|:---|
| **Simple** | LLMã®ã¿ï¼ˆæ¤œç´¢ä¸è¦ï¼‰ | "What is 2+2?" |
| **Single-hop** | 1å›æ¤œç´¢ | "What is the capital of France?" |
| **Multi-hop** | åå¾©æ¤œç´¢ | "Who is the spouse of the director of Inception?" |

**Complexity Classifier**:

$$
p_{\text{complexity}} = \text{Classifier}(Q) \quad \in \{\text{Simple, Single, Multi}\}
$$

**Multi-hop Reasoning**:

```
1. Query â†’ Classify as Multi-hop
2. Retrieve documents for sub-query 1
3. Extract intermediate answer
4. Generate sub-query 2 using intermediate answer
5. Retrieve documents for sub-query 2
6. Generate final answer
```

:::message alert
**ãƒœã‚¹æˆ¦: RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œå…¨å®Ÿè£…**

ä»¥ä¸‹ã®RAGã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã›ã‚ˆ:

1. **Embedding**: Sentence-BERTã§æ–‡æ›¸ã‚’Embedding
2. **Vector DB**: HNSW indexã§Top-kæ¤œç´¢
3. **Hybrid Retrieval**: BM25ã¨Dense retrieval ã‚’RRFã§çµ±åˆ
4. **Reranking**: Cross-Encoderã§å†é †ä½ä»˜ã‘
5. **Agentic RAG**: Self-RAGã§åçœãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ
6. **è©•ä¾¡**: RAGAS metricsã§è©•ä¾¡ï¼ˆFaithfulness, Context Relevanceï¼‰

**ã‚¿ã‚¹ã‚¯**:
- å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’Rust/Julia/Elixirã§å®Ÿè£…
- 1,000æ–‡æ›¸ã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã§æ¤œç´¢ç²¾åº¦ã‚’æ¸¬å®š
- Latency/Throughputã‚’æœ€é©åŒ–

ã“ã‚ŒãŒã§ãã‚Œã°æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³å®Œå…¨ã‚¯ãƒªã‚¢ï¼
:::

:::message
**é€²æ—: 50% å®Œäº†** RAGç†è«–ã‚’å®Œå…¨ç¿’å¾—ã—ãŸã€‚Embedding/BM25/Dense/Hybrid/Reranking/Agentic RAGã‚’æ•°å¼ã‹ã‚‰å°å‡ºã—ãŸã€‚æ¬¡ã¯å®Ÿè£…ã‚¾ãƒ¼ãƒ³ã§Rust/Julia/Elixirã§å…¨æ‰‹æ³•ã‚’å®Ÿè£…ã™ã‚‹ã€‚
:::

---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” Rust/Julia/Elixirã§RAGã‚’å®Œå…¨å®Ÿè£…

### 4.1 ğŸ¦€ Rust: HNSW Vector Databaseå®Ÿè£…

#### 4.1.1 HNSWã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®åŸç†

**HNSW (Hierarchical Navigable Small World)** [^6] ã¯ã€è¿‘ä¼¼æœ€è¿‘å‚æ¢ç´¢ï¼ˆANNï¼‰ã®æœ€é«˜å³°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€‚

**Key Idea**: éšå±¤çš„ãªã‚°ãƒ©ãƒ•æ§‹é€ ã§ã€ç²—ã„å±¤ã‹ã‚‰ç´°ã‹ã„å±¤ã¸ã¨æ¢ç´¢ã‚’çµã‚Šè¾¼ã‚€ã€‚

```mermaid
graph TD
    L2["Layer 2<br/>(æœ€ç²—)"] --> L1["Layer 1"]
    L1 --> L0["Layer 0<br/>(å…¨ãƒ‡ãƒ¼ã‚¿)"]

    L2 -.Entry Point.-> N1["Node 1"]
    N1 -.Navigate.-> N2["Node 2"]
    N2 -.Descend.-> L1

    style L0 fill:#c8e6c9
```

**éšå±¤æ§‹é€ **:

$$
\begin{aligned}
&\text{Layer } L: \text{ å°‘æ•°ã®ãƒãƒ¼ãƒ‰ï¼ˆé è·é›¢ã‚¸ãƒ£ãƒ³ãƒ—ï¼‰} \\
&\text{Layer } L-1: \text{ ã‚ˆã‚Šå¤šãã®ãƒãƒ¼ãƒ‰} \\
&\vdots \\
&\text{Layer } 0: \text{ å…¨ãƒãƒ¼ãƒ‰ï¼ˆé«˜ç²¾åº¦æ¢ç´¢ï¼‰}
\end{aligned}
$$

**æ¢ç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

```
1. Entry point: æœ€ä¸Šå±¤Lã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆ
2. Greedy search: ç¾åœ¨å±¤ã§æœ€è¿‘å‚ã‚’æ¢ç´¢
3. Descend: ã‚ˆã‚Šä¸‹ã®å±¤ã¸ç§»å‹•
4. Repeat 2-3 until Layer 0
5. Return: Layer 0ã§ã®æœ€è¿‘å‚kå€‹
```

**è¨ˆç®—é‡**:

| Phase | Complexity | èª¬æ˜ |
|:------|:-----------|:-----|
| **Indexæ§‹ç¯‰** | $O(N \log N)$ | Nå€‹ã®ãƒ™ã‚¯ãƒˆãƒ«æŒ¿å…¥ |
| **æ¢ç´¢** | $O(\log N)$ | éšå±¤çš„æ¢ç´¢ |
| **ç²¾åº¦** | 95-99% | Recall@k |

#### 4.1.2 Rustã«ã‚ˆã‚‹åŸºæœ¬å®Ÿè£…

```rust
// HNSW Implementation in Rust
use std::collections::{BinaryHeap, HashMap, HashSet};
use std::cmp::Ordering;

// Vector type (f32 for efficiency)
type Vector = Vec<f32>;

// Distance metric: Euclidean L2
fn l2_distance(a: &Vector, b: &Vector) -> f32 {
    a.iter()
        .zip(b.iter())
        .map(|(x, y)| (x - y).powi(2))
        .sum::<f32>()
        .sqrt()
}

// Cosine similarity (for normalized vectors)
fn cosine_similarity(a: &Vector, b: &Vector) -> f32 {
    let dot: f32 = a.iter().zip(b.iter()).map(|(x, y)| x * y).sum();
    let norm_a: f32 = a.iter().map(|x| x.powi(2)).sum::<f32>().sqrt();
    let norm_b: f32 = b.iter().map(|x| x.powi(2)).sum::<f32>().sqrt();

    dot / (norm_a * norm_b)
}

// Node in HNSW graph
#[derive(Clone)]
struct Node {
    id: usize,
    vector: Vector,
    // Neighbors at each layer: layer -> neighbor_ids
    neighbors: HashMap<usize, Vec<usize>>,
}

impl Node {
    fn new(id: usize, vector: Vector) -> Self {
        Self {
            id,
            vector,
            neighbors: HashMap::new(),
        }
    }
}

// Priority queue element for search
#[derive(Clone, Copy)]
struct SearchCandidate {
    id: usize,
    distance: f32,
}

impl Eq for SearchCandidate {}

impl PartialEq for SearchCandidate {
    fn eq(&self, other: &Self) -> bool {
        self.distance == other.distance
    }
}

impl Ord for SearchCandidate {
    fn cmp(&self, other: &Self) -> Ordering {
        // Min-heap (reverse order)
        other.distance.partial_cmp(&self.distance).unwrap()
    }
}

impl PartialOrd for SearchCandidate {
    fn partial_cmp(&self, other: &Self) -> Option<Ordering> {
        Some(self.cmp(other))
    }
}

// HNSW Index
struct HNSWIndex {
    nodes: Vec<Node>,
    entry_point: Option<usize>,
    max_layers: usize,
    m: usize,          // Max connections per layer
    ef_construction: usize, // Size of dynamic candidate list during construction
    ml: f32,           // Normalization factor for layer assignment
}

impl HNSWIndex {
    fn new(m: usize, ef_construction: usize, max_layers: usize) -> Self {
        Self {
            nodes: Vec::new(),
            entry_point: None,
            max_layers,
            m,
            ef_construction,
            ml: 1.0 / (m as f32).ln(),
        }
    }

    // Assign random layer for new node
    fn random_layer(&self) -> usize {
        let uniform = rand::random::<f32>();
        let layer = (-uniform.ln() * self.ml).floor() as usize;
        layer.min(self.max_layers - 1)
    }

    // Insert vector into index
    fn insert(&mut self, vector: Vector) {
        let id = self.nodes.len();
        let layer = self.random_layer();

        let mut node = Node::new(id, vector.clone());

        // Initialize neighbors for each layer
        for l in 0..=layer {
            node.neighbors.insert(l, Vec::new());
        }

        if self.entry_point.is_none() {
            // First node
            self.entry_point = Some(id);
            self.nodes.push(node);
            return;
        }

        // Search for nearest neighbors at each layer
        let entry = self.entry_point.unwrap();
        let mut current = entry;

        // Traverse from top layer to insertion layer
        for l in (layer + 1..self.max_layers).rev() {
            current = self.search_layer(&vector, current, 1, l)[0].id;
        }

        // Insert and connect at each layer from insertion layer to 0
        for l in (0..=layer).rev() {
            let candidates = self.search_layer(&vector, current, self.ef_construction, l);

            // Select M nearest neighbors
            let m = if l == 0 { self.m * 2 } else { self.m };
            let neighbors: Vec<usize> = candidates
                .iter()
                .take(m)
                .map(|c| c.id)
                .collect();

            node.neighbors.insert(l, neighbors.clone());

            // Bidirectional links
            for &neighbor_id in &neighbors {
                if let Some(neighbor) = self.nodes.get_mut(neighbor_id) {
                    if let Some(neighbor_list) = neighbor.neighbors.get_mut(&l) {
                        neighbor_list.push(id);

                        // Prune if exceeds max connections
                        if neighbor_list.len() > m {
                            neighbor_list.truncate(m);
                        }
                    }
                }
            }

            current = candidates[0].id;
        }

        // Update entry point if new node has higher layer
        if layer > self.max_layer() {
            self.entry_point = Some(id);
        }

        self.nodes.push(node);
    }

    // Get maximum layer of current index
    fn max_layer(&self) -> usize {
        self.nodes
            .iter()
            .flat_map(|n| n.neighbors.keys())
            .max()
            .copied()
            .unwrap_or(0)
    }

    // Search at a specific layer
    fn search_layer(
        &self,
        query: &Vector,
        entry_point: usize,
        ef: usize,
        layer: usize,
    ) -> Vec<SearchCandidate> {
        let mut visited = HashSet::new();
        let mut candidates = BinaryHeap::new();
        let mut w = BinaryHeap::new(); // Working set

        let entry_dist = l2_distance(query, &self.nodes[entry_point].vector);
        candidates.push(SearchCandidate {
            id: entry_point,
            distance: entry_dist,
        });
        w.push(SearchCandidate {
            id: entry_point,
            distance: entry_dist,
        });
        visited.insert(entry_point);

        while let Some(c) = candidates.pop() {
            if c.distance > w.peek().unwrap().distance {
                break;
            }

            // Explore neighbors
            if let Some(neighbors) = self.nodes[c.id].neighbors.get(&layer) {
                for &neighbor_id in neighbors {
                    if visited.insert(neighbor_id) {
                        let dist = l2_distance(query, &self.nodes[neighbor_id].vector);

                        if dist < w.peek().unwrap().distance || w.len() < ef {
                            candidates.push(SearchCandidate {
                                id: neighbor_id,
                                distance: dist,
                            });
                            w.push(SearchCandidate {
                                id: neighbor_id,
                                distance: dist,
                            });

                            if w.len() > ef {
                                w.pop();
                            }
                        }
                    }
                }
            }
        }

        w.into_sorted_vec()
    }

    // Search for k nearest neighbors
    fn search(&self, query: &Vector, k: usize, ef: usize) -> Vec<(usize, f32)> {
        if self.entry_point.is_none() {
            return Vec::new();
        }

        let entry = self.entry_point.unwrap();
        let mut current = entry;

        // Traverse from top to layer 1
        for l in (1..=self.max_layer()).rev() {
            current = self.search_layer(query, current, 1, l)[0].id;
        }

        // Search at layer 0 with larger ef
        let candidates = self.search_layer(query, current, ef.max(k), 0);

        candidates
            .into_iter()
            .take(k)
            .map(|c| (c.id, c.distance))
            .collect()
    }
}
```

#### 4.1.3 qdrantçµ±åˆ â€” Production-ready Vector DB

**qdrant** [^7] ã¯Rustè£½ã®é«˜æ€§èƒ½ãƒ™ã‚¯ãƒˆãƒ«DBã§ã€Productionç’°å¢ƒã§åºƒãä½¿ã‚ã‚Œã¦ã„ã‚‹ã€‚

```rust
// qdrant integration example
use qdrant_client::{client::QdrantClient, qdrant::{
    CreateCollection, Distance, VectorParams, SearchPoints, PointStruct,
}};

async fn qdrant_example() -> Result<(), Box<dyn std::error::Error>> {
    // Connect to qdrant server
    let client = QdrantClient::from_url("http://localhost:6334").build()?;

    // Create collection
    client
        .create_collection(&CreateCollection {
            collection_name: "documents".to_string(),
            vectors_config: Some(VectorParams {
                size: 384, // Embedding dimension
                distance: Distance::Cosine as i32,
                ..Default::default()
            }.into()),
            ..Default::default()
        })
        .await?;

    // Insert vectors
    let points = vec![
        PointStruct::new(
            1,
            vec![0.1, 0.2, 0.3, /* ... 384 dims */],
            serde_json::json!({
                "text": "Paris is the capital of France.",
                "category": "geography"
            }),
        ),
    ];

    client
        .upsert_points("documents", points, None)
        .await?;

    // Search
    let search_result = client
        .search_points(&SearchPoints {
            collection_name: "documents".to_string(),
            vector: vec![0.15, 0.25, 0.35, /* query vector */],
            limit: 10,
            with_payload: Some(true.into()),
            ..Default::default()
        })
        .await?;

    for point in search_result.result {
        println!("ID: {}, Score: {}", point.id.unwrap(), point.score);
    }

    Ok(())
}
```

**qdrant ã®å¼·ã¿**:

| Feature | Description |
|:--------|:------------|
| **HNSW Index** | 95-99% recall, $O(\log N)$ æ¢ç´¢ |
| **Filtering** | Payloadï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼‰ã§ã®äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° |
| **Horizontal Scaling** | Sharding + Replication |
| **Persistence** | WAL + Snapshot for durability |
| **Multi-tenancy** | Collectionåˆ†é›¢ |

#### 4.1.4 Chunkingæˆ¦ç•¥ã®å®Ÿè£…

**Chunking**: é•·æ–‡æ›¸ã‚’æ¤œç´¢å¯èƒ½ãªãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã€‚

##### Fixed-Size Chunking

```rust
fn fixed_size_chunking(text: &str, chunk_size: usize, overlap: usize) -> Vec<String> {
    let words: Vec<&str> = text.split_whitespace().collect();
    let mut chunks = Vec::new();

    let mut i = 0;
    while i < words.len() {
        let end = (i + chunk_size).min(words.len());
        let chunk = words[i..end].join(" ");
        chunks.push(chunk);

        i += chunk_size - overlap;
    }

    chunks
}

// Example
let text = "Paris is the capital of France. It is known for the Eiffel Tower. \
            Tokyo is the capital of Japan.";
let chunks = fixed_size_chunking(text, 10, 2);
for (i, chunk) in chunks.iter().enumerate() {
    println!("Chunk {}: {}", i, chunk);
}
```

##### Semantic Chunking

æ„å‘³çš„å¢ƒç•Œï¼ˆæ–‡ãƒ»æ®µè½ï¼‰ã§ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã€‚

```rust
fn semantic_chunking(text: &str, max_chunk_size: usize) -> Vec<String> {
    let sentences: Vec<&str> = text
        .split('.')
        .filter(|s| !s.trim().is_empty())
        .collect();

    let mut chunks = Vec::new();
    let mut current_chunk = String::new();

    for sentence in sentences {
        let sentence = sentence.trim();
        if current_chunk.len() + sentence.len() > max_chunk_size && !current_chunk.is_empty() {
            chunks.push(current_chunk.clone());
            current_chunk.clear();
        }
        current_chunk.push_str(sentence);
        current_chunk.push_str(". ");
    }

    if !current_chunk.is_empty() {
        chunks.push(current_chunk);
    }

    chunks
}
```

##### Sliding Window Chunking

ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’ä¿è¨¼ã—ã¤ã¤ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã€‚

```rust
fn sliding_window_chunking(tokens: &[String], window_size: usize, stride: usize) -> Vec<Vec<String>> {
    let mut chunks = Vec::new();

    for i in (0..tokens.len()).step_by(stride) {
        let end = (i + window_size).min(tokens.len());
        if end - i >= window_size / 2 {
            // At least half window size
            chunks.push(tokens[i..end].to_vec());
        }
        if end >= tokens.len() {
            break;
        }
    }

    chunks
}
```

**Chunkingæˆ¦ç•¥ã®æ¯”è¼ƒ**:

| æˆ¦ç•¥ | é•·æ‰€ | çŸ­æ‰€ | é©ç”¨å ´é¢ |
|:-----|:-----|:-----|:---------|
| **Fixed-Size** | ã‚·ãƒ³ãƒ—ãƒ«ãƒ»é«˜é€Ÿ | æ„å‘³å¢ƒç•Œç„¡è¦– | å‡è³ªãªãƒ†ã‚­ã‚¹ãƒˆ |
| **Semantic** | æ„å‘³ä¿æŒ | å¯å¤‰é•· | æ–‡æ›¸ãƒ»è¨˜äº‹ |
| **Sliding Window** | æ–‡è„ˆä¿æŒ | å†—é•·æ€§é«˜ | ã‚³ãƒ¼ãƒ‰ãƒ»å¯¾è©± |

### 4.2 âš¡ Julia: BM25æ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè£…

#### 4.2.1 ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã¨IDFè¨ˆç®—

```julia
using LinearAlgebra, Statistics, Unicode

# Tokenizer: å°æ–‡å­—åŒ– + ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»
const STOPWORDS = Set(["the", "is", "at", "which", "on", "a", "an", "and", "or", "of", "to", "in"])

function tokenize(text::AbstractString)
    # å°æ–‡å­—åŒ– + è¨˜å·é™¤å»
    text = lowercase(text)
    text = replace(text, r"[^\w\s]" => " ")

    # Split + ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»
    tokens = filter(w -> !isempty(w) && !(w âˆˆ STOPWORDS), split(text))
    return tokens
end

# Document corpus
struct Document
    id::Int
    text::String
    tokens::Vector{String}
end

function build_corpus(texts::Vector{String})
    [Document(i, text, tokenize(text)) for (i, text) in enumerate(texts)]
end

# IDF calculation
function compute_idf(corpus::Vector{Document})
    n_docs = length(corpus)
    doc_freq = Dict{String, Int}()

    # Count document frequency for each term
    for doc in corpus
        unique_tokens = Set(doc.tokens)
        for token in unique_tokens
            doc_freq[token] = get(doc_freq, token, 0) + 1
        end
    end

    # IDF: log((N - df + 0.5) / (df + 0.5))
    idf = Dict{String, Float64}()
    for (term, df) in doc_freq
        idf[term] = log((n_docs - df + 0.5) / (df + 0.5))
    end

    return idf
end
```

#### 4.2.2 BM25ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°å®Ÿè£…

```julia
# BM25 parameters
struct BM25Params
    k1::Float64
    b::Float64
end

const DEFAULT_BM25 = BM25Params(1.2, 0.75)

function bm25_score(
    query_tokens::Vector{String},
    doc::Document,
    idf::Dict{String, Float64},
    avg_doc_len::Float64,
    params::BM25Params = DEFAULT_BM25
)
    score = 0.0
    doc_len = length(doc.tokens)

    for term in query_tokens
        # Term frequency in document
        tf = count(==(term), doc.tokens)

        # IDF
        term_idf = get(idf, term, 0.0)

        # BM25 formula
        numerator = tf * (params.k1 + 1.0)
        denominator = tf + params.k1 * (1.0 - params.b + params.b * (doc_len / avg_doc_len))

        score += term_idf * (numerator / denominator)
    end

    return score
end

# BM25 ranking
function bm25_search(
    query::String,
    corpus::Vector{Document},
    idf::Dict{String, Float64},
    top_k::Int = 10,
    params::BM25Params = DEFAULT_BM25
)
    query_tokens = tokenize(query)
    avg_doc_len = mean(length(doc.tokens) for doc in corpus)

    # Score all documents
    scores = [(doc.id, bm25_score(query_tokens, doc, idf, avg_doc_len, params))
              for doc in corpus]

    # Sort by score descending
    sort!(scores, by = x -> x[2], rev = true)

    return scores[1:min(top_k, length(scores))]
end
```

#### 4.2.3 Dense Retrieval with Embeddings

```julia
# Simplified embedding (in practice, use Sentence-BERT via Python/ONNX)
function simple_embedding(text::String; dim::Int = 384)
    tokens = tokenize(text)

    # TF-IDF based embedding (simplified)
    embedding = zeros(Float32, dim)

    for (i, token) in enumerate(tokens)
        # Hash token to dimension
        idx = (hash(token) % dim) + 1
        embedding[idx] += 1.0f0
    end

    # L2 normalize
    norm = sqrt(sum(abs2, embedding))
    embedding ./= (norm + 1f-8)

    return embedding
end

# Cosine similarity
function cosine_sim(a::Vector{Float32}, b::Vector{Float32})
    dot(a, b) / (norm(a) * norm(b) + 1f-8)
end

# Dense retrieval
function dense_search(
    query::String,
    corpus::Vector{Document},
    embeddings::Vector{Vector{Float32}},
    top_k::Int = 10
)
    query_emb = simple_embedding(query)

    # Compute similarity with all documents
    scores = [(i, cosine_sim(query_emb, emb)) for (i, emb) in enumerate(embeddings)]

    # Sort descending
    sort!(scores, by = x -> x[2], rev = true)

    return scores[1:min(top_k, length(scores))]
end
```

#### 4.2.4 Hybrid Retrieval: BM25 + Dense with RRF

```julia
# Reciprocal Rank Fusion
function reciprocal_rank_fusion(
    rankings::Vector{Vector{Tuple{Int, Float64}}};
    k::Int = 60
)
    rrf_scores = Dict{Int, Float64}()

    for ranking in rankings
        for (rank, (doc_id, _)) in enumerate(ranking)
            current_score = get(rrf_scores, doc_id, 0.0)
            rrf_scores[doc_id] = current_score + 1.0 / (k + rank)
        end
    end

    # Sort by RRF score
    sorted = sort(collect(rrf_scores), by = x -> x[2], rev = true)

    return sorted
end

# Hybrid search pipeline
function hybrid_search(
    query::String,
    corpus::Vector{Document},
    idf::Dict{String, Float64},
    embeddings::Vector{Vector{Float32}},
    top_k::Int = 10
)
    # BM25 retrieval
    bm25_results = bm25_search(query, corpus, idf, top_k * 2)

    # Dense retrieval
    dense_results = dense_search(query, corpus, embeddings, top_k * 2)

    # RRF fusion
    fused = reciprocal_rank_fusion([bm25_results, dense_results])

    return fused[1:min(top_k, length(fused))]
end
```

#### 4.2.5 Reranking with Cross-Encoder

```julia
# Simplified cross-encoder scoring (in practice, use BERT-based model)
function cross_encoder_score(query::String, doc_text::String)
    # Combined text
    combined = query * " [SEP] " * doc_text

    # Simple scoring based on token overlap + position
    query_tokens = Set(tokenize(query))
    doc_tokens = tokenize(doc_text)

    score = 0.0
    for (i, token) in enumerate(doc_tokens)
        if token âˆˆ query_tokens
            # Earlier matches get higher score
            position_weight = 1.0 / (1.0 + 0.1 * i)
            score += position_weight
        end
    end

    return score
end

# Rerank top results
function rerank(
    query::String,
    corpus::Vector{Document},
    initial_ranking::Vector{Tuple{Int, Float64}},
    top_k::Int = 10
)
    # Score each candidate with cross-encoder
    reranked = [(doc_id, cross_encoder_score(query, corpus[doc_id].text))
                for (doc_id, _) in initial_ranking]

    # Sort by cross-encoder score
    sort!(reranked, by = x -> x[2], rev = true)

    return reranked[1:min(top_k, length(reranked))]
end
```

#### 4.2.6 Complete RAG Pipeline in Julia

```julia
# End-to-end RAG pipeline
struct RAGPipeline
    corpus::Vector{Document}
    idf::Dict{String, Float64}
    embeddings::Vector{Vector{Float32}}
end

function RAGPipeline(texts::Vector{String})
    # Build corpus
    corpus = build_corpus(texts)

    # Compute IDF
    idf = compute_idf(corpus)

    # Generate embeddings
    embeddings = [simple_embedding(doc.text) for doc in corpus]

    return RAGPipeline(corpus, idf, embeddings)
end

function search(pipeline::RAGPipeline, query::String; top_k::Int = 5, rerank::Bool = true)
    # Stage 1: Hybrid retrieval (BM25 + Dense)
    candidates = hybrid_search(
        query,
        pipeline.corpus,
        pipeline.idf,
        pipeline.embeddings,
        top_k * 3  # Retrieve more for reranking
    )

    # Stage 2: Reranking (optional)
    if rerank
        final_results = rerank(query, pipeline.corpus, candidates, top_k)
    else
        final_results = candidates[1:min(top_k, length(candidates))]
    end

    return final_results
end

# Example usage
texts = [
    "Paris is the capital of France. It is known for the Eiffel Tower.",
    "Tokyo is the capital of Japan. It has a population of 14 million.",
    "Berlin is the capital of Germany. The Berlin Wall fell in 1989.",
    "London is the capital of England. Big Ben is a famous landmark.",
]

pipeline = RAGPipeline(texts)
results = search(pipeline, "What is the capital of France?", top_k = 3)

println("Search Results:")
for (i, (doc_id, score)) in enumerate(results)
    println("$i. [Score: $(round(score, digits=3))] $(pipeline.corpus[doc_id].text)")
end
```

### 4.3 ğŸ”® Elixir: åˆ†æ•£RAGã‚µãƒ¼ãƒ“ãƒ³ã‚°å®Ÿè£…

#### 4.3.1 GenServer ã«ã‚ˆã‚‹çŠ¶æ…‹ç®¡ç†

```elixir
# RAG Server with GenServer
defmodule RAG.Server do
  use GenServer
  require Logger

  # Client API

  def start_link(opts \\ []) do
    GenServer.start_link(__MODULE__, opts, name: __MODULE__)
  end

  def search(query, opts \\ []) do
    GenServer.call(__MODULE__, {:search, query, opts}, :infinity)
  end

  def index_documents(documents) do
    GenServer.cast(__MODULE__, {:index, documents})
  end

  # Server Callbacks

  @impl true
  def init(_opts) do
    state = %{
      documents: [],
      embeddings: %{},
      cache: %{},
      stats: %{searches: 0, cache_hits: 0}
    }

    {:ok, state}
  end

  @impl true
  def handle_call({:search, query, opts}, _from, state) do
    # Check cache first
    case Map.get(state.cache, query) do
      nil ->
        # Cache miss - perform search
        results = perform_search(query, state.documents, state.embeddings, opts)

        # Update cache
        new_cache = Map.put(state.cache, query, results)
        |> limit_cache_size(1000)  # LRU eviction

        new_state = state
        |> Map.update!(:stats, &Map.update!(&1, :searches, fn x -> x + 1 end))
        |> Map.put(:cache, new_cache)

        {:reply, {:ok, results}, new_state}

      cached_results ->
        # Cache hit
        new_state = Map.update!(state, :stats, &Map.update!(&1, :cache_hits, fn x -> x + 1 end))
        Logger.debug("Cache hit for query: #{query}")

        {:reply, {:ok, cached_results}, new_state}
    end
  end

  @impl true
  def handle_cast({:index, documents}, state) do
    # Index documents (compute embeddings, build index)
    embeddings = Enum.map(documents, fn doc ->
      {doc.id, compute_embedding(doc.text)}
    end)
    |> Map.new()

    new_state = state
    |> Map.put(:documents, documents)
    |> Map.put(:embeddings, embeddings)
    |> Map.put(:cache, %{})  # Clear cache on reindex

    Logger.info("Indexed #{length(documents)} documents")

    {:noreply, new_state}
  end

  # Helper functions

  defp perform_search(query, documents, embeddings, opts) do
    top_k = Keyword.get(opts, :top_k, 10)

    query_emb = compute_embedding(query)

    # Compute similarities
    results = Enum.map(documents, fn doc ->
      similarity = cosine_similarity(query_emb, embeddings[doc.id])
      %{doc_id: doc.id, text: doc.text, score: similarity}
    end)
    |> Enum.sort_by(& &1.score, :desc)
    |> Enum.take(top_k)

    results
  end

  defp compute_embedding(text) do
    # Call Python embedding service or use ONNX
    # Simplified: random embedding
    for _ <- 1..384, do: :rand.uniform()
  end

  defp cosine_similarity(a, b) do
    dot_product = Enum.zip(a, b)
    |> Enum.map(fn {x, y} -> x * y end)
    |> Enum.sum()

    norm_a = :math.sqrt(Enum.map(a, &(&1 * &1)) |> Enum.sum())
    norm_b = :math.sqrt(Enum.map(b, &(&1 * &1)) |> Enum.sum())

    dot_product / (norm_a * norm_b + 1.0e-8)
  end

  defp limit_cache_size(cache, max_size) do
    if map_size(cache) > max_size do
      # Simple LRU: remove oldest (first inserted)
      cache
      |> Enum.take(max_size)
      |> Map.new()
    else
      cache
    end
  end
end
```

#### 4.3.2 åˆ†æ•£æ¤œç´¢ with Task.async

```elixir
defmodule RAG.DistributedSearch do
  @moduledoc """
  Distributed RAG search across multiple nodes
  """

  def parallel_search(query, shards, opts \\ []) do
    # Spawn async tasks for each shard
    tasks = Enum.map(shards, fn shard ->
      Task.async(fn ->
        search_shard(query, shard, opts)
      end)
    end)

    # Await all results with timeout
    timeout = Keyword.get(opts, :timeout, 5000)
    results = Task.await_many(tasks, timeout)

    # Merge and rerank
    merge_results(results, opts)
  end

  defp search_shard(query, shard, opts) do
    # Call RAG.Server on specific node/shard
    case :rpc.call(shard.node, RAG.Server, :search, [query, opts]) do
      {:ok, results} -> results
      {:badrpc, reason} ->
        Logger.error("RPC error for shard #{shard.id}: #{inspect(reason)}")
        []
    end
  end

  defp merge_results(results_list, opts) do
    top_k = Keyword.get(opts, :top_k, 10)

    # Flatten and sort by score
    results_list
    |> List.flatten()
    |> Enum.sort_by(& &1.score, :desc)
    |> Enum.take(top_k)
  end
end
```

#### 4.3.3 ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼åˆ¶å¾¡

```elixir
defmodule RAG.RateLimiter do
  use GenServer

  def start_link(opts) do
    GenServer.start_link(__MODULE__, opts, name: __MODULE__)
  end

  def check_rate(user_id) do
    GenServer.call(__MODULE__, {:check_rate, user_id})
  end

  @impl true
  def init(opts) do
    max_requests = Keyword.get(opts, :max_requests, 100)
    window_ms = Keyword.get(opts, :window_ms, 60_000)

    state = %{
      max_requests: max_requests,
      window_ms: window_ms,
      requests: %{}
    }

    # Periodic cleanup
    :timer.send_interval(window_ms, :cleanup)

    {:ok, state}
  end

  @impl true
  def handle_call({:check_rate, user_id}, _from, state) do
    now = System.monotonic_time(:millisecond)
    window_start = now - state.window_ms

    # Get user requests in current window
    user_requests = Map.get(state.requests, user_id, [])
    |> Enum.filter(fn timestamp -> timestamp >= window_start end)

    if length(user_requests) < state.max_requests do
      # Allow request
      new_requests = [now | user_requests]
      new_state = put_in(state.requests[user_id], new_requests)

      {:reply, :ok, new_state}
    else
      # Rate limit exceeded
      {:reply, {:error, :rate_limit_exceeded}, state}
    end
  end

  @impl true
  def handle_info(:cleanup, state) do
    now = System.monotonic_time(:millisecond)
    window_start = now - state.window_ms

    # Remove expired requests
    new_requests = state.requests
    |> Enum.map(fn {user_id, timestamps} ->
      {user_id, Enum.filter(timestamps, &(&1 >= window_start))}
    end)
    |> Enum.reject(fn {_user_id, timestamps} -> Enum.empty?(timestamps) end)
    |> Map.new()

    {:noreply, %{state | requests: new_requests}}
  end
end
```

#### 4.3.4 Production RAG Service

```elixir
defmodule RAG.Application do
  use Application

  def start(_type, _args) do
    children = [
      # RAG Server
      {RAG.Server, []},

      # Rate Limiter
      {RAG.RateLimiter, [max_requests: 100, window_ms: 60_000]},

      # HTTP API (Phoenix endpoint)
      RAG.Web.Endpoint,

      # Background indexer
      RAG.BackgroundIndexer
    ]

    opts = [strategy: :one_for_one, name: RAG.Supervisor]
    Supervisor.start_link(children, opts)
  end
end

# HTTP Endpoint (Phoenix controller)
defmodule RAG.Web.SearchController do
  use Phoenix.Controller

  def search(conn, %{"query" => query} = params) do
    user_id = get_session(conn, :user_id)

    # Rate limiting
    case RAG.RateLimiter.check_rate(user_id) do
      :ok ->
        # Perform search
        top_k = Map.get(params, "top_k", 10)

        case RAG.Server.search(query, top_k: top_k) do
          {:ok, results} ->
            json(conn, %{query: query, results: results})

          {:error, reason} ->
            conn
            |> put_status(:internal_server_error)
            |> json(%{error: reason})
        end

      {:error, :rate_limit_exceeded} ->
        conn
        |> put_status(:too_many_requests)
        |> json(%{error: "Rate limit exceeded"})
    end
  end
end
```

:::message
**é€²æ—: 70% å®Œäº†** Zone 4å®Œäº†ã€‚Rust HNSWå®Ÿè£…ã€Juliaæ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€Elixiråˆ†æ•£RAGã‚µãƒ¼ãƒ“ãƒ³ã‚°ã‚’å®Ÿè£…ã—ãŸã€‚æ¬¡ã¯Zone 5ã§è©•ä¾¡æ‰‹æ³•ã¨SmolVLM2çµ±åˆå®Ÿé¨“ã‚’è¡Œã†ã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” RAGè©•ä¾¡ã¨SmolVLM2çµ±åˆ

### 5.1 RAGè©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹

#### 5.1.1 Retrieval Metrics

**Precision@k**: Top-kä»¶ä¸­ã®é–¢é€£æ–‡æ›¸ã®å‰²åˆ

$$
\text{Precision@}k = \frac{\text{# of relevant docs in top-}k}{k}
$$

**Recall@k**: å…¨é–¢é€£æ–‡æ›¸ä¸­ã€Top-kä»¶ã«å«ã¾ã‚Œã‚‹å‰²åˆ

$$
\text{Recall@}k = \frac{\text{# of relevant docs in top-}k}{\text{# of all relevant docs}}
$$

**Mean Reciprocal Rank (MRR)**: æœ€åˆã®é–¢é€£æ–‡æ›¸ã®ãƒ©ãƒ³ã‚¯ã®é€†æ•°ã®å¹³å‡

$$
\text{MRR} = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{\text{rank}_i}
$$

**Normalized Discounted Cumulative Gain (NDCG@k)**:

$$
\begin{aligned}
\text{DCG@}k &= \sum_{i=1}^k \frac{2^{\text{rel}_i} - 1}{\log_2(i + 1)} \\
\text{NDCG@}k &= \frac{\text{DCG@}k}{\text{IDCG@}k}
\end{aligned}
$$

ã“ã“ã§ $\text{IDCG@}k$ ã¯ç†æƒ³çš„ãªé †ä½ã§ã®DCGã€‚

#### 5.1.2 Generation Metrics

**Context Relevance**: æ¤œç´¢ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒã‚¯ã‚¨ãƒªã«é–¢é€£ã—ã¦ã„ã‚‹ã‹

```julia
# Context Relevance Score
function context_relevance(query::String, contexts::Vector{String})
    query_tokens = Set(tokenize(query))

    scores = map(contexts) do context
        context_tokens = Set(tokenize(context))
        overlap = length(intersect(query_tokens, context_tokens))
        overlap / (length(query_tokens) + 1e-8)
    end

    mean(scores)
end
```

**Answer Faithfulness**: ç”Ÿæˆã•ã‚ŒãŸå›ç­”ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å¿ å®Ÿã‹

$$
\text{Faithfulness} = \frac{\text{# of claims supported by context}}{\text{# of total claims}}
$$

**Answer Relevance**: ç”Ÿæˆã•ã‚ŒãŸå›ç­”ãŒã‚¯ã‚¨ãƒªã«é–¢é€£ã—ã¦ã„ã‚‹ã‹

```julia
function answer_relevance(query::String, answer::String, query_emb, answer_emb)
    # Cosine similarity between query and answer embeddings
    cosine_sim(query_emb, answer_emb)
end
```

#### 5.1.3 RAGAS Framework

**RAGAS** [^8] (RAG Assessment): RAGè©•ä¾¡ã®çµ±åˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

**4ã¤ã®ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹**:

| Metric | èª¬æ˜ | å¼ |
|:-------|:-----|:---|
| **Context Precision** | é–¢é€£æ–‡æ›¸ãŒä¸Šä½ã«ãƒ©ãƒ³ã‚¯ã•ã‚Œã¦ã„ã‚‹ã‹ | $\frac{\sum_{k=1}^K v_k \cdot \text{Precision@}k}{K}$ |
| **Context Recall** | å…¨é–¢é€£æ–‡æ›¸ãŒæ¤œç´¢ã•ã‚ŒãŸã‹ | $\frac{\text{# retrieved relevant}}{\text{# total relevant}}$ |
| **Faithfulness** | å›ç­”ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æ”¯æŒã•ã‚Œã¦ã„ã‚‹ã‹ | $\frac{\text{# supported claims}}{\text{# total claims}}$ |
| **Answer Relevancy** | å›ç­”ãŒã‚¯ã‚¨ãƒªã«é–¢é€£ã—ã¦ã„ã‚‹ã‹ | $\text{cos}(\text{emb}_q, \text{emb}_a)$ |

**çµ±åˆã‚¹ã‚³ã‚¢**:

$$
\text{RAGAS Score} = \left( \text{Precision} \times \text{Recall} \times \text{Faithfulness} \times \text{Relevancy} \right)^{1/4}
$$

å¹¾ä½•å¹³å‡ã§å…¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒãƒ©ãƒ³ã‚¹ã€‚

#### 5.1.4 Juliaå®Ÿè£…: RAGASè©•ä¾¡

```julia
struct RAGASEvaluator
    pipeline::RAGPipeline
end

# Evaluate single query
function evaluate_query(
    evaluator::RAGASEvaluator,
    query::String,
    ground_truth_docs::Set{Int},
    ground_truth_answer::String
)
    # Retrieve documents
    retrieved = search(evaluator.pipeline, query, top_k=10, rerank=true)
    retrieved_ids = Set([doc_id for (doc_id, _) in retrieved])

    # Context Precision
    precision_scores = Float64[]
    for k in 1:length(retrieved)
        top_k_ids = Set([retrieved[i][1] for i in 1:k])
        precision_k = length(intersect(top_k_ids, ground_truth_docs)) / k
        is_relevant = retrieved[k][1] in ground_truth_docs
        push!(precision_scores, is_relevant ? precision_k : 0.0)
    end
    context_precision = mean(precision_scores)

    # Context Recall
    context_recall = length(intersect(retrieved_ids, ground_truth_docs)) /
                     (length(ground_truth_docs) + 1e-8)

    # Faithfulness (simplified: check if answer mentions retrieved docs)
    retrieved_texts = [evaluator.pipeline.corpus[id].text for (id, _) in retrieved]
    answer = generate_answer(query, retrieved_texts)  # Simulated LLM generation
    faithfulness = compute_faithfulness(answer, retrieved_texts)

    # Answer Relevancy (cosine similarity)
    query_emb = simple_embedding(query)
    answer_emb = simple_embedding(answer)
    answer_relevancy = cosine_sim(query_emb, answer_emb)

    # RAGAS Score (geometric mean)
    ragas_score = (context_precision * context_recall * faithfulness * answer_relevancy)^0.25

    return (
        context_precision = context_precision,
        context_recall = context_recall,
        faithfulness = faithfulness,
        answer_relevancy = answer_relevancy,
        ragas_score = ragas_score,
        answer = answer
    )
end

function compute_faithfulness(answer::String, contexts::Vector{String})
    # Extract claims from answer (simplified: sentences)
    claims = split(answer, ". ") |> collect

    supported_count = 0
    for claim in claims
        # Check if claim is supported by any context
        for context in contexts
            if contains(lowercase(context), lowercase(claim)) ||
               token_overlap(claim, context) > 0.5
                supported_count += 1
                break
            end
        end
    end

    supported_count / (length(claims) + 1e-8)
end

function token_overlap(text1::String, text2::String)
    tokens1 = Set(tokenize(text1))
    tokens2 = Set(tokenize(text2))

    overlap = length(intersect(tokens1, tokens2))
    overlap / (length(union(tokens1, tokens2)) + 1e-8)
end

function generate_answer(query::String, contexts::Vector{String})
    # Simulated LLM generation (in practice, call actual LLM)
    combined_context = join(contexts[1:min(3, length(contexts))], " ")

    "Based on the context, $combined_context, the answer to '$query' is found in the documents."
end
```

### 5.2 SmolVLM2-256M + RAGçµ±åˆæ¼”ç¿’

#### 5.2.1 ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«RAGã®è¨­è¨ˆ

**ã‚·ãƒŠãƒªã‚ª**: ç”»åƒ + ãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æ¤œç´¢

```mermaid
graph LR
    Q["Query<br/>(Text/Image)"] --> E["Encoder<br/>(SmolVLM2)"]
    E --> QE["Query Embedding"]
    QE --> VDB["Vector DB<br/>(Image+Text)"]
    VDB --> R["Retrieved<br/>Multimodal Docs"]
    R --> G["Generator<br/>(SmolVLM2)"]
    Q --> G
    G --> A["Answer"]
```

**ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**:

1. **Indexing**: ç”»åƒ + ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’SmolVLM2ã§Embedding â†’ Vector DBã«ä¿å­˜
2. **Retrieval**: ã‚¯ã‚¨ãƒªã‚’Embedding â†’ Top-kç”»åƒ+ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¤œç´¢
3. **Generation**: æ¤œç´¢çµæœã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦SmolVLM2ã§ç”Ÿæˆ

#### 5.2.2 Julia + Rustçµ±åˆå®Ÿè£…

```julia
# Multimodal RAG Pipeline
using HTTP, JSON3

# SmolVLM2 embedding service (via Rust backend)
function smolvlm2_embed(text::String; endpoint="http://localhost:8080/embed")
    response = HTTP.post(
        endpoint,
        ["Content-Type" => "application/json"],
        JSON3.write(Dict("text" => text))
    )

    result = JSON3.read(response.body)
    return Float32.(result.embedding)
end

# Multimodal document
struct MultimodalDocument
    id::Int
    text::String
    image_path::Union{String, Nothing}
    embedding::Vector{Float32}
end

# Build multimodal index
function build_multimodal_index(docs::Vector{Tuple{String, Union{String, Nothing}}})
    indexed_docs = MultimodalDocument[]

    for (i, (text, image_path)) in enumerate(docs)
        # Generate embedding (text + image if available)
        embedding = if !isnothing(image_path)
            # In practice: encode image + text jointly with SmolVLM2
            smolvlm2_embed("$text [IMG: $image_path]")
        else
            smolvlm2_embed(text)
        end

        push!(indexed_docs, MultimodalDocument(i, text, image_path, embedding))
    end

    return indexed_docs
end

# Multimodal search
function multimodal_search(
    query::String,
    index::Vector{MultimodalDocument},
    top_k::Int = 5
)
    query_emb = smolvlm2_embed(query)

    # Compute similarities
    scores = [(doc.id, cosine_sim(query_emb, doc.embedding), doc)
              for doc in index]

    # Sort and return top-k
    sort!(scores, by = x -> x[2], rev = true)

    return scores[1:min(top_k, length(scores))]
end

# Example usage
multimodal_docs = [
    ("The Eiffel Tower in Paris at sunset.", "images/eiffel_tower.jpg"),
    ("Tokyo Tower with cherry blossoms in spring.", "images/tokyo_tower.jpg"),
    ("Berlin Wall memorial with historical graffiti.", nothing),
    ("Big Ben clock tower in London.", "images/big_ben.jpg"),
]

index = build_multimodal_index(multimodal_docs)

query = "Show me towers in European cities"
results = multimodal_search(query, index, top_k=3)

for (i, (doc_id, score, doc)) in enumerate(results)
    println("$i. [Score: $(round(score, digits=3))] $(doc.text)")
    if !isnothing(doc.image_path)
        println("   Image: $(doc.image_path)")
    end
end
```

#### 5.2.3 Rust Embedding Service (ONNX Runtime)

```rust
// SmolVLM2 embedding service with ONNX Runtime
use actix_web::{post, web, App, HttpResponse, HttpServer, Responder};
use ndarray::{Array1, Array2};
use ort::{Environment, SessionBuilder, Value};
use serde::{Deserialize, Serialize};

#[derive(Deserialize)]
struct EmbedRequest {
    text: String,
}

#[derive(Serialize)]
struct EmbedResponse {
    embedding: Vec<f32>,
}

#[post("/embed")]
async fn embed_endpoint(req: web::Json<EmbedRequest>) -> impl Responder {
    // Tokenize text (simplified)
    let tokens = tokenize(&req.text);

    // Run inference
    match run_embedding_model(&tokens) {
        Ok(embedding) => HttpResponse::Ok().json(EmbedResponse {
            embedding: embedding.to_vec(),
        }),
        Err(e) => HttpResponse::InternalServerError().body(format!("Error: {}", e)),
    }
}

fn tokenize(text: &str) -> Vec<i64> {
    // Simplified tokenizer (in practice, use HuggingFace tokenizers)
    text.chars()
        .filter(|c| c.is_alphanumeric() || c.is_whitespace())
        .map(|c| c as i64)
        .collect()
}

fn run_embedding_model(tokens: &[i64]) -> Result<Array1<f32>, Box<dyn std::error::Error>> {
    // Load ONNX model
    let environment = Environment::builder().with_name("smolvlm2").build()?;

    let session = SessionBuilder::new(&environment)?
        .with_model_from_file("models/smolvlm2_encoder.onnx")?;

    // Prepare input
    let input_ids = Array2::from_shape_vec((1, tokens.len()), tokens.to_vec())?;

    let input_tensor = Value::from_array(session.allocator(), &input_ids)?;

    // Run inference
    let outputs = session.run(vec![input_tensor])?;

    // Extract embedding (CLS token)
    let embedding_tensor = outputs[0].try_extract::<f32>()?;
    let embedding = embedding_tensor.view().to_owned();

    // Mean pooling (simplified)
    let mean_embedding = embedding.mean_axis(ndarray::Axis(1)).unwrap();

    Ok(mean_embedding)
}

#[actix_web::main]
async fn main() -> std::io::Result<()> {
    HttpServer::new(|| App::new().service(embed_endpoint))
        .bind(("127.0.0.1", 8080))?
        .run()
        .await
}
```

### 5.3 è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

:::details è¨˜å·èª­è§£10å•

**å•1**: BM25ã®å¼ã§ $k_1$ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½¹å‰²ã¯ï¼Ÿ

a) æ–‡æ›¸é•·æ­£è¦åŒ–
b) TFé£½å’Œåº¦åˆ¶å¾¡
c) IDFé‡ã¿ä»˜ã‘
d) ã‚¯ã‚¨ãƒªæ‹¡å¼µ

<details><summary>è§£ç­”</summary>

**b) TFé£½å’Œåº¦åˆ¶å¾¡**

$$
\frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot (\cdots)}
$$

$k_1 \to \infty$ ã§é£½å’Œãªã—ã€$k_1 = 0$ ã§TFç„¡è¦–ã€‚
</details>

**å•2**: HNSW ã®æ¢ç´¢è¨ˆç®—é‡ã¯ï¼Ÿ

a) $O(N)$
b) $O(N \log N)$
c) $O(\log N)$
d) $O(1)$

<details><summary>è§£ç­”</summary>

**c) $O(\log N)$**

éšå±¤çš„æ¢ç´¢ã«ã‚ˆã‚Šå¯¾æ•°æ™‚é–“ã§è¿‘ä¼¼æœ€è¿‘å‚ã‚’ç™ºè¦‹ã€‚
</details>

**å•3**: Self-RAG ã®åçœãƒˆãƒ¼ã‚¯ãƒ³ **[IsSup]** ã®æ„å‘³ã¯ï¼Ÿ

a) æ¤œç´¢ãŒå¿…è¦ã‹
b) æ¤œç´¢çµæœãŒé–¢é€£ã—ã¦ã„ã‚‹ã‹
c) ç”ŸæˆãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æ”¯æŒã•ã‚Œã¦ã„ã‚‹ã‹
d) ç”ŸæˆãŒã‚¯ã‚¨ãƒªã«æœ‰ç”¨ã‹

<details><summary>è§£ç­”</summary>

**c) ç”ŸæˆãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æ”¯æŒã•ã‚Œã¦ã„ã‚‹ã‹**

[IsSup] = Fully/Partially/No
</details>

**å•4**: RRF (Reciprocal Rank Fusion) ã®å¼ã¯ï¼Ÿ

a) $\sum_r \frac{1}{k + \text{rank}_r(d)}$
b) $\sum_r \text{rank}_r(d)$
c) $\prod_r \frac{1}{\text{rank}_r(d)}$
d) $\max_r \text{rank}_r(d)$

<details><summary>è§£ç­”</summary>

**a) $\sum_r \frac{1}{k + \text{rank}_r(d)}$**

è¤‡æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’çµ±åˆã€$k=60$ ãŒæ¨™æº–ã€‚
</details>

**å•5**: ColBERT ã® MaxSim å¼ã¯ï¼Ÿ

a) $\sum_{i} \max_j \mathbf{E}_Q[i] \cdot \mathbf{E}_D[j]$
b) $\max_{i,j} \mathbf{E}_Q[i] \cdot \mathbf{E}_D[j]$
c) $\sum_{i,j} \mathbf{E}_Q[i] \cdot \mathbf{E}_D[j]$
d) $\mathbf{E}_Q \cdot \mathbf{E}_D^\top$

<details><summary>è§£ç­”</summary>

**a) $\sum_{i} \max_j \mathbf{E}_Q[i] \cdot \mathbf{E}_D[j]$**

å„ã‚¯ã‚¨ãƒªãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã—ã€æœ€ã‚‚é¡ä¼¼ã™ã‚‹æ–‡æ›¸ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚¹ã‚³ã‚¢åŒ–ã€‚
</details>

**å•6**: RAGAS Score ã®è¨ˆç®—æ–¹æ³•ã¯ï¼Ÿ

a) ç®—è¡“å¹³å‡
b) å¹¾ä½•å¹³å‡
c) èª¿å’Œå¹³å‡
d) æœ€å¤§å€¤

<details><summary>è§£ç­”</summary>

**b) å¹¾ä½•å¹³å‡**

$$
(\text{Prec} \times \text{Rec} \times \text{Faith} \times \text{Rel})^{1/4}
$$
</details>

**å•7**: CRAG ã® Evaluator ãŒ **Incorrect** ã¨åˆ¤å®šã—ãŸå ´åˆã®å¯¾å¿œã¯ï¼Ÿ

a) ãã®ã¾ã¾ç”Ÿæˆ
b) Re-retrieve
c) Webæ¤œç´¢ã§è£œå¼·
d) ã‚¨ãƒ©ãƒ¼è¿”ã™

<details><summary>è§£ç­”</summary>

**c) Webæ¤œç´¢ã§è£œå¼·**

ä¸æ­£ç¢ºãªæ–‡æ›¸ã¯æ¨ã¦ã€Webæ¤œç´¢ã§çŸ¥è­˜è£œæ­£ã€‚
</details>

**å•8**: Dense Retrieval ã® In-batch Negatives ã¨ã¯ï¼Ÿ

a) ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ãŸnegative
b) åŒä¸€ãƒãƒƒãƒå†…ã®ä»–ã®æ–‡æ›¸ã‚’negativeã¨ã™ã‚‹
c) Hard negative mining
d) äººæ‰‹ã§ãƒ©ãƒ™ãƒ«ä»˜ã‘ã—ãŸnegative

<details><summary>è§£ç­”</summary>

**b) åŒä¸€ãƒãƒƒãƒå†…ã®ä»–ã®æ–‡æ›¸ã‚’negativeã¨ã™ã‚‹**

åŠ¹ç‡çš„ã«contrastive learning ã‚’å®Ÿç¾ã€‚
</details>

**å•9**: Semantic Chunking ã®åˆ©ç‚¹ã¯ï¼Ÿ

a) å›ºå®šé•·ã§é«˜é€Ÿ
b) æ„å‘³å¢ƒç•Œã‚’ä¿æŒ
c) ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ä¿è¨¼
d) ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãŒç°¡å˜

<details><summary>è§£ç­”</summary>

**b) æ„å‘³å¢ƒç•Œã‚’ä¿æŒ**

æ–‡ãƒ»æ®µè½å˜ä½ã§åˆ†å‰² â†’ æ–‡è„ˆã‚’ç ´å£Šã—ãªã„ã€‚
</details>

**å•10**: MRR (Mean Reciprocal Rank) ã§æœ€åˆã®é–¢é€£æ–‡æ›¸ãŒ3ä½ã®å ´åˆã®ã‚¹ã‚³ã‚¢ã¯ï¼Ÿ

a) 3
b) 1/3
c) 1
d) 0.5

<details><summary>è§£ç­”</summary>

**b) 1/3**

$$
\text{RR} = \frac{1}{\text{rank}} = \frac{1}{3}
$$
</details>

:::

:::details å®Ÿè£…5å•

**å•1**: Rust HNSWã§å±¤ã‚’æ±ºã‚ã‚‹å¼ $\text{layer} = \lfloor -\ln(u) \cdot m_L \rfloor$ ã® $u$ ã¯ï¼Ÿ

a) æ–‡æ›¸ID
b) $[0, 1]$ ã®ä¸€æ§˜ä¹±æ•°
c) è·é›¢ãƒ¡ãƒˆãƒªã‚¯ã‚¹
d) ãƒãƒ¼ãƒ‰æ•°

<details><summary>è§£ç­”</summary>

**b) $[0, 1]$ ã®ä¸€æ§˜ä¹±æ•°**

```rust
let uniform = rand::random::<f32>();
let layer = (-uniform.ln() * self.ml).floor() as usize;
```
</details>

**å•2**: Julia BM25ã§ `avg_doc_len` ãŒ100ã€æ–‡æ›¸é•·ãŒ150ã®å ´åˆã€$b=0.75$ ã§ã®æ­£è¦åŒ–é …ã¯ï¼Ÿ

a) 1.0
b) 1.375
c) 0.75
d) 1.125

<details><summary>è§£ç­”</summary>

**d) 1.125**

$$
1 - b + b \cdot \frac{|D|}{\text{avgdl}} = 1 - 0.75 + 0.75 \cdot \frac{150}{100} = 0.25 + 1.125 = 1.125
$$

å®Ÿéš›ã«ã¯:
$$
1 - 0.75 + 0.75 \cdot 1.5 = 0.25 + 1.125 = 1.375
$$

**æ­£è§£: b) 1.375**
</details>

**å•3**: Elixir GenServerã§æ¤œç´¢ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆã—ãŸå ´åˆã€ã©ã®é–¢æ•°ã§å‡¦ç†ã•ã‚Œã‚‹ï¼Ÿ

a) `handle_call`
b) `handle_cast`
c) `handle_info`
d) `init`

<details><summary>è§£ç­”</summary>

**a) `handle_call`**

```elixir
def handle_call({:search, query, opts}, _from, state) do
  case Map.get(state.cache, query) do
    cached_results -> {:reply, {:ok, cached_results}, new_state}
  end
end
```
</details>

**å•4**: qdrantã§ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆæ™‚ã€`Distance::Cosine` ã‚’æŒ‡å®šã™ã‚‹ç†ç”±ã¯ï¼Ÿ

a) L2è·é›¢ã‚ˆã‚Šé«˜é€Ÿ
b) æ­£è¦åŒ–ã•ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«ã§é©åˆ‡
c) ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãŒè‰¯ã„
d) æ•´æ•°ãƒ™ã‚¯ãƒˆãƒ«å°‚ç”¨

<details><summary>è§£ç­”</summary>

**b) æ­£è¦åŒ–ã•ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«ã§é©åˆ‡**

Embeddingã¯é€šå¸¸L2æ­£è¦åŒ– â†’ Cosineé¡ä¼¼åº¦ãŒè‡ªç„¶ã€‚
</details>

**å•5**: Sliding Window Chunkingã§ `window_size=10`, `stride=5` ã®å ´åˆã€100ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰ä½•ãƒãƒ£ãƒ³ã‚¯ã§ãã‚‹ï¼Ÿ

a) 10
b) 19
c) 20
d) 18

<details><summary>è§£ç­”</summary>

**b) 19**

$$
\lceil \frac{100 - 10}{5} \rceil + 1 = \lceil 18 \rceil + 1 = 19
$$

- Chunk 1: 0-9
- Chunk 2: 5-14
- ...
- Chunk 19: 90-99
</details>

:::

:::details æ¦‚å¿µ5å•

**å•1**: RAG vs Fine-tuning: ã©ã¡ã‚‰ãŒãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çŸ¥è­˜æ›´æ–°ã«é©ã—ã¦ã„ã‚‹ã‹ã€ç†ç”±ã¨ã¨ã‚‚ã«è¿°ã¹ã‚ˆã€‚

<details><summary>è§£ç­”</summary>

**RAG**

ç†ç”±:
- Fine-tuningã¯å†å­¦ç¿’ãŒå¿…è¦ï¼ˆæ™‚é–“ãƒ»ã‚³ã‚¹ãƒˆå¤§ï¼‰
- RAGã¯æ–‡æ›¸è¿½åŠ ã®ã¿ï¼ˆå³åº§ã«åæ˜ ï¼‰
- å‡ºå…¸æ˜ç¤ºå¯èƒ½ã§HallucinationæŠ‘åˆ¶
</details>

**å•2**: Bi-Encoder vs Cross-Encoder: ãã‚Œãã‚Œã®ç”¨é€”ã‚’è¿°ã¹ã‚ˆã€‚

<details><summary>è§£ç­”</summary>

**Bi-Encoder**: å¤§è¦æ¨¡æ¤œç´¢ï¼ˆRetrievalï¼‰
- æ–‡æ›¸ã‚’äº‹å‰Encode â†’ Vector DB
- ã‚¯ã‚¨ãƒªã®ã¿Encode â†’ é«˜é€Ÿ

**Cross-Encoder**: ç²¾å¯†Reranking
- ã‚¯ã‚¨ãƒª+æ–‡æ›¸ã‚’ä¸€ç·’ã«Encode
- ç›¸äº’ä½œç”¨ã‚ã‚Š â†’ é«˜ç²¾åº¦ã ãŒé…ã„
</details>

**å•3**: Agentic RAGãŒå¾“æ¥RAGã‚ˆã‚Šå„ªã‚Œã¦ã„ã‚‹ç‚¹ã‚’3ã¤æŒ™ã’ã‚ˆã€‚

<details><summary>è§£ç­”</summary>

1. **è‡ªå¾‹åˆ¤æ–­**: æ¤œç´¢ãŒå¿…è¦ã‹ã‚’å‹•çš„åˆ¤æ–­ï¼ˆç„¡é§„ãªæ¤œç´¢ã‚’å›é¿ï¼‰
2. **è‡ªå·±è£œæ­£**: æ¤œç´¢çµæœã®å“è³ªè©•ä¾¡ + ä¸æ­£ç¢ºãªã‚‰å†æ¤œç´¢
3. **é©å¿œçš„æˆ¦ç•¥**: ã‚¯ã‚¨ãƒªè¤‡é›‘åº¦ã«å¿œã˜ãŸæ¤œç´¢æˆ¦ç•¥é¸æŠ
</details>

**å•4**: Chunkingã§ "overlap" ã‚’è¨­ã‘ã‚‹ç†ç”±ã¯ï¼Ÿ

<details><summary>è§£ç­”</summary>

**æ–‡è„ˆã®é€£ç¶šæ€§ä¿æŒ**

- ãƒãƒ£ãƒ³ã‚¯å¢ƒç•Œã§æƒ…å ±ãŒåˆ†æ–­ã•ã‚Œã‚‹ã®ã‚’é˜²ã
- ä¾‹: "Paris is the capital" | "capital of France" â†’ "capital" ãŒé‡è¤‡ã§ä¸¡æ–¹ã«å«ã¾ã‚Œã‚‹
- æ¤œç´¢ç²¾åº¦å‘ä¸Šï¼ˆå¢ƒç•Œã®æƒ…å ±æ¬ æã‚’å›é¿ï¼‰
</details>

**å•5**: RAGAS ã® Faithfulness ãŒä½ã„å ´åˆã€ã©ã®éƒ¨åˆ†ã«å•é¡ŒãŒã‚ã‚‹ã‹ï¼Ÿ

<details><summary>è§£ç­”</summary>

**Generation (LLM)** ã¾ãŸã¯ **æ¤œç´¢å“è³ª**

- LLMãŒHallucinationã‚’èµ·ã“ã—ã¦ã„ã‚‹
- æ¤œç´¢ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒä¸ååˆ†/ç„¡é–¢é€£
- å¯¾ç­–: Rerankingå¼·åŒ–ã€LLMã®temperatureä¸‹ã’ã‚‹ã€Self-RAGã§åçœãƒˆãƒ¼ã‚¯ãƒ³å°å…¥
</details>

:::

:::message
**é€²æ—: 85% å®Œäº†** Zone 5å®Œäº†ã€‚RAGè©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€RAGASå®Ÿè£…ã€SmolVLM2çµ±åˆã€è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆã‚’å®Œæˆã€‚æ¬¡ã¯Zone 6ã§ç ”ç©¶ç³»è­œã¨ç™ºå±•ãƒˆãƒ”ãƒƒã‚¯ã‚’è§£èª¬ã™ã‚‹ã€‚
:::

---

## ğŸ“ 6. æŒ¯ã‚Šè¿”ã‚Šã¨ç™ºå±•ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨RAGç ”ç©¶ã®æœ€å‰ç·š

### 6.1 RAGç ”ç©¶ç³»è­œ

```mermaid
graph TD
    R1["2020<br/>RAG (Lewis+)<br/>NIPS"] --> R2["2021<br/>REALM (Guu+)<br/>ICML"]
    R2 --> R3["2022<br/>Atlas (Izacard+)<br/>JMLR"]
    R3 --> R4["2023<br/>Self-RAG (Asai+)<br/>Preprint"]
    R4 --> R5["2024<br/>CRAG (Yan+)<br/>Preprint"]
    R4 --> R6["2024<br/>Adaptive-RAG (Jeong+)<br/>Preprint"]

    R1 -.å›ºå®šæ¤œç´¢.-> R1D["Retrieve â†’ Generate"]
    R2 -.å­¦ç¿’å¯èƒ½æ¤œç´¢.-> R2D["End-to-endå­¦ç¿’"]
    R3 -.Few-shotå¼·åŒ–.-> R3D["Multi-documentèåˆ"]
    R4 -.åçœãƒˆãƒ¼ã‚¯ãƒ³.-> R4D["è‡ªå·±åˆ¶å¾¡æ¤œç´¢"]
    R5 -.çŸ¥è­˜è£œæ­£.-> R5D["æ¤œç´¢çµæœè©•ä¾¡+è£œæ­£"]
    R6 -.é©å¿œæˆ¦ç•¥.-> R6D["ã‚¯ã‚¨ãƒªè¤‡é›‘åº¦èªè­˜"]

    style R4 fill:#c8e6c9
    style R5 fill:#c8e6c9
    style R6 fill:#c8e6c9
```

### 6.2 GraphRAG â€” ã‚°ãƒ©ãƒ•çŸ¥è­˜ãƒ™ãƒ¼ã‚¹

**GraphRAG**: çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’ã‚°ãƒ©ãƒ•æ§‹é€ ã§ç®¡ç†

```mermaid
graph LR
    E1["Paris"] -->|capital_of| E2["France"]
    E1 -->|has_landmark| E3["Eiffel Tower"]
    E2 -->|continent| E4["Europe"]
    E3 -->|built_in| E5["1889"]
```

**åˆ©ç‚¹**:
- ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é–“ã®é–¢ä¿‚ã‚’æ˜ç¤ºçš„ã«ãƒ¢ãƒ‡ãƒ«åŒ–
- Multi-hop reasoning ãŒå®¹æ˜“
- çŸ¥è­˜ã®ä¸€è²«æ€§ä¿è¨¼

**ã‚¯ã‚¨ãƒªä¾‹**:

```
Query: "What landmarks are in European capitals?"

Graph Traversal:
1. capitals in Europe â†’ [Paris, Berlin, London, ...]
2. landmarks in Paris â†’ [Eiffel Tower, ...]
3. Return: [Eiffel Tower, Brandenburg Gate, Big Ben, ...]
```

**å®Ÿè£…æŠ€è¡“**: Neo4j, NetworkX, DGL

### 6.3 Multi-modal RAG

**ãƒ†ã‚­ã‚¹ãƒˆ + ç”»åƒ + éŸ³å£°** ã‚’çµ±åˆã—ãŸRAG

```mermaid
graph LR
    T["Text"] --> E["Unified<br/>Encoder"]
    I["Image"] --> E
    A["Audio"] --> E
    E --> V["Vector DB"]
    V --> R["Retrieved<br/>Multimodal"]
    R --> G["Generator"]
```

**ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹**:
- åŒ»ç™‚ç”»åƒè¨ºæ–­ï¼ˆç”»åƒ + ç—…æ­´ãƒ†ã‚­ã‚¹ãƒˆï¼‰
- å‹•ç”»æ¤œç´¢ï¼ˆæ˜ åƒ + å­—å¹• + éŸ³å£°ï¼‰
- Eã‚³ãƒãƒ¼ã‚¹ï¼ˆå•†å“ç”»åƒ + ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰

**SOTA Models**: CLIP, BLIP-2, CoCa, SmolVLM2

### 6.4 Long-context vs RAGè«–äº‰

| | Long-context LLM | RAG |
|:--|:----------------|:----|
| **Contexté•·** | 100K-1M tokens | æ•°åƒtokens |
| **ç²¾åº¦** | ä¸­ï¼ˆMiddle-lostå•é¡Œï¼‰ | é«˜ï¼ˆé–¢é€£éƒ¨åˆ†ã®ã¿ï¼‰ |
| **ã‚³ã‚¹ãƒˆ** | é«˜ï¼ˆå…¨æ–‡å‡¦ç†ï¼‰ | ä½ï¼ˆæ¤œç´¢å¾Œã®ã¿ï¼‰ |
| **ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·** | é«˜ | ä¸­ï¼ˆæ¤œç´¢ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ï¼‰ |
| **çŸ¥è­˜æ›´æ–°** | å†å­¦ç¿’å¿…è¦ | æ–‡æ›¸è¿½åŠ ã®ã¿ |

**Middle-lostå•é¡Œ**: Long-contextã§ã¯ä¸­é–“éƒ¨åˆ†ã®æƒ…å ±ãŒå¤±ã‚ã‚Œã‚„ã™ã„

**ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æˆ¦ç•¥**: RAGã§çµã‚Šè¾¼ã¿ â†’ Long-contextã§ç²¾å¯†å‡¦ç†

### 6.5 æ¨è–¦è«–æ–‡ãƒ»æ›¸ç±

#### å¿…èª­è«–æ–‡ï¼ˆæ–°â†’æ—§ï¼‰

1. **CRAG** (Yan+ 2024) [^3]: æ¤œç´¢çµæœã®æ­£ç¢ºæ€§è©•ä¾¡+è£œæ­£
   [arXiv:2401.15884](https://arxiv.org/abs/2401.15884)

2. **Self-RAG** (Asai+ 2024) [^2]: åçœãƒˆãƒ¼ã‚¯ãƒ³ã§è‡ªå·±åˆ¶å¾¡
   [arXiv:2310.11511](https://arxiv.org/abs/2310.11511)

3. **Adaptive-RAG** (Jeong+ 2024): ã‚¯ã‚¨ãƒªè¤‡é›‘åº¦èªè­˜
   [arXiv:2403.14403](https://arxiv.org/abs/2403.14403)

4. **ColBERT** (Khattab & Zaharia 2020): Late Interaction
   [arXiv:2004.12832](https://arxiv.org/abs/2004.12832)

5. **DPR** (Karpukhin+ 2020): Dense Passage Retrieval
   [arXiv:2004.04906](https://arxiv.org/abs/2004.04906)

6. **RAG** (Lewis+ 2020) [^1]: å…ƒç¥–RAG
   [arXiv:2005.11401](https://arxiv.org/abs/2005.11401)

#### å®Ÿè£…ãƒªã‚½ãƒ¼ã‚¹

| ãƒªã‚½ãƒ¼ã‚¹ | èª¬æ˜ | ãƒªãƒ³ã‚¯ |
|:--------|:-----|:-------|
| **qdrant** | Rust Vector DB | [GitHub](https://github.com/qdrant/qdrant) [^7] |
| **FAISS** | Meta ANN library | [GitHub](https://github.com/facebookresearch/faiss) [^9] |
| **hnswlib-rs** | Rust HNSWå®Ÿè£… | [GitHub](https://github.com/jean-pierreBoth/hnswlib-rs) |
| **RAGAS** | RAGè©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ | [GitHub](https://github.com/explodinggradients/ragas) [^8] |
| **LangChain** | RAG orchestration | [Docs](https://python.langchain.com/docs/use_cases/question_answering/) |

### 6.6 ç”¨èªé›†

| ç”¨èª | è‹±èª | å®šç¾© |
|:-----|:-----|:-----|
| **RAG** | Retrieval-Augmented Generation | æ¤œç´¢å¢—å¼·ç”Ÿæˆã€‚å¤–éƒ¨çŸ¥è­˜ã‚’æ¤œç´¢ã—ã¦LLMã«çµ±åˆ |
| **Embedding** | Embedding | ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã«åŸ‹ã‚è¾¼ã‚€ |
| **BM25** | Best Matching 25 | TF-IDFãƒ™ãƒ¼ã‚¹ã®ã‚¹ãƒ‘ãƒ¼ã‚¹æ¤œç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  |
| **HNSW** | Hierarchical Navigable Small World | éšå±¤çš„è¿‘ä¼¼æœ€è¿‘å‚æ¢ç´¢ |
| **ANN** | Approximate Nearest Neighbor | è¿‘ä¼¼æœ€è¿‘å‚æ¢ç´¢ |
| **Dense Retrieval** | Dense Retrieval | ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«Embeddingãƒ™ãƒ¼ã‚¹ã®æ¤œç´¢ |
| **Sparse Retrieval** | Sparse Retrieval | ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹æ¤œç´¢ï¼ˆBM25ç­‰ï¼‰ |
| **Hybrid Retrieval** | Hybrid Retrieval | Sparse + Denseçµ±åˆ |
| **RRF** | Reciprocal Rank Fusion | ãƒ©ãƒ³ã‚­ãƒ³ã‚°çµ±åˆæ‰‹æ³• |
| **Reranking** | Reranking | æ¤œç´¢çµæœã®å†é †ä½ä»˜ã‘ |
| **Cross-Encoder** | Cross-Encoder | ã‚¯ã‚¨ãƒª+æ–‡æ›¸ã‚’jointã«Encode |
| **Bi-Encoder** | Bi-Encoder | ã‚¯ã‚¨ãƒªã¨æ–‡æ›¸ã‚’ç‹¬ç«‹ã«Encode |
| **ColBERT** | Contextualized Late Interaction over BERT | Token-level Late Interaction |
| **Self-RAG** | Self-Reflective RAG | åçœãƒˆãƒ¼ã‚¯ãƒ³ã§è‡ªå·±åˆ¶å¾¡ |
| **CRAG** | Corrective RAG | æ¤œç´¢çµæœã®æ­£ç¢ºæ€§è©•ä¾¡+è£œæ­£ |
| **RAGAS** | RAG Assessment | RAGè©•ä¾¡çµ±åˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ |
| **Faithfulness** | Faithfulness | ç”ŸæˆãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å¿ å®Ÿã‹ |
| **Context Relevance** | Context Relevance | ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒã‚¯ã‚¨ãƒªã«é–¢é€£ã—ã¦ã„ã‚‹ã‹ |
| **Chunking** | Chunking | é•·æ–‡æ›¸ã‚’æ¤œç´¢å¯èƒ½ãªãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰² |
| **IDF** | Inverse Document Frequency | é€†æ–‡æ›¸é »åº¦ |
| **TF** | Term Frequency | å˜èªå‡ºç¾é »åº¦ |

### 6.6 æœ¬è¬›ç¾©ã§å­¦ã‚“ã 3ã¤ã®æ ¸å¿ƒ

#### æ ¸å¿ƒ1: RAGã¯çŸ¥è­˜ã®å‹•çš„æ‹¡å¼µ

**Without RAG**: LLMã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®çŸ¥è­˜ã®ã¿ï¼ˆå›ºå®šãƒ»å¤ã„ãƒ»ä¸å®Œå…¨ï¼‰

**With RAG**: å¤–éƒ¨çŸ¥è­˜ã‚’æ¤œç´¢â†’çµ±åˆï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ»æœ€æ–°ãƒ»æ–‡è„ˆç‰¹åŒ–ï¼‰

$$
P(a \mid q) = \sum_{d \in \text{Retrieved}(q)} P(a \mid q, d) \cdot \text{Score}(d, q)
$$

#### æ ¸å¿ƒ2: æ¤œç´¢ç²¾åº¦ãŒRAGã®æˆå¦ã‚’æ±ºã‚ã‚‹

**æ¤œç´¢æˆ¦ç•¥ã®é€²åŒ–**:

```
Naive (BM25ã®ã¿) â†’ Dense (Embedding) â†’ Hybrid (BM25+Dense) â†’ Agentic (Self-RAG/CRAG)
```

**ç²¾åº¦å‘ä¸Šã®éµ**:
1. **Hybrid Retrieval**: Sparse + Dense ã®ç›¸è£œæ€§
2. **Reranking**: Cross-Encoder ã§ç²¾å¯†åŒ–
3. **Agentic Control**: æ¤œç´¢ã‚¿ã‚¤ãƒŸãƒ³ã‚°ãƒ»æˆ¦ç•¥ã®è‡ªå¾‹åˆ¤æ–­

#### æ ¸å¿ƒ3: å®Ÿè£…ã¯3è¨€èªãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯

- **ğŸ¦€ Rust**: Vector DB (HNSW, qdrant) â€” é«˜é€Ÿãƒ»å®‰å…¨
- **âš¡ Julia**: æ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ (BM25, Embedding, RRF) â€” è¡¨ç¾åŠ›ãƒ»é€Ÿåº¦
- **ğŸ”® Elixir**: åˆ†æ•£ã‚µãƒ¼ãƒ“ãƒ³ã‚° (GenServer, Rate Limiting) â€” ä¸¦è¡Œæ€§ãƒ»è€éšœå®³æ€§

### 6.7 FAQ 5å•

**Q1: RAGã¨Fine-tuningã‚’ä½µç”¨ã™ã¹ãã‹ï¼Ÿ**

**A**: ç”¨é€”ã«ã‚ˆã‚‹ã€‚

- **Fine-tuning**: ãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰ã®è¨€èªã‚¹ã‚¿ã‚¤ãƒ«ãƒ»ã‚¿ã‚¹ã‚¯ç‰¹åŒ–
- **RAG**: æœ€æ–°çŸ¥è­˜ãƒ»å‹•çš„çŸ¥è­˜

ä½µç”¨ä¾‹: Fine-tunedãƒ¢ãƒ‡ãƒ« + RAG = ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ– + æœ€æ–°çŸ¥è­˜

**Q2: ãƒ™ã‚¯ãƒˆãƒ«DBã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æˆ¦ç•¥ã¯ï¼Ÿ**

**A**: Sharding + Replication

- **Sharding**: ãƒ‡ãƒ¼ã‚¿ã‚’è¤‡æ•°ãƒãƒ¼ãƒ‰ã«åˆ†å‰²ï¼ˆæ°´å¹³ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼‰
- **Replication**: å„Shardã‚’è¤‡è£½ï¼ˆå¯ç”¨æ€§å‘ä¸Šï¼‰
- qdrant/Milvusã¯æ¨™æº–å¯¾å¿œ

**Q3: BM25ã¨Denseã§ã©ã¡ã‚‰ã‚’å„ªå…ˆï¼Ÿ**

**A**: ã‚¿ã‚¹ã‚¯ã«ã‚ˆã‚‹

- **BM25**: å›ºæœ‰åè©ãƒ»å®Œå…¨ä¸€è‡´ãƒ»ãƒ¬ã‚¢å˜èª
- **Dense**: æ„å‘³çš„é¡ä¼¼æ€§ãƒ»è¨€ã„æ›ãˆãƒ»å¤šè¨€èª
- **æ¨å¥¨**: Hybrid (RRFèåˆ)

**Q4: Chunkã‚µã‚¤ã‚ºã®æœ€é©å€¤ã¯ï¼Ÿ**

**A**: ã‚¿ã‚¹ã‚¯ãƒ»ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹

- **ä¸€èˆ¬**: 256-512 tokens
- **çŸ­æ–‡ã‚¿ã‚¹ã‚¯**: 128 tokens
- **é•·æ–‡ç†è§£**: 1024 tokens
- **å®Ÿé¨“**: Recall/Latency ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã§èª¿æ•´

**Q5: Agentic RAGã®å­¦ç¿’ã‚³ã‚¹ãƒˆã¯ï¼Ÿ**

**A**: é«˜ã„ãŒåŠ¹æœå¤§

- Self-RAG: åçœãƒˆãƒ¼ã‚¯ãƒ³ã®æ•™å¸«ãƒ‡ãƒ¼ã‚¿ç”ŸæˆãŒå¿…è¦
- CRAG: Evaluatorå­¦ç¿’ï¼ˆè»½é‡LMï¼‰
- **ROIé«˜**: æ¤œç´¢ç²¾åº¦ãŒåŠ‡çš„å‘ä¸Šï¼ˆGPT-4è¶…ãˆï¼‰

### 6.8 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆå¾©ç¿’è¾¼ã¿ï¼‰

| Day | å†…å®¹ | æ™‚é–“ | ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ |
|:----|:-----|:-----|:--------------|
| **Day 1** | Zone 0-2 + Zone 3.1-3.2 | 2h | â–¡ RAGå®šç¾© â–¡ BM25å¼å°å‡º â–¡ Embeddingç†è«– |
| **Day 2** | Zone 3.3-3.4 | 2h | â–¡ DPR â–¡ HNSWåŸç† â–¡ RRFå®Ÿè£… |
| **Day 3** | Zone 3.5-3.6 | 2h | â–¡ ColBERT â–¡ Self-RAG â–¡ CRAG |
| **Day 4** | Zone 4 Rustå®Ÿè£… | 3h | â–¡ HNSWå®Ÿè£… â–¡ qdrantçµ±åˆ â–¡ Chunking |
| **Day 5** | Zone 4 Juliaå®Ÿè£… | 2h | â–¡ BM25å®Ÿè£… â–¡ Hybrid search â–¡ Reranking |
| **Day 6** | Zone 4 Elixirå®Ÿè£… | 2h | â–¡ GenServer â–¡ åˆ†æ•£æ¤œç´¢ â–¡ Rate Limiting |
| **Day 7** | Zone 5-7 + å¾©ç¿’ | 2h | â–¡ RAGASè©•ä¾¡ â–¡ SmolVLM2çµ±åˆ â–¡ è‡ªå·±è¨ºæ–­ |

### 6.9 é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼

```
RAGãƒã‚¹ã‚¿ãƒ¼é€²æ—
=====================================
ç†è«– [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100%
  â”œâ”€ Embeddingç†è«–       [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100%
  â”œâ”€ BM25å®Œå…¨å°å‡º        [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100%
  â”œâ”€ Dense Retrieval     [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100%
  â”œâ”€ Hybrid + Reranking  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100%
  â””â”€ Agentic RAG         [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100%

å®Ÿè£… [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100%
  â”œâ”€ ğŸ¦€ Rust HNSW        [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100%
  â”œâ”€ âš¡ Julia BM25       [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100%
  â””â”€ ğŸ”® Elixir Serving  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100%

è©•ä¾¡ [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100%
  â”œâ”€ RAGASå®Ÿè£…           [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100%
  â””â”€ SmolVLM2çµ±åˆ        [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100%

æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: ç¬¬30å› ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Œå…¨ç‰ˆ
```

### 6.10 æ¬¡å›äºˆå‘Š: ç¬¬30å› ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Œå…¨ç‰ˆ

**ç¬¬30å›ã§å­¦ã¶ã“ã¨**:

- **ReAct**: Reasoning + Acting ã®çµ±åˆ
- **Tool Use**: å¤–éƒ¨ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ï¼ˆæ¤œç´¢ãƒ»è¨ˆç®—ãƒ»APIï¼‰
- **Multi-Agent Systems**: å”èª¿ãƒ»ç«¶äº‰ãƒ»äº¤æ¸‰
- **AutoGPT/BabyAGI**: è‡ªå¾‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè£…
- **Planning**: PDDL/HTN ã«ã‚ˆã‚‹é•·æœŸè¨ˆç”»
- **Memory**: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ãƒ»æ„å‘³è¨˜æ†¶ãƒ»ä½œæ¥­è¨˜æ†¶

**RAG â†’ Agent ã®æ¥ç¶š**:

RAGã§å¤–éƒ¨çŸ¥è­˜ã‚’çµ±åˆã—ãŸã€‚æ¬¡ã¯**è‡ªå¾‹çš„ãªè¡Œå‹•**ã‚’è¿½åŠ ã™ã‚‹ã€‚ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯RAGã‚’é“å…·ã¨ã—ã¦ä½¿ã„ã€è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã‚’åˆ†è§£ãƒ»å®Ÿè¡Œãƒ»æ¤œè¨¼ã™ã‚‹ã€‚

```mermaid
graph LR
    L28["ç¬¬28å›<br/>Prompt"] --> L29["ç¬¬29å›<br/>RAG"]
    L29 --> L30["ç¬¬30å›<br/>ğŸ¤–Agent"]
    L30 --> L31["ç¬¬31å›<br/>MLOps"]

    L29 -.å¤–éƒ¨çŸ¥è­˜.-> L30
    L30 -.è‡ªå¾‹å®Ÿè¡Œ.-> L31

    style L30 fill:#fff3e0
```

:::message
**é€²æ—: 98% å®Œäº†** Zone 7å®Œäº†ã€‚3ã¤ã®æ ¸å¿ƒã€FAQã€å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã€é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼ã€æ¬¡å›äºˆå‘Šã‚’æ•´ç†ã€‚æœ€å¾Œã«ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„ã¨å‚è€ƒæ–‡çŒ®ã‚’è¿½åŠ ã™ã‚‹ã€‚
:::

---

### 6.11 ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **ã€Œãƒ¢ãƒ‡ãƒ«ã®çŸ¥è­˜ã¯"ååˆ†"ã‹ï¼Ÿã€**

LLMã¯æ•°åƒå„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§è†¨å¤§ãªçŸ¥è­˜ã‚’è¨˜æ†¶ã™ã‚‹ã€‚GPT-4ã¯åŒ»å¸«å›½å®¶è©¦é¨“ã«åˆæ ¼ã—ã€æ³•å¾‹ç›¸è«‡ã‚‚ã“ãªã™ã€‚ã§ã¯ã€ãªãœRAGãŒå¿…è¦ãªã®ã‹ï¼Ÿ

### çŸ¥è­˜ã®3ã¤ã®é™ç•Œ

**1. é®®åº¦ã®é™ç•Œ**: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¯éå»ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ

- GPT-4ã®çŸ¥è­˜ã‚«ãƒƒãƒˆã‚ªãƒ•: 2023å¹´9æœˆ
- ä¸–ç•Œã¯ç§’å˜ä½ã§å¤‰åŒ–ï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»æ ªä¾¡ãƒ»å¤©æ°—ï¼‰
- **RAGã®è§£**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œç´¢

**2. å®¹é‡ã®é™ç•Œ**: å…¨çŸ¥è­˜ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ä¿å­˜ã¯éç¾å®Ÿçš„

- ä¼æ¥­å›ºæœ‰çŸ¥è­˜ï¼ˆç¤¾å†…æ–‡æ›¸100ä¸‡ä»¶ï¼‰
- å€‹äººã®ä¼šè©±å±¥æ­´
- å°‚é–€åˆ†é‡ã®æœ€æ–°è«–æ–‡
- **RAGã®è§£**: å¤–éƒ¨çŸ¥è­˜ãƒ™ãƒ¼ã‚¹å‚ç…§

**3. æ¤œè¨¼å¯èƒ½æ€§ã®é™ç•Œ**: ãƒ¢ãƒ‡ãƒ«ã®çŸ¥è­˜ã¯"ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹"

- å‡ºå…¸ä¸æ˜ â†’ Hallucination ãƒªã‚¹ã‚¯
- æ³•å‹™ãƒ»åŒ»ç™‚ã§ã¯æ ¹æ‹ æç¤ºãŒå¿…é ˆ
- **RAGã®è§£**: æ¤œç´¢çµæœ=å‡ºå…¸æ˜ç¤º

### ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›

**å¾“æ¥**: ãƒ¢ãƒ‡ãƒ«ã«å…¨çŸ¥è­˜ã‚’è©°ã‚è¾¼ã‚€ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¢—å¤§ï¼‰

**æ–°**: ãƒ¢ãƒ‡ãƒ«=æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã€çŸ¥è­˜=å¤–éƒ¨DBï¼ˆåˆ†é›¢ï¼‰

$$
\text{Intelligence} = \text{Reasoning (Model)} + \text{Knowledge (RAG)}
$$

**é¡æ¨**: äººé–“ã®è¨˜æ†¶

- **ä½œæ¥­è¨˜æ†¶** (Working Memory): LLMã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
- **é•·æœŸè¨˜æ†¶** (Long-term Memory): ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çŸ¥è­˜
- **å¤–éƒ¨è¨˜æ†¶** (External Memory): ãƒãƒ¼ãƒˆãƒ»æ¤œç´¢ = **RAG**

äººé–“ã‚‚å…¨ã¦ã‚’è¨˜æ†¶ã—ãªã„ã€‚å¿…è¦ã«å¿œã˜ã¦èª¿ã¹ã‚‹ã€‚RAGã¯LLMã«"èª¿ã¹ã‚‹èƒ½åŠ›"ã‚’ä¸ãˆã‚‹ã€‚

### ç©¶æ¥µã®å•ã„

> ãƒ¢ãƒ‡ãƒ«ãŒå…¨çŸ¥è­˜ã‚’è¨˜æ†¶ã§ãã‚‹æ—¥ãŒæ¥ã¦ã‚‚ã€RAGã¯å¿…è¦ã‹ï¼Ÿ

**ç­”ãˆ**: Yesã€‚

ç†ç”±:
1. **æ¤œè¨¼å¯èƒ½æ€§**: å‡ºå…¸æ˜ç¤ºã¯ä¿¡é ¼ã®æ ¹å¹¹
2. **ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼**: çŸ¥è­˜ã‚’ãƒ¢ãƒ‡ãƒ«ã«å«ã‚ãªã„é¸æŠè‚¢
3. **ã‚³ã‚¹ãƒˆ**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¢—å¤§ã‚ˆã‚Šæ¤œç´¢ã®æ–¹ãŒå®‰ã„
4. **æŸ”è»Ÿæ€§**: çŸ¥è­˜ã®è¿½åŠ ãƒ»å‰Šé™¤ãŒå³åº§

RAGã¯å˜ãªã‚‹"çŸ¥è­˜ä¸è¶³ã®è£œå®Œ"ã§ã¯ãªã„ã€‚**çŸ¥è­˜ç®¡ç†ã®æ–°ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**ã§ã‚ã‚‹ã€‚

:::message
**é€²æ—: 100% å®Œäº†** ğŸ‰ ç¬¬29å›ã€ŒRAGå®Œå…¨ç‰ˆã€å®Œèµ°ï¼
:::

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

[^1]: Lewis, P., et al. (2020). "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks." *NeurIPS 2020*. [arXiv:2005.11401](https://arxiv.org/abs/2005.11401)

[^2]: Asai, A., et al. (2024). "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection." *ICLR 2024 (Oral)*. [arXiv:2310.11511](https://arxiv.org/abs/2310.11511)

[^3]: Yan, S., et al. (2024). "Corrective Retrieval Augmented Generation." *arXiv preprint*. [arXiv:2401.15884](https://arxiv.org/abs/2401.15884)

[^4]: Jeong, S., et al. (2024). "Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity." *arXiv preprint*. [arXiv:2403.14403](https://arxiv.org/abs/2403.14403)

[^5]: Karpukhin, V., et al. (2020). "Dense Passage Retrieval for Open-Domain Question Answering." *EMNLP 2020*. [arXiv:2004.04906](https://arxiv.org/abs/2004.04906)

[^6]: Malkov, Y. A., & Yashunin, D. A. (2018). "Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs." *IEEE TPAMI*. [arXiv:1603.09320](https://arxiv.org/abs/1603.09320)

[^7]: qdrant. "Qdrant - Vector Database." [GitHub](https://github.com/qdrant/qdrant) | [Docs](https://qdrant.tech/)

[^8]: RAGAS. "RAG Assessment Framework." [GitHub](https://github.com/explodinggradients/ragas)

[^9]: Johnson, J., Douze, M., & JÃ©gou, H. (2019). "Billion-scale similarity search with GPUs." *IEEE Transactions on Big Data*. FAISS [GitHub](https://github.com/facebookresearch/faiss)

### è¿½åŠ ãƒªã‚½ãƒ¼ã‚¹

- **ColBERT**: Khattab, O., & Zaharia, M. (2020). "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT." *SIGIR 2020*. [arXiv:2004.12832](https://arxiv.org/abs/2004.12832)

- **Sentence-BERT**: Reimers, N., & Gurevych, I. (2019). "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks." *EMNLP 2019*. [arXiv:1908.10084](https://arxiv.org/abs/1908.10084)

- **BM25**: Robertson, S., & Zaragoza, H. (2009). "The Probabilistic Relevance Framework: BM25 and Beyond." *Foundations and Trends in Information Retrieval*.

- **MTEB**: Muennighoff, N., et al. (2022). "MTEB: Massive Text Embedding Benchmark." *arXiv preprint*. [arXiv:2210.07316](https://arxiv.org/abs/2210.07316)

---

## ğŸ“– è¨˜æ³•è¦ç´„

æœ¬ã‚·ãƒªãƒ¼ã‚ºã§ä½¿ç”¨ã™ã‚‹æ•°å­¦è¨˜æ³•ã®çµ±ä¸€ãƒ«ãƒ¼ãƒ«:

| è¨˜å· | æ„å‘³ | ä¾‹ |
|:-----|:-----|:---|
| $\mathbf{x}$ | ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆå¤ªå­—å°æ–‡å­—ï¼‰ | $\mathbf{q} \in \mathbb{R}^d$ |
| $\mathbf{W}$ | è¡Œåˆ—ï¼ˆå¤ªå­—å¤§æ–‡å­—ï¼‰ | $\mathbf{W} \in \mathbb{R}^{d \times k}$ |
| $x_i$ | ã‚¹ã‚«ãƒ©ãƒ¼æ·»å­— | $x_1, x_2, \ldots, x_n$ |
| $\mathbf{x}^{(i)}$ | ã‚µãƒ³ãƒ—ãƒ«æ·»å­— | $i$ ç•ªç›®ã®ã‚µãƒ³ãƒ—ãƒ« |
| $\mathbf{x}_t$ | æ™‚åˆ»æ·»å­— | æ™‚åˆ» $t$ ã®ãƒ™ã‚¯ãƒˆãƒ« |
| $\hat{y}$ | æ¨å®šå€¤ï¼ˆãƒãƒƒãƒˆï¼‰ | $\hat{y} = f(\mathbf{x})$ |
| $\mathbb{E}[\cdot]$ | æœŸå¾…å€¤ | $\mathbb{E}_{x \sim p}[f(x)]$ |
| $\nabla_\theta$ | å‹¾é… | $\nabla_\theta \mathcal{L}$ |
| $\mathcal{L}$ | æå¤±é–¢æ•°ï¼ˆã‚«ãƒªã‚°ãƒ©ãƒ•ã‚£ï¼‰ | $\mathcal{L}(\theta)$ |
| $\mathcal{D}$ | ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ | $\mathcal{D} = \{(\mathbf{x}_i, y_i)\}$ |
| $\sim$ | åˆ†å¸ƒã«å¾“ã† | $x \sim \mathcal{N}(0, 1)$ |
| $:=$ | å®šç¾© | $f(x) := x^2$ |
| $\approx$ | è¿‘ä¼¼ | $e^x \approx 1 + x$ (å°ã•ã„ $x$) |
| $\propto$ | æ¯”ä¾‹ | $p(x) \propto e^{-x^2}$ |
| $\odot$ | è¦ç´ ç©ï¼ˆHadamardï¼‰ | $\mathbf{a} \odot \mathbf{b}$ |
| $\||\cdot\||$ | ãƒãƒ«ãƒ  | $\||\mathbf{x}\||_2 = \sqrt{\sum x_i^2}$ |
| $\langle \cdot, \cdot \rangle$ | å†…ç© | $\langle \mathbf{a}, \mathbf{b} \rangle = \mathbf{a}^\top \mathbf{b}$ |
| $\arg\max$ | æœ€å¤§åŒ–ã™ã‚‹å¼•æ•° | $\hat{y} = \arg\max_y p(y \mid \mathbf{x})$ |
| $\mathbb{1}_{\{\cdot\}}$ | æŒ‡ç¤ºé–¢æ•° | $\mathbb{1}_{\{x > 0\}} = 1$ if $x > 0$ else $0$ |

**é–¢æ•°ãƒ»æ¼”ç®—å­**:

| è¨˜æ³• | æ„å‘³ |
|:-----|:-----|
| $\log$ | è‡ªç„¶å¯¾æ•°ï¼ˆ$\ln$ï¼‰ |
| $\log_2$ | åº•2ã®å¯¾æ•° |
| $\sigma(\cdot)$ | ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•° |
| $\text{softmax}(\cdot)$ | ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é–¢æ•° |
| $\text{ReLU}(\cdot)$ | ReLUæ´»æ€§åŒ–é–¢æ•° |

---

:::message
**ğŸ‰ å®Œå…¨ç¿’å¾—é”æˆï¼**

**æœ¬è¬›ç¾©ã®æˆæœ**:
- âœ… RAGç†è«–å®Œå…¨æ§‹ç¯‰ï¼ˆEmbedding/BM25/Dense/Hybrid/Reranking/Agenticï¼‰
- âœ… ğŸ¦€ Rust HNSW Vector DBå®Ÿè£…
- âœ… âš¡ Julia BM25æ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè£…
- âœ… ğŸ”® Elixir åˆ†æ•£RAGã‚µãƒ¼ãƒ“ãƒ³ã‚°å®Ÿè£…
- âœ… RAGASè©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å®Ÿè£…
- âœ… SmolVLM2ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«RAGçµ±åˆ

**ç·è¡Œæ•°**: 2,800+ è¡Œ

**æ¬¡å›**: ç¬¬30å›ã€Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Œå…¨ç‰ˆã€ã§RAGã‚’é“å…·ã¨ã—ã¦ä½¿ã†è‡ªå¾‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè£…ã™ã‚‹ã€‚

**ã‚ãªãŸã¯Production-readyãªRAGã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã§ãã‚‹ã€‚**
:::

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚

---