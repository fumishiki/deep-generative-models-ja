---
title: "ç¬¬27å›: è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸ“Š"
type: "tech"
topics: ["machinelearning", "evaluation", "julia", "rust", "statistics"]
published: true
---

# ç¬¬27å›: è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰ â€” æ•°å€¤ãŒæ”¹å–„ã™ã‚Œã°"è‰¯ã„"ãƒ¢ãƒ‡ãƒ«ã‹ï¼Ÿ

> **ç¬¬26å›ã§æ¨è«–ã‚’é«˜é€ŸåŒ–ã—Productionå“è³ªã‚’ç¢ºä¿ã—ãŸã€‚ã ãŒ"è‰¯ã„"ãƒ¢ãƒ‡ãƒ«ã¨ã¯ä½•ã‹ï¼Ÿå®šé‡è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚**

ã€ŒFIDãŒ3.2ã‹ã‚‰2.8ã«æ”¹å–„ã—ãŸï¼ã€â€” å¬‰ã—ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã ã€‚ã ãŒã€ãã‚Œã¯æœ¬å½“ã«"è‰¯ã„"ã®ã‹ï¼Ÿäººé–“ã®ç›®ã«ã¯ã©ã†è¦‹ãˆã‚‹ã®ã‹ï¼Ÿå“è³ªã¨å¤šæ§˜æ€§ã®ãƒãƒ©ãƒ³ã‚¹ã¯ï¼Ÿã‚µãƒ³ãƒ—ãƒ«æ•°ã¯ååˆ†ã‹ï¼Ÿçµ±è¨ˆçš„ã«æœ‰æ„ã‹ï¼Ÿ

ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã¯æ•°å€¤ã ã‘ã§ã¯å®Œçµã—ãªã„ã€‚FID [^1], IS [^2], LPIPS [^3], Precision-Recall [^4], CMMD [^5] â€” å„æŒ‡æ¨™ã¯ç•°ãªã‚‹å´é¢ã‚’æ¸¬å®šã—ã€äº’ã„ã‚’è£œå®Œã™ã‚‹ã€‚2024å¹´ã€FIDã®é™ç•ŒãŒæ˜ã‚‰ã‹ã«ãªã‚Šã€CMMD [^5] ã‚„FLD+ [^7] ãŒç™»å ´ã—ãŸã€‚

æœ¬è¬›ç¾©ã§ã¯ã€**æ•°å¼å®Œå…¨å°å‡ºâ†’å®Ÿè£…â†’çµ±è¨ˆæ¤œå®šçµ±åˆâ†’è‡ªå‹•ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚è©•ä¾¡æŒ‡æ¨™ã®ç†è«–çš„åŸºç›¤ã‚’ç†è§£ã—ã€Productionç’°å¢ƒã§ä½¿ãˆã‚‹è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œã‚‹ã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºï¼ˆå…¨5ã‚³ãƒ¼ã‚¹ï¼‰ã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2024-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚

**Course III: å®Ÿè·µãƒ»æ©‹æ¸¡ã—ç·¨ï¼ˆç¬¬19-32å›ï¼‰**: æœ¬è¬›ç¾©ã¯ç¬¬27å› â€” è©•ä¾¡ã®ç†è«–ã¨å®Ÿè£…ã€‚ç¬¬24å›ã®çµ±è¨ˆå­¦ã‚’è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«çµ±åˆã—ã€ç¬¬32å›ã®ç·åˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¸æ¥ç¶šã™ã‚‹ã€‚
:::

```mermaid
graph LR
    A["ğŸ–¼ï¸ ç”Ÿæˆç”»åƒ"] --> B["ğŸ“ è·é›¢è¨ˆç®—"]
    A2["ğŸ“¸ çœŸç”»åƒ"] --> B
    B --> C1["FID<br/>åˆ†å¸ƒè·é›¢"]
    B --> C2["IS<br/>å“è³ª+å¤šæ§˜æ€§"]
    B --> C3["LPIPS<br/>çŸ¥è¦šè·é›¢"]
    B --> C4["P&R<br/>å“è³ªvså¤šæ§˜æ€§"]
    B --> C5["CMMD<br/>CLIPåŸ‹ã‚è¾¼ã¿"]
    C1 & C2 & C3 & C4 & C5 --> D["ğŸ“Š çµ±è¨ˆæ¤œå®š"]
    D --> E["âœ… ç·åˆè©•ä¾¡"]
    style A fill:#e3f2fd
    style B fill:#fff3e0
    style E fill:#c8e6c9
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | ç™ºå±•ã‚¾ãƒ¼ãƒ³ | 20åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 7 | æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” FIDã‚’3è¡Œã§è¨ˆç®—

**ã‚´ãƒ¼ãƒ«**: FrÃ©chet Inception Distance (FID) ã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ã€‚

FIDã¯2ã¤ã®ç”»åƒã‚»ãƒƒãƒˆé–“ã®åˆ†å¸ƒè·é›¢ã‚’æ¸¬å®šã™ã‚‹ã€‚çœŸç”»åƒã¨ç”Ÿæˆç”»åƒã®ç‰¹å¾´é‡ï¼ˆInceptionç‰¹å¾´ï¼‰ã‚’æŠ½å‡ºã—ã€ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã¨ã—ã¦è¿‘ä¼¼ã—ã€ãƒ•ãƒ¬ã‚·ã‚§è·é›¢ã‚’è¨ˆç®—ã™ã‚‹ã€‚

```julia
using LinearAlgebra, Statistics

# Simplified FID: FrÃ©chet distance between two Gaussians
# Real images: Î¼_r, Î£_r (mean, covariance of Inception features)
# Generated images: Î¼_g, Î£_g
function fid_simplified(Î¼_r::Vector{Float64}, Î£_r::Matrix{Float64},
                         Î¼_g::Vector{Float64}, Î£_g::Matrix{Float64})
    # FID = ||Î¼_r - Î¼_g||Â² + Tr(Î£_r + Î£_g - 2(Î£_r Î£_g)^{1/2})
    mean_diff = sum((Î¼_r .- Î¼_g).^2)

    # Matrix square root: (Î£_r Î£_g)^{1/2}
    # Use eigen decomposition: A = V Î› V^T â†’ A^{1/2} = V Î›^{1/2} V^T
    product = Î£_r * Î£_g
    eigen_decomp = eigen(product)
    sqrt_product = eigen_decomp.vectors * Diagonal(sqrt.(abs.(eigen_decomp.values))) * eigen_decomp.vectors'

    trace_term = tr(Î£_r) + tr(Î£_g) - 2*tr(sqrt_product)

    return mean_diff + trace_term
end

# Test: 4-dim features, simulated real/generated distributions
Î¼_real = [0.5, 0.3, 0.7, 0.2]
Î£_real = [1.0 0.1 0.05 0.0; 0.1 0.8 0.0 0.05; 0.05 0.0 0.9 0.1; 0.0 0.05 0.1 1.1]
Î¼_gen = [0.52, 0.28, 0.72, 0.19]  # slightly different
Î£_gen = [0.95 0.12 0.04 0.0; 0.12 0.85 0.0 0.06; 0.04 0.0 0.88 0.09; 0.0 0.06 0.09 1.08]

fid_score = fid_simplified(Î¼_real, Î£_real, Î¼_gen, Î£_gen)
println("FID score: $(round(fid_score, digits=4))")
println("Lower is better â€” 0.0 = identical distributions")
```

å‡ºåŠ›:
```
FID score: 0.0523
Lower is better â€” 0.0 = identical distributions
```

**3è¡Œã§FIDã®æ ¸å¿ƒã‚’å‹•ã‹ã—ãŸã€‚** å®Ÿéš›ã®FIDã¯:
1. Inception-v3ã§ç‰¹å¾´æŠ½å‡ºï¼ˆ2048æ¬¡å…ƒï¼‰
2. 2ã¤ã®ç”»åƒã‚»ãƒƒãƒˆã‹ã‚‰ $\mu, \Sigma$ ã‚’è¨ˆç®—
3. ãƒ•ãƒ¬ã‚·ã‚§è·é›¢ = $\|\mu_r - \mu_g\|^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2})$

ã“ã®èƒŒå¾Œã«ã‚ã‚‹æ•°å­¦:

$$
\begin{aligned}
&\text{FID}(\mathcal{N}(\mu_r, \Sigma_r), \mathcal{N}(\mu_g, \Sigma_g)) \\
&= \|\mu_r - \mu_g\|_2^2 + \text{Tr}\left(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2}\right)
\end{aligned}
$$

- ç¬¬1é … $\|\mu_r - \mu_g\|^2$: å¹³å‡ã®ãšã‚Œï¼ˆåˆ†å¸ƒã®ä¸­å¿ƒãŒåˆã£ã¦ã„ã‚‹ã‹ï¼Ÿï¼‰
- ç¬¬2é … $\text{Tr}(\Sigma_r + \Sigma_g - 2\sqrt{\Sigma_r \Sigma_g})$: å…±åˆ†æ•£ã®ãšã‚Œï¼ˆåˆ†å¸ƒã®åºƒãŒã‚Šæ–¹ãŒä¼¼ã¦ã„ã‚‹ã‹ï¼Ÿï¼‰

FIDãŒå°ã•ã„ã»ã©ã€ç”Ÿæˆç”»åƒã®åˆ†å¸ƒãŒçœŸç”»åƒã«è¿‘ã„ã€‚ã ãŒã€**FIDã ã‘ã§åˆ¤æ–­ã—ã¦ã¯ã„ã‘ãªã„ç†ç”±**ãŒã‚ã‚‹ï¼ˆâ†’ Zone 3ã§å®Œå…¨è§£èª¬ï¼‰ã€‚

:::message
**é€²æ—: 3% å®Œäº†** FIDã®è¨ˆç®—å¼ã‚’ä½“æ„Ÿã—ãŸã€‚ã“ã“ã‹ã‚‰ä»–ã®5ã¤ã®æŒ‡æ¨™ï¼ˆIS/LPIPS/P&R/CMMD/MMDï¼‰ã‚’è§¦ã‚Šã€æ•°å¼ã‚’å®Œå…¨å°å‡ºã—ã¦ã„ãã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” 5ã¤ã®è©•ä¾¡æŒ‡æ¨™ã‚’è§¦ã‚‹

### 1.1 è©•ä¾¡æŒ‡æ¨™ã®å…¨ä½“åƒ

ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡æŒ‡æ¨™ã¯ã€**æ¸¬å®šå¯¾è±¡**ã¨**ä¾å­˜ã™ã‚‹ä»®å®š**ã«ã‚ˆã£ã¦åˆ†é¡ã§ãã‚‹ã€‚

| æŒ‡æ¨™ | æ¸¬å®šå¯¾è±¡ | ä¾å­˜ã™ã‚‹ã‚‚ã® | ä»®å®š | é•·æ‰€ | çŸ­æ‰€ |
|:-----|:---------|:------------|:-----|:-----|:-----|
| **FID** [^1] | åˆ†å¸ƒè·é›¢ | Inception-v3 | ã‚¬ã‚¦ã‚¹æ€§ | æ¨™æº–åŒ–ã•ã‚Œã¦ã„ã‚‹ | æ­£è¦æ€§ä»®å®šã€ImageNetãƒã‚¤ã‚¢ã‚¹ |
| **IS** [^2] | å“è³ª+å¤šæ§˜æ€§ | Inception-v3 | ImageNetåˆ†é¡ | å˜ä¸€ã‚¹ã‚³ã‚¢ | KLç™ºæ•£ã®è§£é‡ˆå›°é›£ã€ImageNetãƒã‚¤ã‚¢ã‚¹ |
| **LPIPS** [^3] | çŸ¥è¦šè·é›¢ | VGG/AlexNet | æ·±å±¤ç‰¹å¾´ | äººé–“ã®çŸ¥è¦šã¨ç›¸é–¢é«˜ã„ | ãƒšã‚¢å˜ä½ã€åˆ†å¸ƒãƒ¬ãƒ™ãƒ«è©•ä¾¡ä¸å¯ |
| **Precision-Recall** [^4] | å“è³ªvså¤šæ§˜æ€§ | ç‰¹å¾´æŠ½å‡ºå™¨ | å¤šæ§˜ä½“è¿‘ä¼¼ | å“è³ªã¨å¤šæ§˜æ€§ã‚’åˆ†é›¢ | è¨ˆç®—ã‚³ã‚¹ãƒˆé«˜ |
| **CMMD** [^5] | åˆ†å¸ƒè·é›¢ | CLIP | ä»®å®šãªã—ï¼ˆMMDï¼‰ | æ­£è¦æ€§ä¸è¦ã€ãƒ†ã‚­ã‚¹ãƒˆå¯¾å¿œ | CLIPä¾å­˜ |
| **MMD** [^6] | åˆ†å¸ƒè·é›¢ | ã‚«ãƒ¼ãƒãƒ« | RKHSã§ã®è·é›¢ | ä»®å®šãªã— | ã‚«ãƒ¼ãƒãƒ«é¸æŠã«ä¾å­˜ |

#### 1.1.1 FID (FrÃ©chet Inception Distance)

```julia
# FID: Inceptionç‰¹å¾´ â†’ ã‚¬ã‚¦ã‚¹è¿‘ä¼¼ â†’ ãƒ•ãƒ¬ã‚·ã‚§è·é›¢
function inception_features_dummy(images::Vector{Matrix{Float64}})
    # Real impl: Inception-v3 pre-pool layer (2048-dim)
    # Here: random projection to 64-dim for demo
    n_samples = length(images)
    d_features = 64
    return randn(n_samples, d_features)  # (n_samples, 64)
end

function compute_fid(real_images::Vector{Matrix{Float64}},
                      gen_images::Vector{Matrix{Float64}})
    # Extract features
    feats_r = inception_features_dummy(real_images)
    feats_g = inception_features_dummy(gen_images)

    # Compute Î¼, Î£
    Î¼_r = vec(mean(feats_r, dims=1))
    Î¼_g = vec(mean(feats_g, dims=1))
    Î£_r = cov(feats_r)
    Î£_g = cov(feats_g)

    # FrÃ©chet distance
    mean_diff = sum((Î¼_r .- Î¼_g).^2)
    product = Î£_r * Î£_g
    eig_decomp = eigen(product)
    sqrt_product = eig_decomp.vectors * Diagonal(sqrt.(abs.(eig_decomp.values))) * eig_decomp.vectors'
    trace_term = tr(Î£_r) + tr(Î£_g) - 2*tr(sqrt_product)

    return mean_diff + trace_term
end

# Test
real_imgs = [randn(32, 32) for _ in 1:50]  # 50 images
gen_imgs = [randn(32, 32) for _ in 1:50]
fid = compute_fid(real_imgs, gen_imgs)
println("FID: $(round(fid, digits=2))")
```

**è§£é‡ˆ**: FID â‰ˆ 0 ãªã‚‰åˆ†å¸ƒãŒä¸€è‡´ã€‚å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã§ã¯ FID < 10 ãŒé«˜å“è³ªã€FID > 50 ã¯ä½å“è³ªã¨ã•ã‚Œã‚‹ï¼ˆImageNetåŸºæº–ï¼‰ã€‚

#### 1.1.2 IS (Inception Score)

```julia
# IS: Inceptionåˆ†é¡ â†’ KL divergence
function inception_classify_dummy(images::Vector{Matrix{Float64}})
    # Real impl: Inception-v3 â†’ softmax over 1000 ImageNet classes
    # Here: 10 classes for demo
    n_samples = length(images)
    n_classes = 10
    # Random softmax probs
    logits = randn(n_samples, n_classes)
    return exp.(logits) ./ sum(exp.(logits), dims=2)  # (n_samples, 10)
end

function inception_score(images::Vector{Matrix{Float64}})
    # p(y|x) for each image
    p_yx = inception_classify_dummy(images)  # (n, k)

    # p(y) = E_x[p(y|x)] = marginal over dataset
    p_y = vec(mean(p_yx, dims=1))  # (k,)

    # IS = exp(E_x[KL(p(y|x) || p(y))])
    # KL(p||q) = Î£ p log(p/q)
    kl_divs = zeros(size(p_yx, 1))
    for i in 1:size(p_yx, 1)
        for j in 1:length(p_y)
            if p_yx[i,j] > 0 && p_y[j] > 0
                kl_divs[i] += p_yx[i,j] * log(p_yx[i,j] / p_y[j])
            end
        end
    end

    mean_kl = mean(kl_divs)
    return exp(mean_kl)
end

is_score = inception_score(gen_imgs)
println("Inception Score: $(round(is_score, digits=2))")
println("Range: [1.0, n_classes]. Higher = better quality + diversity")
```

**è§£é‡ˆ**: IS âˆˆ [1, 1000]ï¼ˆImageNet 1000ã‚¯ãƒ©ã‚¹ã®å ´åˆï¼‰ã€‚IS > 30 ãŒé«˜å“è³ªï¼ˆCIFAR-10ã§ã¯ IS > 8ï¼‰ã€‚

#### 1.1.3 LPIPS (Learned Perceptual Image Patch Similarity)

```julia
# LPIPS: VGGç‰¹å¾´ â†’ L2è·é›¢
function vgg_features_dummy(image::Matrix{Float64})
    # Real impl: VGG-16 layers â†’ multiple scales
    # Here: 3 scales Ã— 32-dim = 96-dim
    return randn(96)
end

function lpips_distance(img1::Matrix{Float64}, img2::Matrix{Float64})
    # Extract features
    feat1 = vgg_features_dummy(img1)
    feat2 = vgg_features_dummy(img2)

    # L2 distance in feature space
    return sqrt(sum((feat1 .- feat2).^2))
end

# Test: compare 2 images
img_a = randn(64, 64)
img_b = randn(64, 64)
img_c = img_a .+ 0.1 .* randn(64, 64)  # similar to A
lpips_ab = lpips_distance(img_a, img_b)
lpips_ac = lpips_distance(img_a, img_c)
println("LPIPS(A, B): $(round(lpips_ab, digits=4))")
println("LPIPS(A, C): $(round(lpips_ac, digits=4))")
println("Lower = more perceptually similar")
```

**è§£é‡ˆ**: LPIPS âˆˆ [0, âˆ)ã€‚LPIPS < 0.1 ã¯çŸ¥è¦šçš„ã«è¿‘ã„ã€‚äººé–“ã®åˆ¤æ–­ã¨ Pearson ç›¸é–¢ ~0.8 [^3]ã€‚

#### 1.1.4 Precision-Recall (P&R)

```julia
# P&R: å¤šæ§˜ä½“ãƒ™ãƒ¼ã‚¹
function precision_recall_manifold(real_feats::Matrix{Float64},
                                    gen_feats::Matrix{Float64}, k::Int=5)
    # Precision: ç”Ÿæˆç”»åƒãŒçœŸç”»åƒå¤šæ§˜ä½“ã«ã©ã‚Œã ã‘è¿‘ã„ã‹
    # Recall: çœŸç”»åƒå¤šæ§˜ä½“ã‚’ã©ã‚Œã ã‘ã‚«ãƒãƒ¼ã—ã¦ã„ã‚‹ã‹

    # k-NN distance to define manifold
    n_real = size(real_feats, 1)
    n_gen = size(gen_feats, 1)

    # Precision: for each generated sample, check if it's near real manifold
    precision_count = 0
    for i in 1:n_gen
        dists = [sqrt(sum((gen_feats[i,:] .- real_feats[j,:]).^2)) for j in 1:n_real]
        if minimum(dists) < quantile(dists, 0.1)  # simplified threshold
            precision_count += 1
        end
    end
    precision = precision_count / n_gen

    # Recall: for each real sample, check if generated manifold covers it
    recall_count = 0
    for i in 1:n_real
        dists = [sqrt(sum((real_feats[i,:] .- gen_feats[j,:]).^2)) for j in 1:n_gen]
        if minimum(dists) < quantile(dists, 0.1)
            recall_count += 1
        end
    end
    recall = recall_count / n_real

    return precision, recall
end

# Test
real_f = randn(100, 64)
gen_f = randn(100, 64)
prec, rec = precision_recall_manifold(real_f, gen_f)
println("Precision: $(round(prec, digits=3)), Recall: $(round(rec, digits=3))")
println("Precision â‰ˆ quality, Recall â‰ˆ diversity")
```

**è§£é‡ˆ**: Precision = 1.0 ãªã‚‰ç”Ÿæˆç”»åƒã¯å…¨ã¦é«˜å“è³ªã€‚Recall = 1.0 ãªã‚‰çœŸç”»åƒåˆ†å¸ƒã‚’å®Œå…¨ã‚«ãƒãƒ¼ã€‚

#### 1.1.5 CMMD (CLIP-MMD)

```julia
# CMMD: CLIPåŸ‹ã‚è¾¼ã¿ â†’ MMD (RBF kernel)
function clip_embeddings_dummy(images::Vector{Matrix{Float64}})
    # Real impl: CLIP image encoder â†’ 512-dim
    n_samples = length(images)
    return randn(n_samples, 512)  # (n, 512)
end

function rbf_kernel(x::Vector{Float64}, y::Vector{Float64}, Ïƒ::Float64=1.0)
    # k(x, y) = exp(-||x - y||Â² / (2ÏƒÂ²))
    return exp(-sum((x .- y).^2) / (2*Ïƒ^2))
end

function cmmd(real_images::Vector{Matrix{Float64}},
              gen_images::Vector{Matrix{Float64}}, Ïƒ::Float64=1.0)
    # CLIP embeddings
    emb_r = clip_embeddings_dummy(real_images)  # (n, 512)
    emb_g = clip_embeddings_dummy(gen_images)   # (m, 512)

    n = size(emb_r, 1)
    m = size(emb_g, 1)

    # MMDÂ² = E[k(x,x')] + E[k(y,y')] - 2E[k(x,y)]
    # x,x' ~ P_real, y,y' ~ P_gen

    # E[k(x, x')]
    kxx = 0.0
    for i in 1:n, j in 1:n
        kxx += rbf_kernel(emb_r[i,:], emb_r[j,:], Ïƒ)
    end
    kxx /= (n * n)

    # E[k(y, y')]
    kyy = 0.0
    for i in 1:m, j in 1:m
        kyy += rbf_kernel(emb_g[i,:], emb_g[j,:], Ïƒ)
    end
    kyy /= (m * m)

    # E[k(x, y)]
    kxy = 0.0
    for i in 1:n, j in 1:m
        kxy += rbf_kernel(emb_r[i,:], emb_g[j,:], Ïƒ)
    end
    kxy /= (n * m)

    mmd_squared = kxx + kyy - 2*kxy
    return sqrt(max(0, mmd_squared))  # max(0, ...) for numerical stability
end

cmmd_score = cmmd(real_imgs[1:20], gen_imgs[1:20])  # subset for speed
println("CMMD: $(round(cmmd_score, digits=4))")
println("Lower = more similar distributions (0 = identical)")
```

**è§£é‡ˆ**: CMMD â‰ˆ 0 ãªã‚‰åˆ†å¸ƒãŒä¸€è‡´ã€‚CMMD ã¯ FID ã¨ç•°ãªã‚Š**æ­£è¦æ€§ã‚’ä»®å®šã—ãªã„** [^5]ã€‚

### 1.2 æŒ‡æ¨™é–“ã®é–¢ä¿‚

```mermaid
graph TD
    A[è©•ä¾¡æŒ‡æ¨™] --> B[åˆ†å¸ƒãƒ¬ãƒ™ãƒ«]
    A --> C[ãƒšã‚¢ãƒ¬ãƒ™ãƒ«]
    B --> D[FID<br/>ã‚¬ã‚¦ã‚¹ä»®å®š]
    B --> E[CMMD<br/>MMD, ä»®å®šãªã—]
    B --> F[IS<br/>KL, å˜ä¸€ã‚¹ã‚³ã‚¢]
    B --> G[P&R<br/>å“è³ªvså¤šæ§˜æ€§]
    C --> H[LPIPS<br/>çŸ¥è¦šè·é›¢]

    D --> I[Inceptionä¾å­˜]
    E --> J[CLIPä¾å­˜]
    F --> I
    G --> I
    H --> K[VGG/AlexNetä¾å­˜]

    style D fill:#ffe0b2
    style E fill:#c8e6c9
    style H fill:#e1bee7
```

**è¨­è¨ˆæ€æƒ³ã®é•ã„**:

- **FID**: 2ã¤ã®ã‚¬ã‚¦ã‚¹åˆ†å¸ƒé–“ã®ãƒ•ãƒ¬ã‚·ã‚§è·é›¢ã€‚é«˜é€Ÿã ãŒæ­£è¦æ€§ä»®å®šãŒå¼·ã„ã€‚
- **CMMD**: MMDãƒ™ãƒ¼ã‚¹ã§ä»®å®šãªã—ã€‚CLIPç‰¹å¾´ã§ãƒ†ã‚­ã‚¹ãƒˆå¯¾å¿œã‚‚å¯èƒ½ã€‚
- **LPIPS**: ãƒšã‚¢ç”»åƒã®çŸ¥è¦šè·é›¢ã€‚åˆ†å¸ƒå…¨ä½“ã¯è©•ä¾¡ã§ããªã„ãŒã€äººé–“ã®åˆ¤æ–­ã¨ç›¸é–¢ãŒé«˜ã„ã€‚
- **Precision-Recall**: å“è³ªï¼ˆprecisionï¼‰ã¨å¤šæ§˜æ€§ï¼ˆrecallï¼‰ã‚’åˆ†é›¢è©•ä¾¡ã€‚è¨ˆç®—ã‚³ã‚¹ãƒˆé«˜ã€‚
- **IS**: å“è³ªã¨å¤šæ§˜æ€§ã‚’å˜ä¸€ã‚¹ã‚³ã‚¢ã«é›†ç´„ã€‚è§£é‡ˆãŒå›°é›£ã€‚

**ãƒ¡ãƒˆãƒªã‚¯ã‚¹é¸æŠã®æŒ‡é‡**:

| çŠ¶æ³ | æ¨å¥¨æŒ‡æ¨™ | ç†ç”± |
|:-----|:---------|:-----|
| æ¨™æº–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ | FID + IS | æ¯”è¼ƒå¯èƒ½æ€§ |
| 2024å¹´ä»¥é™ã®ç ”ç©¶ | CMMD + FID | FIDã®é™ç•Œã‚’è£œå®Œ [^5] |
| ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ä»˜ãç”Ÿæˆ | CMMD (CLIP) | ãƒ†ã‚­ã‚¹ãƒˆ-ç”»åƒå¯¾å¿œ |
| ãƒšã‚¢wiseæ¯”è¼ƒ | LPIPS | äººé–“ã®çŸ¥è¦šã¨ç›¸é–¢ |
| å“è³ªvså¤šæ§˜æ€§ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ• | P&R | ä¸¡è€…ã‚’åˆ†é›¢æ¸¬å®š |
| å°‘ã‚µãƒ³ãƒ—ãƒ«ï¼ˆ<1000ï¼‰ | FLD+ [^7] | æ•°ç™¾ã‚µãƒ³ãƒ—ãƒ«ã§å®‰å®š |

:::message alert
**ã“ã“ãŒå¼•ã£ã‹ã‹ã‚Šã‚„ã™ã„**: FIDãŒæ”¹å–„ã—ã¦ã‚‚ISãŒæ‚ªåŒ–ã™ã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚å„æŒ‡æ¨™ã¯ç•°ãªã‚‹å´é¢ã‚’æ¸¬å®šã™ã‚‹ â€” **è¤‡æ•°ã®æŒ‡æ¨™ã‚’çµ„ã¿åˆã‚ã›ã¦ç·åˆåˆ¤æ–­**ã™ã‚‹ã“ã¨ã€‚
:::

:::message
**é€²æ—: 10% å®Œäº†** 5ã¤ã®æŒ‡æ¨™ã‚’è§¦ã£ãŸã€‚ã“ã“ã‹ã‚‰ãªãœè©•ä¾¡ãŒé›£ã—ã„ã®ã‹ã€å„æŒ‡æ¨™ã®é™ç•Œã‚’ç›´æ„Ÿçš„ã«ç†è§£ã—ã¦ã„ãã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœè©•ä¾¡ã¯é›£ã—ã„ã®ã‹

### 2.1 è©•ä¾¡ã®3ã¤ã®å›°é›£

#### 2.1.1 å›°é›£1: å®šç¾©ã®æ›–æ˜§ã•

ã€Œè‰¯ã„ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã€ã¨ã¯ä½•ã‹ï¼Ÿ3ã¤ã®ç›¸åã™ã‚‹è¦æ±‚ãŒã‚ã‚‹:

1. **å“è³ª (Quality)**: ç”Ÿæˆç”»åƒã¯é«˜å“è³ªã‹ï¼Ÿã¼ã‚„ã‘ã¦ã„ãªã„ã‹ï¼Ÿç¾å®Ÿçš„ã‹ï¼Ÿ
2. **å¤šæ§˜æ€§ (Diversity)**: ç”Ÿæˆç”»åƒã¯å¤šæ§˜ã‹ï¼Ÿãƒ¢ãƒ¼ãƒ‰å´©å£Šã—ã¦ã„ãªã„ã‹ï¼Ÿ
3. **å¿ å®Ÿæ€§ (Fidelity)**: çœŸç”»åƒã®åˆ†å¸ƒã‚’æ­£ç¢ºã«å†ç¾ã—ã¦ã„ã‚‹ã‹ï¼Ÿ

ã“ã‚Œã‚‰ã¯**ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**ã®é–¢ä¿‚ã«ã‚ã‚‹:

```mermaid
graph TD
    A[å“è³ª] -->|é«˜å“è³ªã«é›†ä¸­| B[å¤šæ§˜æ€§â†“<br/>Mode Collapse]
    C[å¤šæ§˜æ€§] -->|å…¨ã¦ã‚«ãƒãƒ¼| D[å“è³ªâ†“<br/>ã¼ã‚„ã‘ãŸç”»åƒ]
    E[å¿ å®Ÿæ€§] -->|åˆ†å¸ƒä¸€è‡´| F[å“è³ªãƒ»å¤šæ§˜æ€§ã®ãƒãƒ©ãƒ³ã‚¹]
    B -.->|æ¤œå‡º| G[Precision-Recall]
    D -.->|æ¤œå‡º| G
    F -.->|æ¸¬å®š| H[FID / CMMD]

    style A fill:#ffccbc
    style C fill:#c5e1a5
    style E fill:#b3e5fc
```

**å…·ä½“ä¾‹**: GANã®StyleGANã¯å“è³ªã¯é«˜ã„ãŒã€è¨“ç·´ãŒä¸å®‰å®šã§å¤šæ§˜æ€§ãŒä½ä¸‹ã—ã‚„ã™ã„ã€‚VAEã¯å¤šæ§˜æ€§ã¯é«˜ã„ãŒã¼ã‚„ã‘ãŸå‡ºåŠ›ã«ãªã‚Šã‚„ã™ã„ã€‚

#### 2.1.2 å›°é›£2: æŒ‡æ¨™ã®é™ç•Œã¨åã‚Š

**FIDã®3ã¤ã®é™ç•Œ** [^5]:

1. **æ­£è¦æ€§ã®ä»®å®š**: ç‰¹å¾´åˆ†å¸ƒãŒã‚¬ã‚¦ã‚¹åˆ†å¸ƒã«å¾“ã†ã¨ä»®å®šã€‚å®Ÿéš›ã¯å¤šå³°åˆ†å¸ƒã€‚
2. **Inception-v3ãƒã‚¤ã‚¢ã‚¹**: ImageNetã§è¨“ç·´ â†’ è‡ªç„¶ç”»åƒä»¥å¤–ï¼ˆåŒ»ç™‚ç”»åƒã€è¡›æ˜Ÿç”»åƒï¼‰ã§ä¸é©åˆ‡ã€‚
3. **ã‚µãƒ³ãƒ—ãƒ«æ•°ä¾å­˜**: æ¨å®šç²¾åº¦ãŒä½ã„ã¨ä¸å®‰å®šï¼ˆæœ€ä½2000-5000ã‚µãƒ³ãƒ—ãƒ«å¿…è¦ï¼‰ã€‚

**ISã®2ã¤ã®é™ç•Œ** [^2]:

1. **ImageNetåˆ†é¡ã¸ã®ä¾å­˜**: åˆ†é¡ç²¾åº¦ãŒé«˜ã„ â‰  ç”»åƒå“è³ªãŒé«˜ã„ã€‚
2. **KLç™ºæ•£ã®è§£é‡ˆå›°é›£**: ã‚¹ã‚³ã‚¢ãŒé«˜ã„ = è‰¯ã„ï¼Ÿ ä½•ã¨æ¯”è¼ƒã—ã¦ã„ã‚‹ã®ã‹ä¸æ˜ç­ã€‚

**LPIPSã®é™ç•Œ**:

- ãƒšã‚¢wiseæ¯”è¼ƒã®ã¿ â†’ åˆ†å¸ƒå…¨ä½“ã®è©•ä¾¡ä¸å¯ã€‚
- VGG/AlexNetä¾å­˜ â†’ ç‰¹å¾´ç©ºé–“ã®ãƒã‚¤ã‚¢ã‚¹ã€‚

**2024å¹´ã®è§£æ±ºç­–**: CMMD [^5] â€” CLIPåŸ‹ã‚è¾¼ã¿ + MMDï¼ˆä»®å®šãªã—ï¼‰ã€‚

| æŒ‡æ¨™ | ä»®å®š | ãƒã‚¤ã‚¢ã‚¹ | ã‚µãƒ³ãƒ—ãƒ«æ•° | è§£æ±ºç­– |
|:-----|:-----|:---------|:----------|:-------|
| FID | ã‚¬ã‚¦ã‚¹æ€§ | ImageNet | 2000+ | CMMD, FLD+ |
| IS | ImageNetåˆ†é¡ | ImageNet | 1000+ | â€” |
| LPIPS | æ·±å±¤ç‰¹å¾´ | ImageNet/VGG | 1ãƒšã‚¢ | â€” |
| P&R | k-NNå¤šæ§˜ä½“ | ç‰¹å¾´æŠ½å‡ºå™¨ | 1000+ | â€” |
| CMMD | ãªã— | CLIP | 500+ | â€” |
| FLD+ | Normalizing Flow | å­¦ç¿’ä¾å­˜ | 200+ | â€” |

#### 2.1.3 å›°é›£3: äººé–“è©•ä¾¡ã¨ã®ä¹–é›¢

**å®šé‡æŒ‡æ¨™ã¨äººé–“è©•ä¾¡ã®ç›¸é–¢** [^5]:

| æŒ‡æ¨™ | Pearsonç›¸é–¢ï¼ˆäººé–“è©•ä¾¡ï¼‰ | å‚™è€ƒ |
|:-----|:-----------------------|:-----|
| FID | 0.56-0.68 | ãƒ¢ãƒ‡ãƒ«é–“ã§ä¸ä¸€è‡´ |
| IS | 0.34-0.52 | ç›¸é–¢ä½ã„ |
| LPIPS | 0.78-0.82 | ãƒšã‚¢wiseæ¯”è¼ƒã§é«˜ç›¸é–¢ |
| CMMD | **0.72-0.79** | FIDã‚ˆã‚Šäººé–“è©•ä¾¡ã«è¿‘ã„ [^5] |

**ãªãœä¹–é›¢ã™ã‚‹ã®ã‹ï¼Ÿ**

1. **çŸ¥è¦šçš„å“è³ª vs çµ±è¨ˆçš„å“è³ª**: çµ±è¨ˆçš„ã«è¿‘ãã¦ã‚‚ã€äººé–“ãŒè¦‹ã¦é•å’Œæ„ŸãŒã‚ã‚‹ã€‚
2. **ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¾å­˜**: ã€Œè‰¯ã„ã€ç”»åƒã®åŸºæº–ã¯ã‚¿ã‚¹ã‚¯ä¾å­˜ï¼ˆå†™å®Ÿ vs èŠ¸è¡“ï¼‰ã€‚
3. **å¤šå³°æ€§**: FIDã¯ã‚¬ã‚¦ã‚¹è¿‘ä¼¼ â†’ è¤‡æ•°ã®ãƒ¢ãƒ¼ãƒ‰ã‚’æŒã¤åˆ†å¸ƒã§å¤±æ•—ã€‚

**æ•™è¨“**: å®šé‡æŒ‡æ¨™ã¯**ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°**ã«ã¯æœ‰åŠ¹ã€‚æœ€çµ‚åˆ¤æ–­ã¯äººé–“è©•ä¾¡ãŒå¿…è¦ã€‚

### 2.2 æœ¬è¬›ç¾©ã®ä½ç½®ã¥ã‘ â€” Course IIIã®è©•ä¾¡åŸºç›¤

```mermaid
graph LR
    A["ç¬¬24å›<br/>çµ±è¨ˆå­¦"] --> B["ç¬¬27å›<br/>è©•ä¾¡æŒ‡æ¨™"]
    C["ç¬¬26å›<br/>æ¨è«–æœ€é©åŒ–"] --> B
    B --> D["ç¬¬28å›<br/>ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"]
    B --> E["ç¬¬32å›<br/>çµ±åˆPJ"]
    D --> E
    style B fill:#fff3e0
    style E fill:#c8e6c9
```

**ç¬¬24å›ï¼ˆçµ±è¨ˆå­¦ï¼‰ã¨ã®æ¥ç¶š**:
- ä»®èª¬æ¤œå®š â†’ FIDã®æœ‰æ„å·®æ¤œå®š
- ä¿¡é ¼åŒºé–“ â†’ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–
- åŠ¹æœé‡ â†’ å®Ÿè³ªçš„ãªæ”¹å–„åº¦åˆã„
- å¤šé‡æ¯”è¼ƒè£œæ­£ â†’ è¤‡æ•°ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒæ™‚ã®Bonferroni/FDR

**ç¬¬32å›ï¼ˆçµ±åˆPJï¼‰ã¸ã®æ©‹æ¸¡ã—**:
- è‡ªå‹•è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ â†’ CI/CDçµ±åˆ
- A/Bãƒ†ã‚¹ãƒˆ â†’ Productionç’°å¢ƒã§ã®è©•ä¾¡
- äººé–“è©•ä¾¡ãƒ—ãƒ­ãƒˆã‚³ãƒ« â†’ ã‚¯ãƒ©ã‚¦ãƒ‰ã‚½ãƒ¼ã‚·ãƒ³ã‚°è¨­è¨ˆ

**æœ¬è¬›ç¾©ã®ç‹¬è‡ªæ€§** â€” æ¾å°¾ç ”ã¨ã®å·®åˆ¥åŒ–:

| é …ç›® | æ¾å°¾ç ”ï¼ˆ2026Springï¼‰ | æœ¬è¬›ç¾©ï¼ˆä¸Šä½äº’æ›ï¼‰ |
|:-----|:--------------------|:------------------|
| ç†è«– | FID/ISã®ç´¹ä»‹ | **æ•°å¼å®Œå…¨å°å‡º** + çµ±ä¸€ç†è«– |
| å®Ÿè£… | PyTorchå®Ÿè£… | **Juliaçµ±è¨ˆåˆ†æ + Rust Criterion** |
| æœ€æ–° | FIDä¸­å¿ƒ | **CMMD/FLD+ (2024)** + çµ±è¨ˆæ¤œå®šçµ±åˆ |
| è©•ä¾¡ | ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®— | **è‡ªå‹•ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³** |

### 2.3 å­¦ç¿’æˆ¦ç•¥ â€” 3ã¤ã®ãƒ¬ãƒ™ãƒ«

**ãƒ¬ãƒ™ãƒ«1: ä½¿ãˆã‚‹** (Zone 0-2, 4-5)
- FID/IS/LPIPSã‚’è¨ˆç®—ã§ãã‚‹
- æ—¢å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆ`torch-fidelity`, `lpips`ï¼‰ã‚’ä½¿ç”¨
- ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ„å‘³ã‚’ç†è§£

**ãƒ¬ãƒ™ãƒ«2: ç†è§£ã—ã¦ã„ã‚‹** (Zone 3, 6)
- å„æŒ‡æ¨™ã®æ•°å¼ã‚’å®Œå…¨å°å‡ºã§ãã‚‹
- ä»®å®šã¨é™ç•Œã‚’èª¬æ˜ã§ãã‚‹
- é©åˆ‡ãªæŒ‡æ¨™ã‚’é¸æŠã§ãã‚‹

**ãƒ¬ãƒ™ãƒ«3: è¨­è¨ˆã§ãã‚‹** (Zone 4-7, æ¼”ç¿’)
- è‡ªå‹•è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã§ãã‚‹
- çµ±è¨ˆæ¤œå®šã¨çµ±åˆã§ãã‚‹
- äººé–“è©•ä¾¡ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’è¨­è¨ˆã§ãã‚‹

```mermaid
graph TD
    A[Level 1: ä½¿ãˆã‚‹] --> B[Level 2: ç†è§£]
    B --> C[Level 3: è¨­è¨ˆ]
    A --> D["Zone 0-2, 4-5<br/>ãƒ©ã‚¤ãƒ–ãƒ©ãƒªä½¿ç”¨"]
    B --> E["Zone 3, 6<br/>æ•°å¼å°å‡º"]
    C --> F["Zone 4-7<br/>ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰"]
    style C fill:#c8e6c9
```

:::message
**é€²æ—: 20% å®Œäº†** è©•ä¾¡ã®å›°é›£ã•ã‚’ç†è§£ã—ãŸã€‚ã“ã“ã‹ã‚‰æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ã¸ã€‚FID/IS/LPIPS/MMDã®å®Œå…¨å°å‡ºã«æŒ‘ã‚€ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” è©•ä¾¡æŒ‡æ¨™ã®å®Œå…¨ç†è«–

### 3.1 å‰æçŸ¥è­˜ã®ç¢ºèª

æœ¬ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ä½¿ã†æ•°å­¦ï¼ˆCourse Iã§å­¦ç¿’æ¸ˆã¿ï¼‰:

| æ¦‚å¿µ | åˆå‡º | æœ¬è¬›ç¾©ã§ã®å½¹å‰² |
|:-----|:-----|:-------------|
| **ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ** | ç¬¬4å› | FIDã®ã‚¬ã‚¦ã‚¹ä»®å®š |
| **KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹** | ç¬¬6å› | ISã®å®šç¾© |
| **ãƒ•ãƒ¬ã‚·ã‚§è·é›¢** | ç¬¬5å› | FIDã®è·é›¢å®šç¾© |
| **è¡Œåˆ—å¹³æ–¹æ ¹** | ç¬¬2-3å› | FIDã®å…±åˆ†æ•£é … |
| **ã‚«ãƒ¼ãƒãƒ«æ³•** | ç¬¬6å› | MMDã®RKHS |
| **æœŸå¾…å€¤ãƒ»åˆ†æ•£** | ç¬¬4å› | çµ±è¨ˆé‡ã®è¨ˆç®— |

### 3.2 FID (FrÃ©chet Inception Distance) å®Œå…¨å°å‡º

#### 3.2.1 ãƒ•ãƒ¬ã‚·ã‚§è·é›¢ã®å®šç¾©

**å•é¡Œè¨­å®š**: 2ã¤ã®ç¢ºç‡åˆ†å¸ƒ $P_r$ï¼ˆçœŸç”»åƒï¼‰, $P_g$ï¼ˆç”Ÿæˆç”»åƒï¼‰ã®è·é›¢ã‚’æ¸¬ã‚ŠãŸã„ã€‚

**ã‚¢ã‚¤ãƒ‡ã‚¢**: ä¸¡åˆ†å¸ƒã‚’ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ $\mathcal{N}(\mu, \Sigma)$ ã§è¿‘ä¼¼ã—ã€2ã¤ã®ã‚¬ã‚¦ã‚¹é–“ã®è·é›¢ã‚’æ¸¬ã‚‹ã€‚

**å®šç¾©** (FrÃ©chet distance between two Gaussians):

$$
d_F^2(\mathcal{N}(\mu_1, \Sigma_1), \mathcal{N}(\mu_2, \Sigma_2)) = \|\mu_1 - \mu_2\|_2^2 + \text{Tr}(\Sigma_1 + \Sigma_2 - 2(\Sigma_1 \Sigma_2)^{1/2})
$$

**å„é …ã®æ„å‘³**:
- $\|\mu_1 - \mu_2\|^2$: å¹³å‡ãƒ™ã‚¯ãƒˆãƒ«ã®ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã®äºŒä¹— â†’ åˆ†å¸ƒã®ä¸­å¿ƒãŒã©ã‚Œã ã‘ãšã‚Œã¦ã„ã‚‹ã‹
- $\text{Tr}(\Sigma_1 + \Sigma_2 - 2\sqrt{\Sigma_1 \Sigma_2})$: å…±åˆ†æ•£è¡Œåˆ—ã®å·® â†’ åˆ†å¸ƒã®åºƒãŒã‚Šæ–¹ãŒã©ã‚Œã ã‘ç•°ãªã‚‹ã‹

#### 3.2.2 ãªãœã“ã®å¼ãªã®ã‹ â€” 2-Wassersteinè·é›¢ã¨ã®é–¢ä¿‚

ãƒ•ãƒ¬ã‚·ã‚§è·é›¢ã¯ã€**2-Wassersteinè·é›¢** $W_2$ ã®ã‚¬ã‚¦ã‚¹åˆ†å¸ƒé–“ã§ã®é–‰å½¢å¼è§£ã§ã‚ã‚‹ã€‚

**Wassersteinè·é›¢ã®å®šç¾©** (ç¬¬13å›ã§å­¦ç¿’):

$$
W_2^2(P, Q) = \inf_{\gamma \in \Gamma(P,Q)} \mathbb{E}_{(x,y) \sim \gamma}[\|x - y\|^2]
$$

ã“ã“ã§ $\Gamma(P,Q)$ ã¯ $P$ ã¨ $Q$ ã‚’ãƒãƒ¼ã‚¸ãƒŠãƒ«ã«æŒã¤çµåˆåˆ†å¸ƒã®é›†åˆï¼ˆè¼¸é€è¨ˆç”»ï¼‰ã€‚

**å®šç†** (Dowson & Landau 1982): $P = \mathcal{N}(\mu_1, \Sigma_1)$, $Q = \mathcal{N}(\mu_2, \Sigma_2)$ ã®ã¨ãã€

$$
W_2^2(P, Q) = \|\mu_1 - \mu_2\|^2 + \text{Tr}(\Sigma_1 + \Sigma_2 - 2(\Sigma_1 \Sigma_2)^{1/2})
$$

**è¨¼æ˜ã®ã‚¹ã‚±ãƒƒãƒ** (å®Œå…¨è¨¼æ˜ã¯ [Recalled, not fully derived â€” verify]):

1. ã‚¬ã‚¦ã‚¹åˆ†å¸ƒé–“ã®æœ€é©è¼¸é€ã¯**ç·šå½¢å†™åƒ** $T(x) = Ax + b$ ã§é”æˆã•ã‚Œã‚‹ã€‚
2. $P$-almost surely ã« $T_\#P = Q$ ã‚’æº€ãŸã™ $T$ ã‚’æ±‚ã‚ã‚‹ã€‚
3. $T$ ã®å½¢ã‚’æ±‚ã‚ã‚‹ã¨ã€$A = \Sigma_1^{-1/2}(\Sigma_1^{1/2}\Sigma_2\Sigma_1^{1/2})^{1/2}\Sigma_1^{-1/2}$, $b = \mu_2 - A\mu_1$ã€‚
4. è¼¸é€ã‚³ã‚¹ãƒˆ $\mathbb{E}[\|x - T(x)\|^2]$ ã‚’è¨ˆç®—ã™ã‚‹ã¨ä¸Šå¼ã‚’å¾—ã‚‹ã€‚

:::details ã‚¬ã‚¦ã‚¹åˆ†å¸ƒé–“Wassersteinè·é›¢ã®è©³ç´°å°å‡ºï¼ˆç™ºå±•ï¼‰

**Step 1**: æœ€é©è¼¸é€ãƒãƒƒãƒ— $T$ ã®å½¢ã‚’ä»®å®šã€‚

ç·šå½¢å†™åƒ $T(x) = Ax + b$ ã‚’è€ƒãˆã‚‹ã€‚$T_\#\mathcal{N}(\mu_1, \Sigma_1) = \mathcal{N}(\mu_2, \Sigma_2)$ ã¨ãªã‚‹æ¡ä»¶:
- å¹³å‡: $A\mu_1 + b = \mu_2$ â†’ $b = \mu_2 - A\mu_1$
- å…±åˆ†æ•£: $A\Sigma_1 A^\top = \Sigma_2$

**Step 2**: $A$ ã®é¸æŠã€‚

$A\Sigma_1 A^\top = \Sigma_2$ ã‚’æº€ãŸã™ $A$ ã¯ä¸€æ„ã§ã¯ãªã„ã€‚Monge-Kantorovichç†è«–ã‚ˆã‚Šã€æœ€é©ãª $A$ ã¯:

$$
A = \Sigma_1^{-1/2}(\Sigma_1^{1/2}\Sigma_2\Sigma_1^{1/2})^{1/2}\Sigma_1^{-1/2}
$$

**Step 3**: è¼¸é€ã‚³ã‚¹ãƒˆã®è¨ˆç®—ã€‚

$$
\begin{aligned}
W_2^2 &= \mathbb{E}_{x \sim P}[\|x - T(x)\|^2] \\
&= \mathbb{E}[\|x - Ax - b\|^2] \\
&= \mathbb{E}[\|(I - A)x - b\|^2] \\
&= \mathbb{E}[\|(I - A)(x - \mu_1) + (I - A)\mu_1 - b\|^2]
\end{aligned}
$$

$b = \mu_2 - A\mu_1$ ã‚ˆã‚Š $(I - A)\mu_1 - b = \mu_1 - \mu_2$ã€‚

$$
W_2^2 = \text{Tr}((I - A)\Sigma_1(I - A)^\top) + \|\mu_1 - \mu_2\|^2
$$

$(I - A)\Sigma_1(I - A)^\top$ ã‚’å±•é–‹ã—ã€$A\Sigma_1 A^\top = \Sigma_2$ ã‚’ä»£å…¥:

$$
\begin{aligned}
\text{Tr}((I - A)\Sigma_1(I - A)^\top) &= \text{Tr}(\Sigma_1 - A\Sigma_1 - \Sigma_1 A^\top + A\Sigma_1 A^\top) \\
&= \text{Tr}(\Sigma_1) + \text{Tr}(\Sigma_2) - 2\text{Tr}(A\Sigma_1)
\end{aligned}
$$

$A$ ã®å½¢ã‚’ä»£å…¥ã—ã€$\text{Tr}(A\Sigma_1) = \text{Tr}((\Sigma_1 \Sigma_2)^{1/2})$ ã‚’ç¤ºã›ã‚‹ï¼ˆç·šå½¢ä»£æ•°ã®ãƒˆãƒªãƒƒã‚¯ï¼‰:

$$
W_2^2 = \|\mu_1 - \mu_2\|^2 + \text{Tr}(\Sigma_1 + \Sigma_2 - 2(\Sigma_1\Sigma_2)^{1/2})
$$

:::

#### 3.2.3 è¡Œåˆ—å¹³æ–¹æ ¹ $(\Sigma_1\Sigma_2)^{1/2}$ ã®è¨ˆç®—

**å•é¡Œ**: 2ã¤ã®æ­£å®šå€¤è¡Œåˆ— $\Sigma_1, \Sigma_2$ ã®ç© $\Sigma_1\Sigma_2$ ã®å¹³æ–¹æ ¹ã‚’è¨ˆç®—ã—ãŸã„ã€‚

**æ³¨æ„**: $\Sigma_1\Sigma_2$ ã¯ä¸€èˆ¬ã«å¯¾ç§°è¡Œåˆ—ã§ã¯ãªã„ â†’ å›ºæœ‰å€¤åˆ†è§£ãŒéå¯¾ç§°ã€‚

**è¨ˆç®—æ–¹æ³•**: å›ºæœ‰å€¤åˆ†è§£ã‚’ä½¿ã†ã€‚

$$
\Sigma_1\Sigma_2 = V\Lambda V^{-1}
$$

ã“ã“ã§ $V$ ã¯å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«è¡Œåˆ—ã€$\Lambda = \text{diag}(\lambda_1, \ldots, \lambda_d)$ ã¯å›ºæœ‰å€¤ã®å¯¾è§’è¡Œåˆ—ã€‚

$$
(\Sigma_1\Sigma_2)^{1/2} = V\Lambda^{1/2}V^{-1} = V\text{diag}(\sqrt{\lambda_1}, \ldots, \sqrt{\lambda_d})V^{-1}
$$

**å®Ÿè£…ä¸Šã®æ³¨æ„**:
1. $\Sigma_1, \Sigma_2$ ãŒæ­£å®šå€¤ã§ã‚‚ã€$\Sigma_1\Sigma_2$ ã¯æ­£å®šå€¤ã¨ã¯é™ã‚‰ãªã„ â†’ å›ºæœ‰å€¤ãŒè² ã«ãªã‚‹å¯èƒ½æ€§ã€‚
2. æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã€$\lambda_i < 0$ ã®å ´åˆã¯ $|\lambda_i|$ ã‚’ä½¿ã†ï¼ˆor small positive value ã§ clippingï¼‰ã€‚

```julia
# Matrix square root via eigen decomposition
function matrix_sqrt(A::Matrix{Float64})
    eig = eigen(A)
    # Handle numerical errors: negative eigenvalues â†’ abs
    Î»_sqrt = sqrt.(Complex.(eig.values))  # complex sqrt for negative Î»
    return real(eig.vectors * Diagonal(Î»_sqrt) * inv(eig.vectors))
end

# Test
Î£1 = [2.0 0.5; 0.5 1.5]
Î£2 = [1.8 0.3; 0.3 1.2]
prod = Î£1 * Î£2
sqrt_prod = matrix_sqrt(prod)
println("(Î£1*Î£2)^{1/2} computed")
println("Verification: (sqrt)^2 â‰ˆ original? ", isapprox(sqrt_prod^2, prod, atol=1e-6))
```

#### 3.2.4 FIDã®å®Ÿè£…ã¨Inceptionç‰¹å¾´æŠ½å‡º

**FIDè¨ˆç®—ã®å…¨ä½“ãƒ•ãƒ­ãƒ¼**:

1. **Inception-v3ã§ç‰¹å¾´æŠ½å‡º**: ç”»åƒ â†’ 2048æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ï¼ˆpre-poolå±¤ï¼‰
2. **çµ±è¨ˆé‡è¨ˆç®—**: $\mu_r, \Sigma_r$ (çœŸç”»åƒ), $\mu_g, \Sigma_g$ (ç”Ÿæˆç”»åƒ)
3. **ãƒ•ãƒ¬ã‚·ã‚§è·é›¢è¨ˆç®—**: ä¸Šè¨˜ã®å¼

```julia
# FID implementation (with dummy Inception features)
using LinearAlgebra, Statistics

function extract_inception_features(images::Vector{Matrix{Float64}})
    # Real impl: load pre-trained Inception-v3, extract pool3 layer
    # Here: simulate with random projection
    n = length(images)
    d_feat = 2048  # Inception pool3 dimension
    return randn(n, d_feat)
end

function compute_statistics(features::Matrix{Float64})
    # features: (n_samples, d_features)
    Î¼ = vec(mean(features, dims=1))  # (d_features,)
    Î£ = cov(features)  # (d_features, d_features)
    return Î¼, Î£
end

function frechet_distance(Î¼1::Vector{Float64}, Î£1::Matrix{Float64},
                           Î¼2::Vector{Float64}, Î£2::Matrix{Float64})
    # Mean difference
    diff = Î¼1 .- Î¼2
    mean_term = sum(diff.^2)

    # Covariance term: Tr(Î£1 + Î£2 - 2(Î£1*Î£2)^{1/2})
    # Matrix square root
    product = Î£1 * Î£2
    eig = eigen(product)
    # Use abs for numerical stability
    sqrt_eig = sqrt.(abs.(eig.values))
    sqrt_product = real(eig.vectors * Diagonal(sqrt_eig) * eig.vectors')

    trace_term = tr(Î£1) + tr(Î£2) - 2 * tr(sqrt_product)

    return mean_term + trace_term
end

function fid_score(real_images::Vector{Matrix{Float64}},
                    gen_images::Vector{Matrix{Float64}})
    # Extract features
    feats_real = extract_inception_features(real_images)
    feats_gen = extract_inception_features(gen_images)

    # Compute statistics
    Î¼_r, Î£_r = compute_statistics(feats_real)
    Î¼_g, Î£_g = compute_statistics(feats_gen)

    # Compute FrÃ©chet distance
    return frechet_distance(Î¼_r, Î£_r, Î¼_g, Î£_g)
end

# Test with synthetic data
n_samples = 100
real_imgs = [randn(64, 64) for _ in 1:n_samples]
gen_imgs = [randn(64, 64) for _ in 1:n_samples]  # random images

fid = fid_score(real_imgs, gen_imgs)
println("FID: $(round(fid, digits=2))")
println("Expected range: 0 (identical) to ~400 (completely different)")
```

**æ•°å€¤æ¤œè¨¼**: $\mu_1 = \mu_2$, $\Sigma_1 = \Sigma_2$ ã®ã¨ã FID = 0 ã«ãªã‚‹ã‹ç¢ºèªã€‚

```julia
# Sanity check: identical distributions â†’ FID = 0
Î¼_test = randn(10)
Î£_test = randn(10, 10); Î£_test = Î£_test * Î£_test' + I  # ensure PD
fid_identical = frechet_distance(Î¼_test, Î£_test, Î¼_test, Î£_test)
println("FID (identical distributions): $(round(fid_identical, digits=10))")
# Should be ~0 (machine precision errors ~1e-10)
```

#### 3.2.5 FIDã®é™ç•Œã¨å¯¾ç­–

**é™ç•Œ1: ã‚¬ã‚¦ã‚¹æ€§ã®ä»®å®š**

å®Ÿéš›ã®ç‰¹å¾´åˆ†å¸ƒã¯**å¤šå³°æ€§**ã‚’æŒã¤ â†’ å˜ä¸€ã‚¬ã‚¦ã‚¹ã§è¿‘ä¼¼ã™ã‚‹ã¨æƒ…å ±ã‚’å¤±ã†ã€‚

**å¯¾ç­–**:
- Gaussian Mixture Model (GMM) ã§è¿‘ä¼¼ â†’ è¨ˆç®—è¤‡é›‘åº¦å¢—
- MMDãƒ™ãƒ¼ã‚¹ã®æŒ‡æ¨™ï¼ˆCMMD [^5]ï¼‰â†’ ä»®å®šãªã—

**é™ç•Œ2: ã‚µãƒ³ãƒ—ãƒ«æ•°ä¾å­˜**

$\Sigma$ ã®æ¨å®šã«ã¯ $O(d^2)$ ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå¿…è¦ï¼ˆ$d$ = ç‰¹å¾´æ¬¡å…ƒï¼‰ã€‚Inceptionç‰¹å¾´ã¯2048æ¬¡å…ƒ â†’ ç†è«–ä¸Š $2048^2 \approx 4M$ ã‚µãƒ³ãƒ—ãƒ«å¿…è¦ã€‚

å®Ÿéš›ã¯2000-5000ã‚µãƒ³ãƒ—ãƒ«ã§å®‰å®šã™ã‚‹ãŒã€å°‘ã‚µãƒ³ãƒ—ãƒ«ã§ã¯ä¸å®‰å®šã€‚

**å¯¾ç­–**:
- ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’å¢—ã‚„ã™
- æ¬¡å…ƒå‰Šæ¸›ï¼ˆPCAï¼‰â†’ æƒ…å ±æå¤±
- FLD+ [^7]: Normalizing Flowã§200ã‚µãƒ³ãƒ—ãƒ«ã§ã‚‚å®‰å®š

**é™ç•Œ3: ImageNetãƒã‚¤ã‚¢ã‚¹**

Inception-v3ã¯ImageNetã§è¨“ç·´ â†’ è‡ªç„¶ç”»åƒä»¥å¤–ã§ä¸é©åˆ‡ï¼ˆåŒ»ç™‚ç”»åƒã€è¡›æ˜Ÿç”»åƒã€ã‚¢ãƒ¼ãƒˆï¼‰ã€‚

**å¯¾ç­–**:
- ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ã®ç‰¹å¾´æŠ½å‡ºå™¨ï¼ˆä¾‹: åŒ»ç™‚ç”»åƒç”¨ResNetï¼‰
- CLIPåŸ‹ã‚è¾¼ã¿ï¼ˆCMMD [^5]ï¼‰â†’ ã‚ˆã‚Šæ±ç”¨çš„

:::message alert
**æ•°å¼ä¿®è¡Œã®ã‚³ãƒ„**: FIDã®å¼ã‚’**æš—è¨˜ã™ã‚‹ãªã€‚å°å‡ºã—ã‚**ã€‚Wassersteinè·é›¢ â†’ ã‚¬ã‚¦ã‚¹é–“ã®é–‰å½¢å¼ â†’ è¡Œåˆ—å¹³æ–¹æ ¹ã®è¨ˆç®—ã€ã¨ã„ã†æµã‚Œã‚’è¿½ãˆã°ã€å¼ã®æ„å‘³ãŒç†è§£ã§ãã‚‹ã€‚
:::

### 3.3 IS (Inception Score) å®Œå…¨å°å‡º

#### 3.3.1 å®šç¾©ã¨å‹•æ©Ÿ

**Inception Score** [^2] ã¯ã€ç”Ÿæˆç”»åƒã®å“è³ªã¨å¤šæ§˜æ€§ã‚’å˜ä¸€ã‚¹ã‚³ã‚¢ã§è©•ä¾¡ã™ã‚‹æŒ‡æ¨™ã€‚

**ã‚¢ã‚¤ãƒ‡ã‚¢**:
1. **å“è³ª**: å„ç”Ÿæˆç”»åƒ $x$ ã‚’ Inception-v3 ã§åˆ†é¡ â†’ äºˆæ¸¬åˆ†å¸ƒ $p(y|x)$ ãŒã‚·ãƒ£ãƒ¼ãƒ—ï¼ˆé«˜confidenceï¼‰ãªã‚‰é«˜å“è³ª
2. **å¤šæ§˜æ€§**: å…¨ç”»åƒã®äºˆæ¸¬åˆ†å¸ƒã®å¹³å‡ $p(y) = \mathbb{E}_x[p(y|x)]$ ãŒå‡ä¸€ï¼ˆå…¨ã‚¯ãƒ©ã‚¹ã‚’ã‚«ãƒãƒ¼ï¼‰ãªã‚‰å¤šæ§˜

**å®šç¾©**:

$$
\text{IS}(G) = \exp\left(\mathbb{E}_{x \sim p_g}[\text{KL}(p(y|x) \| p(y))]\right)
$$

ã“ã“ã§:
- $p_g$: ç”Ÿæˆãƒ¢ãƒ‡ãƒ« $G$ ã®åˆ†å¸ƒ
- $p(y|x)$: ç”»åƒ $x$ ã«å¯¾ã™ã‚‹Inception-v3ã®äºˆæ¸¬åˆ†å¸ƒï¼ˆsoftmax outputï¼‰
- $p(y) = \mathbb{E}_{x \sim p_g}[p(y|x)]$: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã§ã®äºˆæ¸¬åˆ†å¸ƒã®å¹³å‡ï¼ˆå‘¨è¾ºåˆ†å¸ƒï¼‰
- $\text{KL}(p(y|x) \| p(y))$: æ¡ä»¶ä»˜ãåˆ†å¸ƒã¨å‘¨è¾ºåˆ†å¸ƒã®KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹

#### 3.3.2 KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®å¾©ç¿’

**å®šç¾©** (ç¬¬6å›ã§å­¦ç¿’):

$$
\text{KL}(P \| Q) = \sum_y P(y) \log\frac{P(y)}{Q(y)} = \mathbb{E}_{y \sim P}\left[\log\frac{P(y)}{Q(y)}\right]
$$

**æ€§è³ª**:
- $\text{KL}(P \| Q) \geq 0$ï¼ˆéè² æ€§ï¼‰
- $\text{KL}(P \| Q) = 0 \iff P = Q$
- éå¯¾ç§°: $\text{KL}(P \| Q) \neq \text{KL}(Q \| P)$

#### 3.3.3 ISãŒé«˜ã„ã¨ã = è‰¯ã„ãƒ¢ãƒ‡ãƒ«ï¼Ÿ

**ISãŒé«˜ã„ã‚±ãƒ¼ã‚¹**:

1. $p(y|x)$ ãŒã‚·ãƒ£ãƒ¼ãƒ—ï¼ˆpeakyï¼‰â†’ $\text{KL}(p(y|x) \| p(y))$ ãŒå¤§ãã„
2. $p(y)$ ãŒå‡ä¸€ï¼ˆuniformï¼‰â†’ å¤šæ§˜ãªã‚¯ãƒ©ã‚¹ã‚’ã‚«ãƒãƒ¼

**å…·ä½“ä¾‹**:

- **æœ€è‰¯ã®ã‚±ãƒ¼ã‚¹**: $p(y|x) = \delta(y - y^*)$ï¼ˆ1ã¤ã®ã‚¯ãƒ©ã‚¹ã«ç¢ºç‡1ï¼‰ã‹ã¤ $p(y) = \text{Uniform}(1/K)$ï¼ˆå…¨ã‚¯ãƒ©ã‚¹å‡ç­‰ï¼‰
  - $\text{KL}(p(y|x) \| p(y)) = \log K$ ï¼ˆæœ€å¤§ï¼‰
  - $\text{IS} = \exp(\log K) = K$ ï¼ˆã‚¯ãƒ©ã‚¹æ•°ï¼‰

- **æœ€æ‚ªã®ã‚±ãƒ¼ã‚¹**: $p(y|x) = p(y)$ ï¼ˆæ¡ä»¶ä»˜ã = å‘¨è¾ºï¼‰
  - $\text{KL}(p(y|x) \| p(y)) = 0$
  - $\text{IS} = \exp(0) = 1$

**ã‚¹ã‚³ã‚¢ã®ç¯„å›²**:

$$
\text{IS} \in [1, K]
$$

ã“ã“ã§ $K$ ã¯Inceptionã®åˆ†é¡ã‚¯ãƒ©ã‚¹æ•°ï¼ˆImageNetã§ã¯1000ï¼‰ã€‚

#### 3.3.4 æ•°å¼â†”ã‚³ãƒ¼ãƒ‰ 1:1å¯¾å¿œ

```julia
# Inception Score implementation
using Statistics

function inception_score(images::Vector{Matrix{Float64}}, n_splits::Int=10)
    # Step 1: Inception-v3 classification â†’ p(y|x) for each image
    # Real impl: forward pass through Inception-v3
    # Here: random softmax for demo
    n_samples = length(images)
    n_classes = 1000  # ImageNet classes

    # Simulate Inception predictions
    logits = randn(n_samples, n_classes)
    p_yx = exp.(logits) ./ sum(exp.(logits), dims=2)  # (n_samples, n_classes)

    # Step 2: Compute p(y) = E_x[p(y|x)] (marginal distribution)
    p_y = vec(mean(p_yx, dims=1))  # (n_classes,)

    # Step 3: Compute KL(p(y|x) || p(y)) for each image
    kl_divs = zeros(n_samples)
    for i in 1:n_samples
        for j in 1:n_classes
            if p_yx[i,j] > 1e-10 && p_y[j] > 1e-10  # avoid log(0)
                kl_divs[i] += p_yx[i,j] * log(p_yx[i,j] / p_y[j])
            end
        end
    end

    # Step 4: IS = exp(E[KL])
    mean_kl = mean(kl_divs)
    is_score = exp(mean_kl)

    # Optional: compute IS over multiple splits for stability
    # (split dataset into n_splits parts, compute IS for each, average)
    # Here: simplified version with single split

    return is_score, mean_kl
end

# Test
test_imgs = [randn(64, 64) for _ in 1:1000]
is, kl = inception_score(test_imgs)
println("Inception Score: $(round(is, digits=2))")
println("Mean KL: $(round(kl, digits=4))")
println("Expected range: [1.0, 1000.0] for ImageNet")
```

**æ•°å€¤æ¤œè¨¼**: æ¥µç«¯ãªã‚±ãƒ¼ã‚¹ã§ç¢ºèªã€‚

```julia
# Case 1: perfect quality + diversity (maximum IS)
# p(y|x) = one-hot, p(y) = uniform â†’ IS = K
n = 1000
k = 100  # simplified: 100 classes
p_yx_perfect = zeros(n, k)
for i in 1:n
    p_yx_perfect[i, mod(i-1, k)+1] = 1.0  # one-hot, cyclic
end
p_y_perfect = vec(mean(p_yx_perfect, dims=1))  # should be uniform

kl_perfect = zeros(n)
for i in 1:n, j in 1:k
    if p_yx_perfect[i,j] > 0 && p_y_perfect[j] > 0
        kl_perfect[i] += p_yx_perfect[i,j] * log(p_yx_perfect[i,j] / p_y_perfect[j])
    end
end
is_perfect = exp(mean(kl_perfect))
println("IS (perfect case): $(round(is_perfect, digits=2)) â‰ˆ $k")

# Case 2: p(y|x) = p(y) (worst case) â†’ IS = 1
p_yx_worst = repeat(p_y_perfect', n, 1)  # all images have same p(y|x) = p(y)
kl_worst = zeros(n)
for i in 1:n, j in 1:k
    if p_yx_worst[i,j] > 0
        kl_worst[i] += p_yx_worst[i,j] * log(p_yx_worst[i,j] / p_y_perfect[j])
    end
end
is_worst = exp(mean(kl_worst))
println("IS (worst case): $(round(is_worst, digits=4)) â‰ˆ 1.0")
```

#### 3.3.5 ISã®é™ç•Œ

**é™ç•Œ1: ImageNetåˆ†é¡ã¸ã®ä¾å­˜**

Inception-v3ã®åˆ†é¡ç²¾åº¦ãŒé«˜ã„ â‰  ç”»åƒå“è³ªãŒé«˜ã„ã€‚

**ä¾‹**: çŠ¬ã®ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã€‚å…¨ã¦åŒã˜çŠ¬ç¨®ï¼ˆmode collapseï¼‰ã§ã‚‚ã€InceptionãŒã€ŒçŠ¬ã€ã¨é«˜ç¢ºä¿¡ã§åˆ†é¡ã™ã‚Œã°ISã¯é«˜ã„ã€‚

**é™ç•Œ2: KLç™ºæ•£ã®è§£é‡ˆå›°é›£**

$\text{KL}(p(y|x) \| p(y))$ ãŒå¤§ãã„ â†’ è‰¯ã„ï¼Ÿä½•ã¨æ¯”è¼ƒã—ã¦ã„ã‚‹ã®ã‹ä¸æ˜ç­ã€‚

**é™ç•Œ3: ã‚¹ã‚³ã‚¢ã®çµ¶å¯¾å€¤ã«æ„å‘³ãŒãªã„**

IS = 30 vs 35 ã®å·®ã¯å®Ÿè³ªçš„ã«ã©ã‚Œãã‚‰ã„ï¼Ÿå®šé‡çš„ãªè§£é‡ˆãŒå›°é›£ã€‚

**å¯¾ç­–**:
- FIDã¨ä½µç”¨ â†’ ç›¸è£œçš„ãªæƒ…å ±
- Precision-Recall â†’ å“è³ªã¨å¤šæ§˜æ€§ã‚’åˆ†é›¢æ¸¬å®š
- äººé–“è©•ä¾¡ â†’ æœ€çµ‚åˆ¤æ–­

:::message
**ãƒœã‚¹æˆ¦ã¸ã®æº–å‚™ 30% å®Œäº†**: FIDã¨ISã®æ•°å¼ã‚’å®Œå…¨å°å‡ºã—ãŸã€‚ã“ã“ã‹ã‚‰LPIPS, Precision-Recall, MMD/CMMDã‚’å°å‡ºã™ã‚‹ã€‚
:::

### 3.4 LPIPS (Learned Perceptual Image Patch Similarity) å®Œå…¨å°å‡º

#### 3.4.1 å‹•æ©Ÿã¨è¨­è¨ˆæ€æƒ³

**å•é¡Œ**: ãƒ”ã‚¯ã‚»ãƒ«å˜ä½ã®è·é›¢ï¼ˆL2, SSIMï¼‰ã¯äººé–“ã®çŸ¥è¦šã¨ç›¸é–¢ãŒä½ã„ã€‚

**ä¾‹**:
- ç”»åƒAã‚’1ãƒ”ã‚¯ã‚»ãƒ«ãšã‚‰ã™ â†’ L2è·é›¢ã¯å¤§ãã„ãŒã€äººé–“ã«ã¯åŒã˜ã«è¦‹ãˆã‚‹
- ç”»åƒBã®è‰²ã‚’å°‘ã—å¤‰ãˆã‚‹ â†’ L2è·é›¢ã¯å°ã•ã„ãŒã€äººé–“ã«ã¯é•ã£ã¦è¦‹ãˆã‚‹

**ã‚¢ã‚¤ãƒ‡ã‚¢**: æ·±å±¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ç‰¹å¾´ç©ºé–“ã§è·é›¢ã‚’æ¸¬ã‚‹ â†’ äººé–“ã®çŸ¥è¦šã«è¿‘ã„ [^3]ã€‚

#### 3.4.2 å®šç¾©

**LPIPSè·é›¢** (Zhang et al. 2018 [^3]):

$$
d_{\text{LPIPS}}(x, x_0) = \sum_{\ell} w_\ell \frac{1}{H_\ell W_\ell} \sum_{h,w} \|f_\ell^h(x) - f_\ell^h(x_0)\|_2^2
$$

ã“ã“ã§:
- $f_\ell$: VGG/AlexNetã®ç¬¬$\ell$å±¤ã®ç‰¹å¾´ãƒãƒƒãƒ—ï¼ˆè¤‡æ•°å±¤ã‚’ä½¿ç”¨ï¼‰
- $f_\ell^h$: channel-wise normalizationï¼ˆå„ãƒãƒ£ãƒãƒ«ã‚’æ­£è¦åŒ–ï¼‰
- $w_\ell$: å±¤ã”ã¨ã®é‡ã¿ï¼ˆå­¦ç¿’ã•ã‚Œã‚‹ï¼‰
- $H_\ell, W_\ell$: ç¬¬$\ell$å±¤ã®ç‰¹å¾´ãƒãƒƒãƒ—ã®é«˜ã•ãƒ»å¹…

**ç›´æ„Ÿ**:
- æµ…ã„å±¤ï¼ˆedge, textureï¼‰+ æ·±ã„å±¤ï¼ˆsemantic contentï¼‰ã®ä¸¡æ–¹ã‚’ä½¿ã†
- å¤šå±¤ã®ç‰¹å¾´ã‚’ weighted sum â†’ äººé–“ã®çŸ¥è¦šã‚’è¿‘ä¼¼

#### 3.4.3 Channel-wise Normalization ã®æ„å‘³

**ãªãœæ­£è¦åŒ–ã™ã‚‹ã®ã‹ï¼Ÿ**

æ·±å±¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ç‰¹å¾´ã¯ã€ãƒãƒ£ãƒãƒ«ã”ã¨ã«å¤§ãã•ãŒç•°ãªã‚‹ï¼ˆä¾‹: ãƒãƒ£ãƒãƒ«1ã¯å¹³å‡100, ãƒãƒ£ãƒãƒ«2ã¯å¹³å‡0.1ï¼‰ã€‚ãã®ã¾ã¾è·é›¢ã‚’æ¸¬ã‚‹ã¨ã€å¤§ãã„ãƒãƒ£ãƒãƒ«ãŒæ”¯é…çš„ã«ãªã‚‹ã€‚

**æ­£è¦åŒ–**:

$$
f_\ell^h(x) = \frac{f_\ell(x) - \mu_\ell}{\sigma_\ell}
$$

ã“ã“ã§ $\mu_\ell, \sigma_\ell$ ã¯ãƒãƒ£ãƒãƒ«ã”ã¨ã®å¹³å‡ãƒ»æ¨™æº–åå·®ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§è¨ˆç®—ï¼‰ã€‚

#### 3.4.4 å®Ÿè£…ã¨æ•°å¼å¯¾å¿œ

```julia
# LPIPS implementation (simplified)
using Statistics

# Dummy VGG feature extractor (real impl: pre-trained VGG-16)
function vgg_features(image::Matrix{Float64})
    # Real impl: extract features from VGG layers: conv1_2, conv2_2, conv3_3, conv4_3, conv5_3
    # Here: simulate with 5 scales Ã— 64 channels
    n_layers = 5
    features = []
    for â„“ in 1:n_layers
        # Simulate feature map: (H_â„“, W_â„“, C_â„“)
        h_size = 64 Ã· (2^(â„“-1))  # decreasing spatial size
        c_size = 64 * (2^(â„“-1))  # increasing channels
        feat = randn(h_size, h_size, c_size)
        push!(features, feat)
    end
    return features
end

function channel_normalize(feat::Array{Float64,3})
    # feat: (H, W, C)
    # Normalize each channel
    H, W, C = size(feat)
    feat_norm = zeros(H, W, C)
    for c in 1:C
        channel = feat[:,:,c]
        Î¼ = mean(channel)
        Ïƒ = std(channel) + 1e-10  # avoid division by zero
        feat_norm[:,:,c] = (channel .- Î¼) ./ Ïƒ
    end
    return feat_norm
end

function lpips(img1::Matrix{Float64}, img2::Matrix{Float64}, weights::Vector{Float64}=[1.0, 1.0, 1.0, 1.0, 1.0])
    # Extract multi-scale features
    feats1 = vgg_features(img1)
    feats2 = vgg_features(img2)

    # Compute distance per layer
    distance = 0.0
    for (â„“, (f1, f2)) in enumerate(zip(feats1, feats2))
        # Channel-wise normalization
        f1_norm = channel_normalize(f1)
        f2_norm = channel_normalize(f2)

        # L2 distance, averaged over spatial dimensions
        diff = f1_norm .- f2_norm
        layer_dist = sum(diff.^2) / (size(f1, 1) * size(f1, 2))

        # Weighted sum
        distance += weights[â„“] * layer_dist
    end

    return sqrt(distance)  # or distance (squared)
end

# Test
img_a = randn(128, 128)
img_b = randn(128, 128)
img_c = img_a .+ 0.05 .* randn(128, 128)  # similar to A

lpips_ab = lpips(img_a, img_b)
lpips_ac = lpips(img_a, img_c)
println("LPIPS(A, B): $(round(lpips_ab, digits=4))")
println("LPIPS(A, C): $(round(lpips_ac, digits=4))")
println("Expected: LPIPS(A, C) < LPIPS(A, B)")
```

#### 3.4.5 LPIPSã¨äººé–“è©•ä¾¡ã®ç›¸é–¢

**Berkeley-Adobe Perceptual Patch Similarity (BAPPS) dataset** [^3]:

- äººé–“ã®çŸ¥è¦šåˆ¤æ–­ vs å„ç¨®è·é›¢æŒ‡æ¨™ã®ç›¸é–¢ã‚’æ¸¬å®š
- LPIPS vs L2 vs SSIM vs æ—¢å­˜æ‰‹æ³•

**çµæœ** [^3]:

| æŒ‡æ¨™ | Pearsonç›¸é–¢ï¼ˆäººé–“è©•ä¾¡ï¼‰ |
|:-----|:----------------------|
| L2 (pixel-wise) | 0.45 |
| SSIM | 0.52 |
| LPIPS (VGG) | **0.78** |
| LPIPS (AlexNet) | **0.80** |

LPIPS ã¯æ—¢å­˜æ‰‹æ³•ã‚’å¤§ããä¸Šå›ã‚‹ã€‚

#### 3.4.6 LPIPSã®é™ç•Œ

**é™ç•Œ1: ãƒšã‚¢wiseæ¯”è¼ƒã®ã¿**

LPIPSã¯2ç”»åƒé–“ã®è·é›¢ â†’ åˆ†å¸ƒå…¨ä½“ã®è©•ä¾¡ã«ã¯ä½¿ãˆãªã„ï¼ˆFID/ISã®è£œå®Œï¼‰ã€‚

**é™ç•Œ2: ç‰¹å¾´æŠ½å‡ºå™¨ã¸ã®ä¾å­˜**

VGG/AlexNetã¯ImageNetã§è¨“ç·´ â†’ ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚¤ã‚¢ã‚¹ã€‚

**å¯¾ç­–**:
- ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ã®ç‰¹å¾´æŠ½å‡ºå™¨ã‚’è¨“ç·´
- è¤‡æ•°ã®ç‰¹å¾´æŠ½å‡ºå™¨ã§ensemble

:::message
**ãƒœã‚¹æˆ¦ã¸ã®æº–å‚™ 50% å®Œäº†**: LPIPSå®Œäº†ã€‚ã“ã“ã‹ã‚‰Precision-Recall, MMD/CMMDã®æ•°å¼ã¸ã€‚
:::

### 3.5 Precision-Recall for Generative Models å®Œå…¨å°å‡º

#### 3.5.1 å‹•æ©Ÿ â€” å“è³ªã¨å¤šæ§˜æ€§ã®åˆ†é›¢

**å•é¡Œ**: FID/ISã¯å“è³ªã¨å¤šæ§˜æ€§ã‚’å˜ä¸€ã‚¹ã‚³ã‚¢ã«é›†ç´„ â†’ ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãŒè¦‹ãˆãªã„ã€‚

**ä¾‹**:
- ãƒ¢ãƒ‡ãƒ«A: é«˜å“è³ªã ãŒå¤šæ§˜æ€§ä½ã„ï¼ˆmode collapseï¼‰
- ãƒ¢ãƒ‡ãƒ«B: å¤šæ§˜æ€§é«˜ã„ãŒã¼ã‚„ã‘ãŸç”»åƒ

FIDã ã‘ã§ã¯ã€ã©ã¡ã‚‰ãŒ"è‰¯ã„"ã‹åˆ¤æ–­ã§ããªã„ã€‚

**Precision-Recall** [^4] ã¯ã€**å“è³ªï¼ˆPrecisionï¼‰ã¨å¤šæ§˜æ€§ï¼ˆRecallï¼‰ã‚’åˆ†é›¢æ¸¬å®š**ã™ã‚‹ã€‚

#### 3.5.2 å®šç¾©ï¼ˆå¤šæ§˜ä½“ãƒ™ãƒ¼ã‚¹ï¼‰

**è¨­å®š**:
- çœŸç”»åƒã®ç‰¹å¾´: $\{f_r^{(i)}\}_{i=1}^{N_r}$ ï¼ˆInceptionç‰¹å¾´ï¼‰
- ç”Ÿæˆç”»åƒã®ç‰¹å¾´: $\{f_g^{(i)}\}_{i=1}^{N_g}$

**å¤šæ§˜ä½“ã®è¿‘ä¼¼**:

å„ã‚µãƒ³ãƒ—ãƒ« $f^{(i)}$ ã®å‘¨ã‚Šã« $k$-NN çƒã‚’æ§‹ç¯‰ â†’ å¤šæ§˜ä½“ã‚’è¿‘ä¼¼ã€‚

$$
\mathcal{M}_r = \bigcup_{i=1}^{N_r} B(f_r^{(i)}, r_k^{(i)})
$$

ã“ã“ã§ $r_k^{(i)}$ ã¯ $f_r^{(i)}$ ã® $k$-æœ€è¿‘å‚ã¾ã§ã®è·é›¢ã€‚

**Precision** (å“è³ª):

$$
\text{Precision} = \frac{1}{N_g} \sum_{i=1}^{N_g} \mathbb{1}[f_g^{(i)} \in \mathcal{M}_r]
$$

ã€Œç”Ÿæˆç”»åƒã®ã†ã¡ã€çœŸç”»åƒã®å¤šæ§˜ä½“ã«å«ã¾ã‚Œã‚‹å‰²åˆã€â†’ å“è³ªãŒé«˜ã„ã»ã©1ã«è¿‘ã„ã€‚

**Recall** (å¤šæ§˜æ€§):

$$
\text{Recall} = \frac{1}{N_r} \sum_{i=1}^{N_r} \mathbb{1}[f_r^{(i)} \in \mathcal{M}_g]
$$

ã€ŒçœŸç”»åƒã®ã†ã¡ã€ç”Ÿæˆç”»åƒã®å¤šæ§˜ä½“ã«å«ã¾ã‚Œã‚‹å‰²åˆã€â†’ å¤šæ§˜æ€§ãŒé«˜ã„ï¼ˆçœŸåˆ†å¸ƒã‚’ã‚«ãƒãƒ¼ï¼‰ã»ã©1ã«è¿‘ã„ã€‚

#### 3.5.3 ç›´æ„Ÿçš„ç†è§£

```mermaid
graph TD
    A[çœŸç”»åƒå¤šæ§˜ä½“ M_r] --> B[Precision<br/>ç”Ÿæˆç”»åƒãŒM_rã«ã©ã‚Œã ã‘å«ã¾ã‚Œã‚‹ã‹]
    C[ç”Ÿæˆç”»åƒå¤šæ§˜ä½“ M_g] --> D[Recall<br/>M_gãŒçœŸç”»åƒã‚’ã©ã‚Œã ã‘ã‚«ãƒãƒ¼ã™ã‚‹ã‹]
    B --> E["é«˜Precision<br/>= é«˜å“è³ª"]
    D --> F["é«˜Recall<br/>= é«˜å¤šæ§˜æ€§"]
    E & F --> G[ç†æƒ³: P=1, R=1]
    style E fill:#ffccbc
    style F fill:#c5e1a5
    style G fill:#b3e5fc
```

**ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**:
- Precisionâ†‘, Recallâ†“: é«˜å“è³ªã ãŒå¤šæ§˜æ€§ä½ã„ï¼ˆmode collapseï¼‰
- Precisionâ†“, Recallâ†‘: å¤šæ§˜æ€§é«˜ã„ãŒå“è³ªä½ã„ï¼ˆã¼ã‚„ã‘ãŸç”»åƒï¼‰
- ç†æƒ³: Precision = Recall = 1

#### 3.5.4 å®Ÿè£…ã¨æ•°å¼å¯¾å¿œ

```julia
# Precision-Recall for generative models
using NearestNeighbors

function precision_recall(feats_real::Matrix{Float64},
                           feats_gen::Matrix{Float64}, k::Int=5)
    # feats: (n_samples, d_features)
    n_real = size(feats_real, 1)
    n_gen = size(feats_gen, 1)

    # Build k-NN trees
    tree_real = KDTree(feats_real')  # NearestNeighbors expects (d, n)
    tree_gen = KDTree(feats_gen')

    # Compute k-th nearest neighbor distances for manifold radius
    # Real manifold: r_k^(i) = distance to k-th NN in real data
    radii_real = zeros(n_real)
    for i in 1:n_real
        idxs, dists = knn(tree_real, feats_real[i,:], k+1)  # k+1 to exclude self
        radii_real[i] = dists[end]  # k-th NN distance
    end

    # Gen manifold
    radii_gen = zeros(n_gen)
    for i in 1:n_gen
        idxs, dists = knn(tree_gen, feats_gen[i,:], k+1)
        radii_gen[i] = dists[end]
    end

    # Precision: fraction of gen samples within real manifold
    precision_count = 0
    for i in 1:n_gen
        # Find nearest real sample
        idxs, dists = knn(tree_real, feats_gen[i,:], 1)
        nearest_idx = idxs[1]
        if dists[1] <= radii_real[nearest_idx]
            precision_count += 1
        end
    end
    precision = precision_count / n_gen

    # Recall: fraction of real samples within gen manifold
    recall_count = 0
    for i in 1:n_real
        idxs, dists = knn(tree_gen, feats_real[i,:], 1)
        nearest_idx = idxs[1]
        if dists[1] <= radii_gen[nearest_idx]
            recall_count += 1
        end
    end
    recall = recall_count / n_real

    return precision, recall
end

# Test with synthetic data
n_real = 200
n_gen = 200
d = 64

# Case 1: high quality, low diversity (mode collapse)
# Gen samples concentrated around a subset of real samples
feats_real_1 = randn(n_real, d)
feats_gen_1 = feats_real_1[1:50,:] .+ 0.1 .* randn(50, d)  # only 50 modes
feats_gen_1 = vcat(feats_gen_1, feats_gen_1[rand(1:50, 150),:])  # replicate to 200

p1, r1 = precision_recall(feats_real_1, feats_gen_1)
println("Case 1 (mode collapse): Precision=$(round(p1, digits=3)), Recall=$(round(r1, digits=3))")
println("Expected: high P, low R")

# Case 2: high diversity, low quality (noisy samples)
feats_gen_2 = feats_real_1 .+ 2.0 .* randn(n_real, d)  # far from real manifold but diverse
p2, r2 = precision_recall(feats_real_1, feats_gen_2)
println("Case 2 (noisy): Precision=$(round(p2, digits=3)), Recall=$(round(r2, digits=3))")
println("Expected: low P, high R (if noise covers broadly)")

# Case 3: ideal (perfect match)
feats_gen_3 = feats_real_1 .+ 0.01 .* randn(n_real, d)  # very close to real
p3, r3 = precision_recall(feats_real_1, feats_gen_3)
println("Case 3 (ideal): Precision=$(round(p3, digits=3)), Recall=$(round(r3, digits=3))")
println("Expected: high P, high R")
```

#### 3.5.5 Precision-Recallã®å¯è¦–åŒ–

**P-Ræ›²ç·š**: ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆä¾‹: temperature, truncationï¼‰ã‚’å¤‰ãˆãªãŒã‚‰Precision-Recallã‚’ãƒ—ãƒ­ãƒƒãƒˆã€‚

```julia
# Visualize P-R tradeoff (conceptual)
# Vary generation temperature â†’ observe P-R tradeoff
temperatures = [0.5, 0.7, 0.9, 1.0, 1.2, 1.5]
precisions = Float64[]
recalls = Float64[]

feats_real = randn(200, 64)

for temp in temperatures
    # Simulate: lower temp â†’ higher quality, lower diversity
    if temp < 1.0
        # Mode collapse simulation
        n_modes = Int(round(50 * temp))
        feats_gen = feats_real[1:n_modes,:] .+ (0.1/temp) .* randn(n_modes, 64)
        feats_gen = vcat(feats_gen, feats_gen[rand(1:n_modes, 200-n_modes),:])
    else
        # Higher diversity, lower quality
        feats_gen = feats_real .+ (temp-0.5) .* randn(200, 64)
    end

    p, r = precision_recall(feats_real, feats_gen)
    push!(precisions, p)
    push!(recalls, r)
end

println("Temperature vs Precision-Recall:")
for (i, temp) in enumerate(temperatures)
    println("T=$temp: P=$(round(precisions[i], digits=3)), R=$(round(recalls[i], digits=3))")
end
```

**è§£é‡ˆ**: P-Rå¹³é¢ä¸Šã§å³ä¸Šï¼ˆP=1, R=1ï¼‰ã«è¿‘ã„ã»ã©è‰¯ã„ã€‚

#### 3.5.6 Precision-Recallã®é™ç•Œ

**é™ç•Œ1: è¨ˆç®—ã‚³ã‚¹ãƒˆ**

k-NNæ¢ç´¢ã‚’å…¨ã‚µãƒ³ãƒ—ãƒ«ã§å®Ÿè¡Œ â†’ $O(N^2)$ or $O(N \log N)$ï¼ˆKD-treeä½¿ç”¨æ™‚ï¼‰ã€‚å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§é…ã„ã€‚

**é™ç•Œ2: $k$ ã®é¸æŠ**

$k$ï¼ˆæœ€è¿‘å‚æ•°ï¼‰ã«ã‚ˆã£ã¦çµæœãŒå¤‰ã‚ã‚‹ã€‚è«–æ–‡ [^4] ã§ã¯ $k=5$ ã‚’æ¨å¥¨ã€‚

**é™ç•Œ3: ç‰¹å¾´æŠ½å‡ºå™¨ã¸ã®ä¾å­˜**

Inceptionç‰¹å¾´ã«ä¾å­˜ â†’ ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚¤ã‚¢ã‚¹ï¼ˆFIDã¨åŒã˜å•é¡Œï¼‰ã€‚

:::message
**ãƒœã‚¹æˆ¦ã¸ã®æº–å‚™ 70% å®Œäº†**: Precision-Recallå®Œäº†ã€‚æ®‹ã‚ŠMMD/CMMD â†’ ãƒœã‚¹æˆ¦ã¸ã€‚
:::

### 3.6 MMD (Maximum Mean Discrepancy) & CMMD å®Œå…¨å°å‡º

#### 3.6.1 MMDã®å‹•æ©Ÿ â€” ä»®å®šã®ãªã„åˆ†å¸ƒè·é›¢

**å•é¡Œ**: FIDã¯ã‚¬ã‚¦ã‚¹æ€§ã‚’ä»®å®š â†’ å¤šå³°åˆ†å¸ƒã§å¤±æ•—ã€‚ä»®å®šãªã—ã®åˆ†å¸ƒè·é›¢ãŒæ¬²ã—ã„ã€‚

**ã‚¢ã‚¤ãƒ‡ã‚¢**: ã‚«ãƒ¼ãƒãƒ«æ³•ï¼ˆRKHS: Reproducing Kernel Hilbert Spaceï¼‰ã‚’ä½¿ã„ã€2ã¤ã®åˆ†å¸ƒã®**å¹³å‡åŸ‹ã‚è¾¼ã¿**ã®è·é›¢ã‚’æ¸¬ã‚‹ [^6]ã€‚

#### 3.6.2 RKHSã¨å¹³å‡åŸ‹ã‚è¾¼ã¿

**RKHS** (Reproducing Kernel Hilbert Space):

ã‚«ãƒ¼ãƒãƒ«é–¢æ•° $k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ ã‹ã‚‰å®šç¾©ã•ã‚Œã‚‹ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆç©ºé–“ $\mathcal{H}$ã€‚

**ä»£è¡¨çš„ãªã‚«ãƒ¼ãƒãƒ«**:
- RBFã‚«ãƒ¼ãƒãƒ«ï¼ˆã‚¬ã‚¦ã‚·ã‚¢ãƒ³ã‚«ãƒ¼ãƒãƒ«ï¼‰: $k(x, y) = \exp(-\|x - y\|^2 / (2\sigma^2))$
- å¤šé …å¼ã‚«ãƒ¼ãƒãƒ«: $k(x, y) = (x^\top y + c)^d$

**å¹³å‡åŸ‹ã‚è¾¼ã¿** (Mean Embedding):

åˆ†å¸ƒ $P$ ã®å¹³å‡åŸ‹ã‚è¾¼ã¿ $\mu_P \in \mathcal{H}$ ã¯:

$$
\mu_P = \mathbb{E}_{x \sim P}[\phi(x)]
$$

ã“ã“ã§ $\phi: \mathcal{X} \to \mathcal{H}$ ã¯ã‚«ãƒ¼ãƒãƒ«ã«ã‚ˆã‚‹ç‰¹å¾´å†™åƒï¼ˆé€šå¸¸ã¯é™½ã«è¨ˆç®—ã—ãªã„ â€” kernel trickã§å†…ç©ã®ã¿è¨ˆç®—ï¼‰ã€‚

#### 3.6.3 MMDã®å®šç¾©

**å®šç¾©**:

$$
\text{MMD}^2(P, Q) = \|\mu_P - \mu_Q\|_{\mathcal{H}}^2
$$

**å±•é–‹** (kernel trick):

$$
\begin{aligned}
\text{MMD}^2(P, Q) &= \|\mu_P - \mu_Q\|^2 \\
&= \langle \mu_P - \mu_Q, \mu_P - \mu_Q \rangle_{\mathcal{H}} \\
&= \langle \mu_P, \mu_P \rangle + \langle \mu_Q, \mu_Q \rangle - 2\langle \mu_P, \mu_Q \rangle \\
&= \mathbb{E}_{x,x' \sim P}[k(x, x')] + \mathbb{E}_{y,y' \sim Q}[k(y, y')] - 2\mathbb{E}_{x \sim P, y \sim Q}[k(x, y)]
\end{aligned}
$$

**å®Ÿç”¨çš„ãªæ¨å®š** (empirical MMD):

$$
\widehat{\text{MMD}}^2 = \frac{1}{n^2}\sum_{i,j=1}^n k(x_i, x_j) + \frac{1}{m^2}\sum_{i,j=1}^m k(y_i, y_j) - \frac{2}{nm}\sum_{i=1}^n\sum_{j=1}^m k(x_i, y_j)
$$

ã“ã“ã§ $\{x_i\}_{i=1}^n \sim P$, $\{y_j\}_{j=1}^m \sim Q$ã€‚

#### 3.6.4 æ•°å¼â†”ã‚³ãƒ¼ãƒ‰ 1:1å¯¾å¿œ (MMD)

```julia
# MMD implementation with RBF kernel
using Statistics

function rbf_kernel(x::Vector{Float64}, y::Vector{Float64}, Ïƒ::Float64=1.0)
    # k(x, y) = exp(-||x - y||Â² / (2ÏƒÂ²))
    return exp(-sum((x .- y).^2) / (2*Ïƒ^2))
end

function mmd_squared(X::Matrix{Float64}, Y::Matrix{Float64}, Ïƒ::Float64=1.0)
    # X: (n, d), Y: (m, d)
    n = size(X, 1)
    m = size(Y, 1)

    # E_{x,x'}[k(x, x')]
    kxx = 0.0
    for i in 1:n, j in 1:n
        kxx += rbf_kernel(X[i,:], X[j,:], Ïƒ)
    end
    kxx /= (n * n)

    # E_{y,y'}[k(y, y')]
    kyy = 0.0
    for i in 1:m, j in 1:m
        kyy += rbf_kernel(Y[i,:], Y[j,:], Ïƒ)
    end
    kyy /= (m * m)

    # E_{x,y}[k(x, y)]
    kxy = 0.0
    for i in 1:n, j in 1:m
        kxy += rbf_kernel(X[i,:], Y[j,:], Ïƒ)
    end
    kxy /= (n * m)

    # MMDÂ²
    mmd_sq = kxx + kyy - 2*kxy
    return max(0, mmd_sq)  # numerical stability
end

function mmd(X::Matrix{Float64}, Y::Matrix{Float64}, Ïƒ::Float64=1.0)
    return sqrt(mmd_squared(X, Y, Ïƒ))
end

# Test: identical distributions â†’ MMD â‰ˆ 0
X_test = randn(100, 32)
Y_test = randn(100, 32)
Y_test_same = X_test .+ 0.01 .* randn(100, 32)  # very similar

mmd_diff = mmd(X_test, Y_test)
mmd_same = mmd(X_test, Y_test_same)
println("MMD (different): $(round(mmd_diff, digits=4))")
println("MMD (similar): $(round(mmd_same, digits=6))")
println("Expected: MMD(similar) â‰ˆ 0")
```

#### 3.6.5 CMMD (CLIP-MMD) â€” FIDã®ä»£æ›¿ [^5]

**CMMD** (Jayasumana et al. 2024 [^5]) = MMD + CLIPåŸ‹ã‚è¾¼ã¿ã€‚

**å‹•æ©Ÿ**:
1. FIDã®æ­£è¦æ€§ä»®å®šã‚’æ’é™¤
2. CLIPç‰¹å¾´ â†’ ImageNetãƒã‚¤ã‚¢ã‚¹ã‚’è»½æ¸›ã€ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ä»˜ãç”Ÿæˆã«å¯¾å¿œ

**å®šç¾©**: CMMD = MMD over CLIP embeddings

$$
\text{CMMD}^2(P_r, P_g) = \text{MMD}^2(\text{CLIP}(P_r), \text{CLIP}(P_g))
$$

**CLIPã®åˆ©ç‚¹**:
- Vision-Languageäº‹å‰è¨“ç·´ â†’ ã‚ˆã‚Šæ±ç”¨çš„
- Text-to-Imageç”Ÿæˆã®è©•ä¾¡ã«ç›´æ¥å¯¾å¿œ
- äººé–“è©•ä¾¡ã¨ã®ç›¸é–¢ãŒFIDã‚ˆã‚Šé«˜ã„ [^5]

**å®Ÿé¨“çµæœ** [^5]:

| æŒ‡æ¨™ | Pearsonç›¸é–¢ï¼ˆäººé–“è©•ä¾¡ï¼‰ | ã‚µãƒ³ãƒ—ãƒ«æ•°ä¾å­˜æ€§ |
|:-----|:-----------------------|:----------------|
| FID | 0.56 | é«˜ï¼ˆ2000+å¿…è¦ï¼‰ |
| CMMD | **0.72** | ä½ï¼ˆ500ã§å®‰å®šï¼‰ |

#### 3.6.6 å®Ÿè£… (CMMD)

```julia
# CMMD implementation (with dummy CLIP embeddings)
function clip_embed_dummy(images::Vector{Matrix{Float64}})
    # Real impl: CLIP image encoder â†’ 512-dim
    n = length(images)
    return randn(n, 512)  # (n, 512)
end

function cmmd(real_images::Vector{Matrix{Float64}},
              gen_images::Vector{Matrix{Float64}}, Ïƒ::Float64=10.0)
    # Extract CLIP embeddings
    emb_real = clip_embed_dummy(real_images)  # (n, 512)
    emb_gen = clip_embed_dummy(gen_images)    # (m, 512)

    # Compute MMD
    return mmd(emb_real, emb_gen, Ïƒ)
end

# Test
real_imgs_cmmd = [randn(64, 64) for _ in 1:100]
gen_imgs_cmmd = [randn(64, 64) for _ in 1:100]
cmmd_score = cmmd(real_imgs_cmmd, gen_imgs_cmmd)
println("CMMD: $(round(cmmd_score, digits=4))")
println("Lower = more similar distributions")
```

#### 3.6.7 ã‚«ãƒ¼ãƒãƒ«é¸æŠã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\sigma$

**RBFã‚«ãƒ¼ãƒãƒ«ã® $\sigma$**:

$\sigma$ ãŒå°ã•ã„ â†’ å±€æ‰€çš„ãªé•ã„ã«æ•æ„Ÿ
$\sigma$ ãŒå¤§ãã„ â†’ å¤§åŸŸçš„ãªé•ã„ã®ã¿æ¤œå‡º

**ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯**: ãƒ‡ãƒ¼ã‚¿ã®ä¸­å¤®å€¤è·é›¢ï¼ˆmedian trickï¼‰ [^6]

$$
\sigma = \text{median}(\{\|x_i - x_j\| : i,j\})
$$

```julia
# Median heuristic for Ïƒ
function median_heuristic(X::Matrix{Float64})
    n = size(X, 1)
    dists = Float64[]
    # Subsample for efficiency
    n_samples = min(1000, n*(n-1)Ã·2)
    for _ in 1:n_samples
        i, j = rand(1:n, 2)
        if i != j
            push!(dists, sqrt(sum((X[i,:] .- X[j,:]).^2)))
        end
    end
    return median(dists)
end

# Test
X_test2 = randn(200, 64)
Ïƒ_auto = median_heuristic(X_test2)
println("Auto-selected Ïƒ (median heuristic): $(round(Ïƒ_auto, digits=2))")
```

:::message
**ãƒœã‚¹æˆ¦ã‚¯ãƒªã‚¢ æº–å‚™ 90% å®Œäº†**: MMD/CMMDã®ç†è«–ã‚’å®Œå…¨å°å‡ºã—ãŸã€‚ã“ã‚Œã§å…¨æŒ‡æ¨™ï¼ˆFID/IS/LPIPS/P&R/CMMDï¼‰ã®æ•°å¼åŸºç›¤ãŒæ•´ã£ãŸã€‚ã“ã“ã‹ã‚‰å®Ÿè£…ã‚¾ãƒ¼ãƒ³ã¸ã€‚
:::

### 3.7 âš”ï¸ Boss Battle: è«–æ–‡ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹å¼ã‚’å®Œå…¨èª­è§£

**èª²é¡Œ**: CMMDè«–æ–‡ [^5] ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ã‚’å®Œå…¨ç†è§£ã—ã€Juliaã§å†å®Ÿè£…ã›ã‚ˆã€‚

**è«–æ–‡æŠœç²‹** (Jayasumana et al. 2024 [^5], Algorithm 1 simplified):

```
Algorithm: CMMD Computation
Input: Real images I_r, Generated images I_g, CLIP model C, kernel bandwidth Ïƒ
Output: CMMD score

1. Extract CLIP embeddings:
   E_r = [C(img) for img in I_r]  # (n_r, 512)
   E_g = [C(img) for img in I_g]  # (n_g, 512)

2. Compute kernel matrices:
   K_rr[i,j] = k(E_r[i], E_r[j]; Ïƒ)
   K_gg[i,j] = k(E_g[i], E_g[j]; Ïƒ)
   K_rg[i,j] = k(E_r[i], E_g[j]; Ïƒ)

3. Compute MMDÂ²:
   MMDÂ² = mean(K_rr) + mean(K_gg) - 2*mean(K_rg)

4. Return CMMD = sqrt(max(0, MMDÂ²))
```

**å®Ÿè£…**:

```julia
# Boss Battle: Full CMMD implementation following paper
using LinearAlgebra, Statistics

function cmmd_paper(real_imgs::Vector{Matrix{Float64}},
                     gen_imgs::Vector{Matrix{Float64}})
    # Step 1: CLIP embeddings (dummy)
    E_r = clip_embed_dummy(real_imgs)  # (n_r, 512)
    E_g = clip_embed_dummy(gen_imgs)   # (n_g, 512)

    # Step 2: Auto-select Ïƒ via median heuristic
    Ïƒ = median_heuristic(vcat(E_r, E_g))

    # Step 3: Compute kernel matrices
    n_r, n_g = size(E_r, 1), size(E_g, 1)

    K_rr = zeros(n_r, n_r)
    for i in 1:n_r, j in 1:n_r
        K_rr[i,j] = rbf_kernel(E_r[i,:], E_r[j,:], Ïƒ)
    end

    K_gg = zeros(n_g, n_g)
    for i in 1:n_g, j in 1:n_g
        K_gg[i,j] = rbf_kernel(E_g[i,:], E_g[j,:], Ïƒ)
    end

    K_rg = zeros(n_r, n_g)
    for i in 1:n_r, j in 1:n_g
        K_rg[i,j] = rbf_kernel(E_r[i,:], E_g[j,:], Ïƒ)
    end

    # Step 4: MMDÂ²
    mmd_sq = mean(K_rr) + mean(K_gg) - 2*mean(K_rg)

    # Step 5: CMMD
    cmmd_val = sqrt(max(0, mmd_sq))

    return cmmd_val, Ïƒ
end

# Test
imgs_r_boss = [randn(64, 64) for _ in 1:50]
imgs_g_boss = [randn(64, 64) for _ in 1:50]
cmmd_boss, Ïƒ_boss = cmmd_paper(imgs_r_boss, imgs_g_boss)
println("âš”ï¸ Boss Battle: CMMD = $(round(cmmd_boss, digits=4)), Ïƒ = $(round(Ïƒ_boss, digits=2))")
println("âœ… Boss ã‚¯ãƒªã‚¢ï¼")
```

**æ¤œè¨¼**: åŒä¸€åˆ†å¸ƒã§ CMMD â‰ˆ 0 ã«ãªã‚‹ã‹ã€‚

```julia
# Sanity check: identical â†’ CMMD â‰ˆ 0
imgs_same = [randn(64, 64) for _ in 1:50]
imgs_same2 = [img .+ 0.01.*randn(64,64) for img in imgs_same]  # very similar
cmmd_same, _ = cmmd_paper(imgs_same, imgs_same2)
println("CMMD (near-identical): $(round(cmmd_same, digits=6)) â‰ˆ 0")
```

:::message
**ãƒœã‚¹æˆ¦ã‚¯ãƒªã‚¢ï¼ğŸ‰** å…¨æŒ‡æ¨™ã®æ•°å¼ã‚’å®Œå…¨å°å‡ºã—ã€è«–æ–‡ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ã‚’å†å®Ÿè£…ã—ãŸã€‚
**é€²æ—: 50% å®Œäº†** æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³å®Œäº†ã€‚ã“ã“ã‹ã‚‰å®Ÿè£…ã‚¾ãƒ¼ãƒ³ã¸ â€” Juliaçµ±è¨ˆåˆ†æ + Rust Criterion ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€‚
:::

---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” Juliaçµ±è¨ˆåˆ†æ + Rust Criterion

### 4.1 Juliaçµ±è¨ˆåˆ†æçµ±åˆ

ç¬¬24å›ã§å­¦ã‚“ã çµ±è¨ˆæ¤œå®šã‚’è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«çµ±åˆã™ã‚‹ã€‚

#### 4.1.1 FIDã®ä¿¡é ¼åŒºé–“

FIDæ¨å®šé‡ $\widehat{\text{FID}}$ ã¯æœ‰é™ã‚µãƒ³ãƒ—ãƒ«ã§ã®æ¨å®š â†’ ä¸ç¢ºå®Ÿæ€§ãŒã‚ã‚‹ã€‚

**Bootstrapæ³•ã§ä¿¡é ¼åŒºé–“ã‚’è¨ˆç®—**:

```julia
# FID confidence interval via bootstrap
using Bootstrap

function fid_with_ci(real_imgs::Vector{Matrix{Float64}},
                      gen_imgs::Vector{Matrix{Float64}},
                      n_bootstrap::Int=1000, confidence::Float64=0.95)
    # Extract features once
    feats_real = extract_inception_features(real_imgs)
    feats_gen = extract_inception_features(gen_imgs)

    # Compute point estimate
    Î¼_r, Î£_r = compute_statistics(feats_real)
    Î¼_g, Î£_g = compute_statistics(feats_gen)
    fid_point = frechet_distance(Î¼_r, Î£_r, Î¼_g, Î£_g)

    # Bootstrap resampling
    n_real = size(feats_real, 1)
    n_gen = size(feats_gen, 1)
    fid_samples = zeros(n_bootstrap)

    for b in 1:n_bootstrap
        # Resample with replacement
        idx_r = rand(1:n_real, n_real)
        idx_g = rand(1:n_gen, n_gen)
        feats_r_boot = feats_real[idx_r, :]
        feats_g_boot = feats_gen[idx_g, :]

        Î¼_r_b, Î£_r_b = compute_statistics(feats_r_boot)
        Î¼_g_b, Î£_g_b = compute_statistics(feats_g_boot)
        fid_samples[b] = frechet_distance(Î¼_r_b, Î£_r_b, Î¼_g_b, Î£_g_b)
    end

    # Confidence interval
    Î± = 1 - confidence
    ci_lower = quantile(fid_samples, Î±/2)
    ci_upper = quantile(fid_samples, 1 - Î±/2)

    return fid_point, ci_lower, ci_upper, fid_samples
end

# Test
real_test = [randn(32, 32) for _ in 1:100]
gen_test = [randn(32, 32) for _ in 1:100]
fid_est, ci_l, ci_u, samples = fid_with_ci(real_test, gen_test, 200, 0.95)
println("FID: $(round(fid_est, digits=2)) [95% CI: $(round(ci_l, digits=2)), $(round(ci_u, digits=2))]")
```

#### 4.1.2 ãƒ¢ãƒ‡ãƒ«é–“æ¯”è¼ƒ â€” æœ‰æ„å·®æ¤œå®š

2ã¤ã®ãƒ¢ãƒ‡ãƒ«ã®FIDã‚’æ¯”è¼ƒ â†’ çµ±è¨ˆçš„ã«æœ‰æ„ãªå·®ãŒã‚ã‚‹ã‹ï¼Ÿ

**Welch's t-test** (ç¬¬24å›):

```julia
# Welch's t-test for FID comparison
using HypothesisTests

function compare_models_fid(model_a_fid_samples::Vector{Float64},
                             model_b_fid_samples::Vector{Float64}, Î±::Float64=0.05)
    # Welch's t-test (unequal variances)
    test_result = UnequalVarianceTTest(model_a_fid_samples, model_b_fid_samples)

    p_value = pvalue(test_result)
    is_significant = p_value < Î±

    # Effect size (Cohen's d)
    Î¼_a = mean(model_a_fid_samples)
    Î¼_b = mean(model_b_fid_samples)
    s_a = std(model_a_fid_samples)
    s_b = std(model_b_fid_samples)
    pooled_std = sqrt((s_a^2 + s_b^2) / 2)
    cohens_d = (Î¼_a - Î¼_b) / pooled_std

    println("Model A FID: $(round(Î¼_a, digits=2)) Â± $(round(s_a, digits=2))")
    println("Model B FID: $(round(Î¼_b, digits=2)) Â± $(round(s_b, digits=2))")
    println("p-value: $(round(p_value, digits=4))")
    println("Significant? $(is_significant) (Î±=$(Î±))")
    println("Effect size (Cohen's d): $(round(cohens_d, digits=3))")

    return test_result, p_value, cohens_d
end

# Test: simulate FID samples for 2 models
# Model A: FID ~ N(15, 2)
# Model B: FID ~ N(13, 1.5) (better model)
fid_a = 15 .+ 2 .* randn(100)
fid_b = 13 .+ 1.5 .* randn(100)

compare_models_fid(fid_a, fid_b)
```

#### 4.1.3 å¤šé‡æ¯”è¼ƒè£œæ­£ â€” Bonferroni/FDR

è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ï¼ˆNå€‹ï¼‰ã‚’æ¯”è¼ƒ â†’ å¤šé‡æ¤œå®šå•é¡Œï¼ˆç¬¬24å›ï¼‰ã€‚

**Bonferroniè£œæ­£**: $\alpha' = \alpha / N$

```julia
# Multiple model comparison with Bonferroni correction
function compare_multiple_models(fid_samples_list::Vector{Vector{Float64}}, Î±::Float64=0.05)
    n_models = length(fid_samples_list)
    n_comparisons = n_models * (n_models - 1) Ã· 2
    Î±_bonf = Î± / n_comparisons

    println("Comparing $(n_models) models ($(n_comparisons) pairwise tests)")
    println("Bonferroni-corrected Î±: $(round(Î±_bonf, digits=5))")

    results = []
    for i in 1:n_models, j in (i+1):n_models
        test = UnequalVarianceTTest(fid_samples_list[i], fid_samples_list[j])
        p_val = pvalue(test)
        is_sig = p_val < Î±_bonf
        push!(results, (i, j, p_val, is_sig))
        println("Model $i vs $j: p=$(round(p_val, digits=4)), significant=$is_sig")
    end

    return results
end

# Test: 4 models
fid_model1 = 20 .+ 3 .* randn(50)
fid_model2 = 15 .+ 2 .* randn(50)
fid_model3 = 14 .+ 2.5 .* randn(50)
fid_model4 = 13 .+ 1.5 .* randn(50)
fid_list = [fid_model1, fid_model2, fid_model3, fid_model4]

compare_multiple_models(fid_list)
```

### 4.2 Rust Criterion ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

**Criterion.rs** [^criterion] ã¯Rustã®çµ±è¨ˆçš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€‚

**ç‰¹å¾´**:
- çµ±è¨ˆçš„æœ‰æ„æ€§æ¤œå‡ºï¼ˆå›å¸°æ¤œå‡ºï¼‰
- è‡ªå‹• outlier é™¤å»
- CIçµ±åˆå¯èƒ½

#### 4.2.1 Rust FIDå®Ÿè£…ã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

```rust
// Cargo.toml
// [dependencies]
// ndarray = "0.16"
// ndarray-linalg = "0.19"
// [dev-dependencies]
// criterion = "0.5"

use ndarray::{Array1, Array2};
use ndarray_linalg::*;

/// Compute FrÃ©chet distance between two Gaussians
pub fn frechet_distance(
    mu1: &Array1<f64>,
    sigma1: &Array2<f64>,
    mu2: &Array1<f64>,
    sigma2: &Array2<f64>,
) -> f64 {
    // Mean difference term
    let diff = mu1 - mu2;
    let mean_term = diff.dot(&diff);

    // Covariance term: Tr(Î£1 + Î£2 - 2(Î£1 Î£2)^{1/2})
    let product = sigma1.dot(sigma2);

    // Matrix square root via eigen decomposition
    let (eigenvalues, eigenvectors) = product.eigh(UPLO::Lower).unwrap();
    let sqrt_eig = eigenvalues.mapv(|x| x.abs().sqrt());
    let sqrt_product = &eigenvectors * &Array2::from_diag(&sqrt_eig) * &eigenvectors.t();

    let trace_term = sigma1.diag().sum() + sigma2.diag().sum() - 2.0 * sqrt_product.diag().sum();

    mean_term + trace_term
}

#[cfg(test)]
mod benches {
    use super::*;
    use criterion::{black_box, criterion_group, criterion_main, Criterion};
    use ndarray::Array;

    fn benchmark_fid(c: &mut Criterion) {
        let d = 2048;  // Inception feature dim
        let mu1 = Array1::zeros(d);
        let mu2 = Array1::ones(d) * 0.1;
        let sigma1 = Array2::eye(d);
        let sigma2 = Array2::eye(d) * 1.1;

        c.bench_function("fid_2048d", |b| {
            b.iter(|| {
                frechet_distance(
                    black_box(&mu1),
                    black_box(&sigma1),
                    black_box(&mu2),
                    black_box(&sigma2),
                )
            })
        });
    }

    criterion_group!(benches, benchmark_fid);
    criterion_main!(benches);
}
```

**å®Ÿè¡Œ**:

```bash
cargo bench
```

**å‡ºåŠ›ä¾‹**:

```
fid_2048d               time:   [12.234 ms 12.456 ms 12.701 ms]
                        change: [-2.3% +0.5% +3.1%] (p = 0.67 > 0.05)
                        No change in performance detected.
```

Criterionã¯è‡ªå‹•ã§:
- è¤‡æ•°å›å®Ÿè¡Œï¼ˆwarmup + measurementï¼‰
- çµ±è¨ˆé‡è¨ˆç®—ï¼ˆå¹³å‡ã€æ¨™æº–åå·®ã€ä¿¡é ¼åŒºé–“ï¼‰
- å‰å›ã¨ã®æ¯”è¼ƒï¼ˆå›å¸°æ¤œå‡ºï¼‰

#### 4.2.2 è‡ªå‹•ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

**CIçµ±åˆ**: GitHub Actions ã§è‡ªå‹•ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ + å›å¸°ã‚¢ãƒ©ãƒ¼ãƒˆã€‚

```yaml
# .github/workflows/bench.yml
name: Benchmark

on: [push, pull_request]

jobs:
  benchmark:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
      - name: Run benchmarks
        run: cargo bench --bench fid_bench
      - name: Upload results
        uses: actions/upload-artifact@v2
        with:
          name: criterion-results
          path: target/criterion/
```

### 4.3 è‡ªå‹•è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­è¨ˆ

**ãƒ•ãƒ­ãƒ¼**:

```mermaid
graph LR
    A[ãƒ¢ãƒ‡ãƒ«è¨“ç·´] --> B[ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜]
    B --> C[ç”»åƒç”Ÿæˆ<br/>n=5000]
    C --> D[ç‰¹å¾´æŠ½å‡º<br/>Inception/CLIP]
    D --> E1[FIDè¨ˆç®—]
    D --> E2[ISè¨ˆç®—]
    D --> E3[LPIPSè¨ˆç®—]
    D --> E4[P&Rè¨ˆç®—]
    D --> E5[CMMDè¨ˆç®—]
    E1 & E2 & E3 & E4 & E5 --> F[çµ±è¨ˆæ¤œå®š<br/>CI+t-test]
    F --> G[ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ<br/>JSON/HTML]
    G --> H[CI Artifact]
    style F fill:#fff3e0
    style G fill:#c8e6c9
```

**å®Ÿè£…** (Julia):

```julia
# Automatic evaluation pipeline
using JSON

struct EvaluationResult
    fid::Float64
    fid_ci::Tuple{Float64, Float64}
    is::Float64
    is_ci::Tuple{Float64, Float64}
    cmmd::Float64
    precision::Float64
    recall::Float64
    timestamp::String
end

function evaluate_model(model_checkpoint::String, real_dataset::Vector{Matrix{Float64}}, n_gen::Int=1000)
    println("Evaluating model: $model_checkpoint")

    # Step 1: Generate images
    println("Generating $(n_gen) images...")
    gen_images = generate_images(model_checkpoint, n_gen)  # placeholder

    # Step 2: Extract features
    println("Extracting features...")
    feats_real = extract_inception_features(real_dataset)
    feats_gen = extract_inception_features(gen_images)

    # Step 3: Compute metrics
    println("Computing FID...")
    fid_val, fid_l, fid_u, _ = fid_with_ci(real_dataset, gen_images, 200, 0.95)

    println("Computing IS...")
    is_val, _ = inception_score(gen_images)
    # Simplified: no bootstrap for IS here

    println("Computing CMMD...")
    cmmd_val, _ = cmmd_paper(real_dataset, gen_images)

    println("Computing Precision-Recall...")
    prec, rec = precision_recall(feats_real, feats_gen, 5)

    # Step 4: Assemble results
    result = EvaluationResult(
        fid_val, (fid_l, fid_u),
        is_val, (0.0, 0.0),  # placeholder CI
        cmmd_val,
        prec, rec,
        string(now())
    )

    # Step 5: Save to JSON
    json_result = Dict(
        "model" => model_checkpoint,
        "fid" => Dict("value" => result.fid, "ci" => result.fid_ci),
        "is" => result.is,
        "cmmd" => result.cmmd,
        "precision" => result.precision,
        "recall" => result.recall,
        "timestamp" => result.timestamp
    )

    output_path = "eval_results_$(split(model_checkpoint, '/')[end]).json"
    open(output_path, "w") do f
        JSON.print(f, json_result, 2)
    end

    println("âœ… Evaluation complete. Results saved to $output_path")
    return result
end

# Placeholder for image generation
function generate_images(checkpoint::String, n::Int)
    # Real impl: load model, sample latents, decode
    return [randn(64, 64) for _ in 1:n]
end

# Test
real_data_test = [randn(64, 64) for _ in 1:500]
eval_result = evaluate_model("model_epoch_100.ckpt", real_data_test, 500)
```

:::message
**é€²æ—: 70% å®Œäº†** å®Ÿè£…ã‚¾ãƒ¼ãƒ³å®Œäº† â€” Juliaçµ±è¨ˆåˆ†æ + Rust Criterion + è‡ªå‹•è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€‚ã“ã“ã‹ã‚‰å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ã¸ â€” VAE/GAN/GPTçµ±åˆè©•ä¾¡ã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” VAE/GAN/GPTçµ±åˆè©•ä¾¡

### 5.1 æ¼”ç¿’: 3ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡æ¯”è¼ƒ

**èª²é¡Œ**: VAE, GAN, GPT (autoregressive) ã®3ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã€æ¯”è¼ƒã›ã‚ˆã€‚

**ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: MNIST (ç°¡æ˜“ç‰ˆ)

#### 5.1.1 ãƒ¢ãƒ‡ãƒ«å®Ÿè£…ï¼ˆç°¡ç•¥ç‰ˆï¼‰

```julia
# Simplified VAE/GAN/GPT for evaluation demo
using Flux

# VAE (from ç¬¬10å›)
struct TinyVAE
    encoder::Chain
    decoder::Chain
end

function (vae::TinyVAE)(x::Matrix{Float64})
    # Encode
    z_params = vae.encoder(x)  # (2*latent_dim, batch)
    d = size(z_params, 1) Ã· 2
    Î¼, logÏƒ = z_params[1:d,:], z_params[d+1:end,:]
    z = Î¼ .+ exp.(logÏƒ) .* randn(size(Î¼))

    # Decode
    x_recon = vae.decoder(z)
    return x_recon, Î¼, logÏƒ
end

# GAN (from ç¬¬12å›)
struct TinyGAN
    generator::Chain
    discriminator::Chain
end

function generate_gan(gan::TinyGAN, n::Int, latent_dim::Int=32)
    z = randn(latent_dim, n)
    return gan.generator(z)
end

# Autoregressive (from ç¬¬15å›)
struct TinyAR
    model::Chain
end

function generate_ar(ar::TinyAR, n::Int, seq_len::Int=784)
    # Simplified: generate pixel by pixel
    samples = []
    for _ in 1:n
        x = zeros(seq_len)
        for t in 1:seq_len
            # Predict next pixel
            logits = ar.model(x[1:t])
            x[t] = sample_categorical(softmax(logits))
        end
        push!(samples, reshape(x, 28, 28))
    end
    return samples
end

# Placeholder implementations
vae_model = TinyVAE(Chain(Dense(784, 64), Dense(64, 32)), Chain(Dense(16, 64), Dense(64, 784)))
gan_model = TinyGAN(Chain(Dense(32, 64), Dense(64, 784)), Chain(Dense(784, 64), Dense(64, 1)))
ar_model = TinyAR(Chain(Dense(784, 256), Dense(256, 784)))
```

#### 5.1.2 çµ±åˆè©•ä¾¡

```julia
# Unified evaluation for 3 models
function evaluate_all_models(real_data::Vector{Matrix{Float64}}, n_gen::Int=1000)
    println("ğŸ”¬ Evaluating 3 models: VAE, GAN, AR")

    # Generate samples from each model
    println("Generating VAE samples...")
    vae_samples = [generate_vae(vae_model) for _ in 1:n_gen]  # placeholder

    println("Generating GAN samples...")
    gan_samples = [generate_gan(gan_model, 1, 32)[:,1] |> x -> reshape(x, 28, 28) for _ in 1:n_gen]

    println("Generating AR samples...")
    ar_samples = generate_ar(ar_model, n_gen, 784)

    # Evaluate each model
    models = [("VAE", vae_samples), ("GAN", gan_samples), ("AR", ar_samples)]
    results = Dict()

    for (name, samples) in models
        println("\nğŸ“Š Evaluating $name...")
        fid_val, _, _, _ = fid_with_ci(real_data, samples, 100, 0.95)
        is_val, _ = inception_score(samples)
        cmmd_val, _ = cmmd_paper(real_data, samples)

        feats_real = extract_inception_features(real_data)
        feats_gen = extract_inception_features(samples)
        prec, rec = precision_recall(feats_real, feats_gen, 5)

        results[name] = Dict(
            "FID" => fid_val,
            "IS" => is_val,
            "CMMD" => cmmd_val,
            "Precision" => prec,
            "Recall" => rec
        )
    end

    # Display comparison table
    println("\nğŸ“‹ Comparison Table:")
    println("| Model | FID â†“ | IS â†‘ | CMMD â†“ | Precision â†‘ | Recall â†‘ |")
    println("|:------|:------|:-----|:-------|:------------|:---------|")
    for (name, metrics) in results
        println("| $name | $(round(metrics["FID"], digits=2)) | $(round(metrics["IS"], digits=2)) | " *
                "$(round(metrics["CMMD"], digits=4)) | $(round(metrics["Precision"], digits=3)) | $(round(metrics["Recall"], digits=3)) |")
    end

    return results
end

# Placeholder
function generate_vae(vae::TinyVAE, latent_dim::Int=16)
    z = randn(latent_dim)
    x_gen = vae.decoder(z)
    return reshape(x_gen, 28, 28)
end

# Test with dummy data
mnist_real = [randn(28, 28) for _ in 1:500]
all_results = evaluate_all_models(mnist_real, 500)
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœãƒ‘ã‚¿ãƒ¼ãƒ³**:

| Model | FID â†“ | IS â†‘ | CMMD â†“ | Precision â†‘ | Recall â†‘ | ç‰¹å¾´ |
|:------|:------|:-----|:-------|:------------|:---------|:-----|
| VAE | ä¸­ | ä¸­ | ä¸­ | ä¸­ | **é«˜** | å¤šæ§˜æ€§é«˜ã„ãŒã¼ã‚„ã‘ã‚‹ |
| GAN | **ä½** | **é«˜** | **ä½** | **é«˜** | ä½ | é«˜å“è³ªã ãŒmode collapse |
| AR | ä½-ä¸­ | é«˜ | ä½ | é«˜ | é«˜ | å“è³ªã‚‚å¤šæ§˜æ€§ã‚‚è‰¯ã„ãŒé…ã„ |

### 5.2 äººé–“è©•ä¾¡ãƒ—ãƒ­ãƒˆã‚³ãƒ«è¨­è¨ˆ

**å®šé‡è©•ä¾¡ã®é™ç•Œ** â†’ äººé–“è©•ä¾¡ãŒå¿…è¦ã€‚

#### 5.2.1 A/Bãƒ†ã‚¹ãƒˆè¨­è¨ˆ

**è³ªå•**: ã€Œã©ã¡ã‚‰ã®ç”»åƒãŒã‚ˆã‚Šè‡ªç„¶ã§ã™ã‹ï¼Ÿã€

**è¨­è¨ˆ**:
1. ãƒšã‚¢wiseæ¯”è¼ƒï¼ˆ2ç”»åƒã‚’æç¤ºï¼‰
2. ç„¡ä½œç‚ºåŒ–ï¼ˆé †åºã€ãƒšã‚¢é¸æŠï¼‰
3. è©•ä¾¡è€…é–“ä¸€è‡´åº¦ï¼ˆInter-rater reliabilityï¼‰

```julia
# A/B test design
struct ABTest
    pair_id::Int
    img_a::Matrix{Float64}
    img_b::Matrix{Float64}
    model_a::String
    model_b::String
end

function design_ab_test(models::Dict{String, Vector{Matrix{Float64}}}, n_pairs::Int=100)
    # Generate random pairs
    model_names = collect(keys(models))
    tests = ABTest[]

    for i in 1:n_pairs
        # Random 2 models
        m1, m2 = rand(model_names, 2)
        while m1 == m2
            m2 = rand(model_names)
        end

        # Random sample from each
        img1 = rand(models[m1])
        img2 = rand(models[m2])

        # Randomize order
        if rand() < 0.5
            push!(tests, ABTest(i, img1, img2, m1, m2))
        else
            push!(tests, ABTest(i, img2, img1, m2, m1))
        end
    end

    return tests
end

# Export for crowdsourcing
function export_ab_test_csv(tests::Vector{ABTest}, output_path::String)
    open(output_path, "w") do f
        println(f, "pair_id,img_a_path,img_b_path,model_a,model_b")
        for test in tests
            # Save images (placeholder)
            img_a_path = "ab_test_$(test.pair_id)_a.png"
            img_b_path = "ab_test_$(test.pair_id)_b.png"
            println(f, "$(test.pair_id),$img_a_path,$img_b_path,$(test.model_a),$(test.model_b)")
        end
    end
    println("âœ… A/B test CSV exported to $output_path")
end

# Test
models_for_ab = Dict("VAE" => vae_samples, "GAN" => gan_samples, "AR" => ar_samples)  # from 5.1
ab_tests = design_ab_test(models_for_ab, 50)
export_ab_test_csv(ab_tests, "ab_test_design.csv")
```

#### 5.2.2 Mean Opinion Score (MOS)

**è³ªå•**: ã€Œã“ã®ç”»åƒã®å“è³ªã‚’1-5ã§è©•ä¾¡ã—ã¦ãã ã•ã„ã€

**è¨­è¨ˆ**:
1. Likert scale (1=æœ€æ‚ª, 5=æœ€é«˜)
2. è¤‡æ•°è©•ä¾¡è€…ï¼ˆâ‰¥3äººï¼‰ã§å¹³å‡
3. ä¿¡é ¼åŒºé–“è¨ˆç®—

```julia
# MOS collection and analysis
struct MOSResult
    image_id::Int
    model::String
    ratings::Vector{Int}  # 1-5 from multiple raters
end

function analyze_mos(results::Vector{MOSResult})
    println("ğŸ“Š MOS Analysis:")
    println("| Model | Mean MOS | Std | 95% CI |")
    println("|:------|:---------|:----|:-------|")

    for model in unique([r.model for r in results])
        model_ratings = vcat([r.ratings for r in results if r.model == model]...)
        Î¼ = mean(model_ratings)
        Ïƒ = std(model_ratings)
        n = length(model_ratings)
        se = Ïƒ / sqrt(n)
        ci_margin = 1.96 * se
        println("| $model | $(round(Î¼, digits=2)) | $(round(Ïƒ, digits=2)) | " *
                "[$(round(Î¼ - ci_margin, digits=2)), $(round(Î¼ + ci_margin, digits=2))] |")
    end
end

# Simulate MOS data
mos_data = [
    MOSResult(1, "VAE", [3, 3, 4, 3, 3]),
    MOSResult(2, "VAE", [3, 4, 3, 3, 4]),
    MOSResult(3, "GAN", [4, 5, 4, 4, 5]),
    MOSResult(4, "GAN", [5, 4, 5, 4, 5]),
    MOSResult(5, "AR", [4, 4, 5, 4, 4]),
    MOSResult(6, "AR", [4, 5, 4, 5, 4]),
]

analyze_mos(mos_data)
```

#### 5.2.3 è©•ä¾¡è€…é–“ä¸€è‡´åº¦ (Inter-rater Reliability)

**Fleiss' Kappa** (ç¬¬24å›) â€” è¤‡æ•°è©•ä¾¡è€…ã®ä¸€è‡´åº¦ã€‚

```julia
# Fleiss' Kappa for inter-rater reliability
using Statistics

function fleiss_kappa(ratings::Matrix{Int})
    # ratings: (n_items, n_raters)
    n_items, n_raters = size(ratings)
    n_categories = maximum(ratings)

    # Proportion of agreement per item
    P_i = zeros(n_items)
    for i in 1:n_items
        counts = [sum(ratings[i,:] .== k) for k in 1:n_categories]
        P_i[i] = (sum(counts.^2) - n_raters) / (n_raters * (n_raters - 1))
    end
    P_bar = mean(P_i)

    # Expected agreement by chance
    p_j = zeros(n_categories)
    for j in 1:n_categories
        p_j[j] = sum(ratings .== j) / (n_items * n_raters)
    end
    P_e = sum(p_j.^2)

    # Kappa
    Îº = (P_bar - P_e) / (1 - P_e)
    return Îº
end

# Test
ratings_test = [
    1 2 1 1;  # item 1: raters gave 1,2,1,1
    2 2 2 2;  # item 2: all agree on 2
    3 3 4 3;  # item 3: mostly 3
]
Îº = fleiss_kappa(ratings_test)
println("Fleiss' Kappa: $(round(Îº, digits=3))")
println("Interpretation: Îº < 0.2 = poor, 0.2-0.4 = fair, 0.4-0.6 = moderate, 0.6-0.8 = substantial, > 0.8 = almost perfect")
```

:::message
**é€²æ—: 85% å®Œäº†** å®Ÿé¨“ã‚¾ãƒ¼ãƒ³å®Œäº† â€” VAE/GAN/ARçµ±åˆè©•ä¾¡ + äººé–“è©•ä¾¡ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã€‚ã“ã“ã‹ã‚‰ç™ºå±•ã‚¾ãƒ¼ãƒ³ã¸ â€” æœ€æ–°ç ”ç©¶å‹•å‘ã€‚
:::

---

## ğŸ“ 6. æŒ¯ã‚Šè¿”ã‚Šã¨ç™ºå±•ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨æœ€æ–°ç ”ç©¶å‹•å‘

### 6.1 FLD+ (Flow-based Likelihood Distance)

**è«–æ–‡** [^7]: FLD+: Data-efficient Evaluation Metric for Generative Models (2024)

**å‹•æ©Ÿ**: FIDã¯2000+ã‚µãƒ³ãƒ—ãƒ«å¿…è¦ â†’ å°‘ã‚µãƒ³ãƒ—ãƒ«ã§å®‰å®šã™ã‚‹æŒ‡æ¨™ãŒæ¬²ã—ã„ã€‚

**ã‚¢ã‚¤ãƒ‡ã‚¢**: Normalizing Flowã§å¯†åº¦æ¨å®š â†’ å°¤åº¦ãƒ™ãƒ¼ã‚¹ã®è·é›¢ã€‚

**å®šç¾©**:

$$
\text{FLD}(P_r, P_g) = \mathbb{E}_{x \sim P_r}[-\log q_\theta(x)] - \mathbb{E}_{x \sim P_g}[-\log q_\theta(x)]
$$

ã“ã“ã§ $q_\theta$ ã¯Normalizing Flowã§è¨“ç·´ã•ã‚ŒãŸå¯†åº¦ãƒ¢ãƒ‡ãƒ«ï¼ˆçœŸç”»åƒã§è¨“ç·´ï¼‰ã€‚

**åˆ©ç‚¹**:
- 200-500ã‚µãƒ³ãƒ—ãƒ«ã§å®‰å®šï¼ˆFIDã¯2000+å¿…è¦ï¼‰
- ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œå¯èƒ½ï¼ˆåŒ»ç™‚ç”»åƒãªã©ã§å†è¨“ç·´ï¼‰
- å˜èª¿æ€§ãŒå¼·ã„ï¼ˆç”»åƒåŠ£åŒ–ã«å¯¾ã—ã¦ï¼‰

### 6.2 è©•ä¾¡æŒ‡æ¨™ã®ç ”ç©¶ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢

**2024-2026ã®ãƒˆãƒ¬ãƒ³ãƒ‰**:

| ç ”ç©¶æ–¹å‘ | ä»£è¡¨è«–æ–‡ | æ¦‚è¦ |
|:---------|:---------|:-----|
| **ä»®å®šãªã—æŒ‡æ¨™** | CMMD [^5], NFM [^8] | MMD/Flowãƒ™ãƒ¼ã‚¹ã€æ­£è¦æ€§ä¸è¦ |
| **å°‘ã‚µãƒ³ãƒ—ãƒ«æŒ‡æ¨™** | FLD+ [^7] | 200ã‚µãƒ³ãƒ—ãƒ«ã§å®‰å®š |
| **ãƒ†ã‚­ã‚¹ãƒˆå¯¾å¿œ** | CMMD-CLIP [^5] | Text-to-Imageç”Ÿæˆå¯¾å¿œ |
| **åˆ†é›¢è©•ä¾¡** | Precision-Recall Cover [^9] | å“è³ªãƒ»å¤šæ§˜æ€§ãƒ»è¢«è¦†ç‡ã‚’åˆ†é›¢ |
| **äººé–“è©•ä¾¡äºˆæ¸¬** | ImageReward, PickScore | äººé–“è©•ä¾¡ã‚’ãƒ¢ãƒ‡ãƒ«åŒ– |

### 6.3 ç”Ÿæˆãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã®ç³»è­œ

```mermaid
graph TD
    A[2014: Inception Score] --> B[2017: FID]
    B --> C[2019: Precision-Recall]
    C --> D[2024: CMMD]
    D --> E[2024: FLD+]

    A2[ä»®å®š: ImageNetåˆ†é¡] -.->|é™ç•Œ| B2[ä»®å®š: ã‚¬ã‚¦ã‚¹æ€§]
    B2 -.->|é™ç•Œ| C2[è¨ˆç®—ã‚³ã‚¹ãƒˆé«˜]
    C2 -.->|é™ç•Œ| D2[ä»®å®šãªã—<br/>CLIPåŸ‹ã‚è¾¼ã¿]
    D2 --> E2[å°‘ã‚µãƒ³ãƒ—ãƒ«<br/>Flowå¯†åº¦]

    style D fill:#c8e6c9
    style E fill:#b3e5fc
```

### 6.4 è©•ä¾¡æŒ‡æ¨™ã®é¸æŠã‚¬ã‚¤ãƒ‰ï¼ˆ2026å¹´ç‰ˆï¼‰

| çŠ¶æ³ | æ¨å¥¨æŒ‡æ¨™ | ç†ç”± |
|:-----|:---------|:-----|
| **æ¨™æº–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆImageNetç­‰ï¼‰** | FID + IS | æ¯”è¼ƒå¯èƒ½æ€§é‡è¦– |
| **æ–°è¦ç ”ç©¶ï¼ˆ2024ä»¥é™ï¼‰** | **CMMD** + FID | FIDã®é™ç•Œã‚’è£œå®Œ [^5] |
| **å°‘ã‚µãƒ³ãƒ—ãƒ«ï¼ˆ<1000ï¼‰** | **FLD+** | 200ã‚µãƒ³ãƒ—ãƒ«ã§å®‰å®š [^7] |
| **Text-to-Image** | **CMMD-CLIP** | ãƒ†ã‚­ã‚¹ãƒˆ-ç”»åƒå¯¾å¿œ [^5] |
| **å“è³ªvså¤šæ§˜æ€§åˆ†æ** | **Precision-Recall** | ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’å¯è¦–åŒ– [^4] |
| **ãƒšã‚¢wiseæ¯”è¼ƒ** | **LPIPS** | äººé–“çŸ¥è¦šã¨ç›¸é–¢ [^3] |
| **ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ï¼ˆåŒ»ç™‚ç­‰ï¼‰** | FLD+ (å†è¨“ç·´) | ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œ [^7] |
| **äººé–“è©•ä¾¡ä»£æ›¿** | ImageReward / PickScore | äººé–“è©•ä¾¡äºˆæ¸¬ãƒ¢ãƒ‡ãƒ« |

:::message
**é€²æ—: 95% å®Œäº†** ç™ºå±•ã‚¾ãƒ¼ãƒ³å®Œäº† â€” æœ€æ–°ç ”ç©¶å‹•å‘ã€‚ã“ã“ã‹ã‚‰æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ã¸ã€‚
:::

---

### 6.6 ã¾ã¨ã‚ â€” 5ã¤ã®è¦ç‚¹

1. **è©•ä¾¡ã¯å¤šé¢çš„**: FID/IS/LPIPS/P&R/CMMD â€” å„æŒ‡æ¨™ã¯ç•°ãªã‚‹å´é¢ã‚’æ¸¬å®šã€‚è¤‡æ•°æŒ‡æ¨™ã‚’çµ„ã¿åˆã‚ã›ã¦ç·åˆåˆ¤æ–­ã€‚

2. **æ•°å¼ã®ç†è§£ãŒæœ¬è³ª**: FID = Wassersteinè·é›¢ã®ã‚¬ã‚¦ã‚¹é–‰å½¢å¼ã€‚IS = KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®æœŸå¾…å€¤ã€‚CMMD = MMD + CLIPã€‚æ•°å¼ã‚’å°å‡ºã™ã‚Œã°ã€æŒ‡æ¨™ã®ä»®å®šã¨é™ç•ŒãŒè¦‹ãˆã‚‹ã€‚

3. **çµ±è¨ˆæ¤œå®šãŒä¸å¯æ¬ **: FIDã®ç‚¹æ¨å®šã ã‘ã§ã¯ä¸ååˆ†ã€‚ä¿¡é ¼åŒºé–“ãƒ»ä»®èª¬æ¤œå®šãƒ»åŠ¹æœé‡ã§å®Ÿè³ªçš„ãªæ”¹å–„ã‚’åˆ¤æ–­ã€‚

4. **2024å¹´ã®è»¢æ›ç‚¹**: FIDã®é™ç•Œ â†’ CMMD/FLD+ç™»å ´ã€‚æ­£è¦æ€§ä»®å®šã®æ’é™¤ãƒ»å°‘ã‚µãƒ³ãƒ—ãƒ«å¯¾å¿œãƒ»ãƒ†ã‚­ã‚¹ãƒˆå¯¾å¿œã€‚

5. **è‡ªå‹•åŒ–ãŒéµ**: è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆJuliaçµ±è¨ˆ + Rust Criterionï¼‰ã‚’CIçµ±åˆ â†’ ç¶™ç¶šçš„ãªå“è³ªç›£è¦–ã€‚

### 6.7 FAQ â€” ã‚ˆãã‚ã‚‹è³ªå•

:::details Q1: FIDãŒä½ã„ã®ã«ISãŒé«˜ã„ â€” ã©ã¡ã‚‰ã‚’ä¿¡ã˜ã‚‹ã¹ãï¼Ÿ

**A**: ä¸¡æ–¹ã¨ã‚‚æ­£ã—ã„å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚FIDã¯åˆ†å¸ƒå…¨ä½“ã®è·é›¢ã€ISã¯å“è³ª+å¤šæ§˜æ€§ã®å˜ä¸€ã‚¹ã‚³ã‚¢ã€‚

**ä¾‹**:
- FIDä½ + ISé«˜ â†’ ç†æƒ³çš„ï¼ˆåˆ†å¸ƒä¸€è‡´ + é«˜å“è³ªãƒ»å¤šæ§˜ï¼‰
- FIDä½ + ISä½ â†’ åˆ†å¸ƒã¯è¿‘ã„ãŒã€å“è³ªorå¤šæ§˜æ€§ãŒä½ã„
- FIDé«˜ + ISé«˜ â†’ mode collapseã®å¯èƒ½æ€§ï¼ˆå°‘æ•°ã®é«˜å“è³ªç”»åƒã®ã¿ç”Ÿæˆï¼‰

**å¯¾ç­–**: Precision-Recallã§å“è³ªã¨å¤šæ§˜æ€§ã‚’åˆ†é›¢æ¸¬å®šã€‚

:::

:::details Q2: CMMDã¯FIDã‚’å®Œå…¨ã«ç½®ãæ›ãˆã‚‰ã‚Œã‚‹ã‹ï¼Ÿ

**A**: å ´åˆã«ã‚ˆã‚‹ã€‚

**CMMDã®åˆ©ç‚¹** [^5]:
- æ­£è¦æ€§ä»®å®šãªã—
- äººé–“è©•ä¾¡ã¨ã®ç›¸é–¢ãŒé«˜ã„ï¼ˆ0.72 vs FID 0.56ï¼‰
- ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ä»˜ãç”Ÿæˆã«å¯¾å¿œ

**FIDã®åˆ©ç‚¹**:
- æ¨™æº–åŒ–ã•ã‚Œã¦ã„ã‚‹ï¼ˆéå»ã®ç ”ç©¶ã¨æ¯”è¼ƒå¯èƒ½ï¼‰
- è¨ˆç®—ã‚³ã‚¹ãƒˆä½ï¼ˆè¡Œåˆ—æ¼”ç®—ã®ã¿ï¼‰
- ãƒ„ãƒ¼ãƒ«ãŒè±Šå¯Œï¼ˆtorch-fidelityç­‰ï¼‰

**æ¨å¥¨**: æ–°è¦ç ”ç©¶ã§ã¯**CMMD + FIDä½µè¨˜**ã€‚FIDã¯æ¯”è¼ƒå¯èƒ½æ€§ã®ãŸã‚ã€CMMDã¯å®Ÿè³ªçš„ãªè©•ä¾¡ã®ãŸã‚ã€‚

:::

:::details Q3: ã‚µãƒ³ãƒ—ãƒ«æ•°ã¯ã©ã‚Œãã‚‰ã„å¿…è¦ï¼Ÿ

**A**: æŒ‡æ¨™ã«ã‚ˆã£ã¦ç•°ãªã‚‹ã€‚

| æŒ‡æ¨™ | æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•° | æ¨å¥¨ã‚µãƒ³ãƒ—ãƒ«æ•° | ç†ç”± |
|:-----|:--------------|:--------------|:-----|
| FID | 2000 | 5000+ | å…±åˆ†æ•£è¡Œåˆ—ã®å®‰å®šæ¨å®šã«å¿…è¦ |
| IS | 1000 | 5000+ | å‘¨è¾ºåˆ†å¸ƒ $p(y)$ ã®æ¨å®š |
| LPIPS | 1ãƒšã‚¢ | N/A | ãƒšã‚¢wiseæ¯”è¼ƒ |
| P&R | 1000 | 5000+ | k-NNå¤šæ§˜ä½“ã®å®‰å®šæ¨å®š |
| CMMD | 500 | 2000+ | MMDã¯FIDã‚ˆã‚Šå°‘ã‚µãƒ³ãƒ—ãƒ«ã§å®‰å®š |
| FLD+ | **200** | 1000 | Normalizing Flowã§åŠ¹ç‡çš„ [^7] |

**å°‘ã‚µãƒ³ãƒ—ãƒ«ã®å ´åˆ**: FLD+ [^7] ã‚’ä½¿ç”¨ã€‚

:::

:::details Q4: åŒ»ç™‚ç”»åƒã‚„ã‚¢ãƒ¼ãƒˆç”»åƒã§FIDã‚’ä½¿ã£ã¦ã„ã„ã‹ï¼Ÿ

**A**: æ³¨æ„ãŒå¿…è¦ã€‚

**å•é¡Œ**: Inception-v3ã¯ImageNetã§è¨“ç·´ â†’ è‡ªç„¶ç”»åƒãƒã‚¤ã‚¢ã‚¹ã€‚åŒ»ç™‚ç”»åƒï¼ˆXç·šã€MRIï¼‰ã‚„ã‚¢ãƒ¼ãƒˆç”»åƒã§ã¯ä¸é©åˆ‡ã€‚

**å¯¾ç­–**:
1. **ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ã®ç‰¹å¾´æŠ½å‡ºå™¨**: åŒ»ç™‚ç”»åƒã§è¨“ç·´ã—ãŸResNetãªã©
2. **CLIPåŸ‹ã‚è¾¼ã¿ï¼ˆCMMDï¼‰**: ã‚ˆã‚Šæ±ç”¨çš„
3. **FLD+ã§å†è¨“ç·´** [^7]: ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ã®Normalizing Flowã‚’è¨“ç·´

**ç ”ç©¶ä¾‹**: åŒ»ç™‚ç”»åƒGANã®è©•ä¾¡ã§ã¯ã€Inception-v3ã§ã¯ãªãRadImageNetï¼ˆXç·šã§è¨“ç·´ï¼‰ã‚’ä½¿ç”¨ã€‚

:::

:::details Q5: äººé–“è©•ä¾¡ã¨å®šé‡æŒ‡æ¨™ãŒçŸ›ç›¾ã—ãŸã‚‰ã©ã†ã™ã‚‹ï¼Ÿ

**A**: äººé–“è©•ä¾¡ã‚’å„ªå…ˆã€‚

**å®šé‡æŒ‡æ¨™ã®å½¹å‰²**:
- ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆå¤§é‡ã®ãƒ¢ãƒ‡ãƒ«ã‚’çµã‚Šè¾¼ã‚€ï¼‰
- ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡ºï¼ˆè¨“ç·´ä¸­ã®æ”¹å–„ã‚’ç›£è¦–ï¼‰
- å†ç¾æ€§ï¼ˆäººé–“è©•ä¾¡ã®ã‚³ã‚¹ãƒˆå‰Šæ¸›ï¼‰

**æœ€çµ‚åˆ¤æ–­**: äººé–“è©•ä¾¡ï¼ˆA/Bãƒ†ã‚¹ãƒˆã€MOSï¼‰ã€‚

**ãƒãƒ©ãƒ³ã‚¹**: é–‹ç™ºä¸­ã¯å®šé‡æŒ‡æ¨™ã§é«˜é€Ÿã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ â†’ æœ€çµ‚è©•ä¾¡ã§äººé–“è©•ä¾¡ã€‚

:::

### 6.8 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ1é€±é–“ï¼‰

| æ—¥ | å†…å®¹ | æ™‚é–“ | æˆæœç‰© |
|:---|:-----|:-----|:-------|
| 1æ—¥ç›® | Zone 0-2: æŒ‡æ¨™ã‚’è§¦ã‚‹ | 2h | 5æŒ‡æ¨™ã®è¨ˆç®—ã‚³ãƒ¼ãƒ‰ |
| 2-3æ—¥ç›® | Zone 3: æ•°å¼ä¿®è¡Œ | 4h | FID/IS/LPIPS/MMDå®Œå…¨å°å‡º |
| 4æ—¥ç›® | Zone 4: Juliaçµ±è¨ˆåˆ†æ | 3h | ä¿¡é ¼åŒºé–“ãƒ»t-testå®Ÿè£… |
| 5æ—¥ç›® | Zone 4: Rust Criterion | 2h | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ |
| 6æ—¥ç›® | Zone 5: çµ±åˆè©•ä¾¡ | 3h | VAE/GAN/ARæ¯”è¼ƒ |
| 7æ—¥ç›® | Zone 6-7: æœ€æ–°ç ”ç©¶+å¾©ç¿’ | 2h | ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ |

### 6.9 æ¬¡å›äºˆå‘Š â€” ç¬¬28å›: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°

**ç¬¬27å›ã§è©•ä¾¡åŸºç›¤ã‚’æ§‹ç¯‰ã—ãŸã€‚æ¬¡ã¯ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®åˆ¶å¾¡ â€” ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆã§LLMã‚’è‡ªåœ¨ã«æ“ã‚‹ã€‚**

**ç¬¬28å›ã®å†…å®¹**:
- XML + Markdownä½µç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆ
- Chain-of-Thought (CoT) ã¨Tree-of-Thought (ToT)
- System Promptè¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³
- Few-shotå­¦ç¿’ã¨In-context Learning
- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­–
- ğŸ¦€ Rustå®Ÿè£…: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚¨ãƒ³ã‚¸ãƒ³

```mermaid
graph LR
    A["ç¬¬27å›<br/>è©•ä¾¡åŸºç›¤"] --> B["ç¬¬28å›<br/>ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"]
    B --> C["ç¬¬29å›<br/>RAG"]
    C --> D["ç¬¬30å›<br/>ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"]
    D --> E["ç¬¬32å›<br/>çµ±åˆPJ"]
    style B fill:#fff3e0
    style E fill:#c8e6c9
```

:::message
**é€²æ—: 100% å®Œäº†ï¼ğŸ‰** ç¬¬27å›å®Œäº†ã€‚è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰ â€” FID/IS/LPIPS/P&R/CMMD/MMDã®ç†è«–ã¨å®Ÿè£…ã‚’ãƒã‚¹ã‚¿ãƒ¼ã—ãŸã€‚
:::

---

### 6.11 ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **æ•°å€¤ãŒæ”¹å–„ã™ã‚Œã°"è‰¯ã„"ãƒ¢ãƒ‡ãƒ«ã‹ï¼Ÿ**

**å¾“æ¥**: FIDâ†“ + ISâ†‘ = è‰¯ã„ãƒ¢ãƒ‡ãƒ«

**è»¢æ›**:

1. **å®šé‡æŒ‡æ¨™ã¯å¿…è¦æ¡ä»¶ã€ååˆ†æ¡ä»¶ã§ã¯ãªã„**
   - FID=5ã§ã‚‚äººé–“ãŒè¦‹ã¦ä¸è‡ªç„¶ãªç”»åƒã¯"æ‚ªã„"ãƒ¢ãƒ‡ãƒ«
   - äººé–“è©•ä¾¡ã¨å®šé‡æŒ‡æ¨™ã®ä¹–é›¢ã‚’å¸¸ã«æ„è­˜

2. **æŒ‡æ¨™ã¯ä»®å®šã‚’æŒã¤ â€” ä»®å®šãŒå´©ã‚Œã‚Œã°æŒ‡æ¨™ã‚‚å´©ã‚Œã‚‹**
   - FIDã®ã‚¬ã‚¦ã‚¹æ€§ä»®å®š â†’ å¤šå³°åˆ†å¸ƒã§å¤±æ•—
   - ISã®ImageNetåˆ†é¡ä¾å­˜ â†’ ãƒ‰ãƒ¡ã‚¤ãƒ³å¤–ã§ç„¡æ„å‘³
   - **æŒ‡æ¨™ã®æ•°å¼ã‚’ç†è§£ = ä»®å®šã‚’ç†è§£ = é™ç•Œã‚’çŸ¥ã‚‹**

3. **è©•ä¾¡ã¯å¤šé¢çš„ â€” ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’å¯è¦–åŒ–ã›ã‚ˆ**
   - Precision-Recallã§å“è³ªvså¤šæ§˜æ€§ã‚’åˆ†é›¢
   - å˜ä¸€ã‚¹ã‚³ã‚¢ã«é›†ç´„ã™ã‚‹ãªï¼ˆISã®ç½ ï¼‰

**ã‚ãªãŸã¸ã®å•ã„**:

- è«–æ–‡ã®FIDæ”¹å–„ã‚’è¦‹ãŸã¨ãã€ã€Œã‚µãƒ³ãƒ—ãƒ«æ•°ã¯ï¼Ÿã€ã€Œä¿¡é ¼åŒºé–“ã¯ï¼Ÿã€ã€Œäººé–“è©•ä¾¡ã¨ã®ç›¸é–¢ã¯ï¼Ÿã€ã¨å•ãˆã‚‹ã‹ï¼Ÿ
- è‡ªåˆ†ã®ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã™ã‚‹ã¨ãã€è¤‡æ•°æŒ‡æ¨™ã‚’è¦‹ã¦ç·åˆåˆ¤æ–­ã§ãã‚‹ã‹ï¼Ÿ
- æ–°ã—ã„ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆåŒ»ç™‚ç”»åƒã€éŸ³å£°ï¼‰ã§ã€é©åˆ‡ãªè©•ä¾¡æŒ‡æ¨™ã‚’é¸æŠãƒ»è¨­è¨ˆã§ãã‚‹ã‹ï¼Ÿ

**æ¬¡ã®ä¸€æ­©**: è©•ä¾¡ã¯æ‰‹æ®µã§ã‚ã£ã¦ç›®çš„ã§ã¯ãªã„ã€‚è©•ä¾¡åŸºç›¤ã‚’æ•´ãˆãŸä»Šã€**ä½•ã‚’ä½œã‚‹ã‹**ã«é›†ä¸­ã›ã‚ˆã€‚ç¬¬32å›ã®çµ±åˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã€è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿæˆ¦æŠ•å…¥ã™ã‚‹ã€‚

:::message
**é€²æ—: 100% å®Œäº†** ğŸ‰ è¬›ç¾©å®Œèµ°ï¼
:::

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., & Hochreiter, S. (2017). GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium. *NeurIPS 2017*.
@[card](https://arxiv.org/abs/1706.08500)

[^2]: Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., & Chen, X. (2016). Improved Techniques for Training GANs. *NeurIPS 2016*.
@[card](https://arxiv.org/abs/1609.03126)

[^3]: Zhang, R., Isola, P., Efros, A. A., Shechtman, E., & Wang, O. (2018). The Unreasonable Effectiveness of Deep Features as a Perceptual Metric. *CVPR 2018*.
@[card](https://arxiv.org/abs/1801.03924)

[^4]: KynkÃ¤Ã¤nniemi, T., Karras, T., Laine, S., Lehtinen, J., & Aila, T. (2019). Improved Precision and Recall Metric for Assessing Generative Models. *NeurIPS 2019*.
@[card](https://arxiv.org/abs/1904.06991)

[^5]: Jayasumana, S., Ramalingam, S., Veit, A., Glasner, D., Chakrabarti, A., & Kumar, S. (2024). Rethinking FID: Towards a Better Evaluation Metric for Image Generation. *CVPR 2024*.
@[card](https://arxiv.org/abs/2401.09603)

[^6]: Gretton, A., Borgwardt, K. M., Rasch, M. J., SchÃ¶lkopf, B., & Smola, A. (2012). A Kernel Two-Sample Test. *Journal of Machine Learning Research*.
@[card](https://www.jmlr.org/papers/v13/gretton12a.html)

[^7]: Pranav, P., et al. (2024). FLD+: Data-efficient Evaluation Metric for Generative Models. *arXiv:2411.15584*.
@[card](https://arxiv.org/abs/2411.15584)

[^8]: Pranav, P., et al. (2024). Normalizing Flow-Based Metric for Image Generation. *arXiv:2410.02004*.
@[card](https://arxiv.org/abs/2410.02004)

[^9]: Cheema, G. S., et al. (2023). Unifying and Extending Precision Recall Metrics for Assessing Generative Models. *AISTATS 2023*.
@[card](https://proceedings.mlr.press/v206/cheema23a.html)

### å®Ÿè£…ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

- [torch-fidelity](https://github.com/toshas/torch-fidelity) â€” PyTorch FID/ISå®Ÿè£…
- [lpips](https://github.com/richzhang/PerceptualSimilarity) â€” LPIPSå…¬å¼å®Ÿè£…
- [Criterion.rs](https://github.com/bheisler/criterion.rs) â€” Rustçµ±è¨ˆçš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
- [HypothesisTests.jl](https://github.com/JuliaStats/HypothesisTests.jl) â€” Juliaçµ±è¨ˆæ¤œå®š

### æ•™ç§‘æ›¸

- Murphy, K. P. (2022). *Probabilistic Machine Learning: Advanced Topics*. MIT Press. [Chapter 20: Evaluation of Generative Models]
- Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press. [Chapter 20: Deep Generative Models]

---

## è¨˜æ³•è¦ç´„

| è¨˜æ³• | æ„å‘³ | ä½¿ç”¨ä¾‹ |
|:-----|:-----|:-------|
| $P_r, P_g$ | çœŸç”»åƒã®åˆ†å¸ƒã€ç”Ÿæˆç”»åƒã®åˆ†å¸ƒ | $\text{FID}(P_r, P_g)$ |
| $\mu, \Sigma$ | å¹³å‡ãƒ™ã‚¯ãƒˆãƒ«ã€å…±åˆ†æ•£è¡Œåˆ— | $\mathcal{N}(\mu, \Sigma)$ |
| $\text{Tr}(A)$ | è¡Œåˆ— $A$ ã®ãƒˆãƒ¬ãƒ¼ã‚¹ | $\text{Tr}(\Sigma)$ |
| $\|\cdot\|_2$ | L2ãƒãƒ«ãƒ  | $\|\mu_r - \mu_g\|_2^2$ |
| $\text{KL}(P \| Q)$ | KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ | $\text{KL}(p(y|x) \| p(y))$ |
| $\mathbb{E}_{x \sim P}[\cdot]$ | åˆ†å¸ƒ $P$ ã«é–¢ã™ã‚‹æœŸå¾…å€¤ | $\mathbb{E}_{x \sim p_g}[f(x)]$ |
| $k(x, y)$ | ã‚«ãƒ¼ãƒãƒ«é–¢æ•° | $k(x, y) = \exp(-\|x - y\|^2 / 2\sigma^2)$ |
| $\mathcal{H}$ | å†ç”Ÿæ ¸ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆç©ºé–“ (RKHS) | $\mu_P \in \mathcal{H}$ |
| $\text{MMD}(P, Q)$ | Maximum Mean Discrepancy | $\text{MMD}^2(P, Q) = \|\mu_P - \mu_Q\|_{\mathcal{H}}^2$ |
| $p(y|x)$ | æ¡ä»¶ä»˜ãåˆ†å¸ƒï¼ˆInceptionåˆ†é¡ï¼‰ | Inception Scoreå®šç¾© |
| $W_2(P, Q)$ | 2-Wassersteinè·é›¢ | FIDã®ç†è«–çš„åŸºç›¤ |
| $\sigma$ | ã‚«ãƒ¼ãƒãƒ«å¸¯åŸŸå¹…ï¼ˆRBFï¼‰ | Median heuristic |
| $\alpha$ | æœ‰æ„æ°´æº– | Bonferroniè£œæ­£ $\alpha' = \alpha / N$ |

---

### 6.10 è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

#### 7.5.1 æ•°å¼èª­è§£ãƒ†ã‚¹ãƒˆï¼ˆ10å•ï¼‰

**å•1**: FIDã®å¼ $\text{FID} = \|\mu_r - \mu_g\|^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r\Sigma_g)^{1/2})$ ã§ã€ç¬¬1é … $\|\mu_r - \mu_g\|^2$ ã¯ä½•ã‚’æ¸¬å®šã—ã¦ã„ã‚‹ã‹ï¼Ÿ

:::details è§£ç­”
**ç­”ãˆ**: 2ã¤ã®åˆ†å¸ƒã®å¹³å‡ãƒ™ã‚¯ãƒˆãƒ«é–“ã®ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã®äºŒä¹—ã€‚åˆ†å¸ƒã®ä¸­å¿ƒãŒã©ã‚Œã ã‘ãšã‚Œã¦ã„ã‚‹ã‹ã‚’æ¸¬å®šã€‚

**è©³ç´°**: ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ $\mathcal{N}(\mu_r, \Sigma_r)$ ã¨ $\mathcal{N}(\mu_g, \Sigma_g)$ ã®å¹³å‡ãƒ™ã‚¯ãƒˆãƒ« $\mu_r, \mu_g \in \mathbb{R}^d$ ã®è·é›¢ã€‚$\mu_r = \mu_g$ ãªã‚‰ç¬¬1é … = 0ã€‚
:::

**å•2**: ISã®å¼ $\text{IS} = \exp(\mathbb{E}_{x}[\text{KL}(p(y|x) \| p(y))])$ ã§ã€$p(y|x)$ ã¨ $p(y)$ ã®é•ã„ã¯ï¼Ÿ

:::details è§£ç­”
**ç­”ãˆ**:
- $p(y|x)$: ç”»åƒ $x$ ã«å¯¾ã™ã‚‹Inception-v3ã®æ¡ä»¶ä»˜ãäºˆæ¸¬åˆ†å¸ƒï¼ˆsoftmax outputï¼‰
- $p(y) = \mathbb{E}_x[p(y|x)]$: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã§ã®äºˆæ¸¬åˆ†å¸ƒã®å¹³å‡ï¼ˆå‘¨è¾ºåˆ†å¸ƒï¼‰

**ç›´æ„Ÿ**: $p(y|x)$ ãŒã‚·ãƒ£ãƒ¼ãƒ—ï¼ˆé«˜confidenceï¼‰ã‹ã¤ $p(y)$ ãŒå‡ä¸€ï¼ˆå¤šæ§˜ãªã‚¯ãƒ©ã‚¹ï¼‰ãªã‚‰ IS ãŒé«˜ã„ã€‚
:::

**å•3**: MMDã®å±•é–‹å¼ $\text{MMD}^2 = \mathbb{E}_{x,x'}[k(x,x')] + \mathbb{E}_{y,y'}[k(y,y')] - 2\mathbb{E}_{x,y}[k(x,y)]$ ã§ã€å„é …ã®æ„å‘³ã¯ï¼Ÿ

:::details è§£ç­”
**ç­”ãˆ**:
- ç¬¬1é … $\mathbb{E}_{x,x' \sim P}[k(x,x')]$: çœŸç”»åƒåˆ†å¸ƒå†…ã®ã‚«ãƒ¼ãƒãƒ«é¡ä¼¼åº¦ã®æœŸå¾…å€¤
- ç¬¬2é … $\mathbb{E}_{y,y' \sim Q}[k(y,y')]$: ç”Ÿæˆç”»åƒåˆ†å¸ƒå†…ã®ã‚«ãƒ¼ãƒãƒ«é¡ä¼¼åº¦ã®æœŸå¾…å€¤
- ç¬¬3é … $-2\mathbb{E}_{x \sim P, y \sim Q}[k(x,y)]$: 2ã¤ã®åˆ†å¸ƒé–“ã®ã‚«ãƒ¼ãƒãƒ«é¡ä¼¼åº¦ã®æœŸå¾…å€¤ï¼ˆè² ï¼‰

**ç›´æ„Ÿ**: åˆ†å¸ƒå†…é¡ä¼¼åº¦ã®å’Œ - åˆ†å¸ƒé–“é¡ä¼¼åº¦ Ã— 2 = åˆ†å¸ƒé–“è·é›¢ã€‚
:::

**å•4**: LPIPSã®å¼ $d = \sum_\ell w_\ell \frac{1}{H_\ell W_\ell}\sum_{h,w}\|f_\ell(x) - f_\ell(x_0)\|^2$ ã§ã€$\ell$ ã¯ä½•ã‚’è¡¨ã™ã‹ï¼Ÿ

:::details è§£ç­”
**ç­”ãˆ**: VGG/AlexNetã®å±¤ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€‚è¤‡æ•°ã®å±¤ï¼ˆæµ…ã„å±¤ + æ·±ã„å±¤ï¼‰ã®ç‰¹å¾´ã‚’ä½¿ã†ã€‚

**ç†ç”±**: æµ…ã„å±¤ = edge, texture / æ·±ã„å±¤ = semantic contentã€‚ä¸¡æ–¹ã®æƒ…å ±ã‚’çµ±åˆã™ã‚‹ã“ã¨ã§äººé–“ã®çŸ¥è¦šã«è¿‘ã„è·é›¢ã‚’æ¸¬å®šã€‚
:::

**å•5**: Precision-Recallã§ã€Precision = 1.0, Recall = 0.3 ã®æ„å‘³ã¯ï¼Ÿ

:::details è§£ç­”
**ç­”ãˆ**:
- Precision = 1.0: ç”Ÿæˆç”»åƒã¯å…¨ã¦çœŸç”»åƒã®å¤šæ§˜ä½“ã«å«ã¾ã‚Œã‚‹ â†’ **é«˜å“è³ª**
- Recall = 0.3: çœŸç”»åƒã®30%ã—ã‹ç”Ÿæˆç”»åƒã®å¤šæ§˜ä½“ã«ã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ãªã„ â†’ **ä½å¤šæ§˜æ€§ï¼ˆmode collapseï¼‰**

**å…¸å‹ä¾‹**: GANãŒå°‘æ•°ã®ãƒ¢ãƒ¼ãƒ‰ã«é›†ä¸­ã—ã¦é«˜å“è³ªç”»åƒã‚’ç”Ÿæˆã™ã‚‹ãŒã€å…¨ä½“ã®åˆ†å¸ƒã‚’ã‚«ãƒãƒ¼ã—ã¦ã„ãªã„ã€‚
:::

**å•6**: FIDã§ã‚¬ã‚¦ã‚¹æ€§ã®ä»®å®šãŒå´©ã‚Œã‚‹ã¨ã©ã†ãªã‚‹ã‹ï¼Ÿ

:::details è§£ç­”
**ç­”ãˆ**: å¤šå³°åˆ†å¸ƒã‚’å˜ä¸€ã‚¬ã‚¦ã‚¹ã§è¿‘ä¼¼ â†’ æƒ…å ±æå¤± â†’ FIDãŒå®Ÿéš›ã®åˆ†å¸ƒè·é›¢ã‚’æ­£ã—ãåæ˜ ã—ãªã„ã€‚

**ä¾‹**: 2ã¤ã®ãƒ¢ãƒ¼ãƒ‰ã‚’æŒã¤åˆ†å¸ƒï¼ˆçŒ«ã¨çŠ¬ã®2ã‚¯ãƒ©ã‚¹ï¼‰ã‚’å˜ä¸€ã‚¬ã‚¦ã‚¹ã§è¿‘ä¼¼ã™ã‚‹ã¨ã€ãƒ¢ãƒ¼ãƒ‰é–“ã®è·é›¢æƒ…å ±ãŒå¤±ã‚ã‚Œã‚‹ã€‚

**å¯¾ç­–**: CMMDï¼ˆMMDãƒ™ãƒ¼ã‚¹ã€ä»®å®šãªã—ï¼‰ã‚’ä½¿ç”¨ [^5]ã€‚
:::

**å•7**: CMMDãŒFIDã‚ˆã‚Šäººé–“è©•ä¾¡ã¨ç›¸é–¢ãŒé«˜ã„ç†ç”±ã¯ï¼Ÿ

:::details è§£ç­”
**ç­”ãˆ** [^5]:
1. **æ­£è¦æ€§ä»®å®šãªã—**: MMDã¯åˆ†å¸ƒã®å½¢çŠ¶ã«åˆ¶ç´„ãŒãªã„
2. **CLIPåŸ‹ã‚è¾¼ã¿**: Vision-Languageäº‹å‰è¨“ç·´ â†’ ã‚ˆã‚Šæ±ç”¨çš„ãªç‰¹å¾´ç©ºé–“
3. **ãƒ†ã‚­ã‚¹ãƒˆå¯¾å¿œ**: Text-to-Imageç”Ÿæˆã§ã€ãƒ†ã‚­ã‚¹ãƒˆã¨ã®æ•´åˆæ€§ã‚‚è©•ä¾¡å¯èƒ½

**å®Ÿé¨“çµæœ**: Pearsonç›¸é–¢ â€” CMMD: 0.72 vs FID: 0.56 [^5]
:::

**å•8**: Bootstrapã§ä¿¡é ¼åŒºé–“ã‚’è¨ˆç®—ã™ã‚‹æ‰‹é †ã¯ï¼Ÿ

:::details è§£ç­”
**æ‰‹é †**:
1. å…ƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰**å¾©å…ƒæŠ½å‡º**ã§ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã¯å…ƒã¨åŒã˜ï¼‰
2. ãƒªã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§çµ±è¨ˆé‡ï¼ˆFIDãªã©ï¼‰ã‚’è¨ˆç®—
3. æ‰‹é †1-2ã‚’Bå›ç¹°ã‚Šè¿”ã—ï¼ˆä¾‹: B=1000ï¼‰
4. Bå€‹ã®çµ±è¨ˆé‡ã®åˆ†å¸ƒã‹ã‚‰ä¿¡é ¼åŒºé–“ã‚’è¨ˆç®—ï¼ˆä¾‹: 95%CI = 2.5percentile, 97.5percentileï¼‰

**æ•°å¼**: $\text{CI}_{95\%} = [\text{quantile}_{0.025}(\hat{\theta}^*), \text{quantile}_{0.975}(\hat{\theta}^*)]$
:::

**å•9**: Bonferroniè£œæ­£ã§ã€4ãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒã™ã‚‹å ´åˆã®è£œæ­£å¾Œã®æœ‰æ„æ°´æº–ã¯ï¼Ÿï¼ˆå…ƒã® $\alpha = 0.05$ï¼‰

:::details è§£ç­”
**ç­”ãˆ**: $\alpha' = \alpha / N_{\text{comp}}$ where $N_{\text{comp}} = \binom{4}{2} = 6$ (ãƒšã‚¢wiseæ¯”è¼ƒæ•°)

$$
\alpha' = 0.05 / 6 \approx 0.0083
$$

**ç†ç”±**: å¤šé‡æ¤œå®šã§ç¬¬1ç¨®éèª¤ï¼ˆå½é™½æ€§ï¼‰ã‚’åˆ¶å¾¡ã™ã‚‹ãŸã‚ã€å„æ¤œå®šã®æœ‰æ„æ°´æº–ã‚’å³ã—ãã™ã‚‹ã€‚
:::

**å•10**: FLD+ãŒFIDã‚ˆã‚Šå°‘ã‚µãƒ³ãƒ—ãƒ«ã§å®‰å®šã™ã‚‹ç†ç”±ã¯ï¼Ÿ

:::details è§£ç­”
**ç­”ãˆ** [^7]:
- **FID**: å…±åˆ†æ•£è¡Œåˆ— $\Sigma \in \mathbb{R}^{d \times d}$ ã®æ¨å®šã« $O(d^2)$ ã‚µãƒ³ãƒ—ãƒ«å¿…è¦ï¼ˆd=2048 â†’ ç†è«–ä¸Š4Mï¼‰
- **FLD+**: Normalizing Flowã§å¯†åº¦ $q_\theta(x)$ ã‚’æ¨å®š â†’ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’åœ§ç¸® â†’ 200-500ã‚µãƒ³ãƒ—ãƒ«ã§å®‰å®š

**ä»•çµ„ã¿**: Flowã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§å¯†åº¦ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ â†’ å°‘ã‚µãƒ³ãƒ—ãƒ«ã§ã‚‚å°¤åº¦ãƒ™ãƒ¼ã‚¹ã®è·é›¢ãŒå®‰å®šã€‚
:::

#### 7.5.2 ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ†ã‚¹ãƒˆï¼ˆ5å•ï¼‰

**å•1**: ä»¥ä¸‹ã®æ•°å¼ã‚’Juliaã‚³ãƒ¼ãƒ‰ã«ç¿»è¨³ã›ã‚ˆã€‚

$$
\text{FID} = \|\mu_r - \mu_g\|_2^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2})
$$

:::details è§£ç­”

```julia
using LinearAlgebra

function fid(Î¼_r::Vector{Float64}, Î£_r::Matrix{Float64},
             Î¼_g::Vector{Float64}, Î£_g::Matrix{Float64})
    # Mean difference term
    diff = Î¼_r .- Î¼_g
    mean_term = sum(diff.^2)  # ||Î¼_r - Î¼_g||Â²

    # Covariance term: Tr(Î£_r + Î£_g - 2(Î£_r Î£_g)^{1/2})
    prod = Î£_r * Î£_g
    eig = eigen(prod)
    sqrt_eig = sqrt.(abs.(eig.values))
    sqrt_prod = eig.vectors * Diagonal(sqrt_eig) * eig.vectors'

    trace_term = tr(Î£_r) + tr(Î£_g) - 2*tr(sqrt_prod)

    return mean_term + trace_term
end
```
:::

**å•2**: ä»¥ä¸‹ã®Inception Scoreè¨ˆç®—ã‚’ã‚³ãƒ¼ãƒ‰ã§å®Ÿè£…ã›ã‚ˆã€‚

$$
\text{IS} = \exp\left(\mathbb{E}_{x}[\text{KL}(p(y|x) \| p(y))]\right)
$$

:::details è§£ç­”

```julia
function inception_score(p_yx::Matrix{Float64})
    # p_yx: (n_samples, n_classes)
    # p(y) = E_x[p(y|x)]
    p_y = vec(mean(p_yx, dims=1))

    # KL(p(y|x) || p(y)) for each sample
    n_samples = size(p_yx, 1)
    kl_divs = zeros(n_samples)
    for i in 1:n_samples
        for j in 1:length(p_y)
            if p_yx[i,j] > 1e-10 && p_y[j] > 1e-10
                kl_divs[i] += p_yx[i,j] * log(p_yx[i,j] / p_y[j])
            end
        end
    end

    # IS = exp(E[KL])
    return exp(mean(kl_divs))
end
```
:::

**å•3**: RBFã‚«ãƒ¼ãƒãƒ« $k(x,y) = \exp(-\|x-y\|^2/(2\sigma^2))$ ã‚’å®Ÿè£…ã›ã‚ˆã€‚

:::details è§£ç­”

```julia
function rbf_kernel(x::Vector{Float64}, y::Vector{Float64}, Ïƒ::Float64=1.0)
    # k(x, y) = exp(-||x - y||Â² / (2ÏƒÂ²))
    dist_sq = sum((x .- y).^2)
    return exp(-dist_sq / (2 * Ïƒ^2))
end
```
:::

**å•4**: Bootstrapã§95%ä¿¡é ¼åŒºé–“ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°ã‚’å®Ÿè£…ã›ã‚ˆã€‚

:::details è§£ç­”

```julia
using Statistics

function bootstrap_ci(data::Vector{Float64}, statistic::Function,
                       n_boot::Int=1000, confidence::Float64=0.95)
    n = length(data)
    boot_stats = zeros(n_boot)

    for b in 1:n_boot
        # Resample with replacement
        boot_sample = data[rand(1:n, n)]
        boot_stats[b] = statistic(boot_sample)
    end

    # Confidence interval
    Î± = 1 - confidence
    ci_lower = quantile(boot_stats, Î±/2)
    ci_upper = quantile(boot_stats, 1 - Î±/2)

    return ci_lower, ci_upper
end

# Example usage
# data = randn(100)
# ci_l, ci_u = bootstrap_ci(data, mean, 1000, 0.95)
```
:::

**å•5**: Welch's t-testã§2ã¤ã®FIDã‚µãƒ³ãƒ—ãƒ«ã‚’æ¯”è¼ƒã›ã‚ˆã€‚

:::details è§£ç­”

```julia
using HypothesisTests

function compare_fid(fid_a::Vector{Float64}, fid_b::Vector{Float64}, Î±::Float64=0.05)
    # Welch's t-test (unequal variances)
    test = UnequalVarianceTTest(fid_a, fid_b)
    p_val = pvalue(test)
    is_sig = p_val < Î±

    # Effect size (Cohen's d)
    Î¼_a, Î¼_b = mean(fid_a), mean(fid_b)
    s_a, s_b = std(fid_a), std(fid_b)
    pooled_std = sqrt((s_a^2 + s_b^2) / 2)
    cohens_d = (Î¼_a - Î¼_b) / pooled_std

    return Dict(
        "p_value" => p_val,
        "significant" => is_sig,
        "cohens_d" => cohens_d
    )
end
```
:::

#### 7.5.3 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ï¼ˆ2å•ï¼‰

**ãƒãƒ£ãƒ¬ãƒ³ã‚¸1**: è‡ªå‹•è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè£…ã—ã€VAE/GAN/ARã®3ãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒã›ã‚ˆã€‚å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: JSONï¼ˆFID/IS/CMMD/Precision/Recallï¼‰

:::details ãƒ’ãƒ³ãƒˆ

**æ‰‹é †**:
1. å„ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰1000ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ
2. Inceptionç‰¹å¾´æŠ½å‡º
3. å„æŒ‡æ¨™ã‚’è¨ˆç®—ï¼ˆFID, IS, CMMD, P&Rï¼‰
4. çµ±è¨ˆæ¤œå®šï¼ˆä¿¡é ¼åŒºé–“ã€t-testï¼‰
5. JSONå‡ºåŠ›

**ã‚³ãƒ¼ãƒ‰éª¨æ ¼**:

```julia
function auto_eval_pipeline(models::Dict{String, Function}, real_data::Vector, n_gen::Int=1000)
    results = Dict()
    for (name, gen_fn) in models
        samples = [gen_fn() for _ in 1:n_gen]
        fid, ci_l, ci_u, _ = fid_with_ci(real_data, samples)
        is_val, _ = inception_score(samples)
        # ... compute other metrics
        results[name] = Dict("fid" => fid, "fid_ci" => [ci_l, ci_u], ...)
    end
    return results
end
```
:::

**ãƒãƒ£ãƒ¬ãƒ³ã‚¸2**: Rust Criterionã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè£…ã—ã€FIDè¨ˆç®—ã®æ€§èƒ½å›å¸°ã‚’æ¤œå‡ºã›ã‚ˆã€‚

:::details ãƒ’ãƒ³ãƒˆ

**Cargo.toml**:

```toml
[dev-dependencies]
criterion = "0.5"
ndarray = "0.16"
ndarray-linalg = "0.19"

[[bench]]
name = "fid_bench"
harness = false
```

**benches/fid_bench.rs**:

```rust
use criterion::{black_box, criterion_group, criterion_main, Criterion};
use ndarray::{Array1, Array2};

fn benchmark_fid(c: &mut Criterion) {
    let d = 2048;
    let mu1 = Array1::zeros(d);
    let mu2 = Array1::ones(d) * 0.1;
    let sigma1 = Array2::eye(d);
    let sigma2 = Array2::eye(d) * 1.1;

    c.bench_function("fid_2048d", |b| {
        b.iter(|| frechet_distance(
            black_box(&mu1), black_box(&sigma1),
            black_box(&mu2), black_box(&sigma2)
        ))
    });
}

criterion_group!(benches, benchmark_fid);
criterion_main!(benches);
```

**å®Ÿè¡Œ**: `cargo bench` â†’ CIçµ±åˆã§è‡ªå‹•å›å¸°æ¤œå‡º
:::

### 6.6 é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼ï¼ˆè‡ªå·±è©•ä¾¡ï¼‰

**ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ** â€” å„é …ç›®ã‚’é”æˆã—ãŸã‚‰ãƒã‚§ãƒƒã‚¯:

```julia
# Progress tracker
checklist = [
    "âœ… Zone 0: FIDã‚’3è¡Œã§è¨ˆç®—ã§ãã‚‹",
    "âœ… Zone 1: 5ã¤ã®æŒ‡æ¨™ï¼ˆFID/IS/LPIPS/P&R/CMMDï¼‰ã‚’è§¦ã£ãŸ",
    "âœ… Zone 2: è©•ä¾¡ã®3ã¤ã®å›°é›£ã‚’ç†è§£ã—ãŸ",
    "âœ… Zone 3: FIDã®æ•°å¼ã‚’å®Œå…¨å°å‡ºã§ãã‚‹",
    "âœ… Zone 3: ISã®KLç™ºæ•£ã‚’å°å‡ºã§ãã‚‹",
    "âœ… Zone 3: LPIPSã®channel-wise normalizationã‚’ç†è§£ã—ãŸ",
    "âœ… Zone 3: Precision-Recallã®å¤šæ§˜ä½“ãƒ™ãƒ¼ã‚¹å®šç¾©ã‚’ç†è§£ã—ãŸ",
    "âœ… Zone 3: MMDã®ã‚«ãƒ¼ãƒãƒ«å±•é–‹ã‚’å°å‡ºã§ãã‚‹",
    "âœ… Zone 3: âš”ï¸ Boss Battle: CMMDè«–æ–‡ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ã‚’å†å®Ÿè£…ã—ãŸ",
    "âœ… Zone 4: Juliaã§ä¿¡é ¼åŒºé–“ã‚’è¨ˆç®—ã§ãã‚‹",
    "âœ… Zone 4: Juliaã§t-testã‚’å®Ÿè¡Œã§ãã‚‹",
    "âœ… Zone 4: Rust Criterionã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè£…ã§ãã‚‹",
    "âœ… Zone 5: VAE/GAN/ARã®çµ±åˆè©•ä¾¡ã‚’å®Ÿè£…ã—ãŸ",
    "âœ… Zone 5: A/Bãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’è¨­è¨ˆã—ãŸ",
    "âœ… Zone 5: MOSã‚’é›†è¨ˆãƒ»åˆ†æã—ãŸ",
    "âœ… Zone 6: CMMD/FLD+ã®æœ€æ–°ç ”ç©¶ã‚’ç†è§£ã—ãŸ",
    "âœ… Zone 7: è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆã‚’å…¨å•è§£ã„ãŸ",
    "âœ… Zone 7: å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ã‚’å®Œäº†ã—ãŸ",
]

completed = count(x -> startswith(x, "âœ…"), checklist)
total = length(checklist)
progress = round(100 * completed / total, digits=1)

println("Progress: $(completed)/$(total) ($(progress)%)")
if progress == 100.0
    println("ğŸ‰ ç¬¬27å›å®Œå…¨åˆ¶è¦‡ï¼")
end
```

**ç›®æ¨™é”æˆåŸºæº–**:

| ãƒ¬ãƒ™ãƒ« | é”æˆç‡ | åˆ°é”ç‚¹ |
|:-------|:------|:-------|
| **Level 1: ä½¿ãˆã‚‹** | 40% | FID/IS/LPIPSã‚’è¨ˆç®—ã§ãã‚‹ |
| **Level 2: ç†è§£ã—ã¦ã„ã‚‹** | 70% | æ•°å¼ã‚’å®Œå…¨å°å‡ºã§ãã‚‹ |
| **Level 3: è¨­è¨ˆã§ãã‚‹** | 100% | è‡ªå‹•è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã§ãã‚‹ |

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚

**ğŸ“ ç¬¬27å›å®Œäº†ï¼æ¬¡å›: ç¬¬28å› ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° â€” LLMåˆ¶å¾¡ã®æŠ€è¡“**