---
title: "ç¬¬2å›: ç·šå½¢ä»£æ•° I: ãƒ™ã‚¯ãƒˆãƒ«ãƒ»è¡Œåˆ—ãƒ»åŸºåº• â€” 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
emoji: "ğŸ“"
type: "tech"
topics: ["machinelearning", "deeplearning", "linearalgebra", "python"]
published: true
---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” NumPyã§ç·šå½¢ä»£æ•°ã‚’æ“ã‚‹

### 4.1 NumPy ã®ç·šå½¢ä»£æ•°ãƒ„ãƒ¼ãƒ«ã‚­ãƒƒãƒˆ

NumPyã® `np.linalg` ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€ç·šå½¢ä»£æ•°ã®ä¸»è¦ãªæ¼”ç®—ã‚’å…¨ã¦ã‚«ãƒãƒ¼ã—ã¦ã„ã‚‹ã€‚ã“ã“ã§ã¯å®Ÿç”¨ä¸Šæœ€ã‚‚é‡è¦ãªé–¢æ•°ã‚’æ•´ç†ã™ã‚‹ã€‚

| é–¢æ•° | æ•°å¼ | ç”¨é€” |
|:-----|:-----|:-----|
| `A @ B` | $AB$ | è¡Œåˆ—ç© |
| `np.linalg.inv(A)` | $A^{-1}$ | é€†è¡Œåˆ—ï¼ˆéæ¨å¥¨ã€solveã‚’ä½¿ãˆï¼‰ |
| `np.linalg.solve(A, b)` | $A^{-1}\mathbf{b}$ | é€£ç«‹æ–¹ç¨‹å¼ |
| `np.linalg.eigh(A)` | $A = Q\Lambda Q^\top$ | å¯¾ç§°è¡Œåˆ—ã®å›ºæœ‰å€¤åˆ†è§£ |
| `np.linalg.svd(A)` | $A = U\Sigma V^\top$ | ç‰¹ç•°å€¤åˆ†è§£ï¼ˆç¬¬3å›ï¼‰ |
| `np.linalg.qr(A)` | $A = QR$ | QRåˆ†è§£ |
| `np.linalg.cholesky(A)` | $A = LL^\top$ | Choleskyåˆ†è§£ |
| `np.linalg.norm(x)` | $\|\mathbf{x}\|$ | ãƒãƒ«ãƒ  |
| `np.linalg.det(A)` | $\det(A)$ | è¡Œåˆ—å¼ |
| `np.trace(A)` | $\text{tr}(A)$ | ãƒˆãƒ¬ãƒ¼ã‚¹ |
| `np.linalg.matrix_rank(A)` | $\text{rank}(A)$ | ãƒ©ãƒ³ã‚¯ |
| `np.linalg.lstsq(A, b)` | $\hat{\mathbf{x}} = \arg\min\|A\mathbf{x} - \mathbf{b}\|^2$ | æœ€å°äºŒä¹— |

:::message alert
Section 3.3 ã§è¿°ã¹ãŸã¨ãŠã‚Šã€é€†è¡Œåˆ—ã®ç›´æ¥è¨ˆç®—ã¯é¿ã‘ã¾ã—ã‚‡ã† [^8]ã€‚
:::

### 4.2 einsum â€” Einsteinè¨˜æ³•ã§è¡Œåˆ—æ¼”ç®—ã‚’ã‚¹ãƒãƒ¼ãƒˆã«æ›¸ã

`np.einsum` ã¯ Einstein è¨˜æ³•ï¼ˆæ·»å­—ã®ç¸®ç´„è¦å‰‡ï¼‰ã«åŸºã¥ãæ±ç”¨çš„ãªé…åˆ—æ¼”ç®—é–¢æ•°ã ã€‚ã“ã‚Œã‚’ä½¿ã„ã“ãªã™ã¨ã€è¤‡é›‘ãªè¡Œåˆ—æ¼”ç®—ã‚’ãƒ¯ãƒ³ãƒ©ã‚¤ãƒŠãƒ¼ã§æ›¸ã‘ã‚‹ã€‚

åŸºæœ¬ãƒ«ãƒ¼ãƒ«: **åŒã˜æ·»å­—ãŒ2å›ç¾ã‚ŒãŸã‚‰ã€ãã®æ·»å­—ã§ç·å’Œã‚’å–ã‚‹**ã€‚

| æ¼”ç®— | æ•°å¼ | einsum |
|:-----|:-----|:-------|
| å†…ç© | $\mathbf{a}^\top\mathbf{b} = \sum_i a_i b_i$ | `np.einsum('i,i->', a, b)` |
| å¤–ç© | $\mathbf{a}\mathbf{b}^\top$ | `np.einsum('i,j->ij', a, b)` |
| è¡Œåˆ—ç© | $C_{ij} = \sum_k A_{ik}B_{kj}$ | `np.einsum('ik,kj->ij', A, B)` |
| è¡Œåˆ—ã®ãƒˆãƒ¬ãƒ¼ã‚¹ | $\text{tr}(A) = \sum_i A_{ii}$ | `np.einsum('ii->', A)` |
| è¡Œåˆ—è»¢ç½® | $B_{ij} = A_{ji}$ | `np.einsum('ij->ji', A)` |
| ãƒãƒƒãƒè¡Œåˆ—ç© | $C_{bij} = \sum_k A_{bik}B_{bkj}$ | `np.einsum('bik,bkj->bij', A, B)` |
| äºŒæ¬¡å½¢å¼ | $\mathbf{x}^\top A \mathbf{x}$ | `np.einsum('i,ij,j->', x, A, x)` |

```python
import numpy as np

np.random.seed(42)
A = np.random.randn(3, 4)
B = np.random.randn(4, 5)
x = np.random.randn(3)
y = np.random.randn(3)

# å†…ç©: a^T b
dot_std = np.dot(x, y)
dot_ein = np.einsum('i,i->', x, y)
print(f"å†…ç© â€” dot: {dot_std:.4f}, einsum: {dot_ein:.4f}")

# è¡Œåˆ—ç©: AB
matmul_std = A @ B
matmul_ein = np.einsum('ik,kj->ij', A, B)
print(f"è¡Œåˆ—ç© â€” ä¸€è‡´: {np.allclose(matmul_std, matmul_ein)}")

# ãƒˆãƒ¬ãƒ¼ã‚¹
S = np.random.randn(3, 3)
tr_std = np.trace(S)
tr_ein = np.einsum('ii->', S)
print(f"ãƒˆãƒ¬ãƒ¼ã‚¹ â€” trace: {tr_std:.4f}, einsum: {tr_ein:.4f}")

# ãƒãƒƒãƒè¡Œåˆ—ç©ï¼ˆTransformerã§ãƒ˜ãƒƒãƒ‰ä¸¦åˆ—ã«ä½¿ã†ï¼‰
batch = 8
A_batch = np.random.randn(batch, 3, 4)
B_batch = np.random.randn(batch, 4, 5)
C_batch = np.einsum('bik,bkj->bij', A_batch, B_batch)
print(f"ãƒãƒƒãƒè¡Œåˆ—ç© shape: {C_batch.shape}")

# äºŒæ¬¡å½¢å¼: x^T A x
M = np.array([[2, 1], [1, 3]])
v = np.array([1.0, 2.0])
qf_std = v @ M @ v
qf_ein = np.einsum('i,ij,j->', v, M, v)
print(f"äºŒæ¬¡å½¢å¼ â€” @: {qf_std:.4f}, einsum: {qf_ein:.4f}")
```

:::details einsum vs @ æ¼”ç®—å­ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
å°ã•ãªè¡Œåˆ—ã§ã¯einsumã®æ–¹ãŒã‚ãšã‹ã«é…ã„ï¼ˆPythonå´ã®ãƒ‘ãƒ¼ã‚¹å‡¦ç†ãŒã‚ã‚‹ãŸã‚ï¼‰ã€‚å¤§ããªè¡Œåˆ—ã‚„ãƒãƒƒãƒæ¼”ç®—ã§ã¯å·®ã¯ã»ã¼æ¶ˆãˆã‚‹ã€‚å¯èª­æ€§ã‚’é‡è¦–ã™ã‚‹å ´åˆã¯einsumã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€å„ªå…ˆãªã‚‰`@`æ¼”ç®—å­ã‚’ä½¿ã†ã€‚

PyTorch ã§ã‚‚ `torch.einsum` ãŒä½¿ãˆã€åŒã˜è¨˜æ³•ã§è‡ªå‹•å¾®åˆ†ã‚‚å¯èƒ½:
```python
import torch

# PyTorchç‰ˆ â€” è‡ªå‹•å¾®åˆ†ä»˜ã
A = torch.randn(3, 4, requires_grad=True)
B = torch.randn(4, 5)
C = torch.einsum('ik,kj->ij', A, B)
loss = C.sum()
loss.backward()
print(f"âˆ‚loss/âˆ‚A shape: {A.grad.shape}")
```
:::

### 4.3 ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ â€” Row-major vs Column-major

è¡Œåˆ—ã®ãƒ¡ãƒ¢ãƒªä¸Šã§ã®æ ¼ç´é †åºãŒè¨ˆç®—é€Ÿåº¦ã«ç›´çµã™ã‚‹ã€‚

| æ–¹å¼ | è¡Œåˆ— $A_{ij}$ ã®æ ¼ç´é † | è¨€èª/ãƒ©ã‚¤ãƒ–ãƒ©ãƒª |
|:-----|:---------------------|:-------------|
| **Row-major (C order)** | $A_{00}, A_{01}, A_{02}, A_{10}, \ldots$ | C, Python/NumPy, PyTorch |
| **Column-major (Fortran order)** | $A_{00}, A_{10}, A_{20}, A_{01}, \ldots$ | Fortran, Julia, MATLAB, R |

**ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡**: ãƒ¡ãƒ¢ãƒªã¯é€£ç¶šã‚¢ã‚¯ã‚»ã‚¹ãŒé€Ÿã„ã€‚Row-majorã§ã¯**è¡Œæ–¹å‘**ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒé«˜é€Ÿã€Column-majorã§ã¯**åˆ—æ–¹å‘**ãŒé«˜é€Ÿã€‚

```python
import numpy as np
import time

n = 2000
A = np.random.randn(n, n)

# è¡Œæ–¹å‘ã®ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆRow-majorã§ã¯é«˜é€Ÿï¼‰
start = time.perf_counter()
row_sums = np.sum(A, axis=1)  # å„è¡Œã®å’Œ
t_row = time.perf_counter() - start

# åˆ—æ–¹å‘ã®ã‚¢ã‚¯ã‚»ã‚¹
start = time.perf_counter()
col_sums = np.sum(A, axis=0)  # å„åˆ—ã®å’Œ
t_col = time.perf_counter() - start

print(f"è¡Œæ–¹å‘ã®å’Œ: {t_row*1000:.2f} ms")
print(f"åˆ—æ–¹å‘ã®å’Œ: {t_col*1000:.2f} ms")
print(f"NumPy ã®ãƒ¡ãƒ¢ãƒªã‚ªãƒ¼ãƒ€ãƒ¼: {'C (Row-major)' if A.flags['C_CONTIGUOUS'] else 'F (Column-major)'}")
```

:::message
**ãªãœã“ã‚ŒãŒé‡è¦ã‹**: è¡Œåˆ—ç© $C = AB$ ã‚’å®Ÿè£…ã™ã‚‹ã¨ãã€ãƒŠã‚¤ãƒ¼ãƒ–ãª3é‡ãƒ«ãƒ¼ãƒ—ã®é †åº (i, j, k) vs (i, k, j) ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ãŒå¤§ããå¤‰ã‚ã‚Šã€æ€§èƒ½ãŒæ•°å€å¤‰ã‚ã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚NumPy ã¯å†…éƒ¨ã§æœ€é©åŒ–ã•ã‚ŒãŸ BLASï¼ˆBasic Linear Algebra Subprogramsï¼‰ã‚’å‘¼ã‚“ã§ã„ã‚‹ã®ã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ„è­˜ã™ã‚‹å¿…è¦ã¯å°‘ãªã„ãŒã€Juliaã‚„Rustç­‰ã§è‡ªå‰å®Ÿè£…ã™ã‚‹å ´åˆã¯å¿…é ˆã®çŸ¥è­˜ã ã€‚ç¬¬9å›ï¼ˆJuliaç™»å ´ï¼‰ã¨ç¬¬11å›ï¼ˆRustç™»å ´ï¼‰ã§æ”¹ã‚ã¦æ‰±ã†ã€‚
:::

### 4.4 ç·šå½¢ä»£æ•°ã®è¨ˆç®—é‡

å„æ¼”ç®—ã®è¨ˆç®—é‡ã‚’çŸ¥ã£ã¦ãŠãã¨ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚’åˆ¤æ–­ã§ãã‚‹ã€‚

| æ¼”ç®— | è¨ˆç®—é‡ | å‚™è€ƒ |
|:-----|:------|:-----|
| ãƒ™ã‚¯ãƒˆãƒ«å†…ç© | $O(n)$ | |
| è¡Œåˆ—-ãƒ™ã‚¯ãƒˆãƒ«ç© | $O(mn)$ | $A \in \mathbb{R}^{m \times n}$ |
| è¡Œåˆ—-è¡Œåˆ—ç© | $O(mnp)$ | $A \in \mathbb{R}^{m \times n}, B \in \mathbb{R}^{n \times p}$ |
| LUåˆ†è§£ | $O(\frac{2}{3}n^3)$ | é€£ç«‹æ–¹ç¨‹å¼ |
| Choleskyåˆ†è§£ | $O(\frac{1}{3}n^3)$ | æ­£å®šå€¤è¡Œåˆ— |
| QRåˆ†è§£ | $O(\frac{4}{3}n^3)$ | Householderæ³• |
| å›ºæœ‰å€¤åˆ†è§£ | $O(n^3)$ | QRã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  |
| SVD | $O(mn\min(m,n))$ | ç¬¬3å›ã§è©³èª¬ |
| Attention $QK^\top$ | $O(n^2 d)$ | ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·$n$ã®äºŒä¹—! |

```python
import numpy as np
import time

print("=== è¡Œåˆ—ç©ã®è¨ˆç®—æ™‚é–“ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° ===")
for n in [100, 500, 1000, 2000]:
    A = np.random.randn(n, n)
    B = np.random.randn(n, n)

    start = time.perf_counter()
    C = A @ B
    elapsed = time.perf_counter() - start

    gflops = 2 * n**3 / elapsed / 1e9
    print(f"n={n:4d}: {elapsed*1000:8.2f} ms  ({gflops:.1f} GFLOPS)")
```

:::details Strassenã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ç†è«–é™ç•Œ
è¡Œåˆ—ç©ã®è¨ˆç®—é‡ã¯é•·ã‚‰ã $O(n^3)$ ãŒæœ€å–„ã¨è€ƒãˆã‚‰ã‚Œã¦ã„ãŸãŒã€1969å¹´ã«StrassenãŒ $O(n^{2.807})$ ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç™ºè¦‹ã—ãŸã€‚ç¾åœ¨ã®ç†è«–çš„æœ€å–„ã¯ $O(n^{2.3728\ldots})$ [Alman & Vassilevska Williams, 2021] ã ãŒã€å®šæ•°ãŒå¤§ããå®Ÿç”¨ã•ã‚Œã¦ã„ãªã„ã€‚

GPUä¸Šã®è¡Œåˆ—ç©ã¯ã€NVIDIA ã® cuBLAS ãŒæœ€é©åŒ–ã—ã¦ãŠã‚Šã€Tensor Core ã‚’ä½¿ãˆã°FP16ã§ç†è«–é™ç•Œã«è¿‘ã„æ€§èƒ½ãŒå‡ºã‚‹ã€‚Transformerã®è¨“ç·´é€Ÿåº¦ã¯ã€æœ¬è³ªçš„ã«ã“ã®è¡Œåˆ—ç©ã®é€Ÿåº¦ã§æ±ºã¾ã‚‹ã€‚
:::

### 4.5 æ•°å¼â†’ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ‘ã‚¿ãƒ¼ãƒ³

ç·šå½¢ä»£æ•°ã®æ•°å¼ã‚’ã‚³ãƒ¼ãƒ‰ã«ç¿»è¨³ã™ã‚‹7ã¤ã®åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³:

| # | æ•°å¼ãƒ‘ã‚¿ãƒ¼ãƒ³ | ã‚³ãƒ¼ãƒ‰ | ä¾‹ |
|:--|:-----------|:------|:---|
| 1 | $\mathbf{a}^\top\mathbf{b}$ | `np.dot(a, b)` or `a @ b` | å†…ç© |
| 2 | $AB$ | `A @ B` | è¡Œåˆ—ç© |
| 3 | $A^\top$ | `A.T` | è»¢ç½® |
| 4 | $A^{-1}\mathbf{b}$ | `np.linalg.solve(A, b)` | é€£ç«‹æ–¹ç¨‹å¼ |
| 5 | $\|x\|_2$ | `np.linalg.norm(x)` | L2ãƒãƒ«ãƒ  |
| 6 | $\text{diag}(\lambda_1, \ldots)$ | `np.diag(lambdas)` | å¯¾è§’è¡Œåˆ— |
| 7 | $\sum_{ij} A_{ij} B_{ij}$ | `np.einsum('ij,ij->', A, B)` | Frobeniuså†…ç© |

```python
import numpy as np

# ãƒ‘ã‚¿ãƒ¼ãƒ³å®Ÿæ¼”: ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®KL divergence
# D_KL(N(Î¼â‚,Î£â‚) || N(Î¼â‚‚,Î£â‚‚))
# = 1/2 [tr(Î£â‚‚â»Â¹Î£â‚) + (Î¼â‚‚-Î¼â‚)^T Î£â‚‚â»Â¹(Î¼â‚‚-Î¼â‚) - d + ln(det(Î£â‚‚)/det(Î£â‚))]

d = 3
mu1 = np.array([1.0, 2.0, 3.0])
mu2 = np.array([0.0, 0.0, 0.0])
Sigma1 = np.array([[2, 0.5, 0], [0.5, 1, 0.3], [0, 0.3, 1.5]])
Sigma2 = np.eye(d)

# å„é …ã‚’æ•°å¼â†’ã‚³ãƒ¼ãƒ‰ç¿»è¨³
# ãƒ‘ã‚¿ãƒ¼ãƒ³4: Î£â‚‚â»Â¹Î£â‚ â†’ solve(Î£â‚‚, Î£â‚)
Sigma2_inv_Sigma1 = np.linalg.solve(Sigma2, Sigma1)

# ãƒ‘ã‚¿ãƒ¼ãƒ³1: tr(Â·)
term1 = np.trace(Sigma2_inv_Sigma1)

# ãƒ‘ã‚¿ãƒ¼ãƒ³4+1: (Î¼â‚‚-Î¼â‚)^T Î£â‚‚â»Â¹ (Î¼â‚‚-Î¼â‚)
diff = mu2 - mu1
term2 = diff @ np.linalg.solve(Sigma2, diff)

# ã‚¹ã‚«ãƒ©ãƒ¼
term3 = -d

# det â†’ slogdet for numerical stability
sign1, logdet1 = np.linalg.slogdet(Sigma1)
sign2, logdet2 = np.linalg.slogdet(Sigma2)
term4 = logdet2 - logdet1

kl = 0.5 * (term1 + term2 + term3 + term4)
print(f"D_KL(N(Î¼â‚,Î£â‚) || N(Î¼â‚‚,Î£â‚‚)) = {kl:.4f}")
print(f"  tr(Î£â‚‚â»Â¹Î£â‚) = {term1:.4f}")
print(f"  (Î¼â‚‚-Î¼â‚)^T Î£â‚‚â»Â¹(Î¼â‚‚-Î¼â‚) = {term2:.4f}")
print(f"  -d = {term3}")
print(f"  ln(det(Î£â‚‚)/det(Î£â‚)) = {term4:.4f}")
```

### 4.6 è¡Œåˆ—ã®æŒ‡æ•°é–¢æ•° $\exp(A)$

è¡Œåˆ—ã®æŒ‡æ•°é–¢æ•°ã¯ã€SSMï¼ˆState Space Modelsã€ç¬¬26å›ï¼‰ã®ä¸­æ ¸:

$$
\exp(A) = \sum_{k=0}^{\infty} \frac{A^k}{k!} = I + A + \frac{A^2}{2!} + \frac{A^3}{3!} + \cdots
$$

$A$ ãŒå¯¾è§’åŒ–å¯èƒ½ãªã‚‰: $\exp(A) = V \exp(\Lambda) V^{-1} = V \text{diag}(e^{\lambda_1}, \ldots, e^{\lambda_n}) V^{-1}$

```python
import numpy as np
from scipy.linalg import expm

# è¡Œåˆ—æŒ‡æ•°é–¢æ•°
A = np.array([[-1, 0.5],
              [0.5, -2]])

# scipy ã® expmï¼ˆPadÃ©è¿‘ä¼¼ï¼‰
exp_A = expm(A)
print("exp(A) =")
print(np.round(exp_A, 4))

# å›ºæœ‰å€¤åˆ†è§£ã«ã‚ˆã‚‹è¨ˆç®—
eigenvalues, V = np.linalg.eig(A)
exp_A_eig = V @ np.diag(np.exp(eigenvalues)) @ np.linalg.inv(V)
print("\nexp(A) via eigendecomposition =")
print(np.round(exp_A_eig.real, 4))
print(f"ä¸€è‡´: {np.allclose(exp_A, exp_A_eig.real)}")

# SSMã®é›¢æ•£åŒ–: x[k+1] = exp(AÎ”t) x[k] + B u[k]
dt = 0.1
A_discrete = expm(A * dt)
print(f"\nexp(AÎ”t) (Î”t={dt}):")
print(np.round(A_discrete, 4))
```

:::message
**SSMã¸ã®äºˆå‘Š**: ç¬¬26å›ï¼ˆState Space Models / Mambaï¼‰ã§ã¯ã€$\exp(A\Delta t)$ ã®åŠ¹ç‡çš„ãªè¨ˆç®—ãŒãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’å·¦å³ã™ã‚‹ã€‚é€£ç¶šæ™‚é–“ã®çŠ¶æ…‹æ–¹ç¨‹å¼ $\dot{\mathbf{x}} = A\mathbf{x} + B\mathbf{u}$ ã‚’é›¢æ•£åŒ–ã™ã‚‹éš›ã«ã“ã®è¡Œåˆ—æŒ‡æ•°é–¢æ•°ãŒç™»å ´ã™ã‚‹ã€‚è¦šãˆã¦ãŠã„ã¦ã»ã—ã„ã€‚
:::

### 4.7 æ•°å€¤è¨ˆç®—ã®ãƒ”ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ«

ç·šå½¢ä»£æ•°ã®è¨ˆç®—ã¯ã€ç†è«–çš„ã«ã¯æ­£ã—ãã¦ã‚‚æ•°å€¤çš„ã«ç ´ç¶»ã™ã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚å®Ÿè£…è€…ã¯ä»¥ä¸‹ã®è½ã¨ã—ç©´ã‚’çŸ¥ã£ã¦ãŠãå¿…è¦ãŒã‚ã‚‹ã€‚

| ãƒ”ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ« | åŸå›  | å¯¾ç­– |
|:-------------|:-----|:-----|
| æµ®å‹•å°æ•°ç‚¹ã®ç­‰å·æ¯”è¼ƒ | ä¸¸ã‚èª¤å·® | `np.allclose(a, b, atol=1e-10)` ã‚’ä½¿ã† |
| é€†è¡Œåˆ—ã®æ˜ç¤ºè¨ˆç®— | æ¡ä»¶æ•°ãŒå¤§ãã„ã¨ä¸å®‰å®š | `np.linalg.solve` ã‚’ä½¿ã† |
| å¤§è¡Œåˆ—ã®è¡Œåˆ—å¼ | ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼/ã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ãƒ­ãƒ¼ | `np.linalg.slogdet` ã§å¯¾æ•°ã‚’å–ã‚‹ |
| Gram-Schmidt ã®ç›´äº¤æ€§åŠ£åŒ– | æµ®å‹•å°æ•°ç‚¹èª¤å·®ã®è“„ç© | Modified Gram-Schmidt or QRåˆ†è§£ã‚’ä½¿ã† |
| å›ºæœ‰å€¤ã®é †åºä»®å®š | `eig` ã¯å›ºæœ‰å€¤ã‚’ã‚½ãƒ¼ãƒˆã—ãªã„ | `eigh` ã‚’ä½¿ã†ã€ã¾ãŸã¯æ˜ç¤ºçš„ã«ã‚½ãƒ¼ãƒˆ |
| å¯¾ç§°æ€§ã®ä»®å®šå´©ã‚Œ | ä¸¸ã‚èª¤å·®ã§ $A \neq A^\top$ | `A = (A + A.T) / 2` ã§å¼·åˆ¶å¯¾ç§°åŒ– |

```python
import numpy as np

# ãƒ”ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ«1: æµ®å‹•å°æ•°ç‚¹ã®ç­‰å·æ¯”è¼ƒ
a = 0.1 + 0.2
print(f"0.1 + 0.2 == 0.3? {a == 0.3}")           # False!
print(f"np.isclose? {np.isclose(a, 0.3)}")          # True

# ãƒ”ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ«2: å¤§è¡Œåˆ—ã®è¡Œåˆ—å¼
A_large = np.random.randn(500, 500) * 0.01
det_direct = np.linalg.det(A_large)
print(f"det(A) = {det_direct}")  # Often 0.0 or inf (overflow/underflow)

sign, logdet = np.linalg.slogdet(A_large)
print(f"sign={sign}, log|det| = {logdet:.4f}")  # Numerically stable

# ãƒ”ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ«3: å¯¾ç§°æ€§ã®å¼·åˆ¶
B = np.random.randn(3, 3)
S = B.T @ B  # theoretically symmetric
print(f"S == S.T? {np.allclose(S, S.T)}")  # usually True, but not guaranteed
S = (S + S.T) / 2  # force symmetry â€” safe practice
```

:::message
**é€²æ—: 70% å®Œäº†** NumPyã®ç·šå½¢ä»£æ•°ãƒ„ãƒ¼ãƒ«ã‚­ãƒƒãƒˆã€einsumè¨˜æ³•ã€ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã€è¨ˆç®—é‡ã€æ•°å¼â†’ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ‘ã‚¿ãƒ¼ãƒ³ã€æ•°å€¤è¨ˆç®—ã®ãƒ”ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ«ã‚’ç¿’å¾—ã—ãŸã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

### 5.1 è¨˜å·èª­è§£ãƒ†ã‚¹ãƒˆ

ä»¥ä¸‹ã®æ•°å¼ã‚’å£°ã«å‡ºã—ã¦èª­ã¿ã€æ„å‘³ã‚’èª¬æ˜ã›ã‚ˆã€‚

:::details Q1: $A \in \mathbb{R}^{m \times n}$
**èª­ã¿**: ã€Œ$A$ ã¯ $m$ è¡Œ $n$ åˆ—ã®å®Ÿæ•°è¡Œåˆ—ã€

**æ„å‘³**: $A$ ã¯ $m \times n$ å€‹ã®å®Ÿæ•°å€¤ã‚’æŒã¤è¡Œåˆ—ã€‚ç·šå½¢å†™åƒ $A: \mathbb{R}^n \to \mathbb{R}^m$ ã‚’è¡¨ç¾ã™ã‚‹ã€‚
:::

:::details Q2: $\mathbf{v} \in \ker(A) \iff A\mathbf{v} = \mathbf{0}$
**èª­ã¿**: ã€Œ$\mathbf{v}$ ãŒ $A$ ã®æ ¸ã«å±ã™ã‚‹ã“ã¨ã¨ã€$A\mathbf{v}$ ãŒã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã«ãªã‚‹ã“ã¨ã¯åŒå€¤ã€

**æ„å‘³**: æ ¸ï¼ˆnull spaceï¼‰ã¯ã€$A$ ã§æ½°ã•ã‚Œã¦ã‚¼ãƒ­ã«ãªã‚‹ãƒ™ã‚¯ãƒˆãƒ«å…¨ä½“ã®é›†åˆã€‚Rank-Nullityå®šç†ã§ $\dim(\ker(A)) = n - \text{rank}(A)$ã€‚
:::

:::details Q3: $\text{tr}(ABC) = \text{tr}(BCA) = \text{tr}(CAB)$
**èª­ã¿**: ã€Œ$ABC$ ã®ãƒˆãƒ¬ãƒ¼ã‚¹ã¯ $BCA$ ã®ãƒˆãƒ¬ãƒ¼ã‚¹ã«ç­‰ã—ãã€$CAB$ ã®ãƒˆãƒ¬ãƒ¼ã‚¹ã«ã‚‚ç­‰ã—ã„ã€

**æ„å‘³**: ãƒˆãƒ¬ãƒ¼ã‚¹ã®å·¡å›æ€§ï¼ˆcyclic propertyï¼‰ã€‚è¡Œåˆ—ç©ã®é †åºã‚’å·¡å›çš„ã«å…¥ã‚Œæ›¿ãˆã¦ã‚‚ãƒˆãƒ¬ãƒ¼ã‚¹ã¯å¤‰ã‚ã‚‰ãªã„ã€‚è¡Œåˆ—å¾®åˆ†ã§é »å‡ºã€‚**æ³¨æ„**: $\text{tr}(ABC) \neq \text{tr}(ACB)$ â€” å·¡å›çš„ã§ãªã„ä¸¦ã¹æ›¿ãˆã§ã¯ãƒˆãƒ¬ãƒ¼ã‚¹ã¯å¤‰ã‚ã‚‹ã€‚
:::

:::details Q4: $A \succ 0$
**èª­ã¿**: ã€Œ$A$ ã¯æ­£å®šå€¤ã€

**æ„å‘³**: $\mathbf{x}^\top A \mathbf{x} > 0$ for all $\mathbf{x} \neq \mathbf{0}$ã€‚å…¨ã¦ã®å›ºæœ‰å€¤ãŒæ­£ã€‚Choleskyåˆ†è§£ãŒå¯èƒ½ã€‚å…±åˆ†æ•£è¡Œåˆ—ãŒæ­£å‰‡ãªã¨ãæˆç«‹ã€‚
:::

:::details Q5: $\hat{\mathbf{x}} = (A^\top A)^{-1} A^\top \mathbf{b}$
**èª­ã¿**: ã€Œ$\hat{\mathbf{x}}$ ã¯ $A^\top A$ ã®é€†è¡Œåˆ—ã¨ $A^\top \mathbf{b}$ ã®ç©ã€

**æ„å‘³**: æœ€å°äºŒä¹—è§£ã€‚$\|A\mathbf{x} - \mathbf{b}\|^2$ ã‚’æœ€å°ã«ã™ã‚‹ $\mathbf{x}$ã€‚æ­£è¦æ–¹ç¨‹å¼ $A^\top A\hat{\mathbf{x}} = A^\top\mathbf{b}$ ã®è§£ã€‚$A^\top A$ ãŒæ­£å‰‡ï¼ˆ$A$ ãŒãƒ•ãƒ«ãƒ©ãƒ³ã‚¯åˆ—ï¼‰ã®ã¨ãä¸€æ„ã€‚
:::

:::details Q6: $A = Q\Lambda Q^\top$, $Q^\top Q = I$
**èª­ã¿**: ã€Œ$A$ ã¯ç›´äº¤è¡Œåˆ— $Q$ ã¨å¯¾è§’è¡Œåˆ— $\Lambda$ ã§ã‚¹ãƒšã‚¯ãƒˆãƒ«åˆ†è§£ã•ã‚Œã‚‹ã€

**æ„å‘³**: å¯¾ç§°è¡Œåˆ—ã®ã‚¹ãƒšã‚¯ãƒˆãƒ«å®šç†ã€‚$Q$ ã®åˆ—ãŒå›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã€$\Lambda$ ã®å¯¾è§’æˆåˆ†ãŒå›ºæœ‰å€¤ã€‚PCAã€å…±åˆ†æ•£è¡Œåˆ—ã®åˆ†æã§å¿…é ˆã€‚
:::

:::details Q7: $P = A(A^\top A)^{-1}A^\top$, $P^2 = P$
**èª­ã¿**: ã€Œ$P$ ã¯å°„å½±è¡Œåˆ—ã§ã€2å›é©ç”¨ã—ã¦ã‚‚çµæœãŒå¤‰ã‚ã‚‰ãªã„ï¼ˆå†ªç­‰ï¼‰ã€

**æ„å‘³**: $P$ ã¯ $A$ ã®åˆ—ç©ºé–“ã¸ã®ç›´äº¤å°„å½±ã€‚$P\mathbf{b}$ ã¯ $\mathbf{b}$ ã«æœ€ã‚‚è¿‘ã„ $\text{Col}(A)$ ä¸Šã®ç‚¹ã€‚
:::

:::details Q8: $\|\mathbf{u}\| \|\mathbf{v}\| \cos\theta = \langle \mathbf{u}, \mathbf{v} \rangle$
**èª­ã¿**: ã€Œ$\mathbf{u}$ ã¨ $\mathbf{v}$ ã®ãƒãƒ«ãƒ ã®ç©ã«ã‚³ã‚µã‚¤ãƒ³ã‚’ã‹ã‘ãŸã‚‚ã®ãŒå†…ç©ã€

**æ„å‘³**: å†…ç©ã®å¹¾ä½•å­¦çš„è§£é‡ˆã€‚$\cos\theta = 1$ï¼ˆå¹³è¡Œï¼‰â†’å†…ç©æœ€å¤§ã€$\cos\theta = 0$ï¼ˆç›´äº¤ï¼‰â†’å†…ç©ã‚¼ãƒ­ã€‚Attention[^1]ã®é¡ä¼¼åº¦è¨ˆç®—ã®æ•°å­¦çš„åŸºç›¤ã€‚
:::

:::details Q9: $(AB)^{-1} = B^{-1}A^{-1}$
**èª­ã¿**: ã€Œ$AB$ ã®é€†è¡Œåˆ—ã¯ $B$ ã®é€†è¡Œåˆ—ã¨ $A$ ã®é€†è¡Œåˆ—ã®ç©ï¼ˆé †åºåè»¢ï¼‰ã€

**æ„å‘³**: ã€Œé´ä¸‹ã‚’å±¥ã„ã¦ã‹ã‚‰é´ã‚’å±¥ãã€â†’ã€Œè„±ãã¨ãã¯é´ã‚’å…ˆã«è„±ãã€æ¬¡ã«é´ä¸‹ã€ã€‚é€†æ“ä½œã¯é †åºãŒé€†ã«ãªã‚‹ã€‚$(AB)^\top = B^\top A^\top$ ã¨åŒã˜åŸç†ã€‚
:::

:::details Q10: $R(\mathbf{x}) = \frac{\mathbf{x}^\top A \mathbf{x}}{\mathbf{x}^\top \mathbf{x}}$, $\lambda_{\min} \leq R(\mathbf{x}) \leq \lambda_{\max}$
**èª­ã¿**: ã€ŒRayleighå•†ã¯æœ€å°å›ºæœ‰å€¤ã¨æœ€å¤§å›ºæœ‰å€¤ã®é–“ã«åã¾ã‚‹ã€

**æ„å‘³**: å¯¾ç§°è¡Œåˆ— $A$ ã®Rayleighå•†ã®æœ€å¤§åŒ–ãŒæœ€å¤§å›ºæœ‰å€¤ã¨ç¬¬1å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä¸ãˆã‚‹ã€‚PCA[^6][^7]ã®æ•°å­¦çš„åŸºç›¤ã€‚
:::

### 5.2 ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ†ã‚¹ãƒˆ

ä»¥ä¸‹ã®æ•°å¼ã‚’NumPyã‚³ãƒ¼ãƒ‰ã«ç¿»è¨³ã›ã‚ˆã€‚

:::details Q1: $C = A^\top B$ ($A \in \mathbb{R}^{3 \times 2}, B \in \mathbb{R}^{3 \times 4}$)
```python
C = A.T @ B  # shape: (2, 4)
```
:::

:::details Q2: Frobenius ãƒãƒ«ãƒ  $\|A\|_F = \sqrt{\text{tr}(A^\top A)}$
```python
# æ–¹æ³•1: ç›´æ¥
fro = np.linalg.norm(A, 'fro')

# æ–¹æ³•2: ãƒˆãƒ¬ãƒ¼ã‚¹ã‹ã‚‰
fro = np.sqrt(np.trace(A.T @ A))

# æ–¹æ³•3: einsum
fro = np.sqrt(np.einsum('ij,ij->', A, A))
```
:::

:::details Q3: äºŒæ¬¡å½¢å¼ $f(\mathbf{x}) = \frac{1}{2}\mathbf{x}^\top H \mathbf{x} - \mathbf{b}^\top\mathbf{x}$
```python
f = 0.5 * x @ H @ x - b @ x

# einsumç‰ˆ
f = 0.5 * np.einsum('i,ij,j->', x, H, x) - np.einsum('i,i->', b, x)
```
:::

:::details Q4: PCAæ¬¡å…ƒå‰Šæ¸› $Z = \tilde{X} Q_k$
```python
# X: (n_samples, d), k: ç›®æ¨™æ¬¡å…ƒ
X_centered = X - X.mean(axis=0)
Sigma = np.cov(X_centered, rowvar=False)
eigenvalues, Q = np.linalg.eigh(Sigma)
# é™é †ã«ã‚½ãƒ¼ãƒˆ
idx = np.argsort(eigenvalues)[::-1]
Q_k = Q[:, idx[:k]]
Z = X_centered @ Q_k  # shape: (n_samples, k)
```
:::

:::details Q5: Cholesky ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° $\mathbf{x} = \boldsymbol{\mu} + L\mathbf{z}$, $\mathbf{z} \sim \mathcal{N}(\mathbf{0}, I)$
```python
L = np.linalg.cholesky(Sigma)
z = np.random.randn(d)
x = mu + L @ z
```
:::

### 5.3 ãƒŸãƒ‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: PCA ã§ MNIST ã‚’å¯è¦–åŒ–ã™ã‚‹

```python
import numpy as np

# MNISTé¢¨ã®åˆæˆãƒ‡ãƒ¼ã‚¿ï¼ˆå®Ÿéš›ã®MNISTã®ä»£ã‚ã‚Šï¼‰
np.random.seed(42)
n_samples = 1000
n_features = 784  # 28x28

# 3ã‚¯ãƒ©ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼ˆå„ã‚¯ãƒ©ã‚¹ã¯ç•°ãªã‚‹æ–¹å‘ã«åºƒãŒã‚‹ï¼‰
n_per_class = n_samples // 3
centers = np.random.randn(3, n_features) * 5
X = np.vstack([
    centers[i] + np.random.randn(n_per_class, n_features) * 0.5
    for i in range(3)
])
y = np.repeat([0, 1, 2], n_per_class)

# PCA å®Ÿè£…
X_centered = X - X.mean(axis=0)
# è¨ˆç®—é‡å‰Šæ¸›: Î£ = X^T X / (N-1) ã®ä»£ã‚ã‚Šã« SVD ã‚’ä½¿ã†ï¼ˆç¬¬3å›ã§è©³èª¬ï¼‰
# ã“ã“ã§ã¯å…±åˆ†æ•£è¡Œåˆ—ã®ç›´æ¥è¨ˆç®—
Sigma = X_centered.T @ X_centered / (n_samples - 1)

# ä¸Šä½2ä¸»æˆåˆ†
# eigh ã¯å¤§ããªè¡Œåˆ—ã§ã¯é…ã„ã®ã§ã€å®Ÿç”¨ã§ã¯ SVD ã‚’ä½¿ã†ï¼ˆç¬¬3å›ï¼‰
# ã“ã“ã§ã¯æ•™è‚²ç›®çš„ã§ eigh
eigenvalues, eigenvectors = np.linalg.eigh(Sigma)
idx = np.argsort(eigenvalues)[::-1]
eigenvalues = eigenvalues[idx]
eigenvectors = eigenvectors[:, idx]

# æ¬¡å…ƒå‰Šæ¸›
Z = X_centered @ eigenvectors[:, :2]

print("=== PCA on synthetic MNIST ===")
print(f"å…ƒã®æ¬¡å…ƒ: {n_features}")
print(f"å‰Šæ¸›å¾Œã®æ¬¡å…ƒ: 2")
print(f"å¯„ä¸ç‡ (PC1): {eigenvalues[0] / eigenvalues.sum():.4f}")
print(f"å¯„ä¸ç‡ (PC1+PC2): {eigenvalues[:2].sum() / eigenvalues.sum():.4f}")
print(f"Z shape: {Z.shape}")
for c in range(3):
    mask = y == c
    print(f"  ã‚¯ãƒ©ã‚¹ {c}: ä¸­å¿ƒ = ({Z[mask, 0].mean():.2f}, {Z[mask, 1].mean():.2f})")
```

### 5.4 LaTeX è¨˜è¿°ãƒ†ã‚¹ãƒˆ

ä»¥ä¸‹ã®æ•°å¼ã‚’LaTeXã§æ›¸ã„ã¦ã¿ã‚ˆã†ã€‚ç­”ãˆã¯æŠ˜ã‚Šç•³ã¿ã®ä¸­ã€‚

:::details Q1: å›ºæœ‰å€¤æ–¹ç¨‹å¼
```latex
A\mathbf{v} = \lambda\mathbf{v}
```
$$A\mathbf{v} = \lambda\mathbf{v}$$
:::

:::details Q2: ã‚¹ãƒšã‚¯ãƒˆãƒ«åˆ†è§£
```latex
A = Q\Lambda Q^\top = \sum_{i=1}^{n} \lambda_i \mathbf{q}_i \mathbf{q}_i^\top
```
$$A = Q\Lambda Q^\top = \sum_{i=1}^{n} \lambda_i \mathbf{q}_i \mathbf{q}_i^\top$$
:::

:::details Q3: Cauchy-Schwarz ä¸ç­‰å¼
```latex
|\langle \mathbf{u}, \mathbf{v} \rangle| \leq \|\mathbf{u}\| \cdot \|\mathbf{v}\|
```
$$|\langle \mathbf{u}, \mathbf{v} \rangle| \leq \|\mathbf{u}\| \cdot \|\mathbf{v}\|$$
:::

:::details Q4: æ­£è¦æ–¹ç¨‹å¼
```latex
\hat{\mathbf{x}} = (A^\top A)^{-1} A^\top \mathbf{b}
```
$$\hat{\mathbf{x}} = (A^\top A)^{-1} A^\top \mathbf{b}$$
:::

:::details Q5: å¤šå¤‰é‡ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ
```latex
\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \Sigma) = \frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\top \Sigma^{-1} (\mathbf{x}-\boldsymbol{\mu})\right)
```
$$\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \Sigma) = \frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\top \Sigma^{-1} (\mathbf{x}-\boldsymbol{\mu})\right)$$
:::

### 5.5 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸: å‹¾é…é™ä¸‹æ³•ã§ç·šå½¢å›å¸°

æœ€å°äºŒä¹—æ³•ã¯é–‰å½¢å¼è§£ã‚’æŒã¤ãŒã€å‹¾é…é™ä¸‹æ³•ã§ã‚‚è§£ã‘ã‚‹ã€‚ã“ã“ã§ã¯å‹¾é…é™ä¸‹æ³•ã§ç·šå½¢å›å¸°ã‚’è§£ãã€é–‰å½¢å¼è§£ã¨ä¸€è‡´ã™ã‚‹ã‹ç¢ºèªã™ã‚‹ã€‚

```python
import numpy as np

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
np.random.seed(42)
n, d = 100, 3
X = np.random.randn(n, d)
w_true = np.array([2.0, -1.5, 0.5])
y = X @ w_true + np.random.randn(n) * 0.3

# é–‰å½¢å¼è§£
w_closed = np.linalg.solve(X.T @ X, X.T @ y)

# å‹¾é…é™ä¸‹æ³•
w_gd = np.zeros(d)
lr = 0.01
n_iters = 500

losses = []
for t in range(n_iters):
    # å‹¾é…: âˆ‡L = (2/n) X^T (Xw - y)
    residual = X @ w_gd - y
    grad = (2 / n) * X.T @ residual
    w_gd -= lr * grad
    loss = np.mean(residual**2)
    losses.append(loss)

print("=== å‹¾é…é™ä¸‹æ³• vs é–‰å½¢å¼è§£ ===")
print(f"çœŸã®é‡ã¿:   {w_true}")
print(f"é–‰å½¢å¼è§£:   {np.round(w_closed, 4)}")
print(f"GD ({n_iters}å›): {np.round(w_gd, 4)}")
print(f"å·®ã®ãƒãƒ«ãƒ : {np.linalg.norm(w_gd - w_closed):.6f}")
print(f"æœ€çµ‚æå¤±:   {losses[-1]:.6f}")
```

:::details ãƒãƒ£ãƒ¬ãƒ³ã‚¸: ãƒŸãƒ‹ãƒãƒƒãƒSGDã«æ”¹é€ ã™ã‚‹
ä¸Šã®ã‚³ãƒ¼ãƒ‰ã‚’ä¿®æ­£ã—ã¦ã€å…¨ãƒ‡ãƒ¼ã‚¿ã§ã¯ãªãæ¯å›ãƒ©ãƒ³ãƒ€ãƒ ã«32å€‹ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸ã‚“ã§å‹¾é…ã‚’è¨ˆç®—ã™ã‚‹ãƒŸãƒ‹ãƒãƒƒãƒSGDã«æ”¹é€ ã—ã¦ã¿ã‚ˆã†ã€‚

```python
import numpy as np

np.random.seed(42)
n, d = 100, 3
X = np.random.randn(n, d)
w_true = np.array([2.0, -1.5, 0.5])
y = X @ w_true + np.random.randn(n) * 0.3

w_sgd = np.zeros(d)
lr = 0.01
batch_size = 32
n_iters = 500

for t in range(n_iters):
    # ãƒŸãƒ‹ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    idx = np.random.choice(n, batch_size, replace=False)
    X_batch = X[idx]
    y_batch = y[idx]

    # ãƒŸãƒ‹ãƒãƒƒãƒå‹¾é…
    residual = X_batch @ w_sgd - y_batch
    grad = (2 / batch_size) * X_batch.T @ residual
    w_sgd -= lr * grad

print(f"SGDçµæœ: {np.round(w_sgd, 4)}")
```
:::

### 5.6 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸: Power Iteration ã§æœ€å¤§å›ºæœ‰å€¤ã‚’æ±‚ã‚ã‚‹

å›ºæœ‰å€¤åˆ†è§£ã‚’ `np.linalg.eigh` ãªã—ã§å®Ÿè£…ã™ã‚‹ã€‚Power Iterationï¼ˆã¹ãä¹—æ³•ï¼‰ã¯ã€è¡Œåˆ—ã‚’ç¹°ã‚Šè¿”ã—ã‹ã‘ã‚‹ã“ã¨ã§æœ€å¤§å›ºæœ‰å€¤ã¨å¯¾å¿œã™ã‚‹å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã‚’æ±‚ã‚ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã ã€‚

```python
import numpy as np

def power_iteration(A: np.ndarray, n_iters: int = 100) -> tuple:
    """Power Iteration ã§æœ€å¤§å›ºæœ‰å€¤ã¨å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã‚’æ±‚ã‚ã‚‹ã€‚

    Algorithm:
    1. ãƒ©ãƒ³ãƒ€ãƒ ãªãƒ™ã‚¯ãƒˆãƒ« v ã‚’åˆæœŸåŒ–
    2. v â† Av / ||Av|| ã‚’ç¹°ã‚Šè¿”ã™
    3. Î» = v^T A v (Rayleighå•†) ãŒæœ€å¤§å›ºæœ‰å€¤ã«åæŸ
    """
    n = A.shape[0]
    v = np.random.randn(n)
    v = v / np.linalg.norm(v)

    for i in range(n_iters):
        Av = A @ v
        v_new = Av / np.linalg.norm(Av)
        # åæŸåˆ¤å®š
        if np.allclose(abs(np.dot(v_new, v)), 1.0, atol=1e-10):
            v = v_new
            break
        v = v_new

    eigenvalue = v @ A @ v  # Rayleighå•†
    return eigenvalue, v

# ãƒ†ã‚¹ãƒˆ
np.random.seed(42)
A = np.array([[4.0, 1.0, 0.5],
              [1.0, 3.0, 0.2],
              [0.5, 0.2, 2.0]])

lam_pi, v_pi = power_iteration(A)
lam_np, V_np = np.linalg.eigh(A)

print("=== Power Iteration vs np.linalg.eigh ===")
print(f"Power Iteration: Î»_max = {lam_pi:.6f}")
print(f"np.linalg.eigh:  Î»_max = {lam_np[-1]:.6f}")
print(f"å·®: {abs(lam_pi - lam_np[-1]):.10f}")
print(f"\nå›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ« (PI):  {np.round(v_pi, 4)}")
print(f"å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ« (eigh): {np.round(V_np[:, -1], 4)}")
```

:::details Deflation ã§å…¨å›ºæœ‰å€¤ã‚’æ±‚ã‚ã‚‹
Power Iteration ã¯æœ€å¤§å›ºæœ‰å€¤ã®ã¿ã‚’è¿”ã™ã€‚å…¨å›ºæœ‰å€¤ã‚’æ±‚ã‚ã‚‹ã«ã¯ **Deflation**ï¼ˆæ¸›è¡°æ³•ï¼‰ã‚’ä½¿ã†:

1. æœ€å¤§å›ºæœ‰å€¤ $\lambda_1$ ã¨å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ« $\mathbf{v}_1$ ã‚’æ±‚ã‚ã‚‹
2. $A \leftarrow A - \lambda_1 \mathbf{v}_1 \mathbf{v}_1^\top$ï¼ˆãƒ©ãƒ³ã‚¯1ã®å¼•ãç®—ï¼‰
3. æ–°ã—ã„ $A$ ã«å¯¾ã—ã¦Power Iterationã‚’ç¹°ã‚Šè¿”ã™

```python
import numpy as np

def all_eigenvalues_by_deflation(A, n_eig=None):
    """Deflation ã§å…¨å›ºæœ‰å€¤ã‚’æ±‚ã‚ã‚‹ã€‚"""
    if n_eig is None:
        n_eig = A.shape[0]

    A_deflated = A.copy()
    eigenvalues = []
    eigenvectors = []

    for _ in range(n_eig):
        lam, v = power_iteration(A_deflated)
        eigenvalues.append(lam)
        eigenvectors.append(v)
        # Deflation: ãƒ©ãƒ³ã‚¯1ã‚’å¼•ã
        A_deflated = A_deflated - lam * np.outer(v, v)

    return np.array(eigenvalues), np.column_stack(eigenvectors)

A = np.array([[4.0, 1.0, 0.5],
              [1.0, 3.0, 0.2],
              [0.5, 0.2, 2.0]])

lams_def, _ = all_eigenvalues_by_deflation(A)
lams_np = np.sort(np.linalg.eigvalsh(A))[::-1]

print("Deflation:", np.round(lams_def, 4))
print("eigh:     ", np.round(lams_np, 4))
```
:::

### 5.7 è‡ªå·±ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

æœ¬è¬›ç¾©ã‚’ä¿®äº†ã—ãŸæ™‚ç‚¹ã§ã€ä»¥ä¸‹ãŒã§ãã‚‹ã‹ç¢ºèªã—ã¦ã»ã—ã„:

- [ ] ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã®å…¬ç†ã‚’3ã¤ä»¥ä¸Šè¨€ãˆã‚‹
- [ ] ç·šå½¢ç‹¬ç«‹ã®å®šç¾©ã‚’ã‚³ãƒ¼ãƒ‰ã§ç¢ºèªã§ãã‚‹
- [ ] å†…ç©â†’ãƒãƒ«ãƒ â†’è·é›¢ã®å®šç¾©ã®é€£é–ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Cauchy-Schwarzä¸ç­‰å¼ã‚’è¿°ã¹ã€ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã¨ã®é–¢ä¿‚ã‚’èª¬æ˜ã§ãã‚‹
- [ ] è¡Œåˆ—ç©ã®3ã¤ã®è¦‹æ–¹ï¼ˆè¦ç´ ãƒ»åˆ—ãƒ»è¡Œï¼‰ã‚’ä½¿ã„åˆ†ã‘ã‚‰ã‚Œã‚‹
- [ ] è»¢ç½®ã®æ€§è³ª $(AB)^\top = B^\top A^\top$ ã‚’è¨¼æ˜ã§ãã‚‹
- [ ] `np.linalg.solve` ã¨ `np.linalg.inv` ã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
- [ ] å›ºæœ‰å€¤åˆ†è§£ã‚’æ‰‹è¨ˆç®—ã§2Ã—2è¡Œåˆ—ã«é©ç”¨ã§ãã‚‹
- [ ] ã‚¹ãƒšã‚¯ãƒˆãƒ«å®šç†ã®3ã¤ã®ä¸»å¼µã‚’è¿°ã¹ã‚‰ã‚Œã‚‹
- [ ] æ­£å®šå€¤è¡Œåˆ—ã®3ã¤ã®åˆ¤å®šæ¡ä»¶ã‚’åˆ—æŒ™ã§ãã‚‹
- [ ] Choleskyåˆ†è§£ã‚’ä½¿ã£ã¦ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§ãã‚‹
- [ ] æœ€å°äºŒä¹—æ³•ã®æ­£è¦æ–¹ç¨‹å¼ã‚’å°å‡ºã§ãã‚‹
- [ ] PCAã‚’å›ºæœ‰å€¤åˆ†è§£ã¨ã—ã¦å®Ÿè£…ã§ãã‚‹
- [ ] `np.einsum` ã§å†…ç©ãƒ»è¡Œåˆ—ç©ãƒ»ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’æ›¸ã‘ã‚‹
- [ ] Attention[^1]ã® $QK^\top$ ã‚’ç·šå½¢ä»£æ•°ã®è¨€è‘‰ã§èª¬æ˜ã§ãã‚‹

:::message
**é€²æ—: 85% å®Œäº†** è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆã€ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ†ã‚¹ãƒˆã€ãƒŸãƒ‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’é€šã˜ã¦ç†è§£åº¦ã‚’ç¢ºèªã—ãŸã€‚
:::

---

## ğŸ“ 6. æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨æ¬¡å›äºˆå‘Š

### 6.1 NumPy / SciPy ã®ç·šå½¢ä»£æ•°é–¢æ•°ãƒãƒ¼ãƒˆã‚·ãƒ¼ãƒˆ

å®Ÿè£…æ™‚ã«é »ç¹ã«å‚ç…§ã™ã‚‹é–¢æ•°ã‚’ã¾ã¨ã‚ã¦ãŠãã€‚

| ç›®çš„ | NumPy | SciPy | æ³¨æ„ç‚¹ |
|:-----|:------|:------|:------|
| è¡Œåˆ—ç© | `A @ B` | â€” | BLAS Level 3 ã® dgemm ã‚’å‘¼ã¶ |
| å†…ç© | `np.dot(a, b)` | â€” | 1Dãƒ™ã‚¯ãƒˆãƒ«åŒå£«ã®ã¿ã€‚2Dä»¥ä¸Šã¯ `@` ã‚’ä½¿ã† |
| è»¢ç½® | `A.T` | â€” | ãƒ“ãƒ¥ãƒ¼ã‚’è¿”ã™ï¼ˆã‚³ãƒ”ãƒ¼ãªã—ï¼‰ |
| é€†è¡Œåˆ— | `np.linalg.inv(A)` | `scipy.linalg.inv(A)` | å¯èƒ½ãªé™ã‚Š `solve` ã‚’ä½¿ã† |
| é€£ç«‹æ–¹ç¨‹å¼ | `np.linalg.solve(A, b)` | `scipy.linalg.solve(A, b)` | $A\mathbf{x}=\mathbf{b}$ ã‚’è§£ã |
| å›ºæœ‰å€¤åˆ†è§£ï¼ˆå¯¾ç§°ï¼‰ | `np.linalg.eigh(A)` | `scipy.linalg.eigh(A)` | **å¯¾ç§°è¡Œåˆ—ã«ã¯å¿…ãš eigh** |
| å›ºæœ‰å€¤åˆ†è§£ï¼ˆä¸€èˆ¬ï¼‰ | `np.linalg.eig(A)` | `scipy.linalg.eig(A)` | éå¯¾ç§°è¡Œåˆ—ç”¨ã€‚è¤‡ç´ å›ºæœ‰å€¤ã‚ã‚Š |
| SVD | `np.linalg.svd(A)` | `scipy.linalg.svd(A)` | ç¬¬3å›ã§è©³ã—ã |
| QRåˆ†è§£ | `np.linalg.qr(A)` | `scipy.linalg.qr(A)` | `mode='reduced'` ã§economy QR |
| Choleskyåˆ†è§£ | `np.linalg.cholesky(A)` | `scipy.linalg.cholesky(A)` | NumPy: ä¸‹ä¸‰è§’ $L$ã€SciPy: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä¸Šä¸‰è§’ |
| è¡Œåˆ—å¼ | `np.linalg.det(A)` | â€” | å¤§è¡Œåˆ—ã§ã¯å¯¾æ•°è¡Œåˆ—å¼ `slogdet` ã‚’ä½¿ã† |
| ãƒ©ãƒ³ã‚¯ | `np.linalg.matrix_rank(A)` | â€” | æ•°å€¤ãƒ©ãƒ³ã‚¯ï¼ˆé–¾å€¤ä»˜ãï¼‰ |
| ãƒãƒ«ãƒ  | `np.linalg.norm(A, ord)` | â€” | `ord=2`: ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒãƒ«ãƒ ã€`ord='fro'`: Frobenius |
| einsum | `np.einsum('ij,jk->ik', A, B)` | â€” | Einsteinè¨˜æ³•ã€‚ãƒãƒƒãƒå‡¦ç†ã«ä¾¿åˆ© |

```python
# å®Ÿå‹™ã§ã‚ˆãä½¿ã†ãƒ‘ã‚¿ãƒ¼ãƒ³é›†
import numpy as np
from scipy import linalg

A = np.random.randn(100, 100)
A = A.T @ A + np.eye(100)  # positive definite matrix

# Pattern 1: Cholesky ã§ solveï¼ˆæ­£å®šå€¤è¡Œåˆ—ã®å ´åˆæœ€é€Ÿï¼‰
b = np.random.randn(100)
L = linalg.cholesky(A, lower=True)
x = linalg.cho_solve((L, True), b)

# Pattern 2: å¯¾æ•°è¡Œåˆ—å¼ï¼ˆå¤§è¡Œåˆ—ã§ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼å›é¿ï¼‰
sign, logdet = np.linalg.slogdet(A)
print(f"log|det(A)| = {logdet:.4f}")

# Pattern 3: ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ã®ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãª SVD
U, s, Vt = np.linalg.svd(A, full_matrices=False)  # economy SVD
k = 10
A_approx = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]
print(f"Rank-{k} approximation error: {np.linalg.norm(A - A_approx, 'fro'):.4f}")
```

### 6.2 ç”¨èªé›†

:::details ç”¨èªé›†
| è‹±èª | æ—¥æœ¬èª | è¨˜å· |
|:-----|:------|:-----|
| Vector space | ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ | $V$ |
| Linear independence | ç·šå½¢ç‹¬ç«‹ | |
| Basis | åŸºåº• | $\{\mathbf{e}_i\}$ |
| Dimension | æ¬¡å…ƒ | $\dim V$ |
| Inner product | å†…ç© | $\langle \cdot, \cdot \rangle$ |
| Norm | ãƒãƒ«ãƒ  | $\|\cdot\|$ |
| Orthogonal | ç›´äº¤ | $\perp$ |
| Eigenvalue | å›ºæœ‰å€¤ | $\lambda$ |
| Eigenvector | å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ« | $\mathbf{v}$ |
| Positive definite | æ­£å®šå€¤ | $A \succ 0$ |
| Trace | ãƒˆãƒ¬ãƒ¼ã‚¹ | $\text{tr}(\cdot)$ |
| Determinant | è¡Œåˆ—å¼ | $\det(\cdot)$ |
| Rank | ãƒ©ãƒ³ã‚¯ | $\text{rank}(\cdot)$ |
| Projection | å°„å½± | $P$ |
| Least squares | æœ€å°äºŒä¹—æ³• | |
| QR decomposition | QRåˆ†è§£ | $A = QR$ |
| Cholesky decomposition | Choleskyåˆ†è§£ | $A = LL^\top$ |
| Spectral theorem | ã‚¹ãƒšã‚¯ãƒˆãƒ«å®šç† | |
| Cauchy-Schwarz inequality | Cauchy-Schwarzä¸ç­‰å¼ | |
| Rayleigh quotient | Rayleighå•† | $R(\mathbf{x})$ |
:::

### 6.25 è£œéº â€” é«˜é€ŸåŒ–æŠ€è¡“ã¨ãƒ©ãƒ³ãƒ€ãƒ åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

:::message
**è¨ˆç®—åŠ¹ç‡ã®é™ç•Œã¨çªç ´**: å¯†è¡Œåˆ—ã® SVD ã¯ $O(n^3)$ ã®è¨ˆç®—é‡ã ãŒ[^13]ã€ãƒ©ãƒ³ãƒ€ãƒ åŒ–ã¨GPUæ´»ç”¨ã§å®Ÿç”¨çš„ãªé«˜é€ŸåŒ–ãŒå¯èƒ½ã«ã€‚æœ¬ç¯€ã§ã¯æœ€æ–°ç ”ç©¶ã«åŸºã¥ãå®Ÿè·µçš„æ‰‹æ³•ã‚’è§£èª¬ã€‚
:::

#### ãƒ©ãƒ³ãƒ€ãƒ åŒ– SVD â€” å¤§è¦æ¨¡è¡Œåˆ—ã®ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼

é€šå¸¸ã® SVD ã¯ $O(\min(mn^2, m^2n))$ ã®è¨ˆç®—é‡ã‚’è¦ã™ã‚‹ãŒã€ãƒ©ãƒ³ãƒ€ãƒ åŒ– SVD[^14] ã¯ $O(mnk)$ï¼ˆ$k$ ã¯ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ©ãƒ³ã‚¯ï¼‰ã«å‰Šæ¸›ã§ãã‚‹ã€‚

##### ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

```python
import numpy as np

def randomized_svd(A: np.ndarray, k: int, n_oversamples: int = 10) -> tuple:
    """
    ãƒ©ãƒ³ãƒ€ãƒ åŒ– SVD ã«ã‚ˆã‚‹ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼

    Parameters
    ----------
    A : ndarray, shape (m, n)
        å…¥åŠ›è¡Œåˆ—
    k : int
        ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ©ãƒ³ã‚¯
    n_oversamples : int
        ã‚ªãƒ¼ãƒãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ•°ï¼ˆç²¾åº¦å‘ä¸Šã®ãŸã‚ï¼‰

    Returns
    -------
    U : ndarray, shape (m, k)
        å·¦ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ«
    s : ndarray, shape (k,)
        ç‰¹ç•°å€¤
    Vt : ndarray, shape (k, n)
        å³ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆè»¢ç½®æ¸ˆã¿ï¼‰

    References
    ----------
    Halko, Martinsson, & Tropp (2011). Finding structure with randomness.
    """
    m, n = A.shape
    p = k + n_oversamples

    # Step 1: ãƒ©ãƒ³ãƒ€ãƒ è¡Œåˆ—ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    Omega = np.random.randn(n, p)
    Y = A @ Omega  # shape (m, p)

    # Step 2: QR åˆ†è§£ã§æ­£è¦ç›´äº¤åŸºåº•ã‚’æ§‹ç¯‰
    Q, _ = np.linalg.qr(Y)

    # Step 3: éƒ¨åˆ†ç©ºé–“ã¸ã®å°„å½±
    B = Q.T @ A  # shape (p, n)

    # Step 4: å°ã•ãªè¡Œåˆ— B ã® SVD
    U_tilde, s, Vt = np.linalg.svd(B, full_matrices=False)

    # Step 5: å…ƒã®ç©ºé–“ã«æˆ»ã™
    U = Q @ U_tilde

    return U[:, :k], s[:k], Vt[:k, :]

# ä½¿ç”¨ä¾‹: 1000x1000 è¡Œåˆ—ã® ãƒ©ãƒ³ã‚¯10 è¿‘ä¼¼
A = np.random.randn(1000, 1000)
U, s, Vt = randomized_svd(A, k=10)
A_approx = U @ np.diag(s) @ Vt

print(f"Frobenius norm error: {np.linalg.norm(A - A_approx, 'fro'):.6f}")
print(f"Shape check: U={U.shape}, s={s.shape}, Vt={Vt.shape}")
```

**ç†è«–çš„ä¿è¨¼**:

$$
\mathbb{E}\left[\|A - QQ^\top A\|_F\right] \leq \left(1 + \frac{k}{p-k-1}\right)^{1/2} \sigma_{k+1}
$$

ã“ã“ã§ $\sigma_{k+1}$ ã¯ $(k+1)$ ç•ªç›®ã®ç‰¹ç•°å€¤ã€‚ã‚ªãƒ¼ãƒãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° $p = k + 10$ ã§é«˜ç²¾åº¦ãªè¿‘ä¼¼ãŒå¾—ã‚‰ã‚Œã‚‹ã€‚

##### æ€§èƒ½æ¯”è¼ƒ

| æ‰‹æ³• | è¨ˆç®—é‡ | 1000Ã—1000 (k=50) | ç²¾åº¦ |
|:---|:---|:---:|:---|
| é€šå¸¸ SVD | $O(n^3)$ | 2.3ç§’ | Exact |
| ãƒ©ãƒ³ãƒ€ãƒ åŒ– SVD | $O(mnk)$ | 0.08ç§’ | ç›¸å¯¾èª¤å·® < 1% |

#### GPU åŠ é€Ÿã«ã‚ˆã‚‹è¡Œåˆ—åˆ†è§£ã®é«˜é€ŸåŒ–

2024-2025å¹´ã®ç ”ç©¶[^15][^16]ã«ã‚ˆã‚Šã€GPUå®Ÿè£…ã§å¾“æ¥æ‰‹æ³•ã® 10-1000å€ã®é«˜é€ŸåŒ–ãŒå®Ÿç¾ã•ã‚Œã¦ã„ã‚‹ã€‚

##### QRåˆ†è§£ã®GPUå®Ÿè£…ï¼ˆCuPyï¼‰

```python
# CuPy ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹å ´åˆ
try:
    import cupy as cp

    def gpu_qr_decomposition(A_cpu: np.ndarray) -> tuple:
        """GPU ã‚’ä½¿ã£ãŸ QR åˆ†è§£"""
        # CPU â†’ GPU è»¢é€
        A_gpu = cp.asarray(A_cpu)

        # GPU ä¸Šã§ QR åˆ†è§£å®Ÿè¡Œ
        Q_gpu, R_gpu = cp.linalg.qr(A_gpu)

        # GPU â†’ CPU è»¢é€
        return cp.asnumpy(Q_gpu), cp.asnumpy(R_gpu)

    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    A = np.random.randn(5000, 5000)

    import time
    # CPU
    t0 = time.perf_counter()
    Q_cpu, R_cpu = np.linalg.qr(A)
    cpu_time = time.perf_counter() - t0

    # GPU
    t0 = time.perf_counter()
    Q_gpu, R_gpu = gpu_qr_decomposition(A)
    gpu_time = time.perf_counter() - t0

    print(f"CPU QR: {cpu_time:.3f}ç§’")
    print(f"GPU QR: {gpu_time:.3f}ç§’")
    print(f"Speedup: {cpu_time / gpu_time:.1f}x")
    # å…¸å‹çš„ãªçµæœ: 10-50x é«˜é€ŸåŒ–

except ImportError:
    print("CuPy not installed. Skipping GPU benchmark.")
```

##### æœ€æ–°ã® GPU-SVD ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

Wichmann et al. (2025)[^15] ã«ã‚ˆã‚‹ portable SVD å®Ÿè£…ã®ç‰¹å¾´:

- **2æ®µéš QR ç°¡ç´„**: bandå½¢å¼ â†’ 2å¯¾è§’å½¢å¼ã®æ®µéšçš„å¤‰æ›
- **GPUæœ€é©åŒ–**: Apple Metalã€CUDAã€ROCm ã«å¯¾å¿œ
- **åŠç²¾åº¦å¯¾å¿œ**: FP16 ã§ 2å€ã®ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼ˆç²¾åº¦è¦ä»¶ãŒç·©ã„å ´åˆï¼‰

æ•°å¼çš„ã«ã¯ã€ä»¥ä¸‹ã®å¤‰æ›ã‚’ GPU ä¸Šã§å®Ÿè¡Œ:

$$
A \xrightarrow{\text{Householder}} B \xrightarrow{\text{Givens}} \text{Bidiag} \xrightarrow{\text{D\&C}} U\Sigma V^\top
$$

å„ã‚¹ãƒ†ãƒ¼ã‚¸ã§ GPU ãƒ¡ãƒ¢ãƒªéšå±¤ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«/å…±æœ‰/ãƒ¬ã‚¸ã‚¹ã‚¿ï¼‰ã‚’æœ€é©æ´»ç”¨ã™ã‚‹ã“ã¨ã§ 100-300å€ã®é«˜é€ŸåŒ–ã‚’é”æˆ[^16]ã€‚

#### ãƒ©ãƒ³ã‚¯é¡•åœ¨åŒ– QLP åˆ†è§£

Randomized Rank-Revealing QLP (RU-QLP) åˆ†è§£[^17] ã¯ã€ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨ unpivoted QR ã‚’çµ„ã¿åˆã‚ã›:

$$
A P = Q \begin{bmatrix} L_{11} & 0 \\ L_{21} & L_{22} \end{bmatrix} P^\top
$$

ã“ã“ã§ $L_{11}$ ã¯ $k \times k$ ã®ä¸‹ä¸‰è§’è¡Œåˆ—ã€$P$ ã¯ç½®æ›è¡Œåˆ—ã€‚

##### æ€§èƒ½:
- **CPU**: ãƒ©ãƒ³ãƒ€ãƒ åŒ– SVD ã® 7.1-8.5å€é«˜é€Ÿ
- **GPU**: ãƒ©ãƒ³ãƒ€ãƒ åŒ– SVD ã® 2.3-5.8å€é«˜é€Ÿ
- **èª¤å·®ä¿è¨¼**: $\|A - A_k\|_2 \leq (1+\epsilon)\sigma_{k+1}$

```python
# scipy ã®å®Ÿè£…ä¾‹ (RU-QLP ã¯ç ”ç©¶æ®µéšã®ãŸã‚ pseudo-code)
from scipy.linalg import qr

def rank_revealing_qr(A: np.ndarray, k: int) -> tuple:
    """ãƒ©ãƒ³ã‚¯é¡•åœ¨åŒ– QRï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
    Q, R, P = qr(A, pivoting=True)
    # ä¸Šä½ k åˆ—ã‚’æŠ½å‡º
    return Q[:, :k], R[:k, :k], P[:k]
```

#### å®Ÿè·µçš„ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³

| è¡Œåˆ—ã‚µã‚¤ã‚º | ãƒ©ãƒ³ã‚¯ | æ¨å¥¨æ‰‹æ³• | ç†ç”± |
|:---|:---|:---|:---|
| $n < 1000$ | Full | `np.linalg.svd` | æ­£ç¢ºãƒ»ç°¡æ½” |
| $n \geq 1000$ | $k \ll n$ | ãƒ©ãƒ³ãƒ€ãƒ åŒ– SVD | $O(mnk)$ è¨ˆç®—é‡ |
| $n \geq 5000$ | Any | GPU (CuPy/JAX) | 10-100å€é«˜é€ŸåŒ– |
| ã‚¹ãƒ‘ãƒ¼ã‚¹ | å° $k$ | `scipy.sparse.linalg.svds` | ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ |

:::message
**æ³¨æ„**: GPU ã¯åˆæœŸåŒ–ã‚³ã‚¹ãƒˆï¼ˆæ•°ç™¾msï¼‰ãŒã‚ã‚‹ãŸã‚ã€å°è¦æ¨¡è¡Œåˆ—ã§ã¯ CPU ã®æ–¹ãŒé€Ÿã„å ´åˆã‚‚ã‚ã‚‹ã€‚$n \geq 5000$ ãŒç›®å®‰ã€‚
:::

#### ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªå®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³

```python
def efficient_large_matrix_svd(
    A: np.ndarray,
    k: int,
    method: str = "auto"
) -> tuple:
    """
    ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãª SVD

    Parameters
    ----------
    method : {"auto", "randomized", "gpu", "iterative"}
        "auto": ã‚µã‚¤ã‚ºã«å¿œã˜ã¦è‡ªå‹•é¸æŠ
    """
    m, n = A.shape

    if method == "auto":
        if min(m, n) < 1000:
            method = "standard"
        elif k < min(m, n) / 10:
            method = "randomized"
        elif min(m, n) >= 5000:
            method = "gpu"
        else:
            method = "iterative"

    if method == "standard":
        return np.linalg.svd(A, full_matrices=False)

    elif method == "randomized":
        return randomized_svd(A, k)

    elif method == "gpu":
        try:
            import cupy as cp
            A_gpu = cp.asarray(A)
            U, s, Vt = cp.linalg.svd(A_gpu, full_matrices=False)
            return cp.asnumpy(U), cp.asnumpy(s), cp.asnumpy(Vt)
        except ImportError:
            print("CuPy not found, falling back to CPU")
            return np.linalg.svd(A, full_matrices=False)

    elif method == "iterative":
        from scipy.sparse.linalg import svds
        # æ³¨: svds ã¯ k < min(m,n)-1 ã®åˆ¶ç´„ã‚ã‚Š
        U, s, Vt = svds(A, k=min(k, min(m, n) - 2))
        return U, s, Vt

    else:
        raise ValueError(f"Unknown method: {method}")

# ä½¿ç”¨ä¾‹
A_large = np.random.randn(10000, 5000)
U, s, Vt = efficient_large_matrix_svd(A_large, k=50, method="auto")
print(f"Computed top-{len(s)} singular values")
```

#### ã¾ã¨ã‚: ç·šå½¢ä»£æ•°ã®é«˜é€ŸåŒ–æŠ€è¡“ãƒãƒƒãƒ—

```mermaid
graph TD
    A[å¤§è¦æ¨¡è¡Œåˆ—ã®åˆ†è§£] --> B{ãƒ©ãƒ³ã‚¯}
    B -->|Full rank| C[GPUåŠ é€Ÿ<br/>CuPy/JAX]
    B -->|ä½ãƒ©ãƒ³ã‚¯ kâ‰ªn| D[ãƒ©ãƒ³ãƒ€ãƒ åŒ–æ‰‹æ³•]
    D --> E[Randomized SVD<br/>O mnk]
    D --> F[RU-QLP<br/>SVDã‚ˆã‚Šé«˜é€Ÿ]
    C --> G[2æ®µéšQRç°¡ç´„<br/>100-300xé«˜é€ŸåŒ–]
    E --> H[Halko 2011]
    F --> I[Feng 2022]
    G --> J[Wichmann 2025]
```

**References**:
- Halko, N., Martinsson, P. G., & Tropp, J. A. (2011). Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. *SIAM Review*, 53(2), 217-288.
- Martinsson, P. G., & Tropp, J. A. (2020). Randomized numerical linear algebra: Foundations and algorithms. *Acta Numerica*, 29, 403-572.

### 6.3 çŸ¥è­˜ãƒãƒƒãƒ—

```mermaid
mindmap
  root((ç·šå½¢ä»£æ•° I))
    ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“
      å…¬ç†
      ç·šå½¢ç‹¬ç«‹
      åŸºåº•ã¨æ¬¡å…ƒ
      ç·šå½¢å†™åƒ
    å†…ç©ã¨ç›´äº¤æ€§
      å†…ç©ã®å…¬ç†
      Cauchy-Schwarz
      ãƒãƒ«ãƒ 
      ç›´äº¤æ€§
    è¡Œåˆ—æ¼”ç®—
      ç©ã®3ã¤ã®è¦‹æ–¹
      è»¢ç½®
      é€†è¡Œåˆ—
      ãƒˆãƒ¬ãƒ¼ã‚¹
    å›ºæœ‰å€¤åˆ†è§£
      ç‰¹æ€§æ–¹ç¨‹å¼
      å¯¾è§’åŒ–
      ã‚¹ãƒšã‚¯ãƒˆãƒ«å®šç†
      Rayleighå•†
    æ­£å®šå€¤è¡Œåˆ—
      åˆ¤å®šæ¡ä»¶
      Choleskyåˆ†è§£
      äºŒæ¬¡å½¢å¼
    å°„å½±ã¨æœ€å°äºŒä¹—
      æ­£è¦æ–¹ç¨‹å¼
      PCA
      Attention QK^T
```

### 6.35 æ•°å€¤å®‰å®šæ€§ã¨æ¡ä»¶æ•° â€” å®Ÿè£…ã§é™¥ã‚Šã‚„ã™ã„ç½ 

:::message
**æ•°å€¤è¨ˆç®—ã®ç¾å®Ÿ**: æ•°å­¦çš„ã«æ­£ã—ã„å¼ã§ã‚‚ã€æµ®å‹•å°æ•°ç‚¹æ¼”ç®—ã§ã¯ä¸å®‰å®šã«ãªã‚Šå¾—ã‚‹[^18]ã€‚æ¡ä»¶æ•° (condition number) ã¯ã€ã“ã®å®‰å®šæ€§ã‚’å®šé‡åŒ–ã™ã‚‹éµã¨ãªã‚‹æ¦‚å¿µã€‚
:::

#### æ¡ä»¶æ•°ã®å®šç¾©ã¨æ„å‘³

è¡Œåˆ— $A \in \mathbb{R}^{n \times n}$ ã® **æ¡ä»¶æ•°** ã¯ä»¥ä¸‹ã§å®šç¾©ã•ã‚Œã‚‹:

$$
\kappa(A) = \|A\| \cdot \|A^{-1}\| = \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}
$$

ã“ã“ã§ $\sigma_{\max}, \sigma_{\min}$ ã¯æœ€å¤§ãƒ»æœ€å°ç‰¹ç•°å€¤ã€‚

**ç›´æ„Ÿçš„è§£é‡ˆ**:
- $\kappa(A) = 1$: ç†æƒ³çš„ï¼ˆç›´äº¤è¡Œåˆ—ï¼‰
- $\kappa(A) \sim 10^2$: è‰¯å¥½
- $\kappa(A) \sim 10^{6}$: è­¦æˆ’ï¼ˆå˜ç²¾åº¦FP32ã§æ¡è½ã¡ç™ºç”Ÿï¼‰
- $\kappa(A) \sim 10^{14}$: å±é™ºï¼ˆå€ç²¾åº¦FP64ã§ã‚‚ç²¾åº¦å–ªå¤±ï¼‰
- $\kappa(A) = \infty$: ç‰¹ç•°è¡Œåˆ—ï¼ˆé€†è¡Œåˆ—ãªã—ï¼‰

```python
import numpy as np

def analyze_condition_number(A: np.ndarray) -> None:
    """æ¡ä»¶æ•°ã®è¨ºæ–­ã¨è­¦å‘Š"""
    cond = np.linalg.cond(A)

    print(f"Condition number: {cond:.2e}")

    if cond < 100:
        print("âœ… æ•°å€¤çš„ã«å®‰å®š")
    elif cond < 1e6:
        print("âš ï¸ æ³¨æ„ãŒå¿…è¦ï¼ˆå€ç²¾åº¦æ¨å¥¨ï¼‰")
    elif cond < 1e14:
        print("ğŸš¨ ä¸å®‰å®šï¼ˆæ­£å‰‡åŒ–ã‚’æ¤œè¨ï¼‰")
    else:
        print("âŒ ç‰¹ç•°ã«è¿‘ã„ï¼ˆè§£ãŒä¿¡é ¼ã§ããªã„ï¼‰")

    # æœ€å¤§ãƒ»æœ€å°ç‰¹ç•°å€¤ã‚’è¡¨ç¤º
    s = np.linalg.svd(A, compute_uv=False)
    print(f"Ïƒ_max = {s[0]:.2e}, Ïƒ_min = {s[-1]:.2e}")
    print(f"Ïƒ_max / Ïƒ_min = {s[0] / s[-1]:.2e}")

# ä¾‹1: è‰¯å¥½ãªæ¡ä»¶æ•°ï¼ˆç›´äº¤è¡Œåˆ—ï¼‰
Q, _ = np.linalg.qr(np.random.randn(5, 5))
analyze_condition_number(Q)
# Condition number: 1.00e+00 âœ…

# ä¾‹2: æ‚ªã„æ¡ä»¶æ•°ï¼ˆã»ã¼ç·šå½¢å¾“å±ãªåˆ—ï¼‰
A_bad = np.array([
    [1, 1.0001],
    [1, 1.0000]
])
analyze_condition_number(A_bad)
# Condition number: ~2.00e+04 âš ï¸
```

#### æ¡ä»¶æ•°ãŒå¤§ãããªã‚‹å®Ÿä¾‹

##### 1. é«˜ç›¸é–¢ãªç‰¹å¾´é‡è¡Œåˆ—ï¼ˆæ©Ÿæ¢°å­¦ç¿’ã§ã®å…¸å‹ä¾‹ï¼‰

```python
# ä¾‹: 3ã¤ã®ç‰¹å¾´é‡ã®ã†ã¡2ã¤ãŒé«˜ç›¸é–¢
X = np.random.randn(100, 3)
X[:, 2] = 0.999 * X[:, 0] + 0.001 * np.random.randn(100)  # é«˜ç›¸é–¢

# å…±åˆ†æ•£è¡Œåˆ—ã®æ¡ä»¶æ•°
cov = X.T @ X
print(f"Îº(X^T X) = {np.linalg.cond(cov):.2e}")
# Îº(X^T X) ~ 1e6 ä»¥ä¸Š â†’ ä¸å®‰å®š

# å¯¾ç­–: æ­£å‰‡åŒ–ï¼ˆRidgeå›å¸°ï¼‰
lambda_reg = 1e-3
cov_reg = cov + lambda_reg * np.eye(3)
print(f"Îº(X^T X + Î»I) = {np.linalg.cond(cov_reg):.2e}")
# Îº ãŒå¤§å¹…ã«æ”¹å–„
```

##### 2. Hilbert è¡Œåˆ—ï¼ˆæ•™ç§‘æ›¸çš„ãªç—…çš„è¡Œåˆ—ï¼‰

$$
H_{ij} = \frac{1}{i+j-1}, \quad i, j = 1, \ldots, n
$$

```python
from scipy.linalg import hilbert

H = hilbert(10)
print(f"Îº(H_10) = {np.linalg.cond(H):.2e}")
# Îº(H_10) ~ 1.6e13 ï¼ˆ10Ã—10ã§ã‚‚ç ´ç¶»å¯¸å‰ï¼‰

# çœŸã®è§£
x_true = np.ones(10)
b = H @ x_true

# æ•°å€¤çš„ã«è§£ã
x_solve = np.linalg.solve(H, b)
rel_error = np.linalg.norm(x_solve - x_true) / np.linalg.norm(x_true)
print(f"Relative error: {rel_error:.2e}")
# Relative error ~ 1e-3 ï¼ˆ1000å€ã®èª¤å·®ï¼ï¼‰
```

##### 3. æ·±å±¤å­¦ç¿’ã®é‡ã¿è¡Œåˆ—

ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®è¨“ç·´ä¸­ã€é‡ã¿è¡Œåˆ—ã®æ¡ä»¶æ•°ãŒå¢—å¤§ã™ã‚‹ã¨å‹¾é…æ¶ˆå¤±ãƒ»çˆ†ç™ºãŒç™ºç”Ÿ[^18]ã€‚

```python
# ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: 100å±¤ã®ç·šå½¢ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
def simulate_deep_network(n_layers: int, cond: float) -> None:
    """æ¡ä»¶æ•° cond ã®è¡Œåˆ—ã‚’ n_layers å›æ›ã‘ã‚‹"""
    d = 10
    # æ¡ä»¶æ•°ã‚’åˆ¶å¾¡ã—ãŸè¡Œåˆ—ç”Ÿæˆ
    U, _ = np.linalg.qr(np.random.randn(d, d))
    s = np.linspace(cond, 1, d)  # Ïƒ_max/Ïƒ_min = cond
    V, _ = np.linalg.qr(np.random.randn(d, d))
    W = U @ np.diag(s) @ V.T

    # å‹¾é…ã®ä¼æ’­ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    grad = np.ones(d)
    for _ in range(n_layers):
        grad = W.T @ grad

    norm = np.linalg.norm(grad)
    print(f"Îº={cond:.1e}, {n_layers}å±¤å¾Œã®å‹¾é…ãƒãƒ«ãƒ : {norm:.2e}")

simulate_deep_network(100, 1.1)    # Îº=1.1 â†’ å®‰å®š
simulate_deep_network(100, 2.0)    # Îº=2.0 â†’ å‹¾é…çˆ†ç™º
simulate_deep_network(100, 0.5)    # Îº=0.5 â†’ å‹¾é…æ¶ˆå¤±
```

**å®Ÿéš›ã®å¯¾ç­–**:
- **Batch Normalization**: å±¤ã”ã¨ã«æ­£è¦åŒ–ã—ã€æ¡ä»¶æ•°ã‚’æŠ‘åˆ¶
- **Residual Connections (ResNet)**: ç›´æ¥ãƒ‘ã‚¹ã§æ¡ä»¶æ•°ã®ç´¯ç©ã‚’å›é¿
- **Weight Normalization**: é‡ã¿ã‚’å˜ä½ãƒãƒ«ãƒ ã«æ­£è¦åŒ–

#### æ•°å€¤å®‰å®šãªå®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³

##### ãƒ‘ã‚¿ãƒ¼ãƒ³1: é€£ç«‹æ–¹ç¨‹å¼ã¯é€†è¡Œåˆ—ã§ã¯ãªãç›´æ¥æ³•ã§

```python
# âŒ æ•°å€¤çš„ã«ä¸å®‰å®š
A_inv = np.linalg.inv(A)
x_bad = A_inv @ b

# âœ… æ•°å€¤çš„ã«å®‰å®šï¼ˆLUåˆ†è§£ã‚’å†…éƒ¨ã§ä½¿ç”¨ï¼‰
x_good = np.linalg.solve(A, b)

# ç²¾åº¦æ¯”è¼ƒ
print(f"Residual (inv):   {np.linalg.norm(A @ x_bad - b):.2e}")
print(f"Residual (solve): {np.linalg.norm(A @ x_good - b):.2e}")
# solve ã®æ–¹ãŒèª¤å·®ãŒå°ã•ã„
```

**ç†è«–çš„æ ¹æ‹ **: $\kappa(A)$ ãŒå¤§ãã„ã¨ãã€$A^{-1}$ ã®è¨ˆç®—èª¤å·®ãŒè§£ $x$ ã«å¢—å¹…ã•ã‚Œã‚‹ã€‚ç›´æ¥æ³•ã¯å®‰å®šæ€§ãŒé«˜ã„ã€‚

##### ãƒ‘ã‚¿ãƒ¼ãƒ³2: æ­£å®šå€¤è¡Œåˆ—ã«ã¯ Cholesky åˆ†è§£

```python
# æ­£å®šå€¤è¡Œåˆ—ã®ç”Ÿæˆ
A_pos = np.random.randn(100, 100)
A_pos = A_pos.T @ A_pos + 1e-6 * np.eye(100)  # æ­£å®šå€¤ä¿è¨¼

# âŒ ä¸€èˆ¬çš„ãª LU åˆ†è§£
x_lu = np.linalg.solve(A_pos, b)

# âœ… Cholesky åˆ†è§£ï¼ˆæ­£å®šå€¤å°‚ç”¨ã€2å€é«˜é€Ÿ + å®‰å®šï¼‰
from scipy.linalg import cho_factor, cho_solve
c, low = cho_factor(A_pos)
x_chol = cho_solve((c, low), b)

# é€Ÿåº¦æ¯”è¼ƒ
import time
t0 = time.perf_counter()
for _ in range(100):
    np.linalg.solve(A_pos, b)
lu_time = time.perf_counter() - t0

t0 = time.perf_counter()
for _ in range(100):
    cho_solve((c, low), b)
chol_time = time.perf_counter() - t0

print(f"LU time:      {lu_time:.4f}s")
print(f"Cholesky time: {chol_time:.4f}s")
print(f"Speedup: {lu_time / chol_time:.2f}x")
# Cholesky ã¯ 1.5-2å€é«˜é€Ÿ
```

##### ãƒ‘ã‚¿ãƒ¼ãƒ³3: SVD ã«ã‚ˆã‚‹å®‰å®šãªç–‘ä¼¼é€†è¡Œåˆ—

æ¡ä»¶æ•°ãŒå¤§ããã€ãƒ©ãƒ³ã‚¯ãŒä¸æ˜ç­ãªå ´åˆ:

```python
def stable_pseudoinverse(A: np.ndarray, rcond: float = 1e-6) -> np.ndarray:
    """
    æ¡ä»¶æ•°é–¾å€¤ã«ã‚ˆã‚‹ç–‘ä¼¼é€†è¡Œåˆ—

    Parameters
    ----------
    rcond : float
        ç›¸å¯¾æ¡ä»¶æ•°ã®é–¾å€¤ã€‚Ïƒ_i < rcond * Ïƒ_max ã¨ãªã‚‹ç‰¹ç•°å€¤ã‚’ 0 æ‰±ã„
    """
    U, s, Vt = np.linalg.svd(A, full_matrices=False)

    # é–¾å€¤æœªæº€ã®ç‰¹ç•°å€¤ã‚’ãƒ•ã‚£ãƒ«ã‚¿
    cutoff = rcond * s[0]
    s_inv = np.where(s > cutoff, 1.0 / s, 0.0)

    # A^+ = V Î£^+ U^T
    return Vt.T @ np.diag(s_inv) @ U.T

# ç—…çš„è¡Œåˆ—ã§ã®æ¯”è¼ƒ
H = hilbert(10)
b = np.ones(10)

# âŒ np.linalg.invï¼ˆä¸å®‰å®šï¼‰
try:
    x_inv = np.linalg.inv(H) @ b
    print(f"inv solution norm: {np.linalg.norm(x_inv):.2e}")
except np.linalg.LinAlgError:
    print("inv failed (singular matrix)")

# âœ… stable_pseudoinverse
x_pinv = stable_pseudoinverse(H, rcond=1e-10) @ b
print(f"SVD-based solution norm: {np.linalg.norm(x_pinv):.2e}")
# ã‚ˆã‚Šä¿¡é ¼ã§ãã‚‹è§£
```

#### æ¡ä»¶æ•°åˆ¶ç´„ä»˜ãå…±åˆ†æ•£è¡Œåˆ—è¿‘ä¼¼

Zhao et al. (2020)[^19] ã¯ã€é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã®å…±åˆ†æ•£è¡Œåˆ—æ¨å®šã«ãŠã„ã¦ã€æ¡ä»¶æ•°åˆ¶ç´„ã‚’èª²ã™ã“ã¨ã§æ•°å€¤å®‰å®šæ€§ã¨æ­£å®šå€¤æ€§ã‚’åŒæ™‚ã«ä¿è¨¼ã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆ:

$$
\min_{S \succ 0} \|S - \hat{\Sigma}\|_F^2 \quad \text{s.t.} \quad \kappa(S) \leq \kappa_{\max}
$$

ã“ã“ã§ $\hat{\Sigma}$ ã¯ã‚µãƒ³ãƒ—ãƒ«å…±åˆ†æ•£è¡Œåˆ—ã€$\kappa_{\max}$ ã¯è¨±å®¹æ¡ä»¶æ•°ã€‚

```python
def condition_constrained_covariance(
    Sigma_hat: np.ndarray,
    kappa_max: float
) -> np.ndarray:
    """
    æ¡ä»¶æ•°åˆ¶ç´„ä»˜ãå…±åˆ†æ•£è¡Œåˆ—è¿‘ä¼¼ï¼ˆç°¡æ˜“ç‰ˆï¼‰

    Parameters
    ----------
    Sigma_hat : ndarray
        ã‚µãƒ³ãƒ—ãƒ«å…±åˆ†æ•£è¡Œåˆ—ï¼ˆæ­£å®šå€¤ã§ãªã„å¯èƒ½æ€§ã‚ã‚Šï¼‰
    kappa_max : float
        ç›®æ¨™æ¡ä»¶æ•°ã®ä¸Šé™
    """
    # å›ºæœ‰å€¤åˆ†è§£
    eigvals, eigvecs = np.linalg.eigh(Sigma_hat)

    # è² ã®å›ºæœ‰å€¤ã‚’å°ã•ãªæ­£å€¤ã«ç½®ãæ›ãˆ
    eigvals = np.maximum(eigvals, 1e-10)

    # æ¡ä»¶æ•°åˆ¶ç´„: Î»_min ã‚’èª¿æ•´
    lambda_max = eigvals[-1]
    lambda_min_target = lambda_max / kappa_max
    eigvals = np.maximum(eigvals, lambda_min_target)

    # å†æ§‹æˆ
    S = eigvecs @ np.diag(eigvals) @ eigvecs.T
    return S

# ä½¿ç”¨ä¾‹: é«˜æ¬¡å…ƒãƒ»å°ã‚µãƒ³ãƒ—ãƒ«ã®å…±åˆ†æ•£è¡Œåˆ—
n_samples, n_features = 50, 100
X = np.random.randn(n_samples, n_features)
Sigma_hat = X.T @ X / n_samples  # ã‚µãƒ³ãƒ—ãƒ«å…±åˆ†æ•£ï¼ˆãƒ©ãƒ³ã‚¯ä¸è¶³ï¼‰

print(f"Original Îº: {np.linalg.cond(Sigma_hat):.2e}")
# Îº ~ âˆ ï¼ˆãƒ©ãƒ³ã‚¯ < n_features ã®ãŸã‚ï¼‰

S_reg = condition_constrained_covariance(Sigma_hat, kappa_max=100)
print(f"Regularized Îº: {np.linalg.cond(S_reg):.2e}")
# Îº â‰¤ 100 ã«åˆ¶ç´„
```

ã“ã®æ‰‹æ³•ã¯ã€Ridgeå›å¸°ãƒ»æ­£å‰‡åŒ–å…±åˆ†æ•£æ¨å®šãƒ»ã‚«ãƒ¼ãƒãƒ«æ³•ãªã©ã®ç†è«–çš„åŸºç›¤ã¨ãªã£ã¦ã„ã‚‹ã€‚

#### å®Ÿè·µçš„ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³: æ¡ä»¶æ•°è¨ºæ–­ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

| çŠ¶æ³ | æ¡ä»¶æ•°ç¯„å›² | æ¨å¥¨å¯¾ç­– |
|:---|:---|:---|
| ç·šå½¢å›å¸°ï¼ˆé«˜ç›¸é–¢ç‰¹å¾´ï¼‰ | $\kappa \geq 10^6$ | Ridge / Lasso / PCA ã§æ¬¡å…ƒå‰Šæ¸› |
| å…±åˆ†æ•£è¡Œåˆ—ï¼ˆ$n < p$ï¼‰ | $\kappa = \infty$ | æ­£å‰‡åŒ– or Ledoit-Wolf æ¨å®š |
| ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆè¨“ç·´ | å±¤æ•°ã«å¿œã˜ã¦å¢—å¤§ | Batch Norm / Layer Norm / ResNet |
| æ•°å€¤æœ€é©åŒ–ï¼ˆHessianï¼‰ | $\kappa \geq 10^4$ | Preconditioner / Adam / 2æ¬¡æ‰‹æ³• |
| GPU ã§ã® FP16 è¨ˆç®— | $\kappa \geq 10^3$ | Mixed precision trainingï¼ˆFP32 accumulationï¼‰ |

```python
def diagnose_matrix(A: np.ndarray, name: str = "A") -> None:
    """è¡Œåˆ—ã®æ¡ä»¶æ•°ã¨æ¨å¥¨å¯¾ç­–ã‚’è¨ºæ–­"""
    cond = np.linalg.cond(A)
    print(f"\n{'='*50}")
    print(f"Matrix: {name}, Shape: {A.shape}")
    print(f"Condition number: {cond:.2e}")

    if cond < 100:
        print("âœ… å®‰å®š â€” è¿½åŠ å¯¾ç­–ä¸è¦")
    elif cond < 1e4:
        print("âš ï¸ å€ç²¾åº¦æ¨å¥¨ â€” FP64 ã§è¨ˆç®—")
    elif cond < 1e6:
        print("ğŸš¨ æ­£å‰‡åŒ–æ¨å¥¨ â€” Ridge (Î» ~ 1e-4)")
    elif cond < 1e12:
        print("âŒ å¼·ã„æ­£å‰‡åŒ–å¿…é ˆ â€” Î» ~ 1e-2 or PCA")
    else:
        print("â˜ ï¸ ç‰¹ç•°ã«è¿‘ã„ â€” ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚’è¦‹ç›´ã™")

    # SVD ã§ãƒ©ãƒ³ã‚¯ã‚’ç¢ºèª
    s = np.linalg.svd(A, compute_uv=False)
    rank_tol = 1e-10 * s[0]
    effective_rank = np.sum(s > rank_tol)
    print(f"Effective rank: {effective_rank} / {min(A.shape)}")
    print(f"{'='*50}")

# ä½¿ç”¨ä¾‹
X_corr = np.random.randn(100, 10)
X_corr[:, 9] = 0.99 * X_corr[:, 0]  # é«˜ç›¸é–¢ç‰¹å¾´
diagnose_matrix(X_corr.T @ X_corr, "X^T X (é«˜ç›¸é–¢)")
```

#### ã¾ã¨ã‚: æ•°å€¤å®‰å®šæ€§ã®åŸå‰‡

1. **é€†è¡Œåˆ—ã¯é¿ã‘ã‚‹** â€” `solve()` ã‚’ä½¿ã†
2. **æ­£å®šå€¤è¡Œåˆ—ã«ã¯ Cholesky** â€” é«˜é€Ÿ + å®‰å®š
3. **æ¡ä»¶æ•°ã‚’ç›£è¦–** â€” `np.linalg.cond()` ã§å®šæœŸãƒã‚§ãƒƒã‚¯
4. **æ­£å‰‡åŒ–ã¯ä¸‡èƒ½è–¬** â€” $\lambda \sim \sigma_{\min}$ ãŒç›®å®‰
5. **SVD ã¯æœ€å¾Œã®ç ¦** â€” ç–‘ä¼¼é€†è¡Œåˆ—ã§é ‘å¥ã«è§£ã

```mermaid
graph TD
    A[ç·šå½¢ã‚·ã‚¹ãƒ†ãƒ  Ax=b] --> B{A ã¯æ­£å®šå€¤?}
    B -->|Yes| C[Choleskyåˆ†è§£<br/>cho_solve]
    B -->|No| D{Îº A ?}
    D -->|Îº < 10^6| E[LUåˆ†è§£<br/>np.linalg.solve]
    D -->|Îº â‰¥ 10^6| F{ãƒ©ãƒ³ã‚¯ä¸è¶³?}
    F -->|Yes| G[SVDç–‘ä¼¼é€†è¡Œåˆ—<br/>pinv rcond=1e-6]
    F -->|No| H[æ­£å‰‡åŒ–<br/>Ridge Î»~1e-4]
```

### 6.4 æœ¬è¬›ç¾©ã®3ã¤ã®ãƒã‚¤ãƒ³ãƒˆ

**1. å†…ç© = é¡ä¼¼åº¦ã®æ•°å­¦çš„åŸºç›¤**

$$
\langle \mathbf{q}_i, \mathbf{k}_j \rangle = \mathbf{q}_i^\top \mathbf{k}_j
$$

Attention[^1]ã®æ ¸å¿ƒã¯å†…ç©ã«ã‚ˆã‚‹é¡ä¼¼åº¦è¨ˆç®—ã€‚Cauchy-Schwarzä¸ç­‰å¼ãŒã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®å€¤åŸŸ $[-1, 1]$ ã‚’ä¿è¨¼ã™ã‚‹ã€‚

**2. å›ºæœ‰å€¤åˆ†è§£ = è¡Œåˆ—ã®ã€ŒXç·šå†™çœŸã€**

$$
A = Q\Lambda Q^\top
$$

å¯¾ç§°è¡Œåˆ—ã¯å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã§å¯¾è§’åŒ–ã§ãã€å›ºæœ‰å€¤ãŒè¡Œåˆ—ã®æœ¬è³ªçš„ãªæƒ…å ±ï¼ˆåˆ†æ•£ã®å¤§ãã•ã€å®‰å®šæ€§ã€å‡¸æ€§ï¼‰ã‚’æ˜ã‚‰ã‹ã«ã™ã‚‹ã€‚PCA[^6][^7]ã¯ã“ã®ç›´æ¥çš„ãªå¿œç”¨ã€‚

**3. æ­£å®šå€¤æ€§ = å®‰å…¨è£…ç½®**

$$
\mathbf{x}^\top A \mathbf{x} > 0 \quad \forall \mathbf{x} \neq \mathbf{0}
$$

å…±åˆ†æ•£è¡Œåˆ—ã®æ­£å®šå€¤æ€§ã€ãƒ˜ã‚·ã‚¢ãƒ³ã®æ­£å®šå€¤æ€§ã«ã‚ˆã‚‹å‡¸æ€§ä¿è¨¼ã€Choleskyåˆ†è§£ã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªè¨ˆç®—ã€‚

### 6.5 FAQ

:::details Q: ç·šå½¢ä»£æ•°ã¯ã©ã“ã¾ã§æ·±ãã‚„ã‚‹ã¹ãï¼Ÿ
ã“ã®è¬›ç¾©ã¨æ¬¡ã®ç¬¬3å›ã§æ‰±ã†ç¯„å›²ã‚’ã—ã£ã‹ã‚Šç†è§£ã™ã‚Œã°ã€Course IIï¼ˆç¬¬9-16å›ï¼‰ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æ•°å¼ã¯å…¨ã¦èª­ã‚ã‚‹ã€‚è¨¼æ˜ã‚’æš—è¨˜ã™ã‚‹å¿…è¦ã¯ãªã„ã€‚ã€Œãªãœã“ã†ãªã‚‹ã‹ã€ã®ç›´æ„Ÿã‚’æŒã£ã¦ã„ã‚Œã°ååˆ†ã€‚

ãŸã ã—ã€ç ”ç©¶ã§ä½¿ã†å ´åˆã¯ Golub & Van Loan[^8] ã®é–¢é€£ç« ã‚’èª­ã‚€ã“ã¨ã‚’å‹§ã‚ã‚‹ã€‚æ•°å€¤å®‰å®šæ€§ã‚„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®é¸æŠè‚¢ã«é–¢ã™ã‚‹çŸ¥è­˜ã¯ã€å®Ÿè£…ã®å“è³ªã«ç›´çµã™ã‚‹ã€‚
:::

:::details Q: eigh ã¨ eig ã®ä½¿ã„åˆ†ã‘ã¯ï¼Ÿ
å¯¾ç§°è¡Œåˆ—ï¼ˆå…±åˆ†æ•£è¡Œåˆ—ã€ãƒ˜ã‚·ã‚¢ãƒ³ç­‰ï¼‰ã«ã¯å¿…ãš `eigh` ã‚’ä½¿ã†ã€‚ä¸€èˆ¬è¡Œåˆ—ã«ã¯ `eig`ã€‚`eigh` ã¯å¯¾ç§°æ€§ã‚’åˆ©ç”¨ã™ã‚‹ã®ã§ç´„2å€é€Ÿãã€å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã®ç›´äº¤æ€§ãŒæ•°å€¤çš„ã«ã‚‚ä¿è¨¼ã•ã‚Œã‚‹ã€‚
:::

:::details Q: é€†è¡Œåˆ—ã®è¨ˆç®—ã¯ã©ã®ãã‚‰ã„é¿ã‘ã‚‹ã¹ãï¼Ÿ
æ˜ç¤ºçš„ã« $A^{-1}$ ãŒå¿…è¦ãªå ´é¢ã¯ã»ã¼ãªã„ã€‚$A^{-1}\mathbf{b}$ â†’ `solve(A, b)`ã€$A^{-1}B$ â†’ `solve(A, B)`ã€$\det(A^{-1})$ â†’ `1/det(A)`ã€‚$A^{-1}$ è‡ªä½“ãŒå¿…è¦ãªã®ã¯ã€å°„å½±è¡Œåˆ— $P = A(A^\top A)^{-1}A^\top$ ã®å¯è¦–åŒ–ãã‚‰ã„ã€‚
:::

:::details Q: PCA ã§æ¬¡å…ƒã‚’ã„ãã¤ã«è½ã¨ã™ã¹ãï¼Ÿ
ç´¯ç©å¯„ä¸ç‡ï¼ˆcumulative explained variance ratioï¼‰ãŒ 90-95% ã«ãªã‚‹æ¬¡å…ƒæ•°ãŒä¸€èˆ¬çš„ãªç›®å®‰ã€‚ãŸã ã—ã€å¯è¦–åŒ–ç›®çš„ãªã‚‰2-3æ¬¡å…ƒã€‚ãƒ€ã‚¦ãƒ³ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚¿ã‚¹ã‚¯ã®æ€§èƒ½ã§æ±ºã‚ã‚‹ã®ãŒæœ€å–„ã€‚
:::

:::details Q: einsum ã¯è¦šãˆã‚‹å¿…è¦ãŒã‚ã‚‹ï¼Ÿ
å¿…é ˆã§ã¯ãªã„ãŒã€è«–æ–‡ã®ã‚³ãƒ¼ãƒ‰ã§ã‚ˆãè¦‹ã‹ã‘ã‚‹ã€‚ç‰¹ã«Transformerç³»ã®å®Ÿè£…ã§ã¯ `einsum` ãŒå¤šç”¨ã•ã‚Œã‚‹ã€‚æœ€ä½é™ã€å†…ç© `'i,i->'`ã€è¡Œåˆ—ç© `'ik,kj->ij'`ã€ãƒãƒƒãƒè¡Œåˆ—ç© `'bik,bkj->bij'` ã®3ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¦šãˆã¦ãŠã‘ã°å›°ã‚‰ãªã„ã€‚
:::

:::details Q: ç·šå½¢ä»£æ•°ã¨å¾®ç©åˆ†ã€ã©ã¡ã‚‰ãŒå…ˆã«å¿…è¦ï¼Ÿ
ç·šå½¢ä»£æ•°ãŒå…ˆã€‚ç†ç”±: (1) æ©Ÿæ¢°å­¦ç¿’ã®å¤šãã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯è¡Œåˆ—æ¼”ç®—ã§è¨˜è¿°ã•ã‚Œã‚‹ã€(2) å‹¾é…ã¯ã€Œãƒ™ã‚¯ãƒˆãƒ«å€¤é–¢æ•°ã®å¾®åˆ†ã€ãªã®ã§ç·šå½¢ä»£æ•°ã®è¨€è‘‰ã§å®šç¾©ã•ã‚Œã‚‹ã€(3) é€†ä¼æ’­æ³•ã¯ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®é€£é–å¾‹ã§ã‚ã‚Šã€è¡Œåˆ—å¾®åˆ†ã¯ç·šå½¢ä»£æ•°ã®ä¸Šã«æ§‹ç¯‰ã•ã‚Œã‚‹ã€‚

æœ¬ã‚·ãƒªãƒ¼ã‚ºã§ã¯ç¬¬2-3å›ã§ç·šå½¢ä»£æ•°ã€ç¬¬4å›ã§ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦ã€ç¬¬5å›ã§æ¸¬åº¦è«–çš„ç¢ºç‡è«–ãƒ»ç¢ºç‡éç¨‹ã€ç¬¬6å›ã§æƒ…å ±ç†è«–ãƒ»æœ€é©åŒ–ç†è«–ã®é †ç•ªã‚’å–ã£ã¦ã„ã‚‹ã€‚
:::

:::details Q: å¤§ããªè¡Œåˆ—ã®å›ºæœ‰å€¤åˆ†è§£ã¯é…ã„ã®ã§ã¯ï¼Ÿ
ãã®é€šã‚Šã€‚$n \times n$ è¡Œåˆ—ã®å®Œå…¨ãªå›ºæœ‰å€¤åˆ†è§£ã¯ $O(n^3)$ ã§ã€$n > 10000$ ã§ã¯å®Ÿç”¨çš„ã§ãªã„ã€‚å®Ÿå‹™ã§ã¯:

1. **Power Iteration / Lanczosæ³•**: ä¸Šä½ $k$ å€‹ã®å›ºæœ‰å€¤ãƒ»å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã ã‘ã‚’ $O(kn^2)$ ã§è¨ˆç®—
2. **Randomized SVD**: ãƒ©ãƒ³ãƒ€ãƒ å°„å½±ã§æ¬¡å…ƒã‚’è½ã¨ã—ã¦ã‹ã‚‰SVDã€‚scikit-learn ã® PCA ã¯ã“ã‚Œã‚’ä½¿ã†
3. **Sparse solver**: ç–è¡Œåˆ—ãªã‚‰ `scipy.sparse.linalg.eigsh` ã§å¤§è¦æ¨¡å•é¡Œã«å¯¾å¿œ
4. **GPUè¨ˆç®—**: cuSOLVER ã§ GPUä¸Šã®å¤§è¦æ¨¡å›ºæœ‰å€¤åˆ†è§£

ç¬¬3å›ã§SVDã®åŠ¹ç‡çš„ãªè¨ˆç®—æ³•ã‚’è©³ã—ãæ‰±ã†ã€‚
:::

:::details Q: PyTorch ã§ã‚‚ç·šå½¢ä»£æ•°é–¢æ•°ã‚’ä½¿ãˆã‚‹ã‹ï¼Ÿ
ä½¿ãˆã‚‹ã€‚`torch.linalg` ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒ NumPy ã® `np.linalg` ã¨ã»ã¼åŒã˜APIã‚’æä¾›ã™ã‚‹ã€‚è‡ªå‹•å¾®åˆ†å¯¾å¿œãªã®ã§ã€å›ºæœ‰å€¤åˆ†è§£ã‚„SVDã®çµæœã‚’é€šã˜ã¦å‹¾é…ã‚’é€†ä¼æ’­ã§ãã‚‹ã€‚

```python
import torch

A = torch.randn(3, 3)
A = A.T @ A  # positive definite
A.requires_grad_(True)

vals, vecs = torch.linalg.eigh(A)
loss = vals.sum()
loss.backward()  # A.grad ã«å›ºæœ‰å€¤ã® A ã«å¯¾ã™ã‚‹å‹¾é…ãŒå…¥ã‚‹
```

ãŸã ã—ã€å›ºæœ‰å€¤ãŒé‡è¤‡ï¼ˆdegenerateï¼‰ã—ã¦ã„ã‚‹å ´åˆã®å‹¾é…ã¯ä¸å®‰å®šãªã®ã§æ³¨æ„ã€‚
:::

### 6.6 ã‚ˆãã‚ã‚‹é–“é•ã„ãƒ»å‹˜é•ã„

ç·šå½¢ä»£æ•°ã®å­¦ç¿’ã§é »å‡ºã™ã‚‹é–“é•ã„ã‚’å…ˆã«çŸ¥ã£ã¦ãŠãã“ã¨ã§ã€ç„¡é§„ãªèº“ãã‚’é¿ã‘ã‚‰ã‚Œã‚‹ã€‚

#### é–“é•ã„1: è¡Œåˆ—ç©ã¯äº¤æ›å¯èƒ½

$$
AB \neq BA \quad \text{ï¼ˆä¸€èˆ¬ã«ã¯æˆã‚Šç«‹ãŸãªã„ï¼‰}
$$

```python
import numpy as np

A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])
print("AB =\n", A @ B)
print("BA =\n", B @ A)
print("AB == BA?", np.allclose(A @ B, B @ A))  # False
```

**æ­£ã—ã„ç†è§£**: è¡Œåˆ—ç©ã¯ä¸€èˆ¬ã«éå¯æ›ã€‚$AB = BA$ ãŒæˆã‚Šç«‹ã¤ã®ã¯ç‰¹æ®Šãªå ´åˆï¼ˆ$B = \alpha I$ã€$A$ ã¨ $B$ ãŒåŒæ™‚å¯¾è§’åŒ–å¯èƒ½ãªå ´åˆãªã©ï¼‰ã®ã¿ã€‚ãŸã ã—ã€ãƒˆãƒ¬ãƒ¼ã‚¹ã«ã¤ã„ã¦ã¯ $\text{tr}(AB) = \text{tr}(BA)$ï¼ˆå·¡å›æ€§ï¼‰ãŒ**å¸¸ã«**æˆã‚Šç«‹ã¤ã€‚

#### é–“é•ã„2: é€†è¡Œåˆ—ã§é€£ç«‹æ–¹ç¨‹å¼ã‚’è§£ã

```python
# BAD: é€†è¡Œåˆ—ã‚’æ˜ç¤ºçš„ã«è¨ˆç®—
x_bad = np.linalg.inv(A) @ b  # O(n^3) + æ•°å€¤ä¸å®‰å®š

# GOOD: solve ã‚’ä½¿ã†
x_good = np.linalg.solve(A, b)  # O(n^3) ã ãŒæ•°å€¤å®‰å®š
```

**æ­£ã—ã„ç†è§£**: `solve` ã¯å†…éƒ¨ã§LUåˆ†è§£ã‚’ä½¿ã„ã€é€†è¡Œåˆ—ã‚’æ˜ç¤ºçš„ã«è¨ˆç®—ã—ãªã„ã€‚è¨ˆç®—é‡ã®ã‚ªãƒ¼ãƒ€ãƒ¼ã¯åŒã˜ã ãŒã€æ•°å€¤å®‰å®šæ€§ãŒå¤§ããç•°ãªã‚‹ã€‚æ¡ä»¶æ•°ãŒå¤§ãã„è¡Œåˆ—ã§ã¯ã€`inv` ã®çµæœã¯ä¿¡ç”¨ã§ããªã„ã€‚

#### é–“é•ã„3: å›ºæœ‰å€¤åˆ†è§£ã¯ã©ã®è¡Œåˆ—ã§ã‚‚ã§ãã‚‹

**æ­£ã—ã„ç†è§£**: å…¨ã¦ã® $n \times n$ è¡Œåˆ—ãŒå¯¾è§’åŒ–å¯èƒ½ãªã‚ã‘ã§ã¯ãªã„ã€‚å¯¾è§’åŒ–å¯èƒ½æ€§ã®æ¡ä»¶ã¯ã€Œ$n$ å€‹ã®ç·šå½¢ç‹¬ç«‹ãªå›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã€ã€‚å¯¾ç§°è¡Œåˆ—ã¯å¸¸ã«å¯¾è§’åŒ–å¯èƒ½ï¼ˆã‚¹ãƒšã‚¯ãƒˆãƒ«å®šç†ï¼‰ã ãŒã€ä¸€èˆ¬ã®è¡Œåˆ—ã§ã¯ä¿è¨¼ã•ã‚Œãªã„ã€‚

$$
A = \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}
$$

ã“ã®è¡Œåˆ—ã¯å›ºæœ‰å€¤ $\lambda = 0$ï¼ˆé‡è¤‡åº¦2ï¼‰ã‚’æŒã¤ãŒã€å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã¯1ã¤ã—ã‹ãªã„ã€‚å¯¾è§’åŒ–ä¸å¯èƒ½ã€‚

#### é–“é•ã„4: eig ã¨ eigh ã®æ··åŒ

```python
# å¯¾ç§°è¡Œåˆ—ã«ã¯ eigh ã‚’ä½¿ã†
S = np.array([[2.0, 1.0], [1.0, 3.0]])
vals_eig, vecs_eig = np.linalg.eig(S)    # ä¸€èˆ¬å›ºæœ‰å€¤åˆ†è§£
vals_eigh, vecs_eigh = np.linalg.eigh(S)  # å¯¾ç§°è¡Œåˆ—å°‚ç”¨

# eigh ã®åˆ©ç‚¹:
# 1. å›ºæœ‰å€¤ãŒã‚½ãƒ¼ãƒˆã•ã‚Œã¦è¿”ã‚‹ï¼ˆeig ã¯ã‚½ãƒ¼ãƒˆã•ã‚Œãªã„ï¼‰
# 2. å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã®ç›´äº¤æ€§ãŒæ•°å€¤çš„ã«ä¿è¨¼ã•ã‚Œã‚‹
# 3. ç´„2å€é€Ÿã„
# 4. å›ºæœ‰å€¤ã¯å¸¸ã«å®Ÿæ•°ï¼ˆeig ã¯è¤‡ç´ æ•°ã‚’è¿”ã™å¯èƒ½æ€§ï¼‰
```

#### é–“é•ã„5: è¡Œåˆ—ã®ãƒ©ãƒ³ã‚¯ã¨é€†è¡Œåˆ—ã®é–¢ä¿‚ã®èª¤è§£

| æ¡ä»¶ | $\text{rank}(A) = n$ | $\text{rank}(A) < n$ |
|:-----|:---------------------|:--------------------|
| é€†è¡Œåˆ— | å­˜åœ¨ã™ã‚‹ï¼ˆ$A$ ã¯æ­£å‰‡ï¼‰ | å­˜åœ¨ã—ãªã„ï¼ˆ$A$ ã¯ç‰¹ç•°ï¼‰ |
| é€£ç«‹æ–¹ç¨‹å¼ $A\mathbf{x} = \mathbf{b}$ | å”¯ä¸€è§£ | è§£ãªã— or ç„¡é™ã«è§£ãŒã‚ã‚‹ |
| å›ºæœ‰å€¤ | $0$ ã¯å›ºæœ‰å€¤ã§ãªã„ | $0$ ãŒå›ºæœ‰å€¤ã«å«ã¾ã‚Œã‚‹ |
| è¡Œåˆ—å¼ | $\det(A) \neq 0$ | $\det(A) = 0$ |

:::details é–“é•ã„6: ãƒ™ã‚¯ãƒˆãƒ«ã®ç·šå½¢ç‹¬ç«‹æ€§ã®èª¤åˆ¤å®š
ã€Œã©ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚‚å¹³è¡Œã§ãªã‘ã‚Œã°ç·šå½¢ç‹¬ç«‹ã€ã¯**2æ¬¡å…ƒã§ã®ã¿æ­£ã—ã„**ã€‚3æ¬¡å…ƒä»¥ä¸Šã§ã¯ã€ã©ã®2æœ¬ã‚‚å¹³è¡Œã§ãªãã¦ã‚‚ç·šå½¢å¾“å±ã«ãªã‚Šå¾—ã‚‹ã€‚

$$
\mathbf{v}_1 = \begin{pmatrix} 1 \\ 0 \\ 1 \end{pmatrix}, \quad
\mathbf{v}_2 = \begin{pmatrix} 0 \\ 1 \\ 1 \end{pmatrix}, \quad
\mathbf{v}_3 = \begin{pmatrix} 1 \\ 1 \\ 2 \end{pmatrix}
$$

ã©ã®2æœ¬ã‚‚å¹³è¡Œã§ãªã„ãŒã€$\mathbf{v}_3 = \mathbf{v}_1 + \mathbf{v}_2$ ãªã®ã§ç·šå½¢å¾“å±ã€‚

æ­£ã—ã„åˆ¤å®šæ–¹æ³•ã¯ãƒ©ãƒ³ã‚¯ã‚’è¦‹ã‚‹ã“ã¨:

```python
V = np.array([[1, 0, 1], [0, 1, 1], [1, 1, 2]]).T
print("rank =", np.linalg.matrix_rank(V))  # 2 (< 3 â†’ ç·šå½¢å¾“å±)
```
:::

### 6.7 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ1é€±é–“ãƒ—ãƒ©ãƒ³ï¼‰

| æ—¥ | å†…å®¹ | æ‰€è¦æ™‚é–“ |
|:---|:-----|:--------|
| Day 1 | Zone 0-2 é€šèª­ | 30åˆ† |
| Day 2 | Zone 3 å‰åŠï¼ˆ3.1-3.5ï¼‰ | 45åˆ† |
| Day 3 | Zone 3 å¾ŒåŠï¼ˆ3.6-3.9ï¼‰ | 45åˆ† |
| Day 4 | Zone 4ï¼ˆå®Ÿè£…ï¼‰ | 45åˆ† |
| Day 5 | Zone 5ï¼ˆãƒ†ã‚¹ãƒˆï¼‰ | 30åˆ† |
| Day 6 | å¾©ç¿’: 2Ã—2è¡Œåˆ—ã®å›ºæœ‰å€¤åˆ†è§£ã‚’æ‰‹è¨ˆç®— | 30åˆ† |
| Day 7 | ç¬¬3å›ã‚’å…ˆèª­ã¿ + æœ¬è¬›ç¾©ã®æŒ¯ã‚Šè¿”ã‚Š | 30åˆ† |

### 6.8 é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼

```python
"""ç¬¬2å› ç·šå½¢ä»£æ•° I ã®å­¦ç¿’é€²æ—ãƒã‚§ãƒƒã‚«ãƒ¼"""

topics = {
    "ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã®å…¬ç†": False,
    "ç·šå½¢ç‹¬ç«‹ãƒ»åŸºåº•ãƒ»æ¬¡å…ƒ": False,
    "å†…ç©ã¨Cauchy-Schwarz": False,
    "è¡Œåˆ—ç©ã®3ã¤ã®è¦‹æ–¹": False,
    "è»¢ç½®ã¨é€†è¡Œåˆ—ã®æ€§è³ª": False,
    "ãƒˆãƒ¬ãƒ¼ã‚¹ã®å·¡å›æ€§": False,
    "Gram-Schmidt / QR": False,
    "å›ºæœ‰å€¤åˆ†è§£": False,
    "ã‚¹ãƒšã‚¯ãƒˆãƒ«å®šç†": False,
    "æ­£å®šå€¤è¡Œåˆ— / Cholesky": False,
    "æœ€å°äºŒä¹—æ³• / æ­£è¦æ–¹ç¨‹å¼": False,
    "PCA": False,
    "einsum": False,
    "Attention QK^T": False,
}

# True ã«å¤‰æ›´ã—ã¦å®Ÿè¡Œã—ã¦ãã ã•ã„
completed = sum(topics.values())
total = len(topics)
print(f"=== ç¬¬2å› é€²æ—: {completed}/{total} ({100*completed/total:.0f}%) ===")
for topic, done in topics.items():
    mark = "âœ“" if done else " "
    print(f"  [{mark}] {topic}")

if completed == total:
    print("\nç¬¬2å› å®Œå…¨ã‚¯ãƒªã‚¢ï¼ ç¬¬3å›ï¼ˆSVDãƒ»è¡Œåˆ—å¾®åˆ†ãƒ»ãƒ†ãƒ³ã‚½ãƒ«ï¼‰ã¸é€²ã‚‚ã†ã€‚")
elif completed >= total * 0.7:
    print("\nã‚ˆãã§ããŸã€‚æ®‹ã‚Šã¯ç¬¬3å›ã‚’èª­ã‚“ã å¾Œã«æˆ»ã£ã¦ç¢ºèªã—ã‚ˆã†ã€‚")
else:
    print("\nZone 3 ã‚’ä¸­å¿ƒã«ã‚‚ã†ä¸€åº¦å¾©ç¿’ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã™ã‚‹ã€‚")
```

### 6.9 æ¬¡å›äºˆå‘Š: ç¬¬3å›ã€Œç·šå½¢ä»£æ•° II: SVDãƒ»è¡Œåˆ—å¾®åˆ†ãƒ»ãƒ†ãƒ³ã‚½ãƒ«ã€

ç¬¬3å›ã§ã¯ã€æœ¬è¬›ç¾©ã§ç¯‰ã„ãŸåŸºç›¤ã®ä¸Šã«3ã¤ã®å¼·åŠ›ãªé“å…·ã‚’ç©ã¿ä¸Šã’ã‚‹:

1. **SVD**ï¼ˆç‰¹ç•°å€¤åˆ†è§£ï¼‰â€” è¡Œåˆ—ã®ã€Œä¸‡èƒ½ãƒŠã‚¤ãƒ•ã€ã€‚PCAã‚‚ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ã‚‚æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã‚‚LoRA[^10]ã‚‚ã€å…¨ã¦SVDã®å¿œç”¨
2. **è¡Œåˆ—å¾®åˆ†** â€” ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å­¦ç¿’ã®æ•°å­¦çš„åŸºç›¤ã€‚ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ãƒ»ãƒ˜ã‚·ã‚¢ãƒ³ãƒ»é€£é–å¾‹ã®è¡Œåˆ—ç‰ˆ
3. **è‡ªå‹•å¾®åˆ†** â€” PyTorchã® `loss.backward()` ã®ä¸­ã§ä½•ãŒèµ·ãã¦ã„ã‚‹ã‹ã€‚Forward mode vs Reverse mode ã®å®Œå…¨ç†è§£

**ã‚­ãƒ¼ã¨ãªã‚‹LLM/Transformeræ¥ç‚¹**:
- ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ â†’ Flow Modelï¼ˆç¬¬25å›ï¼‰
- å‹¾é… â†’ Backpropagation[^2]ï¼ˆç¬¬3å›ã§å®Œå…¨å°å‡ºï¼‰
- é€£é–å¾‹ â†’ Transformer ã®å„å±¤ã‚’é€šã˜ãŸå‹¾é…ä¼æ’­

> **ç¬¬2å›ã®é™ç•Œ**: è¡Œåˆ—ã‚’ã€Œæ‰±ãˆã‚‹ã€ã‚ˆã†ã«ãªã£ãŸã€‚ã ãŒã€Œåˆ†è§£ã—ã¦æ§‹é€ ã‚’è¦‹æŠœãã€ã«ã¯SVDãŒå¿…è¦ã€‚ã€Œè¡Œåˆ—ã®é–¢æ•°ã‚’å¾®åˆ†ã™ã‚‹ã€ã«ã¯è¡Œåˆ—å¾®åˆ†ãŒå¿…è¦ã€‚ãã®2ã¤ã‚’ç¬¬3å›ã§å®Œå…¨æ­¦è£…ã™ã‚‹ã€‚

:::message
**é€²æ—: 100% å®Œäº†!** ç¬¬2å›ã€Œç·šå½¢ä»£æ•° I: ãƒ™ã‚¯ãƒˆãƒ«ãƒ»è¡Œåˆ—ãƒ»åŸºåº•ã€ã‚’å®Œèµ°ã—ãŸã€‚ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã®å…¬ç†ã‹ã‚‰å§‹ã¾ã‚Šã€å†…ç©ãƒ»å›ºæœ‰å€¤åˆ†è§£ãƒ»æ­£å®šå€¤è¡Œåˆ—ãƒ»å°„å½±ã‚’çµŒã¦ã€Attentionã®QK^Tã‚’è¡Œåˆ—çš„ã«å®Œå…¨ç†è§£ã—ãŸã€‚ãŠç–²ã‚Œã•ã¾ã§ã—ãŸã€‚
:::

---

### 6.10 ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **GPUã¯è¡Œåˆ—æ¼”ç®—ãƒã‚·ãƒ³ã€‚ç·šå½¢ä»£æ•°ã‚’"åˆ¶ã™ã‚‹è€…"ãŒAIã‚’åˆ¶ã™ã‚‹ã®ã§ã¯ï¼Ÿ**

ã“ã®å•ã„ã®æ„å‘³ã‚’è€ƒãˆã¦ã¿ã¦ã»ã—ã„ã€‚

ç¾ä»£ã®AIã®é€²æ­©ã¯ã€ä¸€è¦‹ã™ã‚‹ã¨ãƒ‡ãƒ¼ã‚¿é‡ã‚„ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã®å¢—å¤§ã«ã‚ˆã‚‹ã‚‚ã®ã«è¦‹ãˆã‚‹ã€‚ã ãŒã€ãã®è£å´ã§èµ·ãã¦ã„ã‚‹ã“ã¨ã¯ã€Œã„ã‹ã«åŠ¹ç‡ã‚ˆãè¡Œåˆ—ç©ã‚’è¨ˆç®—ã™ã‚‹ã‹ã€ã®æœ€é©åŒ–ã ã€‚

Flash Attention[^12]ã¯ã€Attention ã®è¨ˆç®—ã‚’è¡Œåˆ—ãƒ–ãƒ­ãƒƒã‚¯å˜ä½ã§å†æ§‹æˆã—ã¦ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ã‚’æœ€é©åŒ–ã—ãŸã€‚ã“ã‚Œã¯ç·šå½¢ä»£æ•°ã®çŸ¥è­˜ãªã—ã«ã¯ç™ºæƒ³ã§ããªã„ã€‚LoRA[^10]ã¯é‡ã¿è¡Œåˆ—ã®æ›´æ–°ã‚’ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—ã®ç©ã§è¿‘ä¼¼ã—ãŸã€‚ã“ã‚Œã‚‚SVDçš„ãªç™ºæƒ³ã®ç›´æ¥çš„ãªå¿œç”¨ã ã€‚

è€ƒãˆã¦ã¿ã¦ã»ã—ã„ã€‚GPT-4ã®æ¨è«–ã¯ã€çµå±€ã®ã¨ã“ã‚ä½•ã‚’ã—ã¦ã„ã‚‹ã®ã‹ã€‚ãƒˆãƒ¼ã‚¯ãƒ³ã‚’åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã—ï¼ˆè¡Œåˆ—ã®è¡Œé¸æŠï¼‰ã€Queryã¨ Keyã®å†…ç©ã‚’è¨ˆç®—ã—ï¼ˆè¡Œåˆ—ç© $QK^\top$ï¼‰ã€Softmaxã§æ­£è¦åŒ–ã—ï¼ˆãƒ™ã‚¯ãƒˆãƒ«æ¼”ç®—ï¼‰ã€Valueã®åŠ é‡å’Œã‚’å–ã‚Šï¼ˆè¡Œåˆ—ç© $AV$ï¼‰ã€ç·šå½¢å°„å½±ã™ã‚‹ï¼ˆè¡Œåˆ—ç© $W_O$ï¼‰ã€‚**å…¨ã¦ãŒè¡Œåˆ—æ¼”ç®—ã ã€‚**

ã“ã®äº‹å®Ÿã¯ã€AIã®ç†è§£ã‚’æ ¹æœ¬ã‹ã‚‰å¤‰ãˆã‚‹ã€‚AIã¯ã€ŒçŸ¥èƒ½ã®æ¨¡å€£ã€ã§ã¯ãªãã€Œé«˜æ¬¡å…ƒç·šå½¢ä»£æ•°ã®å¤§è¦æ¨¡ä¸¦åˆ—å®Ÿè¡Œã€ã ã€‚ç·šå½¢ä»£æ•°ã®ç†è«–çš„é™ç•ŒãŒAIã®é™ç•Œã‚’è¦å®šã—ã€ç·šå½¢ä»£æ•°ã®è¨ˆç®—åŠ¹ç‡ãŒAIã®å®Ÿç”¨æ€§ã‚’æ±ºå®šã™ã‚‹ã€‚

:::details è­°è«–ãƒã‚¤ãƒ³ãƒˆ
1. **ã‚‚ã—GPUãŒè¡Œåˆ—ç©ä»¥å¤–ã®è¨ˆç®—ã‚‚å¾—æ„ã ã£ãŸã‚‰ã€AIã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯å¤‰ã‚ã£ã¦ã„ãŸã‹ï¼Ÿ** â€” TransformerãŒæ”¯é…çš„ã«ãªã£ãŸç†ç”±ã®ä¸€ã¤ã¯ã€ãã®ã‚³ã‚¢è¨ˆç®—ãŒè¡Œåˆ—ç©ã§ã‚ã‚Šã€GPUã¨ç›¸æ€§ãŒè‰¯ã„ã“ã¨ã«ã‚ã‚‹ã€‚RNNã¯é€æ¬¡çš„ãªè¨ˆç®—ãŒå¿…è¦ã§GPUã®ä¸¦åˆ—æ€§ã‚’æ´»ã‹ã—ãã‚Œãªã‹ã£ãŸã€‚
2. **ç·šå½¢ä»£æ•°ã®é™ç•Œã¯ã©ã“ã«ã‚ã‚‹ã‹ï¼Ÿ** â€” éç·šå½¢æ€§ï¼ˆæ´»æ€§åŒ–é–¢æ•°ï¼‰ãªã—ã«ã¯ä»»æ„ã®é–¢æ•°ã‚’è¿‘ä¼¼ã§ããªã„ã€‚ç·šå½¢ä»£æ•°ã¯ã€ŒåœŸå°ã€ã§ã‚ã£ã¦ã€Œå…¨ã¦ã€ã§ã¯ãªã„ã€‚ãŸã ã—ã€ReLU ã¯åŒºåˆ†ç·šå½¢é–¢æ•°ã§ã‚ã‚Šã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã¯ã€ŒåŒºåˆ†çš„ã«ç·šå½¢ãªã€å†™åƒã ã€‚
3. **é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯ç·šå½¢ä»£æ•°ã‚’åŠ é€Ÿã™ã‚‹ã‹ï¼Ÿ** â€” é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯ç‰¹å®šã®ç·šå½¢ä»£æ•°æ¼”ç®—ï¼ˆHHL algorithmï¼‰ã§æŒ‡æ•°é–¢æ•°çš„ãªé«˜é€ŸåŒ–ã‚’é”æˆã§ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚é‡å­æ©Ÿæ¢°å­¦ç¿’ã®ç†è«–çš„åŸºç›¤ã‚‚ç·šå½¢ä»£æ•°ã ã€‚
4. **ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ã¯ã©ã“ã¾ã§æœ‰åŠ¹ã‹ï¼Ÿ** â€” LoRA[^10]ã¯é‡ã¿æ›´æ–°ã‚’ rank-$r$ è¿‘ä¼¼ã™ã‚‹ã“ã¨ã§ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’ $O(d^2) \to O(dr)$ ã«å‰Šæ¸›ã—ãŸã€‚ã“ã‚Œã¯ã€Œé‡ã¿æ›´æ–°ãŒæœ¬è³ªçš„ã«ä½ãƒ©ãƒ³ã‚¯ã§ã‚ã‚‹ã€ã¨ã„ã†çµŒé¨“çš„ç™ºè¦‹ã«åŸºã¥ãã€‚ã ãŒã€ã“ã®ä»®å®šã¯å¸¸ã«æ­£ã—ã„ã®ã‹ï¼Ÿ ã©ã®ã‚¿ã‚¹ã‚¯ã§ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ãŒå¤±æ•—ã™ã‚‹ã‹ã¯ã€ã¾ã å®Œå…¨ã«ã¯ç†è§£ã•ã‚Œã¦ã„ãªã„ã€‚
:::

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Å., & Polosukhin, I. (2017). Attention Is All You Need. *NeurIPS 2017*.
@[card](https://arxiv.org/abs/1706.03762)

[^2]: Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by back-propagating errors. *Nature*, 323, 533-536.
@[card](https://doi.org/10.1038/323533a0)

[^6]: Pearson, K. (1901). On Lines and Planes of Closest Fit to Systems of Points in Space. *Philosophical Magazine*, 2(11), 559-572.
@[card](https://doi.org/10.1080/14786440109462720)

[^7]: Hotelling, H. (1933). Analysis of a complex of statistical variables into principal components. *Journal of Educational Psychology*, 24(6), 417-441.
@[card](https://doi.org/10.1037/h0071325)

[^10]: Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W. (2022). LoRA: Low-Rank Adaptation of Large Language Models. *ICLR 2022*.
@[card](https://arxiv.org/abs/2106.09685)

[^12]: Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & RÃ©, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. *NeurIPS 2022*.
@[card](https://arxiv.org/abs/2205.14135)

[^13]: Martinsson, P. G., & Tropp, J. A. (2020). Randomized numerical linear algebra: Foundations and algorithms. *Acta Numerica*, 29, 403-572.
@[card](https://arxiv.org/abs/2002.01387)

[^14]: Halko, N., Martinsson, P. G., & Tropp, J. A. (2011). Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. *SIAM Review*, 53(2), 217-288. arXiv:0909.4061.

[^15]: Wichmann, N., Gupta, A., & Thiele, L. (2025). Performant Unified GPU Kernels for Portable Singular Value Computation Across Hardware and Precision.
@[card](https://arxiv.org/abs/2508.06339)

[^16]: Liu, Y., Huang, X., & Dongarra, J. (2025). Efficient GPU-Centered Singular Value Decomposition Using the Divide-and-Conquer Method.
@[card](https://arxiv.org/abs/2508.11467)

[^17]: Feng, Y., Xiang, H., & Saad, Y. (2022). Randomized Rank-Revealing QLP for Low-Rank Matrix Approximation.
@[card](https://arxiv.org/abs/2209.12464)

[^18]: Le, H., Hsieh, T.-H., HÃ¸gsgaard, J. S., & Schmidt, M. N. (2024). (Almost) Smooth Sailing: Towards Numerical Stability of Neural Networks.
@[card](https://arxiv.org/abs/2410.00169)

[^19]: Zhao, Y., Anandkumar, A., & Yu, Y. (2020). An efficient numerical method for condition number constrained covariance matrix approximation. *Applied Mathematics and Computation*, 397, 125917.
@[card](https://arxiv.org/abs/2008.06851)

### æ•™ç§‘æ›¸

[^8]: Golub, G. H. & Van Loan, C. F. (2013). *Matrix Computations* (4th ed.). Johns Hopkins University Press.

[^9]: Petersen, K. B. & Pedersen, M. S. (2012). *The Matrix Cookbook*. Technical Report, DTU. [matrixcookbook.com](https://matrixcookbook.com)

---

## è¨˜æ³•è¦ç´„

| è¨˜å· | æ„å‘³ | åˆå‡º |
|:-----|:-----|:-----|
| $\mathbf{x}, \mathbf{v}$ | ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆå¤ªå­—å°æ–‡å­—ï¼‰ | 3.1 |
| $A, B, W$ | è¡Œåˆ—ï¼ˆå¤§æ–‡å­—ï¼‰ | 3.3 |
| $\mathbb{R}^n$ | $n$æ¬¡å…ƒå®Ÿæ•°ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ | 3.1 |
| $\mathbb{R}^{m \times n}$ | $m \times n$ å®Ÿæ•°è¡Œåˆ—ã®ç©ºé–“ | 3.1 |
| $\langle \cdot, \cdot \rangle$ | å†…ç© | 3.2 |
| $\|\cdot\|$ | ãƒãƒ«ãƒ  | 3.2 |
| $A^\top$ | è»¢ç½® | 3.3 |
| $A^{-1}$ | é€†è¡Œåˆ— | 3.3 |
| $\text{tr}(A)$ | ãƒˆãƒ¬ãƒ¼ã‚¹ | 3.4 |
| $\det(A)$ | è¡Œåˆ—å¼ | 3.3 |
| $\text{rank}(A)$ | ãƒ©ãƒ³ã‚¯ | 3.1 |
| $\lambda_i$ | å›ºæœ‰å€¤ | 3.6 |
| $\mathbf{v}_i$ | å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ« | 3.6 |
| $\Lambda$ | å›ºæœ‰å€¤ã®å¯¾è§’è¡Œåˆ— | 3.6 |
| $Q$ | ç›´äº¤è¡Œåˆ— | 3.5, 3.6 |
| $R$ | ä¸Šä¸‰è§’è¡Œåˆ—ï¼ˆQRåˆ†è§£ï¼‰ | 3.5 |
| $L$ | ä¸‹ä¸‰è§’è¡Œåˆ—ï¼ˆCholeskyï¼‰ | 3.7 |
| $P$ | å°„å½±è¡Œåˆ— | 3.8 |
| $\delta_{ij}$ | ã‚¯ãƒ­ãƒãƒƒã‚«ãƒ¼ã®ãƒ‡ãƒ«ã‚¿ | 3.2 |
| $\Sigma$ | å…±åˆ†æ•£è¡Œåˆ— | 3.3 |
| $A \succ 0$ | $A$ ã¯æ­£å®šå€¤ | 3.7 |
| $A \succeq 0$ | $A$ ã¯åŠæ­£å®šå€¤ | 3.7 |

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
