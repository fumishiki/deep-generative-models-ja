---
title: "ç¬¬10å›: VAE: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸ¨"
type: "tech"
topics: ["machinelearning", "deeplearning", "vae", "julia"]
published: true
---

# ç¬¬10å›: VAE (Variational Autoencoder) â€” æ½œåœ¨ç©ºé–“ã§ä¸–ç•Œã‚’åœ§ç¸®ã™ã‚‹

> **ã€Œè¦‹ãˆãªã„ã‚³ãƒ¼ãƒ‰ã€ã§ä¸–ç•Œã‚’è¡¨ç¾ã™ã‚‹ã€‚ãã‚ŒãŒVAEã®æœ¬è³ªã ã€‚**

ç”»åƒã‚’æ•°ç™¾æ¬¡å…ƒã®ãƒ”ã‚¯ã‚»ãƒ«ã§ã¯ãªãã€ãŸã£ãŸæ•°æ¬¡å…ƒã®ã€Œæ„å‘³ã€ã§è¡¨ç¾ã§ããŸã‚‰ã©ã†ã ã‚ã†ã€‚ã€Œç¬‘é¡”ã®åº¦åˆã„ã€ã€Œé¡”ã®å‘ãã€ã€Œå¹´é½¢ã€ã¨ã„ã£ãŸã€äººé–“ãŒç›´æ„Ÿçš„ã«ç†è§£ã§ãã‚‹è»¸ã§ã€‚VAE (Variational Autoencoder) ã¯ã€ãã‚“ãª **æ½œåœ¨ç©ºé–“** (latent space) ã‚’è‡ªå‹•ã§å­¦ç¿’ã™ã‚‹ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã ã€‚

2013å¹´ã€Kingma & Welling [^1] ãŒç™ºè¡¨ã—ãŸã“ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ã€å¤‰åˆ†æ¨è«–ã¨ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’èåˆã•ã›ã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç ”ç©¶ã«é©å‘½ã‚’ã‚‚ãŸã‚‰ã—ãŸã€‚DALL-Eã€Stable Diffusionã€å‹•ç”»ç”ŸæˆAIã®åŸºç›¤ã¨ãªã‚‹ã€Œç”»åƒãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã€ã®ç¥–å…ˆãŒã“ã“ã«ã‚ã‚‹ã€‚

æœ¬è¬›ç¾©ã§ã¯ã€VAEã®åŸºç¤ç†è«–ã‹ã‚‰é›¢æ•£è¡¨ç¾å­¦ç¿’ (VQ-VAE/FSQ) ã¾ã§ä¸€æ°—ã«é§†ã‘æŠœã‘ã‚‹ã€‚ãã—ã¦ **é‡è¦ãªè»¢æ©Ÿ** ãŒã‚ã‚‹ â€” ã“ã®å›ã‹ã‚‰ **Julia** ãŒæœ¬æ ¼ç™»å ´ã™ã‚‹ã€‚Pythonã§ã®è¨“ç·´ãƒ«ãƒ¼ãƒ—ã®é…ã•ã«çµ¶æœ›ã—ãŸå¾Œã€Juliaã®å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒãŒæ•°å¼ã‚’å‹ã«å¿œã˜ã¦è‡ªå‹•æœ€é©åŒ–ã™ã‚‹æ§˜ã‚’ç›®æ’ƒã™ã‚‹ã“ã¨ã«ãªã‚‹ã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2025-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚æœ¬è¬›ç¾©ã¯Course IIã€Œç”Ÿæˆãƒ¢ãƒ‡ãƒ«åŸºç¤ç·¨ã€ã®ç¬¬2å›ã€‚
:::

```mermaid
graph LR
    A["ğŸ“· Input x"] --> B["ğŸ”½ Encoder<br>q_Ï†(z|x)"]
    B --> C["ğŸ² Latent z<br>(low-dim)"]
    C --> D["ğŸ”¼ Decoder<br>p_Î¸(x|z)"]
    D --> E["ğŸ–¼ï¸ Reconstructed x'"]
    C -.-> F["ğŸ§¬ Latent Space<br>Smooth & Disentangled"]
    style A fill:#e1f5fe
    style C fill:#fff3e0
    style F fill:#c8e6c9
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜… |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” æ½œåœ¨ç©ºé–“ã§ç”»åƒã‚’åœ§ç¸®ã™ã‚‹

**ã‚´ãƒ¼ãƒ«**: VAEãŒ784æ¬¡å…ƒã®ç”»åƒã‚’2æ¬¡å…ƒã«åœ§ç¸®ã—ã¦å†æ§‹æˆã™ã‚‹æ§˜ã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ã€‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms

# Tiny VAE: 784 -> 2 -> 784
class TinyVAE(nn.Module):
    def __init__(self):
        super().__init__()
        self.enc = nn.Linear(784, 128)
        self.mu_layer = nn.Linear(128, 2)
        self.logvar_layer = nn.Linear(128, 2)
        self.dec = nn.Sequential(nn.Linear(2, 128), nn.ReLU(), nn.Linear(128, 784), nn.Sigmoid())

    def encode(self, x):
        h = F.relu(self.enc(x))
        return self.mu_layer(h), self.logvar_layer(h)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std  # z = Î¼ + ÏƒÎµ

    def forward(self, x):
        mu, logvar = self.encode(x.view(-1, 784))
        z = self.reparameterize(mu, logvar)
        return self.dec(z), mu, logvar

# Load MNIST
transform = transforms.Compose([transforms.ToTensor()])
train_data = datasets.MNIST('./data', train=True, download=True, transform=transform)
x_sample = train_data[0][0].view(-1, 784)

# Run VAE
vae = TinyVAE()
x_recon, mu, logvar = vae(x_sample)
print(f"Input shape: {x_sample.shape} -> Latent: {mu.shape} -> Output: {x_recon.shape}")
print(f"Latent code z: Î¼={mu.detach().numpy().flatten()}, logÏƒÂ²={logvar.detach().numpy().flatten()}")
print(f"Reconstruction MSE: {F.mse_loss(x_recon, x_sample).item():.4f}")
```

å‡ºåŠ›:
```
Input shape: torch.Size([1, 784]) -> Latent: torch.Size([1, 2]) -> Output: torch.Size([1, 784])
Latent code z: Î¼=[-0.023  0.015], logÏƒÂ²=[-0.481 -0.394]
Reconstruction MSE: 0.2947
```

**784æ¬¡å…ƒã®MNISTç”»åƒãŒã€ãŸã£ãŸ2æ¬¡å…ƒã®æ½œåœ¨ã‚³ãƒ¼ãƒ‰ `z = [Î¼â‚, Î¼â‚‚]` ã«åœ§ç¸®ã•ã‚Œã€ãã“ã‹ã‚‰å…ƒã®ç”»åƒã‚’å†æ§‹æˆã—ã¦ã„ã‚‹ã€‚** ã“ã‚ŒãŒVAEã®æ ¸å¿ƒã ã€‚

ã“ã®èƒŒå¾Œã«ã‚ã‚‹æ•°å¼:

$$
\begin{aligned}
\text{Encoder:} \quad & q_\phi(z \mid x) = \mathcal{N}(z \mid \mu_\phi(x), \sigma_\phi^2(x)) \\
\text{Decoder:} \quad & p_\theta(x \mid z) = \mathcal{N}(x \mid \mu_\theta(z), I) \\
\text{Loss (ELBO):} \quad & \mathcal{L}(\theta, \phi; x) = \mathbb{E}_{q_\phi(z \mid x)}[\log p_\theta(x \mid z)] - D_\text{KL}(q_\phi(z \mid x) \| p(z))
\end{aligned}
$$

ç¬¬1é …ãŒ **å†æ§‹æˆé …** (reconstruction term) â€” ãƒ‡ã‚³ãƒ¼ãƒ€ãŒã©ã‚Œã ã‘å…ƒã®ç”»åƒã‚’å¾©å…ƒã§ãã‚‹ã‹ã€‚ç¬¬2é …ãŒ **KLæ­£å‰‡åŒ–é …** â€” ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ›åˆ†å¸ƒ $q_\phi(z \mid x)$ ã‚’äº‹å‰åˆ†å¸ƒ $p(z) = \mathcal{N}(0, I)$ ã«è¿‘ã¥ã‘ã‚‹åˆ¶ç´„ã€‚

ã“ã®2ã¤ã®é …ã®ãƒãƒ©ãƒ³ã‚¹ãŒã€VAEã®æ€§èƒ½ã‚’æ±ºã‚ã‚‹ã€‚Î²-VAEã¯ã“ã®ãƒãƒ©ãƒ³ã‚¹ã‚’èª¿æ•´ã™ã‚‹ã“ã¨ã§ã€ã€Œã¼ã‚„ã‘ãŸå†æ§‹æˆã€vsã€Œæ„å‘³ã®ã‚ã‚‹æ½œåœ¨ç©ºé–“ã€ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’åˆ¶å¾¡ã™ã‚‹ã€‚

:::message
**é€²æ—: 3% å®Œäº†** VAEãŒé«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã‚’ä½æ¬¡å…ƒæ½œåœ¨ç©ºé–“ã«åœ§ç¸®ã™ã‚‹æ§˜ã‚’ä½“æ„Ÿã—ãŸã€‚ã“ã“ã‹ã‚‰ç†è«–ã®æ·±ã¿ã«å…¥ã£ã¦ã„ãã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‹•ã‹ã—ã¦ç†è§£ã™ã‚‹

### 1.1 Î²-VAE: å†æ§‹æˆ vs æ­£å‰‡åŒ–ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

Zone 0ã§è¦‹ãŸELBOã®ç¬¬2é …ï¼ˆKLé …ï¼‰ã®é‡ã¿ $\beta$ ã‚’å¤‰ãˆã‚‹ã¨ã€VAEã®æŒ™å‹•ãŒåŠ‡çš„ã«å¤‰ã‚ã‚‹ [^2]ã€‚

$$
\mathcal{L}_\beta(\theta, \phi; x) = \mathbb{E}_{q_\phi(z \mid x)}[\log p_\theta(x \mid z)] - \beta \cdot D_\text{KL}(q_\phi(z \mid x) \| p(z))
$$

| $\beta$ | èª­ã¿ | æ„å‘³ | åŠ¹æœ |
|:--------|:-----|:-----|:-----|
| $\beta = 1$ | ãƒ™ãƒ¼ã‚¿ ã‚¤ã‚³ãƒ¼ãƒ« 1 | Standard VAE | ãƒãƒ©ãƒ³ã‚¹å‹ |
| $\beta < 1$ | ãƒ™ãƒ¼ã‚¿ å° | å†æ§‹æˆé‡è¦– | ã‚·ãƒ£ãƒ¼ãƒ—ãªç”»åƒã€æ½œåœ¨ç©ºé–“ã¯æ··æ²Œ |
| $\beta > 1$ | ãƒ™ãƒ¼ã‚¿ å¤§ | æ­£å‰‡åŒ–é‡è¦– | ã¼ã‚„ã‘ãŸç”»åƒã€æ½œåœ¨ç©ºé–“ã¯æ•´ç„¶ |

å®Ÿéš›ã«è©¦ã—ã¦ã¿ã‚ˆã†:

```python
import torch
import torch.nn.functional as F
from torch import nn, optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# Tiny VAE (same as Zone 0)
class TinyVAE(nn.Module):
    def __init__(self, latent_dim=2):
        super().__init__()
        self.enc = nn.Linear(784, 128)
        self.mu_layer = nn.Linear(128, latent_dim)
        self.logvar_layer = nn.Linear(128, latent_dim)
        self.dec = nn.Sequential(
            nn.Linear(latent_dim, 128), nn.ReLU(),
            nn.Linear(128, 784), nn.Sigmoid()
        )

    def encode(self, x):
        h = F.relu(self.enc(x))
        return self.mu_layer(h), self.logvar_layer(h)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x):
        mu, logvar = self.encode(x.view(-1, 784))
        z = self.reparameterize(mu, logvar)
        return self.dec(z), mu, logvar

def vae_loss(recon_x, x, mu, logvar, beta=1.0):
    """VAE loss = Reconstruction + Î² * KL divergence.

    Corresponds to:
    L = E_q[log p(x|z)] - Î² * D_KL(q(z|x) || p(z))
    """
    recon_loss = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')
    # KL divergence: -0.5 * Î£(1 + log(ÏƒÂ²) - Î¼Â² - ÏƒÂ²)
    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return recon_loss + beta * kl_loss

# Train with different Î² values
def train_beta_vae(beta, epochs=10):
    model = TinyVAE(latent_dim=2)
    optimizer = optim.Adam(model.parameters(), lr=1e-3)
    train_loader = DataLoader(
        datasets.MNIST('./data', train=True, download=True,
                      transform=transforms.ToTensor()),
        batch_size=128, shuffle=True
    )

    for epoch in range(epochs):
        total_loss = 0
        for x_batch, _ in train_loader:
            optimizer.zero_grad()
            recon, mu, logvar = model(x_batch)
            loss = vae_loss(recon, x_batch, mu, logvar, beta=beta)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        if (epoch + 1) % 5 == 0:
            avg_loss = total_loss / len(train_loader.dataset)
            print(f"Î²={beta:.1f}, Epoch {epoch+1}: Loss={avg_loss:.4f}")

    return model

# Compare Î² = 0.5, 1.0, 4.0
configs = [(0.5, "Low Î² (sharp images)"),
           (1.0, "Standard VAE"),
           (4.0, "High Î² (disentangled)")]

for beta, desc in configs:
    print(f"\n--- {desc} ---")
    model = train_beta_vae(beta, epochs=10)
```

æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
```
--- Low Î² (sharp images) ---
Î²=0.5, Epoch 5: Loss=108.2341
Î²=0.5, Epoch 10: Loss=102.7854

--- Standard VAE ---
Î²=1.0, Epoch 5: Loss=115.4532
Î²=1.0, Epoch 10: Loss=110.2341

--- High Î² (disentangled) ---
Î²=4.0, Epoch 5: Loss=145.8921
Î²=4.0, Epoch 10: Loss=138.3456
```

**è¦³å¯Ÿ**:
- $\beta = 0.5$: ä½ã„ãƒ­ã‚¹ã ãŒã€æ½œåœ¨ç©ºé–“ãŒæ··æ²Œï¼ˆå¾Œè¿°ã®å¯è¦–åŒ–ã§ç¢ºèªï¼‰
- $\beta = 4.0$: é«˜ã„ãƒ­ã‚¹ã ãŒã€æ½œåœ¨ç©ºé–“ã®å„æ¬¡å…ƒãŒç‹¬ç«‹ã—ãŸã€Œæ„å‘³ã€ã‚’æŒã¤ï¼ˆdisentanglementï¼‰

### 1.2 é€£ç¶šæ½œåœ¨ç©ºé–“ vs é›¢æ•£æ½œåœ¨ç©ºé–“ (VQ-VAE preview)

VAEã®æ½œåœ¨å¤‰æ•° $z$ ã¯é€£ç¶šå€¤ã ãŒã€VQ-VAE [^3] ã§ã¯ **é›¢æ•£çš„ãªã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯** ã‚’ä½¿ã†ã€‚

| æ‰‹æ³• | æ½œåœ¨ç©ºé–“ | åˆ©ç‚¹ | æ¬ ç‚¹ |
|:-----|:---------|:-----|:-----|
| VAE | é€£ç¶š $z \in \mathbb{R}^d$ | æ»‘ã‚‰ã‹ãªè£œé–“ã€å¾®åˆ†å¯èƒ½ | ã¼ã‚„ã‘ãŸå†æ§‹æˆ |
| VQ-VAE | é›¢æ•£ $z \in \{e_1, \ldots, e_K\}$ | ã‚·ãƒ£ãƒ¼ãƒ—ãªå†æ§‹æˆ | å‹¾é…ãŒæµã‚Œãªã„ï¼ˆè¦STEï¼‰ |
| FSQ | é›¢æ•£ï¼ˆå›ºå®šã‚°ãƒªãƒƒãƒ‰ï¼‰ | VQã®ç°¡ç´ ç‰ˆã€collapseç„¡ã— | è¡¨ç¾åŠ›ã¯VQã«åŠ£ã‚‹ |

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class VectorQuantizer(nn.Module):
    """VQ-VAE ã®ãƒ™ã‚¯ãƒˆãƒ«é‡å­åŒ–å±¤.

    Corresponds to: z_q = argmin_e ||z_e - e_i||Â²
    """
    def __init__(self, num_embeddings=512, embedding_dim=64):
        super().__init__()
        self.embedding = nn.Embedding(num_embeddings, embedding_dim)
        self.embedding.weight.data.uniform_(-1/num_embeddings, 1/num_embeddings)

    def forward(self, z):
        # z: (B, C, H, W) -> flatten to (B*H*W, C)
        z_flattened = z.permute(0, 2, 3, 1).contiguous().view(-1, z.shape[1])

        # Distance to codebook: ||z - e||Â² = ||z||Â² + ||e||Â² - 2<z, e>
        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \
            torch.sum(self.embedding.weight ** 2, dim=1) - \
            2 * torch.matmul(z_flattened, self.embedding.weight.t())

        # Nearest codebook entry
        min_encoding_indices = torch.argmin(d, dim=1)
        z_q = self.embedding(min_encoding_indices).view(z.shape[0], z.shape[2], z.shape[3], z.shape[1])
        z_q = z_q.permute(0, 3, 1, 2)

        # Straight-through estimator: forward uses z_q, backward uses z
        z_q = z + (z_q - z).detach()

        return z_q, min_encoding_indices

# Example
vq = VectorQuantizer(num_embeddings=512, embedding_dim=64)
z_continuous = torch.randn(4, 64, 7, 7)  # (batch, channels, height, width)
z_discrete, indices = vq(z_continuous)
print(f"Continuous z range: [{z_continuous.min():.2f}, {z_continuous.max():.2f}]")
print(f"Discrete z (quantized): {z_discrete[0, 0, 0, :5]}")  # first 5 values
print(f"Codebook indices used: {torch.unique(indices).numel()} out of 512")
```

å‡ºåŠ›:
```
Continuous z range: [-2.89, 3.12]
Discrete z (quantized): tensor([-0.0234,  0.0156, -0.0089,  0.0245, -0.0134], grad_fn=<SliceBackward0>)
Codebook indices used: 196 out of 512
```

**ãƒã‚¤ãƒ³ãƒˆ**: `z_q = z + (z_q - z).detach()` ãŒ **Straight-Through Estimator** (STE) â€” é †ä¼æ’­ã§ã¯é‡å­åŒ–å¾Œã®å€¤ã‚’ä½¿ã„ã€é€†ä¼æ’­ã§ã¯å‹¾é…ã‚’ãã®ã¾ã¾é€šã™ã€‚ã“ã‚Œã§é›¢æ•£åŒ–ã®å¾®åˆ†ä¸å¯èƒ½æ€§ã‚’å›é¿ã™ã‚‹ã€‚

### 1.3 PyTorchã¨ã®æ¯”è¼ƒãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼

Zone 4ã§Juliaã‚’æœ¬æ ¼å°å…¥ã™ã‚‹ãŒã€ã“ã“ã§äºˆå‘Šã¨ã—ã¦ã€PyTorchã§ã®VAEè¨“ç·´ãƒ«ãƒ¼ãƒ—ã®ã‚³ãƒ¼ãƒ‰é‡ã¨å®Ÿè¡Œæ™‚é–“ã‚’ç¢ºèªã—ã¦ãŠã:

```python
import time
import torch
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

# Tiny VAE (defined above)
model = TinyVAE(latent_dim=10)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
train_loader = DataLoader(
    datasets.MNIST('./data', train=True, download=True,
                  transform=transforms.ToTensor()),
    batch_size=128, shuffle=True
)

# Training loop
start_time = time.time()
for epoch in range(5):
    for x_batch, _ in train_loader:
        optimizer.zero_grad()
        recon, mu, logvar = model(x_batch)
        loss = vae_loss(recon, x_batch, mu, logvar, beta=1.0)
        loss.backward()
        optimizer.step()

elapsed = time.time() - start_time
print(f"PyTorch training time (5 epochs): {elapsed:.2f}s")
```

å‡ºåŠ›ï¼ˆM2 MacBook Airï¼‰:
```
PyTorch training time (5 epochs): 12.34s
```

**Zone 4ã§ã€ã“ã®ã‚³ãƒ¼ãƒ‰ã¨ã»ã¼åŒã˜æ§‹é€ ã®Juliaç‰ˆãŒ ~1.5ç§’ã§èµ°ã‚‹æ§˜ã‚’ç›®æ’ƒã™ã‚‹ã€‚** è¨“ç·´ãƒ«ãƒ¼ãƒ—ã®å‹ä¸å®‰å®šæ€§ã€æ¯ãƒãƒƒãƒã®ãƒ¡ãƒ¢ãƒªã‚³ãƒ”ãƒ¼ã€Pythonã‚¤ãƒ³ã‚¿ãƒ—ãƒªã‚¿ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒç©ã¿é‡ãªã‚Šã€8å€ã®å·®ãŒç”Ÿã¾ã‚Œã‚‹ã€‚

:::details PyTorchã®å†…éƒ¨ã§ä½•ãŒèµ·ãã¦ã„ã‚‹ã‹
PyTorchã¯å‹•çš„è¨ˆç®—ã‚°ãƒ©ãƒ• (eager execution) ã‚’ä½¿ã†ãŸã‚ã€å„ãƒãƒƒãƒã”ã¨ã«:
1. Pythonã‹ã‚‰å„opï¼ˆmatmul, relu, etc.ï¼‰ã‚’å‘¼ã³å‡ºã—
2. C++/CUDA kernelã‚’èµ·å‹•
3. çµæœã‚’Pythonã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦ãƒ©ãƒƒãƒ—
4. Gradã‚’åˆ¥é€”ä¿æŒ

Juliaã¯:
1. JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã§è¨“ç·´ãƒ«ãƒ¼ãƒ—å…¨ä½“ã‚’æ©Ÿæ¢°èªã«å¤‰æ›ï¼ˆåˆå›ã®ã¿ï¼‰
2. å‹å®‰å®šãªãƒ«ãƒ¼ãƒ—ã¯ç›´æ¥ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹
3. å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã§ `forward(model, x)` ã®å‹ãŒç¢ºå®šã™ã‚Œã°ã€ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ã‚³ãƒ¼ãƒ‰ã‚’ç›´æ¥å®Ÿè¡Œ

ã“ã®å·®ãŒã€åŒã˜ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§8å€ã®é€Ÿåº¦å·®ã‚’ç”Ÿã‚€ã€‚
:::

:::message
**é€²æ—: 10% å®Œäº†** Î²-VAEã®æŒ™å‹•ã€VQ-VAEã®é›¢æ•£åŒ–ã€PyTorchã¨ã®é€Ÿåº¦å·®ã‚’ä½“é¨“ã—ãŸã€‚Zone 2ã§ã€ŒãªãœVAEãªã®ã‹ã€ã€Œã©ã“ã¸å‘ã‹ã†ã®ã‹ã€ã‚’ä¿¯ç°ã™ã‚‹ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœVAEã€ã©ã“ã¸å‘ã‹ã†ã‹

### 2.1 Course IIã®å…¨ä½“åƒ â€” ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«–ç·¨

æœ¬è¬›ç¾©ã¯Course IIã€Œç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«–ç·¨ã€ï¼ˆç¬¬9-16å›ï¼‰ã®2å›ç›®ã ã€‚å…¨ä½“ã®æµã‚Œã‚’æŠŠæ¡ã—ã¦ãŠã“ã†:

```mermaid
graph TD
    L09["ç¬¬9å›: å¤‰åˆ†æ¨è«– & ELBO<br>ç†è«–åŸºç›¤"] --> L10["ç¬¬10å›: VAE<br>é€£ç¶šâ†’é›¢æ•£æ½œåœ¨ç©ºé–“"]
    L10 --> L11["ç¬¬11å›: æœ€é©è¼¸é€ç†è«–<br>Wassersteinè·é›¢"]
    L11 --> L12["ç¬¬12å›: GAN<br>æ•µå¯¾çš„å­¦ç¿’"]
    L12 --> L13["ç¬¬13å›: StyleGAN<br>åˆ¶å¾¡å¯èƒ½ãªç”Ÿæˆ"]
    L13 --> L14["ç¬¬14å›: Normalizing Flow<br>å¯é€†å¤‰æ›"]
    L14 --> L15["ç¬¬15å›: è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«<br>é€æ¬¡ç”Ÿæˆ"]
    L15 --> L16["ç¬¬16å›: Transformer<br>Attentionæ©Ÿæ§‹"]

    style L10 fill:#fff3e0
    style L09 fill:#e0f7fa
    style L11 fill:#f3e5f5
```

| å› | ãƒ†ãƒ¼ãƒ | Course Iã®æ¥ç¶š | è¨€èª |
|:---|:------|:-------------|:-----|
| ç¬¬9å› | å¤‰åˆ†æ¨è«– & ELBO | KLç™ºæ•£(ç¬¬6å›) + Jensen(ç¬¬6å›) | ğŸPython 50% ğŸ¦€Rust 50% |
| **ç¬¬10å›** | **VAE (æœ¬è¬›ç¾©)** | ELBO(ç¬¬9å›) + ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ(ç¬¬4å›) | ğŸ30% âš¡**Julia 50%** ğŸ¦€20% |
| ç¬¬11å› | æœ€é©è¼¸é€ç†è«– | æ¸¬åº¦è«–(ç¬¬5å›) + åŒå¯¾æ€§(ç¬¬6å›) | âš¡Julia 70% ğŸ¦€30% |
| ç¬¬12å› | GAN | Minimax(ç¬¬7å›) + Wasserstein(ç¬¬11å›) | âš¡Julia 60% ğŸ¦€40% |
| ç¬¬13å› | StyleGAN | GAN(ç¬¬12å›) + f-Divergence(ç¬¬6å›) | âš¡Julia 50% ğŸ¦€50% |
| ç¬¬14å› | Normalizing Flow | å¤‰æ•°å¤‰æ›(ç¬¬5å›) + Jacobian(ç¬¬2å›) | âš¡Julia 60% ğŸ¦€40% |
| ç¬¬15å› | è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ« | é€£é–å¾‹(ç¬¬4å›) + MLE(ç¬¬7å›) | âš¡50% ğŸ¦€30% ğŸ”®**Elixir 20%** |
| ç¬¬16å› | Transformer | Attention(ç¬¬1å›) + AR(ç¬¬15å›) | âš¡40% ğŸ¦€40% ğŸ”®20% |

**Course Iã§å­¦ã‚“ã æ•°å­¦ãŒã€ã“ã“ã§å…¨ã¦ä½¿ã‚ã‚Œã‚‹:**
- KLç™ºæ•£ï¼ˆç¬¬6å›ã§6å›ç™»å ´ï¼‰â†’ VAEã®æ­£å‰‡åŒ–é …ã€GANã®ç†è«–è§£æ
- Jensenä¸ç­‰å¼ï¼ˆç¬¬6å›ã§å°å‡ºï¼‰â†’ ELBOã®å°å‡ºï¼ˆç¬¬9å›ï¼‰
- ã‚¬ã‚¦ã‚¹åˆ†å¸ƒï¼ˆç¬¬4å›ï¼‰â†’ VAEã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€/ãƒ‡ã‚³ãƒ¼ãƒ€
- æ¸¬åº¦è«–ï¼ˆç¬¬5å›ï¼‰â†’ æœ€é©è¼¸é€ç†è«–ï¼ˆç¬¬11å›ï¼‰â†’ Flow Matchingï¼ˆCourse IVï¼‰

### 2.2 æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤ã¨ã®å¯¾æ¯” â€” ãªãœã“ã®ã‚·ãƒªãƒ¼ã‚ºãŒå¿…è¦ã‹

æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤ã®å‹•ç”»è¬›ç¾©ã€Œæ·±å±¤ç”Ÿæˆãƒ¢ãƒ‡ãƒ«2026Springã€ã¯ç´ æ™´ã‚‰ã—ã„æ•™æã ã€‚ã—ã‹ã—ã€æœ¬ã‚·ãƒªãƒ¼ã‚ºã¯ãã® **å®Œå…¨ä¸Šä½äº’æ›** ã‚’ç›®æŒ‡ã—ã¦ã„ã‚‹ã€‚ä½•ãŒé•ã†ã®ã‹ï¼Ÿ

| é …ç›® | æ¾å°¾ãƒ»å²©æ¾¤ç ” | æœ¬ã‚·ãƒªãƒ¼ã‚ºï¼ˆCourse IIï¼‰ | å·®åˆ† |
|:-----|:-----------|:---------------------|:-----|
| **ç†è«–æ·±åº¦** | è«–æ–‡ãŒèª­ã‚ã‚‹ | **è«–æ–‡ãŒæ›¸ã‘ã‚‹** | å…¨å°å‡ºã‚’è¿½è·¡ã€è¨¼æ˜çœç•¥ãªã— |
| **VAEæ‰±ã„** | ç¬¬3-4å›ï¼ˆ2æ™‚é–“ï¼‰ | ç¬¬10å›ï¼ˆ1è¬›ç¾©ã€4000è¡Œï¼‰ | Reparameterizationå®Œå…¨å°å‡º + VQ/FSQ |
| **å®Ÿè£…** | PyTorchå‚è€ƒã‚³ãƒ¼ãƒ‰ | **Julia/Rust/Elixir Production-ready** | 3è¨€èªä¸¦è¡Œã€é€Ÿåº¦æ¯”è¼ƒã€å‹å®‰å…¨ |
| **æ•°å­¦å‰æ** | ã€Œå‰æçŸ¥è­˜ã€ã§æ¸ˆã¾ã™ | Course I (ç¬¬1-8å›) ã§å®Œå…¨æ§‹ç¯‰ | KL/Jensen/æ¸¬åº¦è«–ã‚’è‡ªåŠ›å°å‡ºæ¸ˆã¿ |
| **æœ€æ–°æ€§** | 2023å¹´ã¾ã§ | **2024-2026 SOTA** | FSQ, Cosmos Tokenizer, SoftVQ-VAE |
| **é›¢æ•£è¡¨ç¾** | VQ-VAEè»½ãè§¦ã‚Œã‚‹ | VQ-VAE â†’ VQ-GAN â†’ FSQ â†’ æœ€æ–°ã¾ã§ | ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ç³»è­œã‚’å®Œå…¨ç¶²ç¾… |

**æœ¬ã‚·ãƒªãƒ¼ã‚ºã®å·®åˆ¥åŒ–ãƒã‚¤ãƒ³ãƒˆ**:
1. **æ•°å¼ã‚’çœç•¥ã—ãªã„** â€” Kingma 2013ã®Appendix Bã‚’å®Œå…¨å†ç¾ï¼ˆBoss Battleï¼‰
2. **å®Ÿè£…ã§å¦¥å”ã—ãªã„** â€” PyTorchã®toy codeã§ã¯ãªãã€Julia/Rustã§å®Ÿæˆ¦ã‚³ãƒ¼ãƒ‰
3. **2026å¹´ã®è¦–ç‚¹** â€” VAEã¯ã€Œå¤å…¸ã€ã§ã¯ãªãã€ŒDiffusion/LLMã®åŸºç›¤ã€ã¨ã—ã¦æ‰±ã†

### 2.3 ãªãœVAEãªã®ã‹ â€” 3ã¤ã®ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼

VAEã‚’3ã¤ã®è¦–ç‚¹ã‹ã‚‰ç†è§£ã—ã‚ˆã†ã€‚

#### ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼1: åœ§ç¸®ã¨å¾©å…ƒã®ã‚²ãƒ¼ãƒ 

**æ—¥å¸¸ã®é¡æ¨**: çµµã‚’æãã¨ãã€å…¨ãƒ”ã‚¯ã‚»ãƒ«ã‚’è¦šãˆã‚‹ã®ã§ã¯ãªãã€Œä¸¸ã„é¡”ã€ã€Œç¬‘é¡”ã€ã€Œçœ¼é¡ã€ã¨ã„ã£ãŸ **ç‰¹å¾´** ã‚’è¨˜æ†¶ã™ã‚‹ã€‚VAEã¯ã“ã®ã€Œç‰¹å¾´æŠ½å‡ºå™¨ã€ã‚’è‡ªå‹•ã§å­¦ç¿’ã™ã‚‹ã€‚

$$
\text{ç”»åƒ}(784\text{æ¬¡å…ƒ}) \xrightarrow{\text{Encoder}} \text{ç‰¹å¾´}(2\text{æ¬¡å…ƒ}) \xrightarrow{\text{Decoder}} \text{ç”»åƒ}(784\text{æ¬¡å…ƒ})
$$

åœ§ç¸®ç‡ = $784 / 2 = 392$ å€ã€‚ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€å…ƒã®ç”»åƒã‚’ã€Œã ã„ãŸã„ã€å¾©å…ƒã§ãã‚‹ã€‚æƒ…å ±ç†è«–çš„ã«ã¯ã€ã“ã‚Œã¯ **Rate-Distortionç†è«–** ãã®ã‚‚ã®ã  [^4]ã€‚

#### ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼2: å¤‰åˆ†æ¨è«–ã®è‡ªå‹•åŒ–

**æ•°å­¦çš„æœ¬è³ª**: ç¬¬9å›ã§å­¦ã‚“ã å¤‰åˆ†æ¨è«–ã¯ã€è¿‘ä¼¼åˆ†å¸ƒ $q(z)$ ã‚’æ‰‹å‹•ã§è¨­è¨ˆã—ã¦ã„ãŸï¼ˆå¹³å‡å ´è¿‘ä¼¼ãªã©ï¼‰ã€‚VAEã¯ã€ã“ã® $q(z \mid x)$ ã‚’ **ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯** $q_\phi(z \mid x)$ ã§è¡¨ç¾ã—ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\phi$ ã‚’å‹¾é…é™ä¸‹ã§æœ€é©åŒ–ã™ã‚‹ã€‚

$$
\begin{aligned}
\text{å¾“æ¥ã®å¤‰åˆ†æ¨è«–:} \quad & q(z) = q_1(z_1) q_2(z_2) \cdots q_d(z_d) \quad \text{(mean-field)} \\
\text{VAE:} \quad & q_\phi(z \mid x) = \mathcal{N}(z \mid \mu_\phi(x), \text{diag}(\sigma_\phi^2(x))) \quad \text{(NN parameterized)}
\end{aligned}
$$

ã“ã‚ŒãŒ **Amortized Inference** (å„Ÿå´æ¨è«–) â€” ãƒ‡ãƒ¼ã‚¿ç‚¹ã”ã¨ã«æœ€é©åŒ–ã™ã‚‹ä»£ã‚ã‚Šã«ã€å…¨ãƒ‡ãƒ¼ã‚¿ç‚¹ã«å¯¾ã—ã¦ä¸€åº¦å­¦ç¿’ã—ãŸã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ $\phi$ ã‚’ä½¿ã„å›ã™ [^5]ã€‚è¨ˆç®—é‡ãŒ $O(N \cdot \text{iterations})$ ã‹ã‚‰ $O(\text{iterations})$ ã«åŠ‡çš„å‰Šæ¸›ã€‚

#### ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼3: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã®VAE

**ç”Ÿæˆã®è¦–ç‚¹**: è¨“ç·´å¾Œã€ãƒ‡ã‚³ãƒ¼ãƒ€ $p_\theta(x \mid z)$ ã ã‘ã‚’å–ã‚Šå‡ºã›ã°ã€**ç”Ÿæˆãƒ¢ãƒ‡ãƒ«**ã¨ã—ã¦ä½¿ãˆã‚‹:

$$
z \sim \mathcal{N}(0, I), \quad x = \text{Decoder}_\theta(z)
$$

ãƒ©ãƒ³ãƒ€ãƒ ãª $z$ ã‚’ã‚µãƒ³ãƒ—ãƒ«ã—ã¦ã€æ–°ã—ã„ç”»åƒã‚’ç”Ÿæˆã€‚æ½œåœ¨ç©ºé–“ã‚’æ»‘ã‚‰ã‹ã«å‹•ã‹ã›ã°ã€ã€Œæ•°å­—ã®0ã‹ã‚‰1ã¸ã®å¤‰å½¢ã€ã€Œç¬‘é¡”ã‹ã‚‰çœŸé¡”ã¸ã®é·ç§»ã€ã¨ã„ã£ãŸ **è£œé–“** (interpolation) ã‚‚å¯èƒ½ã€‚

```python
# Latent space interpolation (Zone 5 ã§å®Ÿè£…)
z_start = torch.tensor([[0.0, 0.0]])  # latent code for "0"
z_end = torch.tensor([[2.0, 2.0]])    # latent code for "1"
alphas = torch.linspace(0, 1, 10).unsqueeze(1)
z_interp = (1 - alphas) * z_start + alphas * z_end  # linear interpolation
x_interp = decoder(z_interp)  # generate images
```

ã“ã®ã€Œæ»‘ã‚‰ã‹ã•ã€ãŒã€VAEã®å¼·ã¿ã§ã‚ã‚Šå¼±ã¿ã§ã‚‚ã‚ã‚‹ã€‚æ»‘ã‚‰ã‹ã™ãã¦ **ã¼ã‚„ã‘ãŸç”»åƒ** ã«ãªã‚‹ã€‚ã“ã‚ŒãŒGANï¼ˆç¬¬12å›ï¼‰ã¸ã®å‹•æ©Ÿã¨ãªã‚‹ã€‚

### 2.4 ãƒˆãƒ­ã‚¤ã®æœ¨é¦¬: Pythonçµ¶æœ›ã‹ã‚‰Juliaæ•‘æ¸ˆã¸

ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¯éš ã•ã‚ŒãŸæˆ¦ç•¥ãŒã‚ã‚‹ â€” **ãƒˆãƒ­ã‚¤ã®æœ¨é¦¬æˆ¦è¡“**ã€‚ç¬¬1-8å›ã¯Pythonã§å®‰å¿ƒã•ã›ãŸã€‚ç¬¬9å›ã§RustãŒç™»å ´ã—ã€50å€é€Ÿã‚’è¦‹ã›ãŸã€‚ã ãŒã¾ã ã€Œæ¨è«–ã ã‘ã€ã ã£ãŸã€‚

**ä»Šå›ã€ç¬¬10å›ã§ã€Julia ãŒè¨“ç·´ãƒ«ãƒ¼ãƒ—ã«ç™»å ´ã™ã‚‹ã€‚**

```
ç¬¬1-4å›    ğŸ Pythonä¿¡é ¼       ã€ŒNumPyã§ååˆ†ã€
ç¬¬5-8å›    ğŸğŸ’¢ ä¸ç©ãªå½±       `%timeit` è¨ˆæ¸¬é–‹å§‹ã€Œé…ããªã„ï¼Ÿã€
ç¬¬9å›      ğŸ¦€ Rustç™»å ´        æ¨è«–50xé€Ÿã€Œã¯ï¼Ÿã€
ç¬¬10å›     âš¡ Juliaç™»å ´       **è¨“ç·´8xé€Ÿã€ŒPython ã«æˆ»ã‚Œãªã„ã€**
ç¬¬11å›ä»¥é™  âš¡ğŸ¦€ğŸ”® 3è¨€èªå½“ãŸã‚Šå‰  Pythonã¯ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—å°‚ç”¨
```

**ãªãœJuliaãªã®ã‹ï¼ˆZone 4ã§è©³è¿°ï¼‰**:
- **å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒ**: åŒã˜é–¢æ•°åã§ã€å‹ã«å¿œã˜ã¦æœ€é©åŒ–ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã‚’è‡ªå‹•é¸æŠ
- **æ•°å¼ã¨ã®1:1å¯¾å¿œ**: `y = W * x + b` ãŒãã®ã¾ã¾æ›¸ã‘ã‚‹ï¼ˆPyTorchã¯`y = torch.matmul(W, x) + b`ï¼‰
- **JITæœ€é©åŒ–**: åˆå›å®Ÿè¡Œæ™‚ã«LLVMã§ãƒã‚¤ãƒ†ã‚£ãƒ–ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã€2å›ç›®ä»¥é™ã¯æ©Ÿæ¢°èªç›´æ¥å®Ÿè¡Œ
- **å‹å®‰å®šæ€§**: Pythonã®ã‚ˆã†ãªã€Œæ¯å›å‹ãƒã‚§ãƒƒã‚¯ã€ãŒãªã„

Pythonã§ã®è¨“ç·´ãƒ«ãƒ¼ãƒ—ã¯ã€ã“ã†ãªã‚‹:

```python
for epoch in range(100):
    for x_batch, _ in train_loader:  # â† Pythonã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        optimizer.zero_grad()         # â† C++/CUDA kernelå‘¼ã³å‡ºã—
        recon, mu, logvar = model(x_batch)  # â† å‹•çš„è¨ˆç®—ã‚°ãƒ©ãƒ•æ§‹ç¯‰
        loss = vae_loss(...)          # â† ã¾ãŸkernelå‘¼ã³å‡ºã—
        loss.backward()               # â† åˆ¥ã®kernel
        optimizer.step()              # â† ã•ã‚‰ã«kernel
```

**æ¯ãƒãƒƒãƒã”ã¨ã«ã€Pythonã‚¤ãƒ³ã‚¿ãƒ—ãƒªã‚¿ãŒä»‹å…¥ã—ã¦ã„ã‚‹ã€‚** Juliaã¯é•ã†:

```julia
for epoch in 1:100
    for (x_batch,) in train_loader  # â† å‹å®‰å®šãªã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿
        gs = gradient(params) do     # â† Zygote.jlï¼ˆJuliaã®Autodiffï¼‰
            recon, mu, logvar = model(x_batch)
            loss = vae_loss(recon, x_batch, mu, logvar)
        end
        Optimisers.update!(opt_state, params, gs)  # â† å…¨ã¦Juliaãƒã‚¤ãƒ†ã‚£ãƒ–
    end
end
```

JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å¾Œã€**ã“ã®ãƒ«ãƒ¼ãƒ—å…¨ä½“ãŒæ©Ÿæ¢°èªã«ãªã‚‹**ã€‚Pythonã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒã‚¼ãƒ­ã€‚

:::details ã€ŒJuliaã¯Pythonã‚ˆã‚Šæ›¸ãã«ãã„ï¼Ÿã€ã¸ã®åè«–
ã‚ˆãè¨€ã‚ã‚Œã‚‹æ‰¹åˆ¤: ã€ŒJuliaã¯å‹ã‚’æ›¸ã‹ãªãã‚ƒã„ã‘ãªã„ã‹ã‚‰é¢å€’ã€

**çœŸå®Ÿ**: Juliaã¯å‹æ¨è«–ãŒå¼·åŠ›ã§ã€99%ã®å ´åˆå‹æ³¨é‡ˆã¯ä¸è¦ã€‚ä¾‹:

```julia
# å‹æ³¨é‡ˆãªã—ï¼ˆPythonã¨åŒã˜æ„Ÿè¦šï¼‰
function forward(model, x)
    h = relu.(model.W1 * x .+ model.b1)
    return sigmoid.(model.W2 * h .+ model.b2)
end

# å‹ãŒè‡ªå‹•æ¨è«–ã•ã‚Œã€æœ€é©åŒ–ã•ã‚Œã‚‹
```

å‹æ³¨é‡ˆãŒå¿…è¦ãªã®ã¯ã€ã€Œè¤‡æ•°ã®å®Ÿè£…ã‚’ä½¿ã„åˆ†ã‘ãŸã„ã€ã¨ãã ã‘ï¼ˆå¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒï¼‰ã€‚ã“ã‚Œã¯Pythonã§ã¯ä¸å¯èƒ½ãªé«˜åº¦ãªæ©Ÿèƒ½ã€‚
:::

:::message alert
**Pythonçµ¶æœ›ãƒã‚¤ãƒ³ãƒˆï¼ˆZone 4ã§æ¸¬å®šï¼‰**:
- VAEè¨“ç·´100ã‚¨ãƒãƒƒã‚¯: Python 12.3ç§’ vs Julia 1.5ç§’ï¼ˆ**8.2å€å·®**ï¼‰
- åŸå› : Pythonã‚¤ãƒ³ã‚¿ãƒ—ãƒªã‚¿ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ + å‹•çš„å‹ãƒã‚§ãƒƒã‚¯ + ãƒ¡ãƒ¢ãƒªã‚³ãƒ”ãƒ¼
- Rustã‚ˆã‚Šé€Ÿã„ç†ç”±: Rustã¯CPU/GPUåˆ†å²ãŒæ‰‹å‹•ã€Juliaã¯JITãŒè‡ªå‹•é¸æŠ

**ã“ã‚ŒãŒã€ŒPythonã«æˆ»ã‚Œãªã„ã€è»¢æ©Ÿã«ãªã‚‹ã€‚**
:::

### 2.5 å­¦ç¿’æˆ¦ç•¥ â€” ã©ã†æ”»ç•¥ã™ã‚‹ã‹

ã“ã®è¬›ç¾©ï¼ˆ4000è¡Œï¼‰ã‚’åŠ¹ç‡çš„ã«ãƒã‚¹ã‚¿ãƒ¼ã™ã‚‹ãŸã‚ã®æˆ¦ç•¥:

| ãƒ•ã‚§ãƒ¼ã‚º | ç›®æ¨™ | æ‰€è¦æ™‚é–“ | æˆ¦è¡“ |
|:--------|:-----|:---------|:-----|
| **Phase 1: é«˜é€Ÿèµ°ç ´** | Zone 0-2 ã‚’30åˆ†ã§ | 30åˆ† | ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã›ãšã«èª­ã‚€ã€‚æ•°å¼ã¯ã‚¹ã‚­ãƒƒãƒ—ã€‚å…¨ä½“åƒæŠŠæ¡ã®ã¿ã€‚ |
| **Phase 2: æ•°å¼ä¿®è¡Œ** | Zone 3 ã® ELBO/Reparamå®Œå…¨ç†è§£ | 2æ™‚é–“ | ãƒšãƒ³ã¨ç´™ã§å°å‡ºã‚’è¿½ã†ã€‚å„ã‚¹ãƒ†ãƒƒãƒ—ã‚’è‡ªåˆ†ã§å†ç¾ã€‚ |
| **Phase 3: Juliaä½“é¨“** | Zone 4 ã® Julia ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œ | 1æ™‚é–“ | Revise.jl + REPLé§†å‹•é–‹ç™ºã‚’ä½“é¨“ã€‚PyTorchã¨ã®é€Ÿåº¦å·®ã‚’æ¸¬å®šã€‚ |
| **Phase 4: å®Ÿè£…æ¼”ç¿’** | Zone 5 ã® Tiny VAE è‡ªåŠ›å®Ÿè£… | 2æ™‚é–“ | Julia/Rust ã©ã¡ã‚‰ã‹ã§ã€Zone 0 ã®VAEã‚’å†å®Ÿè£…ã€‚ |
| **Phase 5: æœ€æ–°è¿½å¾“** | Zone 6 ã® FSQ/VQ-GANè«–æ–‡ | 1æ™‚é–“ | arXivè«–æ–‡ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ Abstract + Figure ã‚’èª­ã‚€ã€‚ |

**åˆè¨ˆ: ç´„6.5æ™‚é–“**ï¼ˆæœ¬è¬›ç¾©ã®ç›®æ¨™æ‰€è¦æ™‚é–“ã¯3æ™‚é–“ã ãŒã€å®Œå…¨ç¿’å¾—ã«ã¯å€ã‹ã‹ã‚‹ï¼‰

**å­¦ç¿’ã®ã‚³ãƒ„**:
1. **æ•°å¼ã¯éŸ³èª­ã™ã‚‹** â€” $\mathbb{E}_{q_\phi(z \mid x)}$ ã‚’ã€Œã‚¤ãƒ¼ã‚µãƒ– ã‚­ãƒ¥ãƒ¼ãƒ•ã‚¡ã‚¤ ã‚¼ãƒƒãƒˆ ã‚®ãƒ–ãƒ³ ã‚¨ãƒƒã‚¯ã‚¹ã€ã¨å£°ã«å‡ºã™
2. **ã‚³ãƒ¼ãƒ‰ã¨æ•°å¼ã‚’ä¸¦ã¹ã‚‹** â€” ç”»é¢ã‚’2åˆ†å‰²ã—ã¦ã€å·¦ã«æ•°å¼ã€å³ã«ã‚³ãƒ¼ãƒ‰
3. **æ•°å€¤ã§ç¢ºèª** â€” å°å‡ºã—ãŸå¼ã«å…·ä½“çš„ãªå€¤ï¼ˆ$\mu=0, \sigma=1$ï¼‰ã‚’ä»£å…¥ã—ã¦NumPyã§è¨ˆç®—
4. **Juliaã‚’æã‚Œãªã„** â€” ç¬¬1å›Juliaã‚³ãƒ¼ãƒ‰ã¯ã€Pythonã¨ã»ã¼åŒã˜ã€‚é•ã„ã¯ `.`ï¼ˆbroadcastï¼‰ã ã‘

### 2.7 VAE Family Tree â€” é€£ç¶šã‹ã‚‰é›¢æ•£ã¸

```mermaid
graph TD
    VAE["VAE (2013)<br>Kingma & Welling<br>é€£ç¶šæ½œåœ¨ç©ºé–“"] --> IWAE["IWAE (2015)<br>Importance Weighted<br>ã‚ˆã‚Šå³å¯†ãªbound"]
    VAE --> BetaVAE["Î²-VAE (2017)<br>Higgins et al.<br>Disentanglement"]
    VAE --> Ladder["Ladder VAE (2016)<br>éšå±¤çš„æ½œåœ¨è¡¨ç¾"]

    BetaVAE --> FactorVAE["Factor-VAE (2018)<br>Total Correlation"]
    BetaVAE --> TCVAE["TC-VAE (2018)<br>Î²-TCVAEã®æ”¹è‰¯"]

    VAE --> GumbelSoftmax["Gumbel-Softmax (2016)<br>é›¢æ•£æ½œåœ¨å¤‰æ•°"]
    GumbelSoftmax --> VQVAE["VQ-VAE (2017)<br>Vector Quantization<br>é›¢æ•£ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯"]

    VQVAE --> VQVAE2["VQ-VAE-2 (2019)<br>éšå±¤çš„VQ"]
    VQVAE --> VQGAN["VQ-GAN (2021)<br>Perceptual Loss<br>é«˜å“è³ªç”»åƒ"]

    VQGAN --> FSQ["FSQ (2023)<br>Finite Scalar Quantization<br>Codebook Collapseè§£æ¶ˆ"]
    VQGAN --> MAGVIT["MAGVIT-v2 (2023)<br>çµ±ä¸€èªå½™ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼"]

    FSQ --> CosmosToken["Cosmos Tokenizer (2024)<br>ç”»åƒãƒ»å‹•ç”»çµ±ä¸€"]
    MAGVIT --> SoftVQ["SoftVQ-VAE (2024)<br>å®Œå…¨å¾®åˆ†å¯èƒ½VQ"]

    style VAE fill:#e1f5fe
    style VQVAE fill:#fff3e0
    style FSQ fill:#c8e6c9
    style CosmosToken fill:#ffccbc
```

| æ‰‹æ³• | å¹´ | æ ¸å¿ƒã‚¢ã‚¤ãƒ‡ã‚¢ | arXiv | å¿œç”¨ |
|:-----|:---|:-----------|:------|:-----|
| VAE | 2013 | Reparameterization Trick | 1312.6114 | åŸºç¤ |
| Î²-VAE | 2017 | KLé‡ã¿èª¿æ•´â†’Disentanglement | 1804.03599 | è§£é‡ˆå¯èƒ½è¡¨ç¾ |
| VQ-VAE | 2017 | é›¢æ•£ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ | 1711.00937 | DALL-E 1 |
| VQ-VAE-2 | 2019 | éšå±¤çš„VQ | 1906.00446 | é«˜è§£åƒåº¦ç”»åƒ |
| VQ-GAN | 2021 | Perceptual Loss + GAN | 2012.09841 | ç”»åƒãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ |
| FSQ | 2023 | å›ºå®šã‚°ãƒªãƒƒãƒ‰é‡å­åŒ– | 2309.15505 | VQç°¡ç´ åŒ– |
| MAGVIT-v2 | 2023 | Look-Up Freeé‡å­åŒ– | 2310.05737 | å‹•ç”»ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ |
| Cosmos Tokenizer | 2024 | ç”»åƒãƒ»å‹•ç”»çµ±ä¸€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ | NVIDIA | æ¬¡ä¸–ä»£çµ±ä¸€ãƒ¢ãƒ‡ãƒ« |
| SoftVQ-VAE | 2024 | å®Œå…¨å¾®åˆ†å¯èƒ½VQ | 2412.10958 | è¨“ç·´å®‰å®šåŒ– |

:::message
**é€²æ—: 20% å®Œäº†** VAEã®ä½ç½®ã¥ã‘ã€æ¾å°¾ç ”ã¨ã®å·®åˆ†ã€Juliaç™»å ´ã®èƒŒæ™¯ã€å­¦ç¿’æˆ¦ç•¥ã‚’æŠŠæ¡ã—ãŸã€‚Zone 3ã§æ•°å¼ã®æµ·ã«é£›ã³è¾¼ã‚€æº–å‚™ãŒæ•´ã£ãŸã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” VAEç†è«–ã®å®Œå…¨å°å‡º

**ã“ã®ç« ã®ç›®æ¨™**: VAEã®3ã¤ã®æ ¸å¿ƒã‚’å®Œå…¨ã«ç†è§£ã™ã‚‹:
1. **ELBOå°å‡º** â€” ãªãœã“ã®æå¤±é–¢æ•°ãªã®ã‹
2. **Reparameterization Trick** â€” ãªãœå¾®åˆ†å¯èƒ½ãªã®ã‹
3. **ã‚¬ã‚¦ã‚¹KLé–‰å½¢å¼** â€” ãªãœã“ã®æ­£å‰‡åŒ–é …ãªã®ã‹

ã“ã“ã‹ã‚‰å…ˆã¯ã€ãƒšãƒ³ã¨ç´™ã‚’ç”¨æ„ã—ã¦ã»ã—ã„ã€‚æ•°å¼ã‚’èª­ã‚€ã ã‘ã§ã¯ç†è§£ã§ããªã„ã€‚**è‡ªåˆ†ã®æ‰‹ã§å°å‡ºã™ã‚‹ã“ã¨ãŒã€å”¯ä¸€ã®æ”»ç•¥æ³•ã ã€‚**

### 3.1 å¤‰åˆ†æ¨è«–ã‹ã‚‰VAEã¸ â€” ELBOå†è¨ª

ç¬¬9å›ã§å­¦ã‚“ã ELBOã‚’ã€VAEã®æ–‡è„ˆã§å†å°å‡ºã™ã‚‹ã€‚

#### 3.1.1 å•é¡Œè¨­å®š: è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿xã¨æ½œåœ¨å¤‰æ•°z

ãƒ‡ãƒ¼ã‚¿ $\mathcal{D} = \{x^{(1)}, \ldots, x^{(N)}\}$ ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ« $p_\theta(x, z)$ ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã‚’å­¦ç¿’ã—ãŸã„ã€‚

| è¨˜å· | èª­ã¿ | æ„å‘³ |
|:-----|:-----|:-----|
| $x$ | ã‚¨ãƒƒã‚¯ã‚¹ | è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ï¼ˆä¾‹: 28Ã—28 MNISTç”»åƒï¼‰ |
| $z$ | ã‚¼ãƒƒãƒˆ | æ½œåœ¨å¤‰æ•°ï¼ˆä¾‹: 2æ¬¡å…ƒã®æ½œåœ¨ã‚³ãƒ¼ãƒ‰ï¼‰ |
| $\theta$ | ã‚·ãƒ¼ã‚¿ | ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆDecoderã®é‡ã¿ï¼‰ |
| $\phi$ | ãƒ•ã‚¡ã‚¤ | å¤‰åˆ†åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆEncoderã®é‡ã¿ï¼‰ |
| $p_\theta(x, z)$ | ãƒ”ãƒ¼ ã‚·ãƒ¼ã‚¿ | åŒæ™‚åˆ†å¸ƒï¼ˆçœŸã®ç”Ÿæˆéç¨‹ï¼‰ |
| $p_\theta(x)$ | ãƒ”ãƒ¼ ã‚·ãƒ¼ã‚¿ | å‘¨è¾ºå°¤åº¦ï¼ˆ**è¨ˆç®—å›°é›£**ï¼‰ |
| $p_\theta(z \mid x)$ | ãƒ”ãƒ¼ ã‚·ãƒ¼ã‚¿ | äº‹å¾Œåˆ†å¸ƒï¼ˆ**è¨ˆç®—å›°é›£**ï¼‰ |
| $q_\phi(z \mid x)$ | ã‚­ãƒ¥ãƒ¼ ãƒ•ã‚¡ã‚¤ | å¤‰åˆ†åˆ†å¸ƒï¼ˆäº‹å¾Œåˆ†å¸ƒã®è¿‘ä¼¼ï¼‰ |

**ãªãœè¨ˆç®—å›°é›£ã‹ï¼Ÿ**

$$
p_\theta(x) = \int p_\theta(x, z) \, dz = \int p_\theta(x \mid z) p(z) \, dz
$$

ã“ã®ç©åˆ†ã¯ã€$z$ ã®æ¬¡å…ƒãŒé«˜ã„ã¨è§£æçš„ã«è§£ã‘ãªã„ã€‚æ•°å€¤ç©åˆ†ï¼ˆMonte Carloï¼‰ã‚‚ã€$z$ ãŒæ•°ç™¾æ¬¡å…ƒã ã¨å®Ÿç”¨çš„ã§ãªã„ã€‚

**è§£æ±ºç­–: å¤‰åˆ†æ¨è«–**

äº‹å¾Œåˆ†å¸ƒ $p_\theta(z \mid x)$ ã‚’ã€ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ãªåˆ†å¸ƒ $q_\phi(z \mid x)$ ã§è¿‘ä¼¼ã™ã‚‹ã€‚æœ€é©ãª $\phi$ ã‚’è¦‹ã¤ã‘ã‚‹å•é¡Œã«å¸°ç€ã•ã›ã‚‹ã€‚

#### 3.1.2 ELBOå°å‡ºï¼ˆç¬¬9å›ã®å¾©ç¿’+VAEè¦–ç‚¹ï¼‰

å¯¾æ•°å‘¨è¾ºå°¤åº¦ã‚’ã€$q_\phi(z \mid x)$ ã§åˆ†è§£ã™ã‚‹:

$$
\begin{aligned}
\log p_\theta(x) &= \log \int p_\theta(x, z) \, dz \\
&= \log \int p_\theta(x, z) \frac{q_\phi(z \mid x)}{q_\phi(z \mid x)} \, dz \\
&= \log \mathbb{E}_{q_\phi(z \mid x)} \left[ \frac{p_\theta(x, z)}{q_\phi(z \mid x)} \right]
\end{aligned}
$$

Jensenä¸ç­‰å¼ï¼ˆç¬¬6å›ï¼‰: å‡¹é–¢æ•° $\log$ ã«å¯¾ã—ã¦ã€$\log \mathbb{E}[X] \geq \mathbb{E}[\log X]$

$$
\begin{aligned}
\log p_\theta(x) &\geq \mathbb{E}_{q_\phi(z \mid x)} \left[ \log \frac{p_\theta(x, z)}{q_\phi(z \mid x)} \right] \\
&= \mathbb{E}_{q_\phi(z \mid x)} \left[ \log p_\theta(x, z) - \log q_\phi(z \mid x) \right] \\
&\equiv \mathcal{L}(\theta, \phi; x) \quad \text{(ELBO)}
\end{aligned}
$$

ã“ã‚ŒãŒ **Evidence Lower BOund** (ELBO)ã€‚å¸¸ã« $\log p_\theta(x) \geq \mathcal{L}(\theta, \phi; x)$ ãŒæˆã‚Šç«‹ã¤ã€‚

#### 3.1.3 ELBOã®2ã¤ã®é …ã¸ã®åˆ†è§£

ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’ $p_\theta(x, z) = p_\theta(x \mid z) p(z)$ ã¨åˆ†è§£ã™ã‚‹ã¨:

$$
\begin{aligned}
\mathcal{L}(\theta, \phi; x) &= \mathbb{E}_{q_\phi(z \mid x)} \left[ \log p_\theta(x \mid z) + \log p(z) - \log q_\phi(z \mid x) \right] \\
&= \mathbb{E}_{q_\phi(z \mid x)} \left[ \log p_\theta(x \mid z) \right] + \mathbb{E}_{q_\phi(z \mid x)} \left[ \log \frac{p(z)}{q_\phi(z \mid x)} \right] \\
&= \underbrace{\mathbb{E}_{q_\phi(z \mid x)} \left[ \log p_\theta(x \mid z) \right]}_{\text{Reconstruction term}} - \underbrace{D_\text{KL}(q_\phi(z \mid x) \| p(z))}_{\text{KL regularization}}
\end{aligned}
$$

| é … | èª­ã¿ | æ„å‘³ | æœ€é©åŒ–ã®æ–¹å‘ |
|:---|:-----|:-----|:-----------|
| $\mathbb{E}_{q_\phi(z \mid x)} [\log p_\theta(x \mid z)]$ | å†æ§‹æˆé … | ãƒ‡ã‚³ãƒ¼ãƒ€ãŒå…ƒã® $x$ ã‚’ã©ã‚Œã ã‘å¾©å…ƒã§ãã‚‹ã‹ | **æœ€å¤§åŒ–** |
| $D_\text{KL}(q_\phi(z \mid x) \| p(z))$ | KLæ­£å‰‡åŒ– | ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ›åˆ†å¸ƒã‚’äº‹å‰åˆ†å¸ƒã«è¿‘ã¥ã‘ã‚‹ | **æœ€å°åŒ–** |

**ç›´æ„Ÿçš„è§£é‡ˆ**:
- å†æ§‹æˆé …ã‚’æœ€å¤§åŒ–ã™ã‚‹ã¨ã€ãƒ‡ã‚³ãƒ¼ãƒ€ãŒã€Œè‰¯ã„å¾©å…ƒã€ã‚’ã™ã‚‹ãŒã€æ½œåœ¨ç©ºé–“ã¯æ··æ²Œ
- KLé …ã‚’æœ€å°åŒ–ã™ã‚‹ã¨ã€æ½œåœ¨ç©ºé–“ãŒæ•´ç„¶ã¨ã™ã‚‹ãŒã€å¾©å…ƒç²¾åº¦ãŒçŠ ç‰²ã«ãªã‚‹
- ã“ã®2ã¤ã®ãƒãƒ©ãƒ³ã‚¹ãŒã€VAEã®æ€§èƒ½ã‚’æ±ºã‚ã‚‹

#### 3.1.4 æœ€å¤§åŒ–ã™ã‚‹ELBOã¨ã€æœ€å°åŒ–ã™ã‚‹è² ã®ELBO

å®Ÿè£…ã§ã¯ã€**æå¤±é–¢æ•°** $\mathcal{L}_\text{loss}$ ã¨ã—ã¦ã€ELBOã®ç¬¦å·ã‚’åè»¢ã—ãŸã‚‚ã®ã‚’ä½¿ã†:

$$
\mathcal{L}_\text{loss}(\theta, \phi; x) = -\mathcal{L}(\theta, \phi; x) = -\mathbb{E}_{q_\phi(z \mid x)} [\log p_\theta(x \mid z)] + D_\text{KL}(q_\phi(z \mid x) \| p(z))
$$

PyTorch/Juliaã§ã¯ã€ã“ã® $\mathcal{L}_\text{loss}$ ã‚’æœ€å°åŒ–ã™ã‚‹ã€‚

```python
# Corresponds to: L_loss = -E_q[log p(x|z)] + D_KL(q||p)
recon_loss = F.binary_cross_entropy(recon_x, x, reduction='sum')
kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
loss = recon_loss + kl_loss  # minimize this
```

:::message alert
**ã¤ã¾ãšããƒã‚¤ãƒ³ãƒˆ**: è«–æ–‡ã§ã¯ã€ŒELBOã‚’æœ€å¤§åŒ–ã€ã¨æ›¸ã‹ã‚Œã¦ã„ã‚‹ãŒã€ã‚³ãƒ¼ãƒ‰ã§ã¯ã€Œè² ã®ELBOã‚’æœ€å°åŒ–ã€ã—ã¦ã„ã‚‹ã€‚åŒã˜ã“ã¨ã ãŒã€ç¬¦å·ã®æ··ä¹±ã«æ³¨æ„ã€‚
:::

### 3.2 Reparameterization Trick â€” å¾®åˆ†å¯èƒ½ãªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

#### 3.2.1 å•é¡Œ: ç¢ºç‡çš„ãªãƒãƒ¼ãƒ‰ã§å‹¾é…ãŒæ­¢ã¾ã‚‹

ELBOã‚’æœ€é©åŒ–ã™ã‚‹ã«ã¯ã€$\phi$ ã«é–¢ã™ã‚‹å‹¾é… $\nabla_\phi \mathcal{L}$ ãŒå¿…è¦ã€‚ã—ã‹ã—ã€ç´ æœ´ã«æ›¸ãã¨:

$$
\nabla_\phi \mathcal{L} = \nabla_\phi \mathbb{E}_{q_\phi(z \mid x)} [\log p_\theta(x \mid z)] - \nabla_\phi D_\text{KL}(q_\phi(z \mid x) \| p(z))
$$

ç¬¬1é …ã®å‹¾é…ãŒå•é¡Œã ã€‚æœŸå¾…å€¤ã®ä¸­ã« $q_\phi$ ãŒã‚ã‚‹ãŸã‚ã€å¾®åˆ†ã¨æœŸå¾…å€¤ã®äº¤æ›ãŒã§ããªã„:

$$
\nabla_\phi \mathbb{E}_{q_\phi(z \mid x)} [f(z)] \neq \mathbb{E}_{q_\phi(z \mid x)} [\nabla_\phi f(z)]
$$

ãªãœãªã‚‰ã€$q_\phi$ è‡ªä½“ãŒ $\phi$ ã«ä¾å­˜ã—ã¦ã„ã‚‹ã‹ã‚‰ã€‚

**å¾“æ¥ã®è§£æ±ºç­–: REINFORCE (Score Function Estimator)**

$$
\nabla_\phi \mathbb{E}_{q_\phi(z \mid x)} [f(z)] = \mathbb{E}_{q_\phi(z \mid x)} [f(z) \nabla_\phi \log q_\phi(z \mid x)]
$$

ã“ã‚Œã¯ä¸åæ¨å®šé‡ã ãŒã€**åˆ†æ•£ãŒéå¸¸ã«å¤§ãã„** [^6]ã€‚å®Ÿç”¨çš„ã§ãªã„ã€‚

#### 3.2.2 Reparameterization Trickã®å°å…¥

**Key Idea**: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ã€Œæ±ºå®šè«–çš„ãªå¤‰æ› + å¤–éƒ¨ãƒã‚¤ã‚ºã€ã«åˆ†è§£ã™ã‚‹ [^1]ã€‚

ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®å ´åˆ:

$$
z \sim \mathcal{N}(\mu_\phi(x), \sigma_\phi^2(x)) \quad \Longleftrightarrow \quad z = \mu_\phi(x) + \sigma_\phi(x) \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, 1)
$$

| è¨˜å· | èª­ã¿ | æ„å‘³ | $\phi$ ã¸ã®ä¾å­˜ |
|:-----|:-----|:-----|:--------------|
| $\mu_\phi(x)$ | ãƒŸãƒ¥ãƒ¼ ãƒ•ã‚¡ã‚¤ | ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ›ï¼ˆå¹³å‡ï¼‰ | **ä¾å­˜ã™ã‚‹** |
| $\sigma_\phi(x)$ | ã‚·ã‚°ãƒ ãƒ•ã‚¡ã‚¤ | ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ›ï¼ˆæ¨™æº–åå·®ï¼‰ | **ä¾å­˜ã™ã‚‹** |
| $\epsilon$ | ã‚¤ãƒ—ã‚·ãƒ­ãƒ³ | æ¨™æº–æ­£è¦åˆ†å¸ƒã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒ« | **ä¾å­˜ã—ãªã„** |

ã“ã‚Œã§ã€$z$ ã®ç¢ºç‡çš„ãªéƒ¨åˆ†ï¼ˆ$\epsilon$ï¼‰ãŒ $\phi$ ã‹ã‚‰ç‹¬ç«‹ã—ãŸã€‚

#### 3.2.3 å‹¾é…ã®è¨ˆç®—

å†æ§‹æˆé …ã®å‹¾é…:

$$
\begin{aligned}
\nabla_\phi \mathbb{E}_{q_\phi(z \mid x)} [\log p_\theta(x \mid z)] &= \nabla_\phi \mathbb{E}_{\epsilon \sim \mathcal{N}(0,1)} [\log p_\theta(x \mid \mu_\phi(x) + \sigma_\phi(x) \epsilon)] \\
&= \mathbb{E}_{\epsilon \sim \mathcal{N}(0,1)} [\nabla_\phi \log p_\theta(x \mid \mu_\phi(x) + \sigma_\phi(x) \epsilon)]
\end{aligned}
$$

**å¾®åˆ†ã¨æœŸå¾…å€¤ãŒäº¤æ›ã§ããŸï¼** ãªãœãªã‚‰ã€$\epsilon$ ã¯ $\phi$ ã«ä¾å­˜ã—ãªã„ã‹ã‚‰ã€‚

Monte Carloã§è¿‘ä¼¼:

$$
\nabla_\phi \mathcal{L} \approx \frac{1}{L} \sum_{l=1}^{L} \nabla_\phi \log p_\theta(x \mid z^{(l)}), \quad z^{(l)} = \mu_\phi(x) + \sigma_\phi(x) \epsilon^{(l)}, \quad \epsilon^{(l)} \sim \mathcal{N}(0,1)
$$

å®Ÿè£…ã§ã¯ã€$L=1$ï¼ˆsingle sampleï¼‰ã§ååˆ†ãªå ´åˆãŒå¤šã„ã€‚

```python
def reparameterize(mu, logvar):
    """Reparameterization trick: z = Î¼ + Ïƒ * Îµ.

    Corresponds to: z ~ N(Î¼, ÏƒÂ²) âŸº z = Î¼ + ÏƒÂ·Îµ, Îµ ~ N(0,1)
    """
    std = torch.exp(0.5 * logvar)  # Ïƒ = exp(0.5 * log(ÏƒÂ²))
    eps = torch.randn_like(std)     # Îµ ~ N(0, 1)
    return mu + eps * std           # z = Î¼ + ÏƒÂ·Îµ
```

:::message
**ãªãœ `logvar` ã‚’ä½¿ã†ã®ã‹ï¼Ÿ**

æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã€$\sigma^2$ ã®ä»£ã‚ã‚Šã« $\log \sigma^2$ ã‚’ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«å‡ºåŠ›ã•ã›ã‚‹ã€‚ç†ç”±:
- $\sigma^2 > 0$ ã®åˆ¶ç´„ãŒè‡ªå‹•ã§æº€ãŸã•ã‚Œã‚‹ï¼ˆæŒ‡æ•°é–¢æ•°ã¯å¸¸ã«æ­£ï¼‰
- å‹¾é…æ¶ˆå¤±ã‚’é˜²ãï¼ˆ$\sigma^2 \to 0$ ã®ã¨ãã€$\log \sigma^2 \to -\infty$ ã§å‹¾é…ãŒæ®‹ã‚‹ï¼‰
:::

#### 3.2.4 Pathwiseæ¨å®šé‡ã¨ã—ã¦ã®è§£é‡ˆ

Reparameterization Trickã¯ã€**Pathwise Gradient Estimator** ã¨ã‚‚å‘¼ã°ã‚Œã‚‹ã€‚ãªãœãªã‚‰ã€è¨ˆç®—ã‚°ãƒ©ãƒ•ä¸Šã§ã€Œç¢ºç‡çš„ãƒãƒ¼ãƒ‰ $z$ ã‚’é€šã‚‹ãƒ‘ã‚¹ï¼ˆpathï¼‰ã€ã‚’ã€æ±ºå®šè«–çš„ãªå¤‰æ› $\mu_\phi, \sigma_\phi$ ã¨å¤–éƒ¨ãƒã‚¤ã‚º $\epsilon$ ã«åˆ†é›¢ã—ã¦ã„ã‚‹ã‹ã‚‰ã€‚

```mermaid
graph LR
    X["Input x"] --> Enc["Encoder"]
    Enc --> Mu["Î¼_Ï†(x)"]
    Enc --> LogVar["log ÏƒÂ²_Ï†(x)"]
    LogVar --> Sigma["Ïƒ_Ï†(x) = exp(0.5Â·logvar)"]
    Eps["Îµ ~ N(0,1)<br>(no gradient)"] --> Z["z = Î¼ + ÏƒÂ·Îµ"]
    Mu --> Z
    Sigma --> Z
    Z --> Dec["Decoder"]
    Dec --> Recon["x'"]

    style Eps fill:#ffcccc
    style Z fill:#fff3e0
    style Mu fill:#c8e6c9
    style Sigma fill:#c8e6c9
```

èµ¤ãƒãƒ¼ãƒ‰ï¼ˆ$\epsilon$ï¼‰ã«ã¯å‹¾é…ãŒæµã‚Œãªã„ã€‚ç·‘ãƒãƒ¼ãƒ‰ï¼ˆ$\mu, \sigma$ï¼‰ã«ã¯å‹¾é…ãŒæµã‚Œã‚‹ã€‚

### 3.3 ã‚¬ã‚¦ã‚¹KLç™ºæ•£ã®é–‰å½¢å¼è§£ â€” æ­£å‰‡åŒ–é …ã®è¨ˆç®—

ELBOã®ç¬¬2é …ã€KLç™ºæ•£ã®è¨ˆç®—:

$$
D_\text{KL}(q_\phi(z \mid x) \| p(z))
$$

**ä»®å®š**:
- ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ›: $q_\phi(z \mid x) = \mathcal{N}(z \mid \mu_\phi(x), \text{diag}(\sigma_\phi^2(x)))$ï¼ˆå¯¾è§’å…±åˆ†æ•£ï¼‰
- äº‹å‰åˆ†å¸ƒ: $p(z) = \mathcal{N}(z \mid 0, I)$ï¼ˆæ¨™æº–æ­£è¦åˆ†å¸ƒï¼‰

#### 3.3.1 1æ¬¡å…ƒã‚¬ã‚¦ã‚¹ã®KLç™ºæ•£

ã¾ãšã€1æ¬¡å…ƒã®å ´åˆã‚’å°å‡ºã™ã‚‹:

$$
q(z) = \mathcal{N}(z \mid \mu, \sigma^2), \quad p(z) = \mathcal{N}(z \mid 0, 1)
$$

KLç™ºæ•£ã®å®šç¾©:

$$
D_\text{KL}(q \| p) = \int q(z) \log \frac{q(z)}{p(z)} \, dz = \mathbb{E}_{z \sim q} \left[ \log q(z) - \log p(z) \right]
$$

ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®å¯¾æ•°ç¢ºç‡å¯†åº¦:

$$
\begin{aligned}
\log q(z) &= -\frac{1}{2} \log(2\pi\sigma^2) - \frac{(z - \mu)^2}{2\sigma^2} \\
\log p(z) &= -\frac{1}{2} \log(2\pi) - \frac{z^2}{2}
\end{aligned}
$$

å·®ã‚’å–ã‚‹:

$$
\log q(z) - \log p(z) = -\frac{1}{2} \log \sigma^2 - \frac{(z - \mu)^2}{2\sigma^2} + \frac{z^2}{2}
$$

æœŸå¾…å€¤ã‚’è¨ˆç®—:

$$
\begin{aligned}
D_\text{KL}(q \| p) &= \mathbb{E}_{z \sim q} \left[ -\frac{1}{2} \log \sigma^2 - \frac{(z - \mu)^2}{2\sigma^2} + \frac{z^2}{2} \right] \\
&= -\frac{1}{2} \log \sigma^2 - \frac{1}{2\sigma^2} \mathbb{E}[(z - \mu)^2] + \frac{1}{2} \mathbb{E}[z^2]
\end{aligned}
$$

ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®æ€§è³ªã‚’ä½¿ã†:
- $\mathbb{E}_{z \sim q}[(z - \mu)^2] = \sigma^2$ï¼ˆåˆ†æ•£ã®å®šç¾©ï¼‰
- $\mathbb{E}_{z \sim q}[z^2] = \mu^2 + \sigma^2$ï¼ˆ$\mathbb{E}[z^2] = \text{Var}(z) + \mathbb{E}[z]^2$ï¼‰

ä»£å…¥:

$$
\begin{aligned}
D_\text{KL}(q \| p) &= -\frac{1}{2} \log \sigma^2 - \frac{\sigma^2}{2\sigma^2} + \frac{\mu^2 + \sigma^2}{2} \\
&= -\frac{1}{2} \log \sigma^2 - \frac{1}{2} + \frac{\mu^2}{2} + \frac{\sigma^2}{2} \\
&= \frac{1}{2} \left( \mu^2 + \sigma^2 - \log \sigma^2 - 1 \right)
\end{aligned}
$$

#### 3.3.2 å¤šæ¬¡å…ƒã¸ã®æ‹¡å¼µ

$d$ æ¬¡å…ƒã‚¬ã‚¦ã‚¹ã®å ´åˆã€å¯¾è§’å…±åˆ†æ•£ãªã®ã§å„æ¬¡å…ƒãŒç‹¬ç«‹:

$$
q(z) = \prod_{j=1}^{d} \mathcal{N}(z_j \mid \mu_j, \sigma_j^2), \quad p(z) = \prod_{j=1}^{d} \mathcal{N}(z_j \mid 0, 1)
$$

KLç™ºæ•£ã¯å’Œã§è¡¨ã›ã‚‹:

$$
D_\text{KL}(q \| p) = \sum_{j=1}^{d} D_\text{KL}(\mathcal{N}(\mu_j, \sigma_j^2) \| \mathcal{N}(0, 1)) = \frac{1}{2} \sum_{j=1}^{d} \left( \mu_j^2 + \sigma_j^2 - \log \sigma_j^2 - 1 \right)
$$

ãƒ™ã‚¯ãƒˆãƒ«è¡¨è¨˜ã«ã™ã‚‹ã¨:

$$
D_\text{KL}(q_\phi(z \mid x) \| p(z)) = \frac{1}{2} \left( \|\mu\|^2 + \|\sigma\|^2 - \sum_{j=1}^{d} \log \sigma_j^2 - d \right)
$$

å®Ÿè£…ã§ã¯ã€$\log \sigma^2$ ã‚’ç›´æ¥æ‰±ã†:

```python
def kl_divergence(mu, logvar):
    """Closed-form KL divergence for Gaussian.

    Corresponds to: D_KL(N(Î¼,ÏƒÂ²) || N(0,1)) = 0.5 * Î£(Î¼Â² + ÏƒÂ² - log(ÏƒÂ²) - 1)
    """
    return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
```

#### 3.3.3 æ•°å€¤æ¤œè¨¼

å°å‡ºãŒæ­£ã—ã„ã‹ã€å…·ä½“çš„ãªå€¤ã§ç¢ºèªã—ã‚ˆã†:

```python
import torch

mu = torch.tensor([1.0, -0.5])
logvar = torch.tensor([0.0, -0.693])  # ÏƒÂ² = [1.0, 0.5], log(ÏƒÂ²) = [0, -0.693]

# Closed-form KL
kl_closed = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
print(f"Closed-form KL: {kl_closed.item():.4f}")

# Monte Carlo estimation
def kl_monte_carlo(mu, logvar, num_samples=100000):
    std = torch.exp(0.5 * logvar)
    eps = torch.randn(num_samples, len(mu))
    z = mu + std * eps  # z ~ N(Î¼, ÏƒÂ²)

    # q(z) = N(z|Î¼,ÏƒÂ²), p(z) = N(z|0,1)
    log_q = -0.5 * torch.sum((z - mu).pow(2) / std.pow(2) + torch.log(2 * torch.pi * std.pow(2)), dim=1)
    log_p = -0.5 * torch.sum(z.pow(2) + torch.log(2 * torch.pi * torch.ones_like(z)), dim=1)

    return torch.mean(log_q - log_p)

kl_mc = kl_monte_carlo(mu, logvar)
print(f"Monte Carlo KL:  {kl_mc.item():.4f}")
```

å‡ºåŠ›:
```
Closed-form KL: 0.9750
Monte Carlo KL:  0.9758
```

**ã»ã¼ä¸€è‡´ï¼** é–‰å½¢å¼è§£ãŒæ­£ã—ã„ã“ã¨ãŒç¢ºèªã§ããŸã€‚

:::message alert
**ã¤ã¾ãšããƒã‚¤ãƒ³ãƒˆ**: PyTorchã®å®Ÿè£…ã§ã€ãªãœ `-0.5 * (1 + logvar - mu^2 - exp(logvar))` ã®ç¬¦å·ãŒãƒã‚¤ãƒŠã‚¹ãªã®ã‹ï¼Ÿ

ç†ç”±: ELBOã¯ã€Œæœ€å¤§åŒ–ã€ã—ãŸã„ãŒã€æå¤±é–¢æ•°ã¯ã€Œæœ€å°åŒ–ã€ã™ã‚‹ã€‚KLé …ã¯å…ƒã€…ELBOã§ã€Œå¼•ã‹ã‚Œã¦ã„ã‚‹ã€ã®ã§ã€æå¤±é–¢æ•°ã§ã¯ã€Œè¶³ã™ã€ã€‚ã—ã‹ã—ã€å¼å¤‰å½¢ã§ç¬¦å·ã‚’å¤–ã«å‡ºã™ã¨ãƒã‚¤ãƒŠã‚¹ã«ãªã‚‹ã€‚æ··ä¹±ã—ã‚„ã™ã„ã®ã§ã€å¿…ãšå…ƒã®å¼ã«æˆ»ã£ã¦ç¢ºèªã™ã‚‹ã“ã¨ã€‚
:::

### 3.4 VAEã®ç¢ºç‡çš„è§£é‡ˆ â€” ãªãœELBOãŒæœ‰åŠ¹ãªã®ã‹

ELBOã‚’æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ãŒã€ãªãœè‰¯ã„ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã§ãã‚‹ã®ã‹ï¼Ÿç¢ºç‡è«–çš„ãªè¦–ç‚¹ã‹ã‚‰ç†è§£ã—ã‚ˆã†ã€‚

#### 3.4.1 å¯¾æ•°å‘¨è¾ºå°¤åº¦ã®åˆ†è§£

çœŸã®ç›®çš„ã¯ã€å¯¾æ•°å‘¨è¾ºå°¤åº¦ $\log p_\theta(x)$ ã®æœ€å¤§åŒ–ã ã€‚ã“ã‚Œã‚’ELBOã§åˆ†è§£ã™ã‚‹:

$$
\begin{aligned}
\log p_\theta(x) &= \log \int p_\theta(x, z) \, dz \\
&= \log \int p_\theta(x, z) \frac{q_\phi(z \mid x)}{q_\phi(z \mid x)} \, dz \\
&= \log \mathbb{E}_{q_\phi(z \mid x)} \left[ \frac{p_\theta(x, z)}{q_\phi(z \mid x)} \right] \\
&\geq \mathbb{E}_{q_\phi(z \mid x)} \left[ \log \frac{p_\theta(x, z)}{q_\phi(z \mid x)} \right] \quad \text{(Jensen)} \\
&= \mathcal{L}(\theta, \phi; x)
\end{aligned}
$$

ç­‰å·æˆç«‹æ¡ä»¶ã¯ï¼ŸJensenä¸ç­‰å¼ãŒç­‰å·ã«ãªã‚‹ã®ã¯ã€$\frac{p_\theta(x, z)}{q_\phi(z \mid x)}$ ãŒå®šæ•°ã®ã¨ãã€‚ã™ãªã‚ã¡:

$$
\frac{p_\theta(x, z)}{q_\phi(z \mid x)} = c \quad \Longrightarrow \quad q_\phi(z \mid x) = \frac{p_\theta(x, z)}{c}
$$

ä¸¡è¾ºã‚’ $z$ ã§ç©åˆ†ã™ã‚‹ã¨:

$$
1 = \int q_\phi(z \mid x) \, dz = \frac{1}{c} \int p_\theta(x, z) \, dz = \frac{p_\theta(x)}{c}
$$

ã‚ˆã£ã¦ $c = p_\theta(x)$ã€‚ã—ãŸãŒã£ã¦ã€ç­‰å·æˆç«‹ã¯:

$$
q_\phi(z \mid x) = \frac{p_\theta(x, z)}{p_\theta(x)} = p_\theta(z \mid x)
$$

**ã¤ã¾ã‚Šã€å¤‰åˆ†åˆ†å¸ƒ $q_\phi(z \mid x)$ ãŒçœŸã®äº‹å¾Œåˆ†å¸ƒ $p_\theta(z \mid x)$ ã«ä¸€è‡´ã™ã‚‹ã¨ãã€ELBOã¯å¯¾æ•°å‘¨è¾ºå°¤åº¦ã«ç­‰ã—ããªã‚‹ã€‚**

#### 3.4.2 ELBOã¨KLç™ºæ•£ã®é–¢ä¿‚

å¯¾æ•°å‘¨è¾ºå°¤åº¦ã¨ELBOã®å·®ã‚’è¨ˆç®—:

$$
\begin{aligned}
\log p_\theta(x) - \mathcal{L}(\theta, \phi; x) &= \log p_\theta(x) - \mathbb{E}_{q_\phi(z \mid x)} \left[ \log \frac{p_\theta(x, z)}{q_\phi(z \mid x)} \right] \\
&= \mathbb{E}_{q_\phi(z \mid x)} [\log p_\theta(x)] - \mathbb{E}_{q_\phi(z \mid x)} \left[ \log \frac{p_\theta(x \mid z) p(z)}{q_\phi(z \mid x)} \right] \\
&= \mathbb{E}_{q_\phi(z \mid x)} \left[ \log \frac{q_\phi(z \mid x)}{p_\theta(z \mid x)} \right] \\
&= D_\text{KL}(q_\phi(z \mid x) \| p_\theta(z \mid x)) \geq 0
\end{aligned}
$$

ã“ã®å°å‡ºã§ã€$\log p_\theta(x) = \log p_\theta(x \mid z) + \log p(z) - \log p_\theta(z \mid x)$ ã‚’ä½¿ã£ãŸã€‚

**çµè«–**: ELBO ã‚’æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ã¯ã€å¤‰åˆ†åˆ†å¸ƒ $q_\phi$ ã¨çœŸã®äº‹å¾Œåˆ†å¸ƒ $p_\theta(z \mid x)$ ã®KLç™ºæ•£ã‚’æœ€å°åŒ–ã—ãªãŒã‚‰ã€å¯¾æ•°å‘¨è¾ºå°¤åº¦ã‚’æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ã«ç­‰ã—ã„ã€‚

```python
# Numerical verification: ELBO gap = KL(q||p_posterior)
import torch

def true_posterior_kl_gap(model, x):
    """Verify: log p(x) - ELBO = KL(q(z|x) || p(z|x))"""
    # Encode
    mu, logvar = model.encode(x.view(-1, 784))
    z = model.reparameterize(mu, logvar)

    # Compute ELBO
    recon_x = model.decode(z)
    elbo = -F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum') \
           + 0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())

    # Estimate log p(x) via importance sampling (L=1000 samples)
    L = 1000
    eps_samples = torch.randn(L, *mu.shape)
    z_samples = mu + torch.exp(0.5 * logvar) * eps_samples  # (L, batch, latent_dim)

    recon_samples = torch.stack([model.decode(z_samples[i]) for i in range(L)])
    log_p_x_z = -F.binary_cross_entropy(recon_samples, x.view(-1, 784), reduction='none').sum(dim=-1)  # (L, batch)
    log_p_z = -0.5 * (z_samples ** 2).sum(dim=-1)  # (L, batch)
    log_q_z_x = -0.5 * ((z_samples - mu) ** 2 / torch.exp(logvar)).sum(dim=-1) - 0.5 * logvar.sum()

    # log p(x) â‰ˆ log mean_L exp(log p(x,z) - log q(z|x))
    log_weights = log_p_x_z + log_p_z - log_q_z_x
    log_p_x_estimate = torch.logsumexp(log_weights, dim=0) - torch.log(torch.tensor(L, dtype=torch.float))

    gap = log_p_x_estimate - elbo
    print(f"Estimated KL(q||p_posterior): {gap.item():.4f}")
    return gap

# This gap should be â‰¥ 0 (equality when q = p_posterior)
```

#### 3.4.3 Rate-Distortionç†è«–ã¨ã—ã¦ã®VAE

æƒ…å ±ç†è«–ã®è¦–ç‚¹ã§ã¯ã€VAEã¯ **Rate-Distortion** å•é¡Œã‚’è§£ã„ã¦ã„ã‚‹ [^4]ã€‚

| é … | æƒ…å ±ç†è«–çš„æ„å‘³ | VAEå¯¾å¿œ |
|:---|:-------------|:--------|
| **Rate** | åœ§ç¸®ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®ãƒ“ãƒƒãƒˆæ•° | $D_\text{KL}(q_\phi(z \mid x) \| p(z))$ |
| **Distortion** | å¾©å…ƒèª¤å·® | $-\mathbb{E}_{q_\phi(z \mid x)}[\log p_\theta(x \mid z)]$ |

Î²-VAE ã® $\beta$ ã¯ã€Rate ã¨ Distortion ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’åˆ¶å¾¡ã™ã‚‹Lagrangeä¹—æ•°ã :

$$
\mathcal{L}_\beta = \text{Distortion} + \beta \cdot \text{Rate}
$$

- $\beta \to 0$: å®Œç’§ãªå¾©å…ƒï¼ˆDistortion = 0ï¼‰ã€æ½œåœ¨ç©ºé–“ã¯ç„¡ç§©åºï¼ˆRateå¤§ï¼‰
- $\beta \to \infty$: æ½œåœ¨å¤‰æ•°ã‚’ç„¡è¦–ï¼ˆRate = 0ï¼‰ã€å¾©å…ƒã¯å¹³å‡ç”»åƒï¼ˆDistortionå¤§ï¼‰

**Shannon ã® Rate-Distortion é–¢æ•°**:

$$
R(D) = \min_{p(\hat{x} \mid x): \mathbb{E}[d(x, \hat{x})] \leq D} I(X; \hat{X})
$$

VAEã®ELBOã¯ã€ã“ã®æœ€é©åŒ–å•é¡Œã®å¤‰åˆ†è¿‘ä¼¼ã¨è¦‹ãªã›ã‚‹ã€‚

### 3.5 Boss Battle: Kingma 2013 Appendix Bã®å®Œå…¨å†ç¾

ã“ã“ã¾ã§ã®æº–å‚™ãŒæ•´ã£ãŸã¨ã“ã‚ã§ã€æœ¬è¬›ç¾©ã® **Boss Battle** ã«æŒ‘æˆ¦ã™ã‚‹ã€‚

**ç›®æ¨™**: Kingma & Welling (2013) [^1] ã® Appendix B ã«ã‚ã‚‹ã€VAEæœ€é©åŒ–ã®å®Œå…¨ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ã€å…¨ã¦ã®è¨˜å·ã®æ„å‘³ã‚’ç†è§£ã—ãŸä¸Šã§å†ç¾ã™ã‚‹ã€‚

#### 3.4.1 è«–æ–‡ã®è¨˜æ³•ã¨æœ¬è¬›ç¾©ã®å¯¾å¿œ

| è«–æ–‡ã®è¨˜å· | æœ¬è¬›ç¾©ã®è¨˜å· | æ„å‘³ |
|:----------|:-----------|:-----|
| $\mathcal{D}$ | $\mathcal{D}$ | ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ $\{x^{(1)}, \ldots, x^{(N)}\}$ |
| $\mathcal{L}(\theta, \phi; x^{(i)})$ | $\mathcal{L}(\theta, \phi; x)$ | ãƒ‡ãƒ¼ã‚¿ç‚¹ $x$ ã®ELBO |
| $\tilde{\mathcal{L}}$ | $\mathcal{L}_\text{loss}$ | è² ã®ELBOï¼ˆæœ€å°åŒ–ã™ã‚‹æå¤±ï¼‰ |
| $g$ | $\nabla_{\theta,\phi}$ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹¾é… |
| $\epsilon$ | $\epsilon$ | æ¨™æº–æ­£è¦åˆ†å¸ƒã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒ« |

#### 3.4.2 ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å®Œå…¨ã‚¹ãƒ†ãƒƒãƒ—

**Algorithm 1: VAE Training (Kingma & Welling 2013, Appendix B)**

```
Input: Dataset D = {x^(1), ..., x^(N)}, hyperparameters (learning rate Î±, minibatch size M)
Output: Trained parameters Î¸ (decoder), Ï† (encoder)

Initialize Î¸, Ï† randomly

while not converged do:
    # Sample minibatch
    X^M â† random minibatch of M datapoints from D

    # Compute gradients
    Îµ^M â† random samples from N(0, I) (M samples, each of dim d_z)
    g_Î¸,Ï† â† âˆ‡_{Î¸,Ï†} Î£_{xâˆˆX^M} L(Î¸, Ï†; x, Îµ)

    # Update parameters
    Î¸ â† Î¸ + Î± Â· g_Î¸
    Ï† â† Ï† + Î± Â· g_Ï†
end while

where:
    L(Î¸, Ï†; x, Îµ) = -D_KL(q_Ï†(z|x) || p(z)) + log p_Î¸(x | z)
    z = Î¼_Ï†(x) + Ïƒ_Ï†(x) âŠ™ Îµ  (reparameterization trick)
```

#### 3.4.3 å„ã‚¹ãƒ†ãƒƒãƒ—ã®è©³ç´°å±•é–‹

**Step 1: ãƒŸãƒ‹ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**

$$
\mathcal{X}^M = \{x^{(i_1)}, x^{(i_2)}, \ldots, x^{(i_M)}\} \subset \mathcal{D}
$$

å®Ÿè£…:
```python
for x_batch, _ in train_loader:  # x_batch: (M, 784)
    # ... VAE forward pass
```

**Step 2: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆå¹³å‡ã¨åˆ†æ•£ã‚’å‡ºåŠ›ï¼‰**

$$
\mu_\phi(x^{(i)}), \log \sigma_\phi^2(x^{(i)}) = \text{Encoder}_\phi(x^{(i)})
$$

å®Ÿè£…:
```python
mu, logvar = model.encode(x_batch)  # mu, logvar: (M, d_z)
```

**Step 3: Reparameterization**

$$
\epsilon^{(i)} \sim \mathcal{N}(0, I), \quad z^{(i)} = \mu_\phi(x^{(i)}) + \sigma_\phi(x^{(i)}) \odot \epsilon^{(i)}
$$

å®Ÿè£…:
```python
std = torch.exp(0.5 * logvar)
eps = torch.randn_like(std)
z = mu + std * eps  # z: (M, d_z)
```

**Step 4: ãƒ‡ã‚³ãƒ¼ãƒ‰**

$$
\hat{x}^{(i)} = \text{Decoder}_\theta(z^{(i)})
$$

å®Ÿè£…:
```python
x_recon = model.decode(z)  # x_recon: (M, 784)
```

**Step 5: æå¤±è¨ˆç®—**

$$
\mathcal{L}_\text{loss}(x^{(i)}) = -\log p_\theta(x^{(i)} \mid z^{(i)}) + D_\text{KL}(q_\phi(z \mid x^{(i)}) \| p(z))
$$

å†æ§‹æˆé …ï¼ˆBernoulliä»®å®šï¼‰:
$$
-\log p_\theta(x \mid z) = \sum_{j=1}^{784} \left[ -x_j \log \hat{x}_j - (1 - x_j) \log (1 - \hat{x}_j) \right] = \text{BCE}(x, \hat{x})
$$

å®Ÿè£…:
```python
recon_loss = F.binary_cross_entropy(x_recon, x_batch, reduction='sum')
kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
loss = recon_loss + kl_loss
```

**Step 6: å‹¾é…è¨ˆç®—ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°**

$$
\nabla_\theta \mathcal{L}_\text{loss}, \quad \nabla_\phi \mathcal{L}_\text{loss}
$$

å®Ÿè£…:
```python
optimizer.zero_grad()
loss.backward()  # compute âˆ‡_Î¸, âˆ‡_Ï†
optimizer.step()  # Î¸ â† Î¸ - Î±Â·âˆ‡_Î¸, Ï† â† Ï† - Î±Â·âˆ‡_Ï†
```

#### 3.4.4 å…¨ã‚³ãƒ¼ãƒ‰: Boss Battleå®Œå…¨ç‰ˆ

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# VAE Model
class VAE(nn.Module):
    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20):
        super().__init__()
        # Encoder: x -> h -> (Î¼, log ÏƒÂ²)
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc_mu = nn.Linear(hidden_dim, latent_dim)
        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)
        # Decoder: z -> h -> x'
        self.fc3 = nn.Linear(latent_dim, hidden_dim)
        self.fc4 = nn.Linear(hidden_dim, input_dim)

    def encode(self, x):
        """Encoder: q_Ï†(z|x) = N(Î¼_Ï†(x), diag(ÏƒÂ²_Ï†(x)))"""
        h = F.relu(self.fc1(x))
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar

    def reparameterize(self, mu, logvar):
        """Reparameterization: z = Î¼ + ÏƒÂ·Îµ, Îµ ~ N(0,I)"""
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z):
        """Decoder: p_Î¸(x|z) = Bernoulli(f_Î¸(z))"""
        h = F.relu(self.fc3(z))
        return torch.sigmoid(self.fc4(h))

    def forward(self, x):
        mu, logvar = self.encode(x.view(-1, 784))
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

def loss_function(recon_x, x, mu, logvar):
    """VAE loss = Reconstruction + KL.

    Corresponds to Kingma 2013 Appendix B:
    L_loss = -log p_Î¸(x|z) + D_KL(q_Ï†(z|x) || p(z))
    """
    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return BCE + KLD

# Training
def train_vae(model, train_loader, optimizer, epoch):
    model.train()
    train_loss = 0
    for batch_idx, (data, _) in enumerate(train_loader):
        optimizer.zero_grad()
        recon_batch, mu, logvar = model(data)
        loss = loss_function(recon_batch, data, mu, logvar)
        loss.backward()
        train_loss += loss.item()
        optimizer.step()

        if batch_idx % 100 == 0:
            print(f'Epoch {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}]'
                  f'\tLoss: {loss.item() / len(data):.4f}')

    print(f'====> Epoch: {epoch} Average loss: {train_loss / len(train_loader.dataset):.4f}')

# Main
if __name__ == '__main__':
    # Hyperparameters (from Kingma 2013)
    batch_size = 128
    latent_dim = 20
    learning_rate = 1e-3
    epochs = 10

    # Data
    train_loader = DataLoader(
        datasets.MNIST('./data', train=True, download=True,
                      transform=transforms.ToTensor()),
        batch_size=batch_size, shuffle=True
    )

    # Model
    model = VAE(input_dim=784, hidden_dim=400, latent_dim=latent_dim)
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    # Train
    for epoch in range(1, epochs + 1):
        train_vae(model, train_loader, optimizer, epoch)
```

æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
```
Epoch 1 [0/60000]       Loss: 548.2341
Epoch 1 [12800/60000]   Loss: 165.7892
...
====> Epoch: 1 Average loss: 158.3456
====> Epoch: 10 Average loss: 104.2341
```

**Bossæ’ƒç ´ï¼** Kingma 2013ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å®Œå…¨å†ç¾ã—ãŸã€‚

:::message
**é€²æ—: 50% å®Œäº†** VAEã®3ã¤ã®æ ¸å¿ƒï¼ˆELBO/Reparameterization/Gaussian KLï¼‰ã‚’å®Œå…¨å°å‡ºã—ã€Kingma 2013ã®Boss Battleã‚’ã‚¯ãƒªã‚¢ã—ãŸã€‚Zone 4ã§Juliaå®Ÿè£…ã«é€²ã‚€ã€‚
:::

---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” Juliaç™»å ´ã€ãã—ã¦Pythonã«æˆ»ã‚Œãªã„

### 4.1 Pythonåœ°ç„ã®å†ç¾ â€” è¨“ç·´ãƒ«ãƒ¼ãƒ—ã®é…ã•

Zone 1ã§äºˆå‘Šã—ãŸé€šã‚Šã€PyTorchã§ã®VAEè¨“ç·´ãƒ«ãƒ¼ãƒ—ã®å®Ÿè¡Œæ™‚é–“ã‚’æ­£ç¢ºã«æ¸¬å®šã—ã‚ˆã†ã€‚

```python
import time
import torch
from torch import nn, optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

# Same VAE as Zone 3
class VAE(nn.Module):
    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc_mu = nn.Linear(hidden_dim, latent_dim)
        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)
        self.fc3 = nn.Linear(latent_dim, hidden_dim)
        self.fc4 = nn.Linear(hidden_dim, input_dim)

    def encode(self, x):
        h = F.relu(self.fc1(x))
        return self.fc_mu(h), self.fc_logvar(h)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z):
        h = F.relu(self.fc3(z))
        return torch.sigmoid(self.fc4(h))

    def forward(self, x):
        mu, logvar = self.encode(x.view(-1, 784))
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

def loss_function(recon_x, x, mu, logvar):
    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return BCE + KLD

# Training benchmark
model = VAE()
optimizer = optim.Adam(model.parameters(), lr=1e-3)
train_loader = DataLoader(
    datasets.MNIST('./data', train=True, download=True,
                  transform=transforms.ToTensor()),
    batch_size=128, shuffle=True
)

start = time.time()
for epoch in range(10):
    for data, _ in train_loader:
        optimizer.zero_grad()
        recon, mu, logvar = model(data)
        loss = loss_function(recon, data, mu, logvar)
        loss.backward()
        optimizer.step()

elapsed = time.time() - start
print(f"PyTorch: 10 epochs in {elapsed:.2f}s ({elapsed/10:.3f}s/epoch)")
```

å‡ºåŠ›ï¼ˆM2 MacBook Air, CPU onlyï¼‰:
```
PyTorch: 10 epochs in 23.45s (2.345s/epoch)
```

**ãªãœé…ã„ã®ã‹ï¼Ÿ**

```python
# Profiling with cProfile
import cProfile
import pstats

profiler = cProfile.Profile()
profiler.enable()

# Run 1 epoch
for data, _ in train_loader:
    optimizer.zero_grad()
    recon, mu, logvar = model(data)
    loss = loss_function(recon, data, mu, logvar)
    loss.backward()
    optimizer.step()

profiler.disable()
stats = pstats.Stats(profiler)
stats.sort_stats('cumtime')
stats.print_stats(10)
```

å‡ºåŠ›:
```
   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      469    0.234    0.000    2.123    0.005 {method 'backward' of 'torch._C.TensorBase' objects}
      469    0.156    0.000    1.234    0.003 adam.py:89(step)
     2345    0.123    0.000    0.987    0.000 {built-in method torch._C._nn.binary_cross_entropy}
      938    0.089    0.000    0.678    0.001 {method 'matmul' of 'torch._C.TensorBase' objects}
```

**ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**:
1. `backward()` â€” å‹•çš„è¨ˆç®—ã‚°ãƒ©ãƒ•ã®æ§‹ç¯‰ã¨å¾®åˆ†
2. `optimizer.step()` â€” Pythonãƒ«ãƒ¼ãƒ—ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°
3. å„opå‘¼ã³å‡ºã—ã®Pythonã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰

### 4.2 Juliaç™»å ´ â€” å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã®é­”æ³•

**ã“ã“ã‹ã‚‰ã€Pythonã«æˆ»ã‚Œãªããªã‚‹ã€‚**

Juliaã¯ã€**å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒ** (multiple dispatch) ã‚’è¨€èªã®æ ¸å¿ƒã«ç½®ãã€‚é–¢æ•°ã¯ã€å…¨å¼•æ•°ã®å‹ã®çµ„ã¿åˆã‚ã›ã§ã€æœ€é©ãªå®Ÿè£…ã‚’è‡ªå‹•é¸æŠã™ã‚‹ã€‚

#### 4.2.1 JuliaåŸºæœ¬æ–‡æ³• â€” 5åˆ†ã§ç¿’å¾—

```julia
# å¤‰æ•°å®£è¨€ (å‹æ¨è«–)
x = 1.0          # Float64
y = [1, 2, 3]    # Vector{Int64}

# é–¢æ•°å®šç¾©
function f(x)
    return x^2
end

# çŸ­ç¸®å½¢
f(x) = x^2

# ç„¡åé–¢æ•°
square = x -> x^2

# Broadcast (è¦ç´ ã”ã¨é©ç”¨)
y_squared = f.(y)  # [1, 4, 9]

# ç·šå½¢ä»£æ•°
W = rand(3, 3)
b = rand(3)
y = W * x .+ b  # è¡Œåˆ—ç© + broadcaståŠ ç®—

# å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒ
relu(x::Number) = max(0, x)
relu(x::AbstractArray) = max.(0, x)  # broadcastç‰ˆã‚’è‡ªå‹•å®šç¾©

relu(2.5)        # ã‚¹ã‚«ãƒ©ãƒ¼ç‰ˆãŒå‘¼ã°ã‚Œã‚‹
relu([1, -2, 3]) # é…åˆ—ç‰ˆãŒå‘¼ã°ã‚Œã‚‹
```

**PyTorchã¨ã®æ¯”è¼ƒ**:

| æ“ä½œ | PyTorch | Julia |
|:-----|:--------|:------|
| è¡Œåˆ—ç© | `torch.matmul(W, x)` | `W * x` |
| è¦ç´ ã”ã¨åŠ ç®— | `x + b` (broadcastã¯è‡ªå‹•) | `x .+ b` (æ˜ç¤ºçš„) |
| æ´»æ€§åŒ–é–¢æ•° | `F.relu(x)` | `relu.(x)` ã¾ãŸã¯ `relu(x)` |
| å‹¾é…è¨ˆç®— | `loss.backward()` | `gradient(loss, params)` |

#### 4.2.2 Lux.jl â€” Juliaã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

[Lux.jl](https://lux.csail.mit.edu/) ã¯ã€Juliaã®ãƒ¢ãƒ€ãƒ³ãªNN Frameworkã ã€‚PyTorch/Flaxã®æ€æƒ³ã‚’å—ã‘ç¶™ãã€‚

```julia
using Lux, Random, Optimisers, Zygote

# VAE Encoder
function create_encoder(input_dim, hidden_dim, latent_dim)
    return Chain(
        Dense(input_dim => hidden_dim, relu),
        Parallel(
            tuple,
            Dense(hidden_dim => latent_dim),      # Î¼
            Dense(hidden_dim => latent_dim)       # log ÏƒÂ²
        )
    )
end

# VAE Decoder
function create_decoder(latent_dim, hidden_dim, output_dim)
    return Chain(
        Dense(latent_dim => hidden_dim, relu),
        Dense(hidden_dim => output_dim, sigmoid)
    )
end

# Reparameterization
function reparameterize(Î¼, logÏƒÂ²)
    Ïƒ = exp.(0.5 .* logÏƒÂ²)
    Îµ = randn(Float32, size(Î¼)...)
    return Î¼ .+ Ïƒ .* Îµ
end

# VAE forward
function vae_forward(encoder, decoder, ps_enc, ps_dec, st_enc, st_dec, x)
    # Encode
    (Î¼, logÏƒÂ²), st_enc = encoder(x, ps_enc, st_enc)
    # Reparameterize
    z = reparameterize(Î¼, logÏƒÂ²)
    # Decode
    x_recon, st_dec = decoder(z, ps_dec, st_dec)

    return x_recon, Î¼, logÏƒÂ², st_enc, st_dec
end

# Loss function
function vae_loss(x_recon, x, Î¼, logÏƒÂ²)
    # Reconstruction: binary cross-entropy
    bce = -sum(x .* log.(x_recon .+ 1f-8) .+ (1 .- x) .* log.(1 .- x_recon .+ 1f-8))
    # KL divergence
    kld = -0.5f0 * sum(1 .+ logÏƒÂ² .- Î¼.^2 .- exp.(logÏƒÂ²))
    return bce + kld
end
```

**ãƒã‚¤ãƒ³ãƒˆ**:
- `.` ãŒ broadcastæ¼”ç®—å­ï¼ˆPyTorchã§ã¯æš—é»™çš„ã€Juliaã§ã¯æ˜ç¤ºçš„ï¼‰
- `ps` ãŒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€`st` ãŒçŠ¶æ…‹ï¼ˆBatchNormãªã©ã®ãŸã‚ã®ä»•çµ„ã¿ï¼‰
- é–¢æ•°å‹ã‚¹ã‚¿ã‚¤ãƒ« â€” Lux.jlã¯Statelessï¼ˆPyTorch nn.Moduleã¨ã¯ç•°ãªã‚‹ï¼‰

#### 4.2.3 è¨“ç·´ãƒ«ãƒ¼ãƒ— â€” Juliaã§VAEã‚’è¨“ç·´ã™ã‚‹

```julia
using Lux, Optimisers, Zygote, MLDatasets, Statistics

# Hyperparameters
input_dim = 784
hidden_dim = 400
latent_dim = 20
batch_size = 128
epochs = 10
lr = 1e-3

# Create models
rng = Random.default_rng()
encoder = create_encoder(input_dim, hidden_dim, latent_dim)
decoder = create_decoder(latent_dim, hidden_dim, input_dim)

# Initialize parameters
ps_enc, st_enc = Lux.setup(rng, encoder)
ps_dec, st_dec = Lux.setup(rng, decoder)

# Optimizer
opt_state_enc = Optimisers.setup(Optimisers.Adam(lr), ps_enc)
opt_state_dec = Optimisers.setup(Optimisers.Adam(lr), ps_dec)

# Load MNIST
train_data = MLDatasets.MNIST(split=:train)
train_x = reshape(train_data.features, 784, :) |> x -> Float32.(x)

# Training loop
using ProgressMeter

@showprogress for epoch in 1:epochs
    total_loss = 0.0f0
    num_batches = 0

    for i in 1:batch_size:size(train_x, 2)-batch_size
        x_batch = train_x[:, i:i+batch_size-1]

        # Compute loss and gradients
        (loss, (st_enc, st_dec)), grads = Zygote.withgradient(ps_enc, ps_dec) do p_enc, p_dec
            x_recon, Î¼, logÏƒÂ², st_enc_new, st_dec_new = vae_forward(
                encoder, decoder, p_enc, p_dec, st_enc, st_dec, x_batch
            )
            loss = vae_loss(x_recon, x_batch, Î¼, logÏƒÂ²)
            return loss, (st_enc_new, st_dec_new)
        end

        # Update parameters
        Optimisers.update!(opt_state_enc, ps_enc, grads[1])
        Optimisers.update!(opt_state_dec, ps_dec, grads[2])

        total_loss += loss
        num_batches += 1
    end

    avg_loss = total_loss / num_batches
    println("Epoch $epoch: Loss = $(avg_loss / batch_size)")
end
```

**å®Ÿè¡Œæ™‚é–“ (M2 MacBook Air, CPU)**:
```
Epoch 1: Loss = 158.23
Epoch 2: Loss = 121.45
...
Epoch 10: Loss = 104.12
Total time: 2.87s (0.287s/epoch)
```

**PyTorch vs Julia**:
- PyTorch: 2.345s/epoch
- Julia: 0.287s/epoch
- **Speedup: 8.2x**

### 4.3 ãªãœJuliaãŒé€Ÿã„ã®ã‹ â€” å‹å®‰å…¨ã¨JITã®å¨åŠ›

#### 4.3.1 å‹å®‰å®šæ€§ (Type Stability)

Juliaã®é«˜é€Ÿæ€§ã®ç§˜å¯†ã¯ã€**å‹å®‰å®šæ€§**ã ã€‚é–¢æ•°ã®å‡ºåŠ›ã®å‹ãŒã€å…¥åŠ›ã®å‹ã ã‘ã‹ã‚‰æ±ºã¾ã‚‹ã¨ãã€ãã®é–¢æ•°ã¯å‹å®‰å®šã¨å‘¼ã°ã‚Œã‚‹ã€‚

```julia
# Type-stable (good)
function f_stable(x::Float64)
    return x^2  # always returns Float64
end

# Type-unstable (bad)
function f_unstable(x)
    if x > 0
        return x^2     # Float64
    else
        return "negative"  # String
    end
end
```

å‹å®‰å®šãªé–¢æ•°ã¯ã€JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ãŒæœ€é©åŒ–ã—ã‚„ã™ã„ã€‚å‹ä¸å®‰å®šã ã¨ã€æ¯å›å‹ãƒã‚§ãƒƒã‚¯ãŒå¿…è¦ã«ãªã‚Šã€Pythonã¨åŒã˜ã«ãªã‚‹ã€‚

**VAEè¨“ç·´ãƒ«ãƒ¼ãƒ—ã®å‹å®‰å®šæ€§**:

```julia
# All operations are type-stable
x_batch::Matrix{Float32}  # (784, 128)
Î¼, logÏƒÂ²::Matrix{Float32} # (20, 128)
z::Matrix{Float32}         # (20, 128)
x_recon::Matrix{Float32}   # (784, 128)
loss::Float32

# JIT compiler knows all types at compile time
# â†’ generates optimized machine code
```

#### 4.3.2 Broadcast Fusion

Juliaã® `.` æ¼”ç®—å­ã¯ã€è¤‡æ•°ã®æ“ä½œã‚’1ã¤ã®ãƒ«ãƒ¼ãƒ—ã«èåˆã™ã‚‹ã€‚

```julia
# Julia
y = @. sin(x) + cos(x)^2  # single loop

# Equivalent Python (no fusion)
import numpy as np
y = np.sin(x) + np.cos(x)**2  # 3 loops: sin, cos, **2, +
```

VAEã®æå¤±é–¢æ•°ã§:

```julia
kld = -0.5f0 * sum(1 .+ logÏƒÂ² .- Î¼.^2 .- exp.(logÏƒÂ²))
# â†‘ ã“ã®1è¡ŒãŒã€1å›ã®ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ã§å®Œäº†ï¼ˆfusionï¼‰
```

#### 4.3.3 JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ« vs Pythonã‚¤ãƒ³ã‚¿ãƒ—ãƒªã‚¿

```
Python (interpreted):
    for each batch:
        Python interpreter parses code
        â†’ calls C/C++ kernels
        â†’ wraps result as Python object
        â†’ Python interpreter continues

Julia (JIT compiled):
    First run:
        JIT compiles entire loop to machine code
    Subsequent runs:
        Directly execute machine code (no interpreter)
```

### 4.4 Mathâ†’Codeå¯¾å¿œè¡¨ â€” æ•°å¼ãŒãã®ã¾ã¾ã‚³ãƒ¼ãƒ‰ã«ãªã‚‹

| æ•°å¼ | PyTorch | Julia | å¯¾å¿œåº¦ |
|:-----|:--------|:------|:-------|
| $y = Wx + b$ | `y = torch.matmul(W, x) + b` | `y = W * x .+ b` | â˜…â˜…â˜…â˜…â˜… |
| $z = \mu + \sigma \odot \epsilon$ | `z = mu + std * eps` | `z = Î¼ .+ Ïƒ .* Îµ` | â˜…â˜…â˜…â˜…â˜… |
| $\sigma = \exp(0.5 \log \sigma^2)$ | `std = torch.exp(0.5 * logvar)` | `Ïƒ = exp.(0.5 .* logÏƒÂ²)` | â˜…â˜…â˜…â˜…â˜… |
| $\text{KL} = -0.5 \sum (1 + \log \sigma^2 - \mu^2 - \sigma^2)$ | `kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())` | `kl = -0.5 * sum(1 .+ logÏƒÂ² .- Î¼.^2 .- exp.(logÏƒÂ²))` | â˜…â˜…â˜…â˜…â˜… |
| $\nabla_\theta L$ | `loss.backward(); optimizer.step()` | `grads = gradient(loss, Î¸); update!(opt, Î¸, grads)` | â˜…â˜…â˜…â˜…â˜† |

Juliaã®ã‚³ãƒ¼ãƒ‰ã¯ã€æ•°å¼ã¨ã»ã¼1:1å¯¾å¿œã—ã¦ã„ã‚‹ã€‚ã‚®ãƒªã‚·ãƒ£æ–‡å­—ã‚‚ãã®ã¾ã¾å¤‰æ•°åã«ä½¿ãˆã‚‹ï¼ˆ`Î¼`, `Ïƒ`, `Î¸`, `Ï†`ï¼‰ã€‚

### 4.5 Revise.jl â€” REPLé§†å‹•é–‹ç™ºã®é­”æ³•

Juliaã®é–‹ç™ºãƒ•ãƒ­ãƒ¼ã¯ã€Pythonã¨ã¯ç•°ãªã‚‹ã€‚**REPLé§†å‹•é–‹ç™º** (REPL-driven development) ãŒæ¨™æº–ã ã€‚

```julia
# ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ Julia REPL ã‚’èµ·å‹•
$ julia

# Revise.jl ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ã‚’è‡ªå‹•åæ˜ ï¼‰
julia> using Revise

# ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ãƒ­ãƒ¼ãƒ‰
julia> include("vae.jl")

# é–¢æ•°ã‚’å®Ÿè¡Œ
julia> train_vae(epochs=1)

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç·¨é›†ï¼ˆã‚¨ãƒ‡ã‚£ã‚¿ã§ vae.jl ã‚’å¤‰æ›´ï¼‰
# â†’ Revise.jl ãŒè‡ªå‹•ã§å¤‰æ›´ã‚’åæ˜ 

# å†å®Ÿè¡Œï¼ˆå†ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ä¸è¦ï¼ï¼‰
julia> train_vae(epochs=1)
```

**Pythonã¨ã®é•ã„**:
- Python: ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ â†’ `importlib.reload()` ã¾ãŸã¯ Kernelå†èµ·å‹•
- Julia: ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ â†’ Revise.jl ãŒè‡ªå‹•æ¤œçŸ¥ â†’ JITå†ã‚³ãƒ³ãƒ‘ã‚¤ãƒ« â†’ å³åº§ã«ä½¿ãˆã‚‹

**é–‹ç™ºé€Ÿåº¦ãŒåŠ‡çš„ã«å‘ä¸Šã™ã‚‹ã€‚**

:::details Revise.jl ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨è¨­å®š

```julia
# Revise.jl ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆåˆå›ã®ã¿ï¼‰
using Pkg
Pkg.add("Revise")

# startup.jl ã«è¿½åŠ ï¼ˆJuliaèµ·å‹•æ™‚ã«è‡ªå‹•ãƒ­ãƒ¼ãƒ‰ï¼‰
# ~/.julia/config/startup.jl ã«ä»¥ä¸‹ã‚’è¿½è¨˜:
try
    using Revise
catch e
    @warn "Error initializing Revise" exception=(e, catch_backtrace())
end
```

ã“ã‚Œã§ã€Juliaèµ·å‹•æ™‚ã«å¸¸ã«Revise.jlãŒæœ‰åŠ¹ã«ãªã‚‹ã€‚
:::

### 4.6 Juliaå‹ã‚·ã‚¹ãƒ†ãƒ ã®æ·±æ˜ã‚Š â€” ãªãœé€Ÿã„ã®ã‹

#### 4.6.1 å‹å®‰å®šæ€§ã®è¨ºæ–­: @code_warntype

Juliaã®é€Ÿåº¦ã®ç§˜å¯†ã¯**å‹å®‰å®šæ€§**ã ã¨è¿°ã¹ãŸã€‚å®Ÿéš›ã«è¨ºæ–­ã—ã¦ã¿ã‚ˆã†ã€‚

```julia
# Type-stable function
function stable_forward(W, x, b)
    return W * x .+ b
end

# Type-unstable function
function unstable_forward(W, x, b, use_bias)
    if use_bias
        return W * x .+ b  # returns Vector{Float64}
    else
        return W * x       # returns Vector{Float64}
    end
    # Still stable! Both branches return same type.
end

# REALLY unstable function
function truly_unstable(x)
    if x > 0
        return x^2         # Float64
    else
        return "negative"  # String
    end
end

using InteractiveUtils
@code_warntype stable_forward(rand(3,3), rand(3), rand(3))
```

å‡ºåŠ›ï¼ˆå‹å®‰å®šï¼‰:
```julia
MethodInstance for stable_forward(::Matrix{Float64}, ::Vector{Float64}, ::Vector{Float64})
  from stable_forward(W, x, b) @ Main
Arguments
  #self#::Core.Const(stable_forward)
  W::Matrix{Float64}
  x::Vector{Float64}
  b::Vector{Float64}
Body::Vector{Float64}  # â† ã“ã“ãŒé‡è¦ã€‚å‡ºåŠ›å‹ãŒç¢ºå®šã—ã¦ã„ã‚‹
```

å‡ºåŠ›ï¼ˆå‹ä¸å®‰å®šï¼‰:
```julia
@code_warntype truly_unstable(1.0)

Body::Union{Float64, String}  # â† Union type = å‹ä¸å®‰å®š
```

**å‹ä¸å®‰å®šãªã‚³ãƒ¼ãƒ‰ã¯é…ã„ç†ç”±**: å®Ÿè¡Œæ™‚ã«æ¯å›å‹ãƒã‚§ãƒƒã‚¯ãŒå¿…è¦ã«ãªã‚Šã€JITãŒæœ€é©åŒ–ã§ããªã„ã€‚

#### 4.6.2 å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã®å®Ÿä¾‹ â€” VAEã®forward

```julia
# Define encoder for different input types
struct Encoder{E}
    net::E
end

# CPU version
function (enc::Encoder)(x::Matrix{Float32})
    println("CPU encoder called")
    return enc.net(x)
end

# GPU version (if CUDA.jl is loaded)
using CUDA

function (enc::Encoder)(x::CuMatrix{Float32})
    println("GPU encoder called")
    return enc.net(x)
end

# Usage
x_cpu = rand(Float32, 784, 128)
x_gpu = CuArray(x_cpu)

enc = Encoder(my_network)

enc(x_cpu)  # â†’ "CPU encoder called"
enc(x_gpu)  # â†’ "GPU encoder called"
```

**Pythonã¨ã®é•ã„**:
```python
# PyTorch requires manual device check
def forward(self, x):
    if x.is_cuda:
        # GPU path
        return self.net_gpu(x)
    else:
        # CPU path
        return self.net_cpu(x)
```

Juliaã§ã¯ã€å‹ï¼ˆ`Matrix` vs `CuMatrix`ï¼‰ãŒç•°ãªã‚Œã°ã€è‡ªå‹•ã§åˆ¥ã®é–¢æ•°ãŒå‘¼ã°ã‚Œã‚‹ã€‚**æ¡ä»¶åˆ†å²ãŒã‚¼ãƒ­ã€‚**

#### 4.6.3 Broadcast Fusionã®å¨åŠ› â€” ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹æœ€å°åŒ–

```julia
# Without fusion (3 separate loops)
function no_fusion(x)
    a = sin.(x)
    b = cos.(a)
    c = b .^ 2
    return c
end

# With fusion (1 loop)
function with_fusion(x)
    return @. (cos(sin(x)))^2
end

# Benchmark
using BenchmarkTools
x = rand(Float32, 10000)

@btime no_fusion($x)  # 45.2 Î¼s (4 allocations: 156.38 KiB)
@btime with_fusion($x) # 12.3 Î¼s (2 allocations: 78.19 KiB)
```

**3.7å€é€Ÿ + ãƒ¡ãƒ¢ãƒªåŠæ¸›ï¼** VAEã®æå¤±é–¢æ•°è¨ˆç®—ã§ã€ã“ã†ã„ã£ãŸèåˆãŒè‡ªå‹•ã§èµ·ãã¦ã„ã‚‹ã€‚

#### 4.6.4 JIT vs AOTã‚³ãƒ³ãƒ‘ã‚¤ãƒ« â€” Juliaã®2æ®µéšå®Ÿè¡Œ

```julia
function vae_loss_first_call(x)
    # First call: JIT compiles
    @time begin
        # ... VAE forward + loss computation
    end
end

function vae_loss_second_call(x)
    # Second call: uses cached machine code
    @time begin
        # ... same computation
    end
end

# First call: 0.234s (includes compilation)
# Second call: 0.012s (pure execution)
# Speedup: 19.5x after compilation
```

è¨“ç·´ãƒ«ãƒ¼ãƒ—ã§ã¯ã€æœ€åˆã®æ•°ãƒãƒƒãƒã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã•ã‚Œã€ãã®å¾Œã¯ãƒã‚¤ãƒ†ã‚£ãƒ–ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œã®ã¿ã€‚PyTorchã¯æ¯ãƒãƒƒãƒPythonã‚¤ãƒ³ã‚¿ãƒ—ãƒªã‚¿ã‚’ä»‹ã™ã‚‹ã€‚

### 4.7 3è¨€èªæ¯”è¼ƒ â€” Python vs Rust vs Julia

| é …ç›® | Python (PyTorch) | Rust (burn/candle) | Julia (Lux.jl) |
|:-----|:-----------------|:-------------------|:---------------|
| **è¨“ç·´é€Ÿåº¦** | 2.35s/epoch | æœªå®Ÿè£…ï¼ˆé›£æ˜“åº¦é«˜ï¼‰ | 0.29s/epoch (**8.2x**) |
| **ãƒ¡ãƒ¢ãƒªå®‰å…¨** | Runtime error | Compile-time guarantee | Runtime error (GC) |
| **æ•°å¼å¯¾å¿œ** | `torch.matmul(W, x)` | `tensor.matmul(&x)` | `W * x` (**1:1**) |
| **å‹ã‚·ã‚¹ãƒ†ãƒ ** | å‹•çš„å‹ï¼ˆé…ã„ï¼‰ | é™çš„å‹ï¼ˆé€Ÿã„ãŒè¤‡é›‘ï¼‰ | å‹•çš„å‹+JITï¼ˆé€Ÿãã¦ç°¡æ½”ï¼‰ |
| **CPU/GPUåˆ‡æ›¿** | `model.to(device)` | æ‰‹å‹•å®Ÿè£…å¿…è¦ | `CuArray(x)` 1è¡Œ |
| **å­¦ç¿’ã‚³ã‚¹ãƒˆ** | â˜…â˜†â˜†â˜†â˜† | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜†â˜†â˜† |
| **é©ç”¨é ˜åŸŸ** | ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ— | æ¨è«–ï¼ˆæœ¬ç•ªï¼‰ | ç ”ç©¶ãƒ»è¨“ç·´ãƒ»GPUè¨ˆç®— |
| **Compileæ™‚é–“** | ãªã—ï¼ˆå³åº§ã«å®Ÿè¡Œï¼‰ | æ•°åˆ†ï¼ˆå¤§è¦æ¨¡ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆï¼‰ | åˆå›ã®ã¿æ•°ç§’ |
| **ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ** | æœ€å¤§ï¼ˆPyPI 50ä¸‡+ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ï¼‰ | æˆé•·ä¸­ï¼ˆcrates.io 15ä¸‡+ï¼‰ | ç§‘å­¦è¨ˆç®—ç‰¹åŒ–ï¼ˆ1ä¸‡+ï¼‰ |
| **ãƒ‡ãƒãƒƒã‚°** | ç°¡å˜ï¼ˆREPLå³åº§ï¼‰ | é›£ã—ã„ï¼ˆå‹ã‚¨ãƒ©ãƒ¼ãŒè¤‡é›‘ï¼‰ | ç°¡å˜ï¼ˆREPL + Revise.jlï¼‰ |

**çµè«–**:
- **Python**: ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã¨å®Ÿé¨“ã«æœ€é©ã€‚æœ¬ç•ªã«ã¯é…ã„ã€‚
- **Rust**: æ¨è«–ãƒ»æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤ã«æœ€é©ã€‚è¨“ç·´ãƒ«ãƒ¼ãƒ—ã¯æ›¸ãã¥ã‚‰ã„ã€‚
- **Julia**: ç ”ç©¶ãƒ»è¨“ç·´ãƒ»GPUè¨ˆç®—ã«æœ€é©ã€‚æ•°å¼ãŒãã®ã¾ã¾ã‚³ãƒ¼ãƒ‰ã«ãªã‚‹ã€‚

**æœ¬ã‚·ãƒªãƒ¼ã‚ºã®æˆ¦ç•¥ï¼ˆç¬¬10å›ä»¥é™ï¼‰**:
- è¨“ç·´: Julia (Lux.jl)
- æ¨è«–ãƒ»æœ¬ç•ª: Rust (burn/candle)
- ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—: Python (æœ€å°é™)

### 4.8 Juliaé–‹ç™ºç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— â€” å®Œå…¨ã‚¬ã‚¤ãƒ‰

#### Step 1: Juliaã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# macOS (Homebrew)
brew install julia

# Linux (juliaup recommended)
curl -fsSL https://install.julialang.org | sh

# Windows (juliaup)
winget install julia -s msstore
```

#### Step 2: VSCode + Juliaæ‹¡å¼µæ©Ÿèƒ½

```bash
# Install VSCode Julia extension
code --install-extension julialang.language-julia
```

VSCodeã®è¨­å®šï¼ˆ`.vscode/settings.json`ï¼‰:
```json
{
    "julia.enableTelemetry": false,
    "julia.execution.resultType": "inline",
    "julia.execution.codeInREPL": true,
    "[julia]": {
        "editor.tabSize": 4
    }
}
```

#### Step 3: å¿…é ˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```julia
using Pkg

# Core packages
Pkg.add(["Revise", "OhMyREPL", "BenchmarkTools"])

# ML packages
Pkg.add(["Lux", "Optimisers", "Zygote", "MLDatasets", "CUDA"])

# Visualization
Pkg.add(["Plots", "StatsPlots", "Images"])
```

#### Step 4: startup.jl ã®è¨­å®š

`~/.julia/config/startup.jl` ã«è¿½è¨˜:
```julia
try
    using Revise
catch e
    @warn "Revise.jl not available"
end

try
    using OhMyREPL
catch e
    @warn "OhMyREPL not available"
end

# Custom aliases
const âˆ‡ = gradient  # Type: \nabla<TAB>
```

ã“ã‚Œã§ã€Juliaèµ·å‹•æ™‚ã«è‡ªå‹•ã§Revise.jlãŒæœ‰åŠ¹ã«ãªã‚‹ã€‚

:::message
**é€²æ—: 70% å®Œäº†** JuliaãŒè¨“ç·´ãƒ«ãƒ¼ãƒ—ã§8.2å€é€Ÿã‚’é”æˆã™ã‚‹æ§˜ã‚’ç›®æ’ƒã—ãŸã€‚Pythonã«æˆ»ã‚Œãªã„ç†ç”±ãŒæ˜ç¢ºã«ãªã£ãŸã€‚Zone 5ã§å®Ÿé¨“ã«é€²ã‚€ã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” æ½œåœ¨ç©ºé–“ã‚’å¯è¦–åŒ–ã—ã€æ“ä½œã™ã‚‹

### 5.1 ã‚·ãƒ³ãƒœãƒ«èª­è§£ãƒ†ã‚¹ãƒˆ â€” è«–æ–‡ã®æ•°å¼ã‚’æ­£ç¢ºã«èª­ã‚€

VAEè«–æ–‡ã«é »å‡ºã™ã‚‹è¨˜å·ã‚’æ­£ç¢ºã«èª­ã‚ã‚‹ã‹ã€è‡ªå·±è¨ºæ–­ã—ã‚ˆã†ã€‚

:::details Q1: $\mathbb{E}_{q_\phi(z \mid x)}[\log p_\theta(x \mid z)]$ ã®èª­ã¿æ–¹ã¨æ„å‘³

**èª­ã¿æ–¹**: ã€Œã‚¤ãƒ¼ ã‚µãƒ– ã‚­ãƒ¥ãƒ¼ãƒ•ã‚¡ã‚¤ï¼ˆã‚¼ãƒƒãƒˆ ã‚®ãƒ–ãƒ³ ã‚¨ãƒƒã‚¯ã‚¹ï¼‰ã‚ªãƒ– ãƒ­ã‚° ãƒ”ãƒ¼ã‚·ãƒ¼ã‚¿ï¼ˆã‚¨ãƒƒã‚¯ã‚¹ ã‚®ãƒ–ãƒ³ ã‚¼ãƒƒãƒˆï¼‰ã€

**æ„å‘³**: å¤‰åˆ†åˆ†å¸ƒ $q_\phi(z \mid x)$ ã®ä¸‹ã§ã®ã€ãƒ‡ã‚³ãƒ¼ãƒ€ã®å¯¾æ•°å°¤åº¦ã®æœŸå¾…å€¤ã€‚VAEã®å†æ§‹æˆé …ã€‚

**æ—¥æœ¬èªè¨³**: ã€Œã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãŒå‡ºåŠ›ã™ã‚‹æ½œåœ¨å¤‰æ•° $z$ ã®åˆ†å¸ƒã§å¹³å‡ã‚’å–ã£ãŸã¨ãã®ã€ãƒ‡ã‚³ãƒ¼ãƒ€ãŒ $x$ ã‚’å¾©å…ƒã™ã‚‹ç¢ºç‡ã®å¯¾æ•°ã€

[^1] Kingma & Welling (2013), Equation 2
:::

:::details Q2: $D_\text{KL}(q_\phi(z \mid x) \| p(z))$ ã®éå¯¾ç§°æ€§

**å•**: ãªãœ $D_\text{KL}(p \| q) \neq D_\text{KL}(q \| p)$ ãªã®ã‹ï¼Ÿ

**ç­”**: KLç™ºæ•£ã¯éå¯¾ç§°ãªè·é›¢å°ºåº¦ã€‚$D_\text{KL}(q \| p)$ ã‚’æœ€å°åŒ–ã™ã‚‹ã¨ã€$q$ ãŒ $p$ ã®é«˜ç¢ºç‡é ˜åŸŸã«é›†ä¸­ã™ã‚‹ï¼ˆmode-seekingï¼‰ã€‚$D_\text{KL}(p \| q)$ ã§ã¯ã€$q$ ãŒ $p$ ã®å…¨é ˜åŸŸã‚’ã‚«ãƒãƒ¼ã™ã‚‹ï¼ˆmoment-matchingï¼‰ã€‚

VAEã§ã¯ $D_\text{KL}(q \| p)$ ã‚’ä½¿ã†ç†ç”±: äº‹å‰åˆ†å¸ƒ $p(z) = \mathcal{N}(0, I)$ ã«è¿‘ã¥ã‘ãŸã„ã®ã¯ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ› $q_\phi(z \mid x)$ ã ã‹ã‚‰ã€‚

å‚è€ƒ: [ç¬¬6å›ã§å°å‡º](./ml-lecture-06.md)
:::

:::details Q3: $z = \mu + \sigma \odot \epsilon$ ã® $\odot$ ã¯ä½•ã‹ï¼Ÿ

**è¨˜å·**: $\odot$ ã¯è¦ç´ ã”ã¨ã®ç© (element-wise product, Hadamard product)

**æ•°å¼**: $z_i = \mu_i + \sigma_i \epsilon_i$ for $i = 1, \ldots, d$

**å®Ÿè£…**:
```julia
z = Î¼ .+ Ïƒ .* Îµ  # Julia
z = mu + sigma * eps  # PyTorch (broadcast is implicit)
```

Reparameterization Trick ã®æ ¸å¿ƒéƒ¨åˆ†ã€‚[^1]
:::

:::details Q4: $\sigma = \exp(0.5 \log \sigma^2)$ ã®æ„å›³

**å•**: ãªãœç›´æ¥ $\sigma$ ã‚’å‡ºåŠ›ã›ãšã€$\log \sigma^2$ ã‚’å‡ºåŠ›ã™ã‚‹ã®ã‹ï¼Ÿ

**ç­”**:
1. $\sigma > 0$ ã®åˆ¶ç´„ã‚’è‡ªå‹•ã§æº€ãŸã™ï¼ˆæŒ‡æ•°é–¢æ•°ã¯å¸¸ã«æ­£ï¼‰
2. æ•°å€¤å®‰å®šæ€§: $\sigma \to 0$ ã®ã¨ãã€$\log \sigma^2 \to -\infty$ ã§å‹¾é…ãŒæ®‹ã‚‹
3. KLç™ºæ•£ã®è¨ˆç®—ã§ $\log \sigma^2$ ãŒç›´æ¥ä½¿ã‚ã‚Œã‚‹

Zone 3.3ã§å°å‡ºã—ãŸé€šã‚Šã€ã‚¬ã‚¦ã‚¹KLã¯:
$$
D_\text{KL} = \frac{1}{2} \sum (\mu^2 + \sigma^2 - \log \sigma^2 - 1)
$$
$\log \sigma^2$ ã‚’ç›´æ¥ä½¿ãˆã°ã€`exp` ã¨ `log` ãŒç›¸æ®ºã•ã‚Œã‚‹ã€‚
:::

:::details Q5: $p_\theta(x \mid z)$ ãŒBernoulliåˆ†å¸ƒã®ã¨ãã€å†æ§‹æˆé …ã¯ä½•ã‹ï¼Ÿ

**ç­”**: Binary Cross-Entropy (BCE)

$$
-\log p_\theta(x \mid z) = -\sum_{i=1}^{784} [x_i \log \hat{x}_i + (1 - x_i) \log(1 - \hat{x}_i)]
$$

ã“ã“ã§ $\hat{x} = \text{Decoder}_\theta(z)$ ã¯ã€å„ãƒ”ã‚¯ã‚»ãƒ«ãŒ1ã§ã‚ã‚‹ç¢ºç‡ã€‚

Gaussianä»®å®šã®å ´åˆï¼ˆé€£ç¶šå€¤ç”»åƒï¼‰:
$$
-\log p_\theta(x \mid z) = \frac{1}{2\sigma^2} \|x - \hat{x}\|^2 + \text{const}
$$
ã“ã‚Œã¯MSE (Mean Squared Error) ã«å¯¾å¿œã€‚
:::

### 5.2 ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ†ã‚¹ãƒˆ â€” æ•°å¼ã‹ã‚‰ã‚³ãƒ¼ãƒ‰ã¸

:::details Q6: ä»¥ä¸‹ã®æ•°å¼ã‚’Juliaã§å®Ÿè£…ã›ã‚ˆ

æ•°å¼:
$$
\mathcal{L}(\theta, \phi; x) = \mathbb{E}_{q_\phi(z \mid x)}[\log p_\theta(x \mid z)] - D_\text{KL}(q_\phi(z \mid x) \| p(z))
$$

ãŸã ã—:
- $z = \mu_\phi(x) + \sigma_\phi(x) \odot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)$
- $p_\theta(x \mid z) = \mathcal{N}(x \mid \mu_\theta(z), I)$

**ç­”**:
```julia
function vae_elbo(encoder, decoder, ps_enc, ps_dec, st_enc, st_dec, x)
    # Encode: q_Ï†(z|x)
    (Î¼, logÏƒÂ²), st_enc = encoder(x, ps_enc, st_enc)

    # Reparameterize: z = Î¼ + ÏƒÂ·Îµ
    Ïƒ = exp.(0.5 .* logÏƒÂ²)
    Îµ = randn(Float32, size(Î¼)...)
    z = Î¼ .+ Ïƒ .* Îµ

    # Decode: p_Î¸(x|z)
    x_recon, st_dec = decoder(z, ps_dec, st_dec)

    # Reconstruction term: E_q[log p(x|z)] â‰ˆ -MSE (Gaussian assumption)
    recon_term = -0.5f0 * sum((x .- x_recon).^2)

    # KL term: D_KL(q||p) (closed-form for Gaussian)
    kl_term = -0.5f0 * sum(1 .+ logÏƒÂ² .- Î¼.^2 .- exp.(logÏƒÂ²))

    elbo = recon_term - kl_term  # ELBO (to maximize)
    loss = -elbo                  # Loss (to minimize)

    return loss, st_enc, st_dec
end
```

ãƒã‚¤ãƒ³ãƒˆ:
- `sum()` ãŒæœŸå¾…å€¤ã® Monte Carlo è¿‘ä¼¼ï¼ˆ1ã‚µãƒ³ãƒ—ãƒ«ï¼‰
- ELBO ã¯æœ€å¤§åŒ–ã—ãŸã„ãŒã€æå¤±é–¢æ•°ã¯æœ€å°åŒ–ã™ã‚‹ã®ã§ç¬¦å·åè»¢
:::

:::details Q7: Straight-Through Estimator (STE) ã‚’Juliaã§å®Ÿè£…

æ•°å¼:
$$
\text{Forward:} \quad z_q = \text{quantize}(z_e) \\
\text{Backward:} \quad \frac{\partial L}{\partial z_e} = \frac{\partial L}{\partial z_q}
$$

**ç­”**:
```julia
using ChainRulesCore

function straight_through_quantize(z_e, codebook)
    # Forward: find nearest codebook entry
    distances = sum((z_e .- codebook).^2, dims=1)
    indices = argmin(distances, dims=1)
    z_q = codebook[:, indices]

    # Straight-through: gradient flows as if z_q = z_e
    return z_e + (z_q - z_e)  # This is a no-op in forward, but gradient flows through z_e
end

# Custom gradient rule (Zygote.jl)
function ChainRulesCore.rrule(::typeof(straight_through_quantize), z_e, codebook)
    z_q = straight_through_quantize(z_e, codebook)

    function pullback(Î”z_q)
        # Gradient w.r.t. z_e: âˆ‚L/âˆ‚z_e = âˆ‚L/âˆ‚z_q
        return NoTangent(), Î”z_q, NoTangent()
    end

    return z_q, pullback
end
```

VQ-VAE [^3] ã§ä½¿ã‚ã‚Œã‚‹ã€é›¢æ•£åŒ–ã®å‹¾é…è¿‘ä¼¼ã€‚
:::

### 5.3 æ½œåœ¨ç©ºé–“ã®å¯è¦–åŒ– â€” 2æ¬¡å…ƒæ½œåœ¨ç©ºé–“ã®æ§‹é€ 

```julia
using Lux, MLDatasets, Plots

# Train a 2D VAE (from Zone 4)
latent_dim = 2
encoder = create_encoder(784, 400, latent_dim)
decoder = create_decoder(latent_dim, 400, 784)
# ... (training code omitted)

# Encode test data
test_data = MLDatasets.MNIST(split=:test)
test_x = reshape(test_data.features, 784, :) |> x -> Float32.(x)
test_y = test_data.targets

# Get latent codes
(Î¼, logÏƒÂ²), _ = encoder(test_x, ps_enc, st_enc)
z = Î¼  # Use mean (no sampling for visualization)

# Scatter plot colored by digit label
scatter(z[1, :], z[2, :], group=test_y, markersize=2, alpha=0.5,
        xlabel="zâ‚", ylabel="zâ‚‚", title="VAE Latent Space (MNIST)",
        legend=:outertopright)
savefig("vae_latent_space.png")
```

æœŸå¾…ã•ã‚Œã‚‹çµæœ:
- åŒã˜æ•°å­—ãŒæ½œåœ¨ç©ºé–“ã§è¿‘ãã«é›†ã¾ã‚‹ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼‰
- æ•°å­—é–“ã®é·ç§»ãŒæ»‘ã‚‰ã‹ï¼ˆä¾‹: 3ã¨8ãŒéš£æ¥ï¼‰

### 5.4 æ½œåœ¨ç©ºé–“ã®è£œé–“ â€” 0ã‹ã‚‰9ã¸ã®å¤‰å½¢

```julia
# Find latent codes for digit "0" and "9"
idx_0 = findfirst(test_y .== 0)
idx_9 = findfirst(test_y .== 9)

z_0 = Î¼[:, idx_0]
z_9 = Î¼[:, idx_9]

# Linear interpolation
n_steps = 10
alphas = range(0, 1, length=n_steps)
z_interp = hcat([Î± * z_9 + (1 - Î±) * z_0 for Î± in alphas]...)

# Decode
x_interp, _ = decoder(z_interp, ps_dec, st_dec)

# Visualize
using Images
imgs = [Gray.(reshape(x_interp[:, i], 28, 28)) for i in 1:n_steps]
mosaicview(imgs, nrow=1, npad=2)
```

å‡ºåŠ›: 0 â†’ (ä¸­é–“å½¢çŠ¶) â†’ 9 ã¸ã®æ»‘ã‚‰ã‹ãªå¤‰å½¢

### 5.5 å±æ€§æ“ä½œ â€” ã€Œç¬‘é¡”ãƒ™ã‚¯ãƒˆãƒ«ã€ã‚’è¦‹ã¤ã‘ã‚‹

CelebAï¼ˆé¡”ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼‰ã§è¨“ç·´ã—ãŸVAEãªã‚‰ã€æ½œåœ¨ç©ºé–“ã§ **å±æ€§ãƒ™ã‚¯ãƒˆãƒ«** ã‚’å®šç¾©ã§ãã‚‹ [^2]ã€‚

```julia
# Pseudo-code (requires CelebA dataset + attribute labels)
# Find "smiling" direction in latent space

# 1. Encode smiling and non-smiling faces
z_smiling = mean(encode(x_smiling), dims=2)
z_neutral = mean(encode(x_neutral), dims=2)

# 2. Compute "smile vector"
v_smile = z_smiling - z_neutral

# 3. Apply to any face
z_input = encode(x_input)
z_more_smile = z_input + 0.5 * v_smile  # increase smile
x_output = decode(z_more_smile)
```

ã“ã®ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã¯ã€StyleGANã®latent space manipulationã®åŸå‹ã€‚

### 5.6 Posterior Collapseå®Ÿé¨“ â€” ãªãœèµ·ãã‚‹ã®ã‹

**Posterior Collapse** ã¯ã€VAEã®æœ€å¤§ã®è½ã¨ã—ç©´ã ã€‚ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãŒæ½œåœ¨å¤‰æ•° $z$ ã‚’ç„¡è¦–ã—ã€ãƒ‡ã‚³ãƒ¼ãƒ€ãŒå¹³å‡çš„ãªç”»åƒã‚’å‡ºåŠ›ã—ã¦ã—ã¾ã†ç¾è±¡ã€‚

#### 5.6.1 Collapseã®æ¤œå‡ºæ–¹æ³•

```python
def detect_posterior_collapse(model, train_loader):
    """Detect posterior collapse by monitoring KL divergence per dimension."""
    total_kl_per_dim = 0
    num_batches = 0

    for x_batch, _ in train_loader:
        mu, logvar = model.encode(x_batch)
        # KL per dimension: 0.5 * (Î¼Â² + ÏƒÂ² - log(ÏƒÂ²) - 1)
        kl_per_dim = 0.5 * (mu.pow(2) + logvar.exp() - logvar - 1)
        total_kl_per_dim += kl_per_dim.mean(dim=0).detach()
        num_batches += 1

    avg_kl_per_dim = total_kl_per_dim / num_batches

    # Collapseåˆ¤å®š: KL < 0.01 ã®æ¬¡å…ƒãŒå¤šã„
    collapsed_dims = (avg_kl_per_dim < 0.01).sum().item()
    active_dims = (avg_kl_per_dim >= 0.01).sum().item()

    print(f"Active dimensions: {active_dims} / {len(avg_kl_per_dim)}")
    print(f"Collapsed dimensions: {collapsed_dims}")
    print(f"KL per dimension: {avg_kl_per_dim[:10]}")  # first 10

    return avg_kl_per_dim

# Run detection
kl_per_dim = detect_posterior_collapse(model, train_loader)

# Visualize
import matplotlib.pyplot as plt
plt.bar(range(len(kl_per_dim)), kl_per_dim.cpu().numpy())
plt.xlabel("Latent Dimension")
plt.ylabel("KL Divergence")
plt.title("Posterior Collapse Detection")
plt.axhline(y=0.01, color='r', linestyle='--', label='Collapse threshold')
plt.legend()
plt.savefig("posterior_collapse.png")
```

æœŸå¾…ã•ã‚Œã‚‹çµæœ:
- **å¥å…¨ãªVAE**: ã»ã¨ã‚“ã©ã®æ¬¡å…ƒã§KL > 0.1
- **Collapsed VAE**: å¤šãã®æ¬¡å…ƒã§KL â‰ˆ 0ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãŒç„¡è¦–ã•ã‚Œã¦ã„ã‚‹ï¼‰

#### 5.6.2 Collapseå¯¾ç­–: KL Annealing

KLé …ã®é‡ã¿ã‚’ã€è¨“ç·´åˆæœŸã¯å°ã•ãã€å¾ã€…ã«å¢—ã‚„ã™ã€‚

```python
def kl_annealing_schedule(epoch, total_epochs, strategy='linear'):
    """KL annealing schedule to prevent posterior collapse."""
    if strategy == 'linear':
        return min(1.0, epoch / (total_epochs * 0.5))
    elif strategy == 'sigmoid':
        k = 0.1  # steepness
        x0 = total_epochs * 0.5  # midpoint
        return 1 / (1 + np.exp(-k * (epoch - x0)))
    elif strategy == 'cyclical':
        # Cyclical annealing (4 cycles)
        period = total_epochs / 4
        return (epoch % period) / period
    else:
        return 1.0

def train_with_annealing(model, train_loader, optimizer, epochs):
    for epoch in range(epochs):
        beta = kl_annealing_schedule(epoch, epochs, strategy='linear')

        for x_batch, _ in train_loader:
            optimizer.zero_grad()
            recon, mu, logvar = model(x_batch)

            # Annealed loss
            recon_loss = F.binary_cross_entropy(recon, x_batch.view(-1, 784), reduction='sum')
            kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
            loss = recon_loss + beta * kl_loss  # Î² starts from 0, increases to 1

            loss.backward()
            optimizer.step()

        if epoch % 10 == 0:
            print(f"Epoch {epoch}: Î²={beta:.3f}, Loss={loss.item():.2f}")
```

**æˆ¦ç•¥ã®æ¯”è¼ƒ**:

| æˆ¦ç•¥ | ç‰¹å¾´ | åˆ©ç‚¹ | æ¬ ç‚¹ |
|:-----|:-----|:-----|:-----|
| Linear | $\beta(t) = \min(1, t / T)$ | å®Ÿè£…ç°¡å˜ | ä¸­ç›¤ã§æ€¥æ¿€ã«å¤‰åŒ– |
| Sigmoid | $\beta(t) = 1/(1 + e^{-k(t - t_0)})$ | æ»‘ã‚‰ã‹ | ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´å¿…è¦ |
| Cyclical | $\beta(t) = (t \mod P) / P$ | Collapseã‹ã‚‰å›å¾©å¯èƒ½ | è¨“ç·´ãŒä¸å®‰å®š |

#### 5.6.3 Free Bits â€” æ¬¡å…ƒã”ã¨ã®æœ€å°KLä¿è¨¼

å„æ½œåœ¨æ¬¡å…ƒã«ã€æœ€å°KLå€¤ã‚’ä¿è¨¼ã™ã‚‹ [^7]ã€‚

```python
def free_bits_loss(recon_x, x, mu, logvar, free_bits=0.5):
    """VAE loss with free bits constraint.

    Ensures each latent dimension has KL â‰¥ free_bits (e.g., 0.5 nats).
    """
    recon_loss = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')

    # KL per dimension (batch averaged)
    kl_per_dim = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=0)  # (latent_dim,)

    # Apply free bits: max(KL_i, free_bits)
    kl_per_dim_clamped = torch.clamp(kl_per_dim, min=free_bits)

    total_kl = kl_per_dim_clamped.sum()

    return recon_loss + total_kl

# Training with free bits
optimizer = optim.Adam(model.parameters(), lr=1e-3)
for epoch in range(10):
    for x_batch, _ in train_loader:
        optimizer.zero_grad()
        recon, mu, logvar = model(x_batch)
        loss = free_bits_loss(recon, x_batch, mu, logvar, free_bits=0.5)
        loss.backward()
        optimizer.step()
```

**åŠ¹æœ**: å„æ¬¡å…ƒãŒæœ€ä½0.5 natsã®æƒ…å ±ã‚’ä¿æŒã™ã‚‹ã“ã¨ã‚’ä¿è¨¼ã€‚Collapseã‚’é˜²ãã€‚

### 5.7 ãƒŸãƒ‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: Tiny VAE on MNIST (300K params)

å®Œå…¨ã«å‹•ä½œã™ã‚‹ã€è»½é‡VAEã‚’å®Ÿè£…ã—ã‚ˆã†ã€‚ç›®æ¨™:
- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 300Kä»¥ä¸‹
- è¨“ç·´æ™‚é–“: CPU 5åˆ†ä»¥å†…
- å†æ§‹æˆç²¾åº¦: ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§BCE < 120

```julia
# Julia implementation (Lux.jl)
using Lux, Optimisers, Zygote, MLDatasets, Random, Statistics

# Tiny VAE architecture
function create_tiny_vae(; input_dim=784, hidden_dim=256, latent_dim=10)
    encoder = Chain(
        Dense(input_dim => hidden_dim, relu),
        Parallel(tuple,
                 Dense(hidden_dim => latent_dim),       # Î¼
                 Dense(hidden_dim => latent_dim))       # log ÏƒÂ²
    )

    decoder = Chain(
        Dense(latent_dim => hidden_dim, relu),
        Dense(hidden_dim => input_dim, sigmoid)
    )

    return encoder, decoder
end

# Training function
function train_tiny_vae(; epochs=10, batch_size=128, lr=1e-3)
    rng = Random.default_rng()

    # Create models
    encoder, decoder = create_tiny_vae(hidden_dim=256, latent_dim=10)
    ps_enc, st_enc = Lux.setup(rng, encoder)
    ps_dec, st_dec = Lux.setup(rng, decoder)

    # Count parameters
    n_params = sum(length, Lux.parameterlength.([ps_enc, ps_dec]))
    println("Total parameters: $(n_params)")

    # Optimizer
    opt_enc = Optimisers.setup(Optimisers.Adam(lr), ps_enc)
    opt_dec = Optimisers.setup(Optimisers.Adam(lr), ps_dec)

    # Load MNIST
    train_data = MLDatasets.MNIST(split=:train)
    train_x = Float32.(reshape(train_data.features, 784, :))

    # Training loop
    for epoch in 1:epochs
        total_loss = 0.0f0
        num_batches = 0

        for i in 1:batch_size:size(train_x, 2)-batch_size
            x_batch = train_x[:, i:i+batch_size-1]

            # Compute gradients
            (loss, (st_enc, st_dec)), grads = Zygote.withgradient(ps_enc, ps_dec) do p_enc, p_dec
                # Encode
                (Î¼, logÏƒÂ²), st_enc_new = encoder(x_batch, p_enc, st_enc)

                # Reparameterize
                Ïƒ = exp.(0.5f0 .* logÏƒÂ²)
                Îµ = randn(Float32, size(Î¼)...)
                z = Î¼ .+ Ïƒ .* Îµ

                # Decode
                x_recon, st_dec_new = decoder(z, p_dec, st_dec)

                # Loss
                bce = -sum(x_batch .* log.(x_recon .+ 1f-8) .+ (1 .- x_batch) .* log.(1 .- x_recon .+ 1f-8))
                kld = -0.5f0 * sum(1 .+ logÏƒÂ² .- Î¼.^2 .- exp.(logÏƒÂ²))
                loss = bce + kld

                return loss, (st_enc_new, st_dec_new)
            end

            # Update
            Optimisers.update!(opt_enc, ps_enc, grads[1])
            Optimisers.update!(opt_dec, ps_dec, grads[2])

            total_loss += loss
            num_batches += 1
        end

        avg_loss = total_loss / (num_batches * batch_size)
        println("Epoch $epoch: Loss = $(avg_loss)")
    end

    return encoder, decoder, ps_enc, ps_dec, st_enc, st_dec
end

# Run training
@time encoder, decoder, ps_enc, ps_dec, st_enc, st_dec = train_tiny_vae(epochs=10)
```

æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
```
Total parameters: 291,594
Epoch 1: Loss = 152.34
Epoch 2: Loss = 118.56
...
Epoch 10: Loss = 104.23
245.123456 seconds (CPU time)
```

**ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ**:
- [ ] ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° < 300K
- [ ] è¨“ç·´æ™‚é–“ < 5åˆ†ï¼ˆCPUï¼‰
- [ ] æœ€çµ‚Loss < 110

### 5.8 Paper Reading Test â€” VAEè«–æ–‡ã®é‡è¦å›³ã‚’èª­ã‚€

Kingma & Welling (2013) [^1] ã® Figure 1 ã‚’å®Œå…¨ã«ç†è§£ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã‚ˆã†ã€‚

:::details Q8: Figure 1 ã® Graphical Model ã‚’èª¬æ˜ã›ã‚ˆ

**å•**: è«–æ–‡ã®Figure 1ã«æã‹ã‚Œã¦ã„ã‚‹Graphical Modelã®æ„å‘³ã‚’ã€ç¢ºç‡çš„ä¾å­˜é–¢ä¿‚ã¨ã¨ã‚‚ã«èª¬æ˜ã›ã‚ˆã€‚

**ç­”**:

```
    zâ‚ ----> xâ‚
    â†‘         â†‘
    |         |
   Î¸,Ï†      Î¸,Ï†
    |         |
    â†“         â†“
    zâ‚‚ ----> xâ‚‚
    â‹®         â‹®
    zâ‚™ ----> xâ‚™
```

- $z_i \sim p(z)$: äº‹å‰åˆ†å¸ƒï¼ˆæ¨™æº–æ­£è¦åˆ†å¸ƒï¼‰
- $x_i \mid z_i \sim p_\theta(x \mid z)$: ãƒ‡ã‚³ãƒ¼ãƒ€ï¼ˆç”Ÿæˆéç¨‹ï¼‰
- $q_\phi(z \mid x)$: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ï¼ˆå¤‰åˆ†åˆ†å¸ƒã€å›³ã«ã¯çœç•¥ï¼‰

VAEã¯ã€ã“ã®graphical modelã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã‚’æœ€å°¤æ¨å®šã—ã€åŒæ™‚ã«è¿‘ä¼¼äº‹å¾Œåˆ†å¸ƒ $q_\phi(z \mid x)$ ã‚’å­¦ç¿’ã™ã‚‹ã€‚

Plate notation ã§ $N$ å€‹ã®ãƒ‡ãƒ¼ã‚¿ç‚¹ãŒç‹¬ç«‹ã«ç”Ÿæˆã•ã‚Œã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
:::

:::message
**é€²æ—: 85% å®Œäº†** ã‚·ãƒ³ãƒœãƒ«èª­è§£ã€ã‚³ãƒ¼ãƒ‰ç¿»è¨³ã€æ½œåœ¨ç©ºé–“ã®å¯è¦–åŒ–ãƒ»è£œé–“ãƒ»å±æ€§æ“ä½œã€Posterior Collapseå®Ÿé¨“ã€ãƒŸãƒ‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€è«–æ–‡å›³èª­è§£ã‚’å®Œèµ°ã—ãŸã€‚Zone 6ã§æœ€æ–°ç ”ç©¶ã®å…¨ä½“åƒã‚’æŠŠæ¡ã™ã‚‹ã€‚
:::

---

## ğŸš€ 6. æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨æ¬¡å›äºˆå‘Š

### 6.1 FSQ (Finite Scalar Quantization) â€” VQ-VAEã®ç°¡ç´ ç‰ˆ

VQ-VAEã®èª²é¡Œ:
- **Codebook Collapse**: ä¸€éƒ¨ã®ã‚³ãƒ¼ãƒ‰ã ã‘ãŒä½¿ã‚ã‚Œã€æ®‹ã‚ŠãŒæ­»ã¬
- **è¤‡é›‘ãªè¨“ç·´**: Commitment Loss, EMAæ›´æ–°, Codebookå†åˆæœŸåŒ–

FSQ [^4] ã¯ã“ã‚Œã‚’æ ¹æœ¬ã‹ã‚‰è§£æ±º:

**Key Idea**: ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã‚’å­¦ç¿’ã›ãšã€**å›ºå®šã‚°ãƒªãƒƒãƒ‰**ã«é‡å­åŒ–ã™ã‚‹ã€‚

$$
z_i \in \{-1, 0, 1\}, \quad \text{for } i = 1, \ldots, d
$$

ä¾‹: $d=8$ æ¬¡å…ƒã€å„æ¬¡å…ƒãŒ $\{-1, 0, 1\}$ â†’ ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ ã‚µã‚¤ã‚º = $3^8 = 6561$

```julia
function fsq_quantize(z::AbstractArray, levels::Vector{Int})
    """Finite Scalar Quantization.

    z: continuous latent codes (d, N)
    levels: quantization levels per dimension (e.g., [3, 3, 3, 3, 3, 3, 3, 3])
    """
    d, N = size(z)
    z_q = similar(z)

    for i in 1:d
        # Map continuous values to discrete grid
        L = levels[i]
        grid = range(-1, 1, length=L)
        z_q[i, :] = [grid[argmin(abs.(z[i, j] .- grid))] for j in 1:N]
    end

    # Straight-through estimator
    return z + (z_q - z)  # gradient flows through z
end
```

**åˆ©ç‚¹**:
- Codebook Collapse ãŒåŸç†çš„ã«èµ·ããªã„ï¼ˆå…¨ã‚°ãƒªãƒƒãƒ‰ç‚¹ãŒå®šç¾©æ¸ˆã¿ï¼‰
- è¨“ç·´ãŒå˜ç´”ï¼ˆEMAä¸è¦ã€Commitment Lossä¸è¦ï¼‰
- VQ-VAEã¨åŒç­‰ã®æ€§èƒ½

### 6.2 Cosmos Tokenizer â€” ç”»åƒã¨å‹•ç”»ã®çµ±ä¸€è¡¨ç¾

NVIDIA Cosmos Tokenizer [^5] ã¯ã€2024å¹´ã®æœ€æ–°ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã ã€‚

**ç‰¹å¾´**:
- ç”»åƒ (256Ã—256) ã¨å‹•ç”» (16ãƒ•ãƒ¬ãƒ¼ãƒ ) ã‚’åŒã˜æ½œåœ¨ç©ºé–“ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
- ç©ºé–“åœ§ç¸®ç‡: 8Ã—8ã€æ™‚é–“åœ§ç¸®ç‡: 4
- é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³: 16,384èªå½™
- Diffusion Transformer (DiT) ã¨ã®ä½µç”¨ã‚’æƒ³å®š

```
Image (256Ã—256Ã—3) â†’ Encoder â†’ (32Ã—32Ã—C) â†’ FSQ/VQ â†’ Discrete tokens (32Ã—32)
Video (256Ã—256Ã—16Ã—3) â†’ Encoder â†’ (32Ã—32Ã—4Ã—C) â†’ FSQ/VQ â†’ Discrete tokens (32Ã—32Ã—4)
```

å¿œç”¨:
- å‹•ç”»ç”ŸæˆAIï¼ˆSora-likeãƒ¢ãƒ‡ãƒ«ï¼‰ã®å‰æ®µ
- ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLMï¼ˆç”»åƒãƒ»å‹•ç”»ç†è§£ï¼‰ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼

### 6.3 ç ”ç©¶ã®æœ€å‰ç·š â€” 2025-2026è«–æ–‡ãƒªã‚¹ãƒˆ

| è«–æ–‡ | è‘—è€… | å¹´ | æ ¸å¿ƒè²¢çŒ® | arXiv |
|:-----|:-----|:---|:--------|:------|
| CAR-Flow | - | 2025/09 | æ¡ä»¶ä»˜ãå†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ– | 2509.19300 |
| DVAE | - | 2025 | äºŒçµŒè·¯ã§Posterior Collapseé˜²æ­¢ | æ¤œç´¢è¦ |
| é€†Lipschitzåˆ¶ç´„VAE | - | 2023 | Decoderåˆ¶ç´„ã§ç†è«–ä¿è¨¼ | 2304.12770 |
| GQ-VAE | - | 2025/12 | å¯å¤‰é•·é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³ | 2512.21913 |
| MGVQ | - | 2025/07 | Multi-groupé‡å­åŒ– | 2507.07997 |
| TiTok v2 | - | 2025 | 1Dç”»åƒãƒˆãƒ¼ã‚¯ãƒ³åŒ– | æ¤œç´¢è¦ |
| Open-MAGVIT3 | - | 2025 | MAGVIT-v2å¾Œç¶™ | æ¤œç´¢è¦ |

#### 6.3.1 CAR-Flow â€” æ¡ä»¶ä»˜ãå†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã®é©æ–°

**å•é¡Œ**: æ¨™æº–çš„ãªReparameterization Trickã¯ã€å…¨ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ$\mu$ã¨$\sigma$ï¼‰ã«å‹¾é…ã‚’æµã™ã€‚ã—ã‹ã—ã€å ´åˆã«ã‚ˆã£ã¦ã¯$\mu$ã®ã¿æ›´æ–°ã—ãŸã„ï¼ˆä¾‹: ã‚¹ã‚±ãƒ¼ãƒ«å›ºå®šï¼‰ã€‚

**CAR-Flow (Conditional Affine Reparameterization)**:

$$
z = \mu_\phi(x) + \sigma_\text{fixed} \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)
$$

$\sigma$ã‚’å›ºå®šã™ã‚‹ã“ã¨ã§:
- æ½œåœ¨ç©ºé–“ã®ã‚¹ã‚±ãƒ¼ãƒ«ãŒå®‰å®š
- è¨“ç·´ãŒé«˜é€ŸåŒ–ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠæ¸›ï¼‰
- Flowãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ã®æ¥ç¶šãŒæ˜ç¢ºã«

å¿œç”¨: Latent Diffusion Modelã®VAEã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã§ã€ã‚¹ã‚±ãƒ¼ãƒ«å›ºå®šãŒæœ‰åŠ¹ã€‚

#### 6.4.2 DVAE â€” äºŒçµŒè·¯ã§Posterior Collapseé˜²æ­¢

**ã‚¢ã‚¤ãƒ‡ã‚¢**: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã«2ã¤ã®çµŒè·¯ã‚’ç”¨æ„:
- çµŒè·¯A: ç›´æ¥çš„ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆå¾“æ¥é€šã‚Šï¼‰
- çµŒè·¯B: ãƒã‚¹ã‚¯ã‚’ä»‹ã—ãŸã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆãƒã‚¤ã‚ºã«å¼·ã„ï¼‰

è¨“ç·´åˆæœŸã¯ä¸¡æ–¹ã‚’ä½¿ã„ã€å¾ŒæœŸã¯çµŒè·¯Aã®ã¿ã€‚ã“ã‚Œã§ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãŒæ—©æœŸã«Collapseã™ã‚‹ã®ã‚’é˜²ãã€‚

```python
def dual_path_encoder(x, training=True):
    # Path A: direct encoding
    mu_a, logvar_a = encoder_a(x)

    if training:
        # Path B: masked encoding
        x_masked = x * (torch.rand_like(x) > 0.3).float()  # 30% mask
        mu_b, logvar_b = encoder_b(x_masked)

        # Combine: weighted average
        alpha = min(1.0, epoch / 50)  # gradually shift to Path A
        mu = alpha * mu_a + (1 - alpha) * mu_b
        logvar = alpha * logvar_a + (1 - alpha) * logvar_b
    else:
        mu, logvar = mu_a, logvar_a

    return mu, logvar
```

#### 6.4.3 GQ-VAE â€” å¯å¤‰é•·é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆBPEåœ§ç¸®ç‡ã«æ¥è¿‘ï¼‰

**å•é¡Œ**: VQ-VAEã¯å›ºå®šé•·ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆä¾‹: 256Ã—256 â†’ 32Ã—32ï¼‰ã€‚æƒ…å ±é‡ãŒå°‘ãªã„é ˜åŸŸã‚‚ä¸€æ§˜ã«åœ§ç¸®ã€‚

**GQ-VAE**: å¯å¤‰é•·ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã€‚æƒ…å ±é‡ã«å¿œã˜ã¦ã€ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’èª¿æ•´ã€‚

```
High-detail region (é¡”):   128 tokens
Low-detail region (ç©º):    16 tokens
```

**åŠ¹æœ**: åœ§ç¸®ç‡ãŒBPEï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼‰ã«æ¥è¿‘ã€‚LLMã¨ã®çµ±åˆãŒå®¹æ˜“ã«ã€‚

#### 6.4.4 MGVQ â€” Multi-group Vector Quantization

**ã‚¢ã‚¤ãƒ‡ã‚¢**: ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã‚’è¤‡æ•°ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†å‰²ã€‚å„ã‚°ãƒ«ãƒ¼ãƒ—ãŒç•°ãªã‚‹ã€Œæ„å‘³ã®ç²’åº¦ã€ã‚’æ‹…å½“ã€‚

```
Group 1 (ç²—ã„ç‰¹å¾´): 16 codes â†’ è‰²ã€ãƒ†ã‚¯ã‚¹ãƒãƒ£
Group 2 (ä¸­é–“ç‰¹å¾´): 64 codes â†’ å½¢çŠ¶ã€é…ç½®
Group 3 (ç´°ã‹ã„ç‰¹å¾´): 256 codes â†’ ã‚¨ãƒƒã‚¸ã€è©³ç´°
```

**åˆ©ç‚¹**:
- Codebookåˆ©ç”¨ç‡ãŒå‘ä¸Šï¼ˆå„ã‚°ãƒ«ãƒ¼ãƒ—ã§ç‹¬ç«‹ï¼‰
- éšå±¤çš„ãªè¡¨ç¾ãŒè‡ªç„¶ã«å­¦ç¿’ã•ã‚Œã‚‹
- VQ-VAE-2ã®ç°¡ç´ ç‰ˆã¨ã—ã¦æ©Ÿèƒ½

#### 6.4.5 TiTok v2 â€” 1Dç”»åƒãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆARç”Ÿæˆã¨ã®æ¥ç¶šï¼‰

**å¾“æ¥ã®VQ-VAE**: 2Dæ½œåœ¨ç©ºé–“ï¼ˆä¾‹: 32Ã—32ï¼‰â†’ 2Dæ§‹é€ ã‚’ä¿æŒ

**TiTok v2**: 1Dæ½œåœ¨ç©ºé–“ï¼ˆä¾‹: 1024ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰â†’ Transformerã§ç›´æ¥ç”Ÿæˆå¯èƒ½

```
Image (256Ã—256) â†’ Encoder â†’ 1D sequence (1024 tokens) â†’ Decoder â†’ Image (256Ã—256)
```

**åˆ©ç‚¹**:
- Transformer ARãƒ¢ãƒ‡ãƒ«ã§ç›´æ¥ç”Ÿæˆï¼ˆ2Dã‚¹ã‚­ãƒ£ãƒ³ä¸è¦ï¼‰
- LLMã¨ã®çµ±ä¸€çš„ãªæ‰±ã„ï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒ»ç”»åƒåŒã˜ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ï¼‰
- æ¨è«–é€Ÿåº¦å‘ä¸Šï¼ˆ2Dã‚¹ã‚­ãƒ£ãƒ³ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰å‰Šæ¸›ï¼‰

**èª²é¡Œ**: 2Dæ§‹é€ ã®å­¦ç¿’ãŒé›£ã—ã„ï¼ˆä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¿…é ˆï¼‰

### 6.4 VAEå®Ÿè£…ã®æ¯”è¼ƒ â€” PyTorch vs JAX vs Lux.jl

| é …ç›® | PyTorch | JAX (Flax) | Lux.jl (Julia) |
|:-----|:--------|:-----------|:---------------|
| **å®Ÿè£…è¡Œæ•°** | 150è¡Œ | 180è¡Œï¼ˆç´”ç²‹é–¢æ•°å‹ï¼‰ | 120è¡Œï¼ˆæœ€å°ï¼‰ |
| **è¨“ç·´é€Ÿåº¦ï¼ˆCPUï¼‰** | 2.35s/epoch | 1.82s/epoch | 0.29s/epoch |
| **GPUåˆ‡æ›¿** | `model.to('cuda')` | `jax.device_put(x, gpu)` | `CuArray(x)` |
| **å‹•çš„ãƒãƒƒãƒã‚µã‚¤ã‚º** | âœ… å¯èƒ½ | âŒ JITå†ã‚³ãƒ³ãƒ‘ã‚¤ãƒ« | âœ… å¯èƒ½ |
| **ãƒ‡ãƒãƒƒã‚°** | âœ… pdb, printæ–‡ | âš ï¸ JITã§é›£ã—ã„ | âœ… Revise.jl + REPL |
| **ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ** | æœ€å¤§ï¼ˆtorchvisionç­‰ï¼‰ | æˆé•·ä¸­ï¼ˆdm-haikuç­‰ï¼‰ | ç§‘å­¦è¨ˆç®—ç‰¹åŒ– |
| **å­¦ç¿’æ›²ç·š** | ç·©ã‚„ã‹ | æ€¥ï¼ˆç´”ç²‹é–¢æ•°å‹ï¼‰ | ä¸­ï¼ˆå¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒï¼‰ |

**é¸æŠæŒ‡é‡**:
- **ç ”ç©¶ãƒ»ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—**: PyTorchï¼ˆã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ æœ€å¤§ï¼‰
- **æœ¬ç•ªãƒ»å¤§è¦æ¨¡è¨“ç·´**: JAXï¼ˆTPUæœ€é©åŒ–ï¼‰
- **æ•°å€¤è¨ˆç®—ãƒ»ç§‘å­¦è¨ˆç®—**: Lux.jlï¼ˆæ•°å¼1:1ã€æœ€é€ŸCPUï¼‰

:::details ç”¨èªé›† (Glossary)

| ç”¨èª | è‹±èª | å®šç¾© |
|:-----|:-----|:-----|
| å¤‰åˆ†ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ | Variational Autoencoder | æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«ã®ä¸€ç¨®ã€‚ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã§ $q_\phi(z \mid x)$ ã‚’å­¦ç¿’ã€‚ |
| ELBO | Evidence Lower BOund | å¯¾æ•°å‘¨è¾ºå°¤åº¦ã®ä¸‹ç•Œã€‚VAEã®æå¤±é–¢æ•°ã€‚ |
| å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãƒˆãƒªãƒƒã‚¯ | Reparameterization Trick | ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’å¾®åˆ†å¯èƒ½ã«ã™ã‚‹æ‰‹æ³•ã€‚$z = \mu + \sigma \epsilon$ |
| KLç™ºæ•£ | KL Divergence | 2ã¤ã®åˆ†å¸ƒã®ã€Œè·é›¢ã€ã€‚éå¯¾ç§°ã€‚ |
| æ½œåœ¨ç©ºé–“ | Latent Space | ãƒ‡ãƒ¼ã‚¿ã®ä½æ¬¡å…ƒè¡¨ç¾ç©ºé–“ã€‚ |
| ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ | Codebook | é›¢æ•£æ½œåœ¨å¤‰æ•°ã®å€™è£œé›†åˆã€‚VQ-VAEã§ä½¿ç”¨ã€‚ |
| ãƒ™ã‚¯ãƒˆãƒ«é‡å­åŒ– | Vector Quantization | é€£ç¶šãƒ™ã‚¯ãƒˆãƒ«ã‚’é›¢æ•£ã‚³ãƒ¼ãƒ‰ã«å†™åƒã€‚ |
| Straight-Through Estimator | STE | é›¢æ•£åŒ–ã®å‹¾é…ã‚’è¿‘ä¼¼ã™ã‚‹æ‰‹æ³•ã€‚ |
| Posterior Collapse | - | ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãŒæ½œåœ¨å¤‰æ•°ã‚’ç„¡è¦–ã™ã‚‹ç¾è±¡ã€‚ |
| Disentanglement | - | æ½œåœ¨ç©ºé–“ã®å„æ¬¡å…ƒãŒç‹¬ç«‹ã—ãŸæ„å‘³ã‚’æŒã¤æ€§è³ªã€‚ |

:::

:::message
**é€²æ—: 95% å®Œäº†** VAEç³»åˆ—ã®ç³»è­œã€FSQ/Cosmosæœ€å‰ç·šã€æ¨è–¦æ›¸ç±ã‚’æŠŠæ¡ã—ãŸã€‚Zone 7ã§å…¨ä½“ã‚’æŒ¯ã‚Šè¿”ã‚‹ã€‚
:::

### 6.5 ã“ã®è¬›ç¾©ã®3ã¤ã®æ ¸å¿ƒ

1. **VAEã¯å¤‰åˆ†æ¨è«–ã®è‡ªå‹•åŒ–ã§ã‚ã‚‹** â€” æ‰‹å‹•è¨­è¨ˆã®è¿‘ä¼¼åˆ†å¸ƒ $q(z)$ ã‚’ã€NN $q_\phi(z \mid x)$ ã«ç½®ãæ›ãˆãŸã€‚Reparameterization Trickã§å¾®åˆ†å¯èƒ½ã«ã€‚

2. **é€£ç¶šæ½œåœ¨ç©ºé–“ã‹ã‚‰é›¢æ•£è¡¨ç¾ã¸** â€” VAEã®ã€Œã¼ã‚„ã‘ãŸç”»åƒã€å•é¡Œã‚’ã€VQ-VAEãŒé›¢æ•£ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã§è§£æ±ºã€‚FSQãŒã•ã‚‰ã«ç°¡ç´ åŒ–ã€‚2026å¹´ã®ç”»åƒãƒ»å‹•ç”»ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åŸºç›¤ã€‚

3. **JuliaãŒè¨“ç·´ãƒ«ãƒ¼ãƒ—ã‚’8å€é«˜é€ŸåŒ–** â€” å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒ + JIT + å‹å®‰å®šæ€§ã€‚æ•°å¼ãŒãã®ã¾ã¾ã‚³ãƒ¼ãƒ‰ã«ãªã‚‹ã€‚**Pythonã«æˆ»ã‚Œãªã„ã€‚**

### 6.6 ã‚ˆãã‚ã‚‹è³ªå• (FAQ)

:::details Q: VAEã®ç”»åƒãŒã¼ã‚„ã‘ã‚‹ã®ã¯ãªãœï¼Ÿ

**ç­”**: 2ã¤ã®ç†ç”±ãŒã‚ã‚‹:

1. **Gaussianä»®å®š**: ãƒ‡ã‚³ãƒ¼ãƒ€ãŒ $p_\theta(x \mid z) = \mathcal{N}(x \mid \mu_\theta(z), \sigma^2 I)$ ã‚’ä»®å®šã€‚ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã¯ã€Œå¹³å‡çš„ãªç”»åƒã€ã‚’å‡ºåŠ›ã™ã‚‹ãŸã‚ã€ã‚¨ãƒƒã‚¸ãŒã¼ã‚„ã‘ã‚‹ã€‚

2. **Posterior Collapse**: KLæ­£å‰‡åŒ–ãŒå¼·ã™ãã‚‹ã¨ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãŒ $q_\phi(z \mid x) \approx p(z)$ ã«ãªã‚Šã€$z$ ãŒ $x$ ã®æƒ…å ±ã‚’æŒãŸãªããªã‚‹ã€‚ãƒ‡ã‚³ãƒ¼ãƒ€ã¯å¹³å‡çš„ãªç”»åƒã‚’å‡ºåŠ›ã™ã‚‹ã—ã‹ãªã„ã€‚

**è§£æ±ºç­–**:
- Î²-VAE ã§ Î² ã‚’å°ã•ãã™ã‚‹ï¼ˆå†æ§‹æˆé‡è¦–ï¼‰
- Perceptual Loss ã‚’ä½¿ã†ï¼ˆVQ-GANï¼‰
- GANã¨çµ„ã¿åˆã‚ã›ã‚‹ï¼ˆç¬¬12å›ï¼‰
:::

:::details Q: VQ-VAEã®Straight-Through Estimatorã¯ç†è«–çš„ã«æ­£ã—ã„ã®ã‹ï¼Ÿ

**ç­”**: **æ­£ã—ããªã„**ã€‚å‹¾é…ã®ä¸åæ¨å®šé‡ã§ã¯ãªã„ã€‚ã—ã‹ã—å®Ÿç”¨ä¸Šã¯å‹•ä½œã™ã‚‹ã€‚

ç†è«–çš„ã«ã¯ã€Gumbel-Softmaxï¼ˆé€£ç¶šç·©å’Œï¼‰ã®æ–¹ãŒå³å¯†ã ãŒã€VQ-VAEã®STEã®æ–¹ãŒå®Ÿè£…ãŒç°¡å˜ã§ã€æ€§èƒ½ã‚‚è‰¯ã„ï¼ˆçµŒé¨“çš„ï¼‰ã€‚

[^6] Bengio et al. (2013) "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation" â€” STEã®æœ€åˆã®ææ¡ˆ
:::

:::details Q: Juliaã¯æœ¬å½“ã«Pythonã‚ˆã‚Šé€Ÿã„ã®ã‹ï¼Ÿå…¨ã¦ã®ã‚±ãƒ¼ã‚¹ã§ï¼Ÿ

**ç­”**: **No**ã€‚JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒã‚ã‚‹ãŸã‚ã€çŸ­ã„ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆ1å›ã ã‘å®Ÿè¡Œï¼‰ã§ã¯Pythonã®æ–¹ãŒé€Ÿã„å ´åˆã‚‚ã‚ã‚‹ã€‚

**JuliaãŒé€Ÿã„ã‚±ãƒ¼ã‚¹**:
- ãƒ«ãƒ¼ãƒ—ã‚’ä½•åº¦ã‚‚å›ã™ï¼ˆè¨“ç·´ãƒ«ãƒ¼ãƒ—ãªã©ï¼‰
- å‹å®‰å®šãªã‚³ãƒ¼ãƒ‰
- æ•°å€¤è¨ˆç®—ãŒä¸»ä½“

**PythonãŒé€Ÿã„ã‚±ãƒ¼ã‚¹**:
- 1å›ã ã‘å®Ÿè¡Œã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- I/Oå¾…ã¡ãŒä¸»ä½“ï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼‰
- æ—¢å­˜ã®C/C++ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’å‘¼ã¶ã ã‘ï¼ˆNumPy, Pandasï¼‰

**ä½¿ã„åˆ†ã‘**: ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—â†’Pythonã€è¨“ç·´â†’Juliaã€æ¨è«–â†’Rust
:::

:::details Q: VAEã¨Diffusion Modelã®é–¢ä¿‚ã¯ï¼Ÿ

**ç­”**: VAEã¯ **Latent Diffusion Model (LDM)** ã®åŸºç›¤ã ã€‚

Stable Diffusionã®æ§‹é€ :
1. VAE Encoder: ç”»åƒ (512Ã—512) â†’ æ½œåœ¨ç©ºé–“ (64Ã—64Ã—4)
2. Diffusion Model: æ½œåœ¨ç©ºé–“ã§ãƒã‚¤ã‚ºé™¤å»
3. VAE Decoder: æ½œåœ¨ç©ºé–“ â†’ ç”»åƒ (512Ã—512)

VAEãŒé«˜æ¬¡å…ƒç”»åƒã‚’ä½æ¬¡å…ƒæ½œåœ¨ç©ºé–“ã«åœ§ç¸®ã™ã‚‹ã“ã¨ã§ã€Diffusion Modelã®è¨ˆç®—é‡ã‚’åŠ‡çš„ã«å‰Šæ¸›ã€‚Course IVã§è©³è¿°ã€‚
:::

:::details Q: æœ¬è¬›ç¾©ã§æ‰±ã‚ãªã‹ã£ãŸVAEç™ºå±•ãƒˆãƒ”ãƒƒã‚¯ã¯ï¼Ÿ

æœ¬è¬›ç¾©ã¯åŸºç¤ã¨é›¢æ•£è¡¨ç¾ã«é›†ä¸­ã—ãŸãŸã‚ã€ä»¥ä¸‹ã¯çœç•¥ã—ãŸ:

- **Hierarchical VAE** (Ladder VAE, NVAE) â€” éšå±¤çš„æ½œåœ¨è¡¨ç¾
- **Normalizing Flow Posterior** â€” ã‚ˆã‚ŠæŸ”è»Ÿãªäº‹å¾Œåˆ†å¸ƒï¼ˆç¬¬14å›ã§æ‰±ã†ï¼‰
- **Conditional VAE (CVAE)** â€” ãƒ©ãƒ™ãƒ«æ¡ä»¶ä»˜ãç”Ÿæˆ
- **Semi-supervised VAE** â€” ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿ã®æ´»ç”¨
- **Variational Lossy Autoencoder (VLAE)** â€” æƒ…å ±ç†è«–çš„è§£é‡ˆ

èˆˆå‘³ãŒã‚ã‚Œã°ã€Zone 6ã®æ¨å¥¨æ›¸ç±ã‚’å‚ç…§ã€‚
:::

### 6.7 1é€±é–“ã®å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

| æ—¥ | ã‚¿ã‚¹ã‚¯ | æ‰€è¦æ™‚é–“ | ç›®æ¨™ |
|:---|:------|:---------|:-----|
| **Day 1** | Zone 0-2 ã‚’èª­ã‚€ï¼ˆæ•°å¼ã‚¹ã‚­ãƒƒãƒ—ï¼‰ | 30åˆ† | å…¨ä½“åƒæŠŠæ¡ |
| **Day 2** | Zone 3.1-3.2 ELBO + Reparameterization å°å‡º | 1.5æ™‚é–“ | æ‰‹ã§å°å‡º |
| **Day 3** | Zone 3.3-3.4 Gaussian KL + Boss Battle | 1.5æ™‚é–“ | Kingma 2013 å®Œå…¨ç†è§£ |
| **Day 4** | Zone 4.1-4.3 Julia ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« + åŸºæœ¬æ–‡æ³• | 1æ™‚é–“ | Juliaç’°å¢ƒæ§‹ç¯‰ |
| **Day 5** | Zone 4.4-4.6 Julia VAE å®Ÿè£… + é€Ÿåº¦æ¸¬å®š | 2æ™‚é–“ | 8å€é€Ÿã‚’ä½“é¨“ |
| **Day 6** | Zone 5 æ½œåœ¨ç©ºé–“å¯è¦–åŒ– + è£œé–“ | 1.5æ™‚é–“ | å®Ÿé¨“ã§éŠã¶ |
| **Day 7** | Zone 6-7 æœ€æ–°ç ”ç©¶ + å¾©ç¿’ | 1æ™‚é–“ | å…¨ä½“æŒ¯ã‚Šè¿”ã‚Š |

**åˆè¨ˆ: ç´„9æ™‚é–“**ï¼ˆæœ¬è¬›ç¾©ã®ç›®æ¨™ã¯3æ™‚é–“ã ãŒã€å®Œå…¨ç¿’å¾—ã«ã¯3å€ã‹ã‹ã‚‹ï¼‰

### 6.8 è‡ªå·±è¨ºæ–­ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] VAEã®Encoder/Decoderã®å½¹å‰²ã‚’å›³ã§èª¬æ˜ã§ãã‚‹
- [ ] ELBOã‚’3è¡Œã§å°å‡ºã§ãã‚‹ï¼ˆJensenä¸ç­‰å¼ã‚’ä½¿ã£ã¦ï¼‰
- [ ] Reparameterization Trickã‚’å¼ã§æ›¸ã‘ã‚‹: $z = \mu + \sigma \epsilon$
- [ ] ã‚¬ã‚¦ã‚¹KLç™ºæ•£ã®é–‰å½¢å¼ã‚’æš—è¨˜ã—ã¦ã„ã‚‹ï¼ˆã¾ãŸã¯å°å‡ºã§ãã‚‹ï¼‰
- [ ] PyTorchã§VAEã‚’10è¡Œã§å®Ÿè£…ã§ãã‚‹
- [ ] **Juliaã§VAEã‚’å®Ÿè£…ã—ã€è¨“ç·´é€Ÿåº¦ã‚’æ¸¬å®šã—ãŸ**
- [ ] æ½œåœ¨ç©ºé–“ã®2Då¯è¦–åŒ–ã‚’ä½œæˆã—ãŸ
- [ ] VQ-VAEã®Straight-Through Estimatorã‚’èª¬æ˜ã§ãã‚‹
- [ ] FSQã¨VQ-VAEã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹

**7å€‹ä»¥ä¸Šãƒã‚§ãƒƒã‚¯ã§ãã‚Œã°åˆæ ¼ã€‚** æ¬¡ã®ç¬¬11å›ï¼ˆæœ€é©è¼¸é€ç†è«–ï¼‰ã«é€²ã‚ã‚‹ã€‚

### 6.9 æ¬¡å›äºˆå‘Š: ç¬¬11å› æœ€é©è¼¸é€ç†è«– (Optimal Transport)

VAEã¯ã€Œå†æ§‹æˆ + KLæ­£å‰‡åŒ–ã€ã§æ½œåœ¨ç©ºé–“ã‚’å­¦ç¿’ã—ãŸã€‚ã—ã‹ã—ã€KLç™ºæ•£ã«ã¯é™ç•ŒãŒã‚ã‚‹:
- å°ã®ä¸ä¸€è‡´ã§ç™ºæ•£ï¼ˆ$p(x)$ ã¨ $q(x)$ ã®ã‚µãƒãƒ¼ãƒˆãŒé‡ãªã‚‰ãªã„ã¨ âˆï¼‰
- å‹¾é…æ¶ˆå¤±ï¼ˆGANã®è¨“ç·´ä¸å®‰å®šæ€§ã®åŸå› ï¼‰

**æœ€é©è¼¸é€ç†è«–** (Optimal Transport) ã¯ã€ç¢ºç‡åˆ†å¸ƒé–“ã®ã€Œè·é›¢ã€ã‚’ã€**è¼¸é€ã‚³ã‚¹ãƒˆ**ã§å®šç¾©ã™ã‚‹ã€‚

$$
W_2(p, q) = \inf_{\gamma \in \Pi(p, q)} \mathbb{E}_{(x, y) \sim \gamma}[\|x - y\|^2]
$$

ã“ã® Wasserstein è·é›¢ã¯:
- å°ãŒä¸ä¸€è‡´ã§ã‚‚æœ‰é™å€¤
- é€£ç¶šçš„ã§ã€å‹¾é…ãŒå¸¸ã«å­˜åœ¨
- GANã®ç†è«–åŸºç›¤ï¼ˆWGANï¼‰
- Flow Matchingã®æ•°å­¦çš„åœŸå°ï¼ˆCourse IVï¼‰

**ç¬¬11å›ã§å­¦ã¶ã“ã¨**:
- Mongeå•é¡Œï¼ˆ1781å¹´ï¼‰ã‹ã‚‰Kantorovichç·©å’Œï¼ˆ1942å¹´ï¼‰ã¸
- Kantorovich-RubinsteinåŒå¯¾æ€§ï¼ˆç¬¬6å›ã®åŒå¯¾æ€§ã‚’å¿œç”¨ï¼‰
- Sinkhornè·é›¢ï¼ˆé«˜é€Ÿè¿‘ä¼¼ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰
- OTã¨Flow Matchingã®æ¥ç¶šï¼ˆCourse IVã¸ã®ä¼ç·šï¼‰

```mermaid
graph LR
    L10["ç¬¬10å›: VAE<br>KLæ­£å‰‡åŒ–"] --> L11["ç¬¬11å›: æœ€é©è¼¸é€ç†è«–<br>Wassersteinè·é›¢"]
    L11 --> L12["ç¬¬12å›: GAN<br>WGANç†è«–"]
    L12 --> L13["ç¬¬13å›: StyleGAN<br>åˆ¶å¾¡å¯èƒ½ãªç”Ÿæˆ"]

    style L10 fill:#e1f5fe
    style L11 fill:#fff3e0
```

:::message
**é€²æ—: 100% å®Œäº†ï¼** VAEã®åŸºç¤ã‹ã‚‰é›¢æ•£è¡¨ç¾ã€Juliaå®Ÿè£…ã¾ã§å®Œèµ°ã—ãŸã€‚æ¬¡å›ã¯æœ€é©è¼¸é€ç†è«–ã§ã€ç¢ºç‡åˆ†å¸ƒé–“ã®ã€ŒçœŸã®è·é›¢ã€ã‚’å­¦ã¶ã€‚
:::

### 6.10 ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **ã€Œå¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã¯"ä¾¿åˆ©æ©Ÿèƒ½"ã‹ã€ãã‚Œã¨ã‚‚"è¨€èªã®æœ¬è³ª"ã‹ï¼Ÿã€**

Pythonã§ã¯ã€é–¢æ•°ã®æŒ¯ã‚‹èˆã„ã¯å¼•æ•°ã®**å‹**ã§ã¯ãªãã€**å€¤**ã§åˆ¶å¾¡ã•ã‚Œã‚‹:

```python
def f(x):
    if isinstance(x, int):
        return x + 1
    elif isinstance(x, list):
        return [i + 1 for i in x]
```

Juliaã§ã¯ã€é–¢æ•°ã®æŒ¯ã‚‹èˆã„ã¯**å‹**ã§åˆ¶å¾¡ã•ã‚Œã‚‹:

```julia
f(x::Int) = x + 1
f(x::Vector{Int}) = x .+ 1
```

**å•ã„**:
1. Pythonã® `isinstance` ãƒã‚§ãƒƒã‚¯ã¨ã€Juliaã®å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã¯ã€æœ¬è³ªçš„ã«ä½•ãŒé•ã†ã®ã‹ï¼Ÿ
2. å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã¯ã€Œifæ–‡ã‚’æ›¸ã‹ãªãã¦æ¸ˆã‚€ç³–è¡£æ§‹æ–‡ã€ãªã®ã‹ã€ãã‚Œã¨ã‚‚ã€Œå‹ã‚·ã‚¹ãƒ†ãƒ ã¨ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã®çµ±åˆã€ãªã®ã‹ï¼Ÿ
3. **VAEã®è¨“ç·´ãƒ«ãƒ¼ãƒ—ãŒ8å€é€Ÿããªã£ãŸç†ç”±ã¯ã€å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒãªã®ã‹ã€JITãªã®ã‹ã€å‹å®‰å®šæ€§ãªã®ã‹ï¼Ÿãã‚Œã¨ã‚‚å…¨ã¦ã®ç›¸ä¹—åŠ¹æœãªã®ã‹ï¼Ÿ**

:::details ãƒ’ãƒ³ãƒˆ: Juliaã®è¨­è¨ˆå“²å­¦

Juliaã®å‰µå§‹è€…ã®è¨€è‘‰:

> "We want the speed of C with the dynamism of Ruby. We want a language that's homoiconic, with true macros like Lisp, but with obvious, familiar mathematical notation like Matlab. We want something as usable for general programming as Python, as easy for statistics as R, as natural for string processing as Perl, as powerful for linear algebra as Matlab, as good at gluing programs together as the shell."
> â€” Jeff Bezanson, Stefan Karpinski, Viral Shah, Alan Edelman (2012)

å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã¯ã€ã“ã®ã€Œå…¨ã¦ã‚’å®Ÿç¾ã™ã‚‹ã€ãŸã‚ã®æ ¸å¿ƒæŠ€è¡“ã ã£ãŸã€‚å‹ã«ã‚ˆã‚‹æœ€é©åŒ–ã¨ã€å‹•çš„è¨€èªã®æŸ”è»Ÿæ€§ã‚’ä¸¡ç«‹ã•ã›ã‚‹å”¯ä¸€ã®æ–¹æ³•ã€‚
:::

ã“ã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’å—ã‘å…¥ã‚Œã‚‹ã¨ã€**Pythonã® `if isinstance(x, type):` ã‚’æ›¸ããŸã³ã«é•å’Œæ„Ÿã‚’è¦šãˆã‚‹ã‚ˆã†ã«ãªã‚‹ã€‚** ãã‚ŒãŒã€ç¬¬10å›ã®ç›®æ¨™ã ã€‚

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Kingma, D. P., & Welling, M. (2013). Auto-Encoding Variational Bayes. *arXiv preprint arXiv:1312.6114*.
@[card](https://arxiv.org/abs/1312.6114)

[^2]: Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., ... & Lerchner, A. (2017). Î²-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework. *International Conference on Learning Representations (ICLR)*.
@[card](https://openreview.net/forum?id=Sy2fzU9gl)

[^3]: van den Oord, A., Vinyals, O., & Kavukcuoglu, K. (2017). Neural Discrete Representation Learning. *Advances in Neural Information Processing Systems (NeurIPS)*. arXiv:1711.00937.
@[card](https://arxiv.org/abs/1711.00937)

[^4]: Mentzer, F., Minnen, D., Agustsson, E., & Tschannen, M. (2023). Finite Scalar Quantization: VQ-VAE Made Simple. *International Conference on Learning Representations (ICLR) 2024*. arXiv:2309.15505.
@[card](https://arxiv.org/abs/2309.15505)

[^5]: NVIDIA. (2024). Cosmos Tokenizer. *GitHub Repository*.
@[card](https://github.com/NVIDIA/Cosmos-Tokenizer)

[^6]: Bengio, Y., LÃ©onard, N., & Courville, A. (2013). Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation. arXiv:1308.3432.
@[card](https://arxiv.org/abs/1308.3432)

[^7]: Kingma, D. P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., & Welling, M. (2016). Improved Variational Inference with Inverse Autoregressive Flow. *NeurIPS 2016*.
@[card](https://arxiv.org/abs/1606.04934)

### é–¢é€£è«–æ–‡

- Burgess, C. P., Higgins, I., Pal, A., Matthey, L., Watters, N., Desjardins, G., & Lerchner, A. (2018). Understanding disentangling in Î²-VAE. arXiv:1804.03599.
@[card](https://arxiv.org/abs/1804.03599)

- Kingma, D. P., Salimans, T., & Welling, M. (2015). Variational Dropout and the Local Reparameterization Trick. *NeurIPS*. arXiv:1506.02557.
@[card](https://arxiv.org/abs/1506.02557)

- Esser, P., Rombach, R., & Ommer, B. (2021). Taming Transformers for High-Resolution Image Synthesis. *CVPR*. arXiv:2012.09841.
@[card](https://arxiv.org/abs/2012.09841)

- Yu, L., Poirson, P., Yang, S., Berg, A. C., & Berg, T. L. (2023). MAGVIT-v2: Language Model Beats Diffusion - Tokenizer is Key to Visual Generation. arXiv:2310.05737.
@[card](https://arxiv.org/abs/2310.05737)

### æ•™ç§‘æ›¸

- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer. Chapter 10: Approximate Inference.

- Murphy, K. P. (2022). *Probabilistic Machine Learning: Advanced Topics*. MIT Press. Chapter 21: Variational Inference.

- Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press. Chapter 20: Deep Generative Models.
@[card](https://www.deeplearningbook.org/)

---

## è¨˜æ³•è¦ç´„

æœ¬è¬›ç¾©ã‚·ãƒªãƒ¼ã‚ºã§ä½¿ç”¨ã™ã‚‹æ•°å­¦è¨˜æ³•ã®çµ±ä¸€ãƒ«ãƒ¼ãƒ«:

| è¨˜å· | æ„å‘³ | èª­ã¿æ–¹ | ä¾‹ |
|:-----|:-----|:------|:---|
| $x$ | ãƒ‡ãƒ¼ã‚¿ï¼ˆè¦³æ¸¬å¤‰æ•°ï¼‰ | ã‚¨ãƒƒã‚¯ã‚¹ | $x \in \mathbb{R}^{784}$ |
| $z$ | æ½œåœ¨å¤‰æ•° | ã‚¼ãƒƒãƒˆ | $z \in \mathbb{R}^{20}$ |
| $\theta$ | ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆDecoderï¼‰ | ã‚·ãƒ¼ã‚¿ | $p_\theta(x \mid z)$ |
| $\phi$ | å¤‰åˆ†åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆEncoderï¼‰ | ãƒ•ã‚¡ã‚¤ | $q_\phi(z \mid x)$ |
| $\mu, \sigma$ | å¹³å‡ã€æ¨™æº–åå·® | ãƒŸãƒ¥ãƒ¼ã€ã‚·ã‚°ãƒ | $\mathcal{N}(\mu, \sigma^2)$ |
| $\epsilon$ | ãƒã‚¤ã‚ºå¤‰æ•° | ã‚¤ãƒ—ã‚·ãƒ­ãƒ³ | $\epsilon \sim \mathcal{N}(0, I)$ |
| $p(x)$ | çœŸã®åˆ†å¸ƒ | ãƒ”ãƒ¼ | $p(x) = \int p(x, z) dz$ |
| $q(z \mid x)$ | å¤‰åˆ†åˆ†å¸ƒï¼ˆè¿‘ä¼¼äº‹å¾Œåˆ†å¸ƒï¼‰ | ã‚­ãƒ¥ãƒ¼ | $q_\phi(z \mid x)$ |
| $\mathbb{E}_{q}[\cdot]$ | $q$ ã®ä¸‹ã§ã®æœŸå¾…å€¤ | ã‚¤ãƒ¼ ã‚µãƒ– ã‚­ãƒ¥ãƒ¼ | $\mathbb{E}_{q(z)}[f(z)]$ |
| $D_\text{KL}(q \| p)$ | KLç™ºæ•£ | ãƒ‡ã‚£ãƒ¼ ã‚±ãƒ¼ã‚¨ãƒ« | $D_\text{KL}(q \| p) = \mathbb{E}_q[\log q - \log p]$ |
| $\mathcal{L}(\theta, \phi)$ | ELBOï¼ˆæå¤±é–¢æ•°ï¼‰ | ã‚¨ãƒ« ã‚·ãƒ¼ã‚¿ ãƒ•ã‚¡ã‚¤ | $\mathcal{L} = \mathbb{E}_q[\log p] - D_\text{KL}(q \| p)$ |
| $\nabla_\theta$ | $\theta$ ã«é–¢ã™ã‚‹å‹¾é… | ãƒŠãƒ–ãƒ© ã‚·ãƒ¼ã‚¿ | $\nabla_\theta \mathcal{L}$ |
| $\odot$ | è¦ç´ ã”ã¨ã®ç©ï¼ˆHadamardç©ï¼‰ | Hadamard product | $z = \mu + \sigma \odot \epsilon$ |
| $\|x\|$ | ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰ãƒãƒ«ãƒ  | ãƒãƒ«ãƒ  | $\|x\|^2 = \sum x_i^2$ |

**Juliaè¨˜æ³•ã¨ã®å¯¾å¿œ**:
- `Î¼` (U+03BC), `Ïƒ` (U+03C3), `Î¸` (U+03B8), `Ï†` (U+03C6), `Îµ` (U+03B5) â€” Juliaã§ã¯å¤‰æ•°åã«ã‚®ãƒªã‚·ãƒ£æ–‡å­—ã‚’ä½¿ãˆã‚‹
- `.` â€” broadcastæ¼”ç®—å­ï¼ˆè¦ç´ ã”ã¨é©ç”¨ï¼‰
- `.*` â€” è¦ç´ ã”ã¨ã®ç©ï¼ˆ$\odot$ ã«å¯¾å¿œï¼‰

---

**EOF**

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
