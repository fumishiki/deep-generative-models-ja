---
title: "ç¬¬32å›: Productionçµ±åˆ: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸ†"
type: "tech"
topics: ["machinelearning", "production", "rust", "julia", "elixir"]
published: true
---

# ç¬¬32å›: Production & ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ— + çµ±åˆPJ ğŸ†

:::message
**å‰æçŸ¥è­˜**: ç¬¬31å›ã§MLOpsåŸºç›¤ã‚’æ•´ãˆãŸã€‚ã“ã®ç¬¬32å›ã¯Course IIIæœ€çµ‚å› â€” 14å›ã®å…¨æŠ€è¡“ã‚’çµ±åˆã—ã¦E2Eã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
:::

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” 3è¡Œã§E2Eã‚·ã‚¹ãƒ†ãƒ ã‚’ä½“æ„Ÿ

ç¬¬31å›ã§MLOpsãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã—ãŸã€‚æœ€çµ‚å›ã®ä»Šå›ã€**å…¨ã¦ã‚’çµ±åˆã—ãŸProduction E2Eã‚·ã‚¹ãƒ†ãƒ **ã‚’3è¡Œã®ã‚³ãƒ¼ãƒ‰ã§ä½“æ„Ÿã—ã‚ˆã†ã€‚

```julia
# SmolVLM2-256Mæ¨è«– â†’ Elixir API â†’ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›† â†’ Juliaå†è¨“ç·´
using SmolVLM2Inference, ElixirGateway, FeedbackLoop
result = deploy_e2e_system("models/smolvlm2-256m.onnx", port=4000)
# => "E2E system deployed: Juliaè¨“ç·´â†’Rustæ¨è«–â†’Elixiré…ä¿¡â†’Feedbackâ†’å†è¨“ç·´"
```

**å‡ºåŠ›**:
```
ğŸ¯ E2E System Status:
  âš¡ Julia Training Pipeline: Ready (SmolVLM2-256M, VAE, GANçµ±åˆ)
  ğŸ¦€ Rust Inference Server: Running on port 8080 (Axum, ONNX Runtime)
  ğŸ”® Elixir API Gateway: Running on port 4000 (Phoenix, JWT auth, Rate limit)
  ğŸ“Š Monitoring: Prometheus metrics at :9090
  ğŸ”„ Feedback Loop: Active (implicit+explicit feedback collected)

âœ… System Health: All components operational
ğŸ“ˆ Current throughput: 1,247 req/s (95th %ile latency: 12ms)
```

**ã“ã®è£ã«ã‚ã‚‹æ•°å¼**: ç¬¬19å›ã‹ã‚‰ç¬¬31å›ã§å­¦ã‚“ã **å…¨ã¦ã®æŠ€è¡“ãŒçµ±åˆã•ã‚Œã¦ã„ã‚‹**:

$$
\text{Production System} = \underbrace{\text{Train}_{\text{Julia}}}_{\text{ç¬¬20,23å›}} \xrightarrow{\text{Export}_{\text{ONNX}}} \underbrace{\text{Infer}_{\text{Rust}}}_{\text{ç¬¬26å›}} \xrightarrow{\text{Serve}_{\text{Elixir}}} \underbrace{\text{Feedback}}_{\text{ç¬¬32å›}} \circlearrowleft
$$

ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—ã®æ•°å¼:

$$
\theta_{t+1} \leftarrow \theta_t - \eta \nabla_\theta \mathcal{L}(\theta_t; \mathcal{D}_{\text{feedback}})
$$

3è¡Œã®ã‚³ãƒ¼ãƒ‰ã®è£ã§ã€**Juliaè¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**ãŒVAE/GAN/GPTã‚’è¨“ç·´ã—ã€**Rustæ¨è«–ã‚µãƒ¼ãƒãƒ¼**ãŒONNXãƒ¢ãƒ‡ãƒ«ã‚’é«˜é€Ÿæ¨è«–ã€**Elixir APIã‚²ãƒ¼ãƒˆã‚¦ã‚§ã‚¤**ãŒåˆ†æ•£é…ä¿¡ã¨èªè¨¼ã‚’æ‹…å½“ã€**ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—**ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è©•ä¾¡ã‚’åé›†ã—ã¦å†è¨“ç·´ã«ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã™ã‚‹ â€” å…¨ã¦ãŒè‡ªå‹•çš„ã«å‹•ä½œã™ã‚‹ã€‚

**ã“ã‚ŒãŒCourse III 14å›ã®é›†å¤§æˆã ã€‚**

:::message
**é€²æ—: 3%å®Œäº†ï¼** ç¬¬32å›ã®ã‚´ãƒ¼ãƒ«ã¯ã€ŒProduction E2Eã‚·ã‚¹ãƒ†ãƒ ã‚’è‡ªåŠ›ã§æ§‹ç¯‰ãƒ»é‹ç”¨ã§ãã‚‹ã€ã“ã¨ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” AIã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆ & ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’è§¦ã‚‹

### 1.1 AIã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã®è¨­è¨ˆ

AIã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã®æœ¬è³ªã¯**å•ã„åˆã‚ã›ã®è‡ªå‹•åˆ†é¡**ã¨**äººé–“ã¸ã®ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æˆ¦ç•¥**ã ã€‚

```julia
using CustomerSupport, Embeddings

# å•ã„åˆã‚ã›ã‚’è‡ªå‹•åˆ†é¡
inquiry = "å•†å“ãŒå±Šã‹ãªã„ã€‚æ³¨æ–‡ç•ªå·ã¯12345ã§ã™ã€‚"
category, confidence = classify_inquiry(inquiry)
# => ("é…é€å•é¡Œ", 0.92)

if confidence < 0.7
    escalate_to_human(inquiry, reason="ä½ä¿¡é ¼åº¦")
elseif category == "è¿”é‡‘è¦æ±‚"
    escalate_to_human(inquiry, reason="é«˜ãƒªã‚¹ã‚¯")
else
    auto_response = generate_faq_response(category, inquiry)
    send_response(auto_response)
end
```

**æ•°å¼**: å•ã„åˆã‚ã›åˆ†é¡ã¯Softmaxåˆ†é¡

$$
p(c_i | \mathbf{x}) = \frac{\exp(\mathbf{w}_i^\top \mathbf{x})}{\sum_{j=1}^C \exp(\mathbf{w}_j^\top \mathbf{x})}
$$

ã“ã“ã§ $\mathbf{x}$ ã¯å•ã„åˆã‚ã›ã®Embeddingã€$\mathbf{w}_i$ ã¯ã‚«ãƒ†ã‚´ãƒª $c_i$ ã®é‡ã¿ãƒ™ã‚¯ãƒˆãƒ«ã€‚

**ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æˆ¦ç•¥**:

| æ¡ä»¶ | ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ | ç†ç”± |
|:-----|:----------|:-----|
| `confidence < 0.7` | äººé–“ã«ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ | ãƒ¢ãƒ‡ãƒ«ãŒè‡ªä¿¡ã‚’æŒã¦ãªã„ |
| `category == "è¿”é‡‘"` | äººé–“ã«ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ | é«˜ãƒªã‚¹ã‚¯ãƒ»é«˜ã‚³ã‚¹ãƒˆåˆ¤æ–­ |
| `sentiment < -0.5` | äººé–“ã«ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ | æ€’ã£ã¦ã„ã‚‹é¡§å®¢ |
| ãã®ä»– | è‡ªå‹•å¿œç­” | æ¨™æº–çš„ãªå•ã„åˆã‚ã› |

### 1.2 ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†: æš—é»™çš„ vs æ˜ç¤ºçš„

ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«ã¯**æš—é»™çš„**ã¨**æ˜ç¤ºçš„**ã®2ç¨®é¡ãŒã‚ã‚‹ã€‚

```julia
# æš—é»™çš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯: ã‚¯ãƒªãƒƒã‚¯ãƒ»æ»åœ¨æ™‚é–“ãƒ»ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«æ·±åº¦
implicit_feedback = collect_implicit_feedback(
    click_through=true,
    dwell_time=45.3,  # ç§’
    scroll_depth=0.78  # 78%ã¾ã§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
)
# => ImplicitFeedback(positive_signal=0.82)

# æ˜ç¤ºçš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯: è©•ä¾¡ãƒœã‚¿ãƒ³ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆãƒ»NPS
explicit_feedback = collect_explicit_feedback(
    rating=4,  # 1-5 stars
    comment="å›ç­”ã¯å½¹ç«‹ã£ãŸãŒã€ã‚‚ã†å°‘ã—å…·ä½“ä¾‹ãŒæ¬²ã—ã‹ã£ãŸ",
    nps=8      # Net Promoter Score (0-10)
)
# => ExplicitFeedback(sentiment=0.65, topics=["å…·ä½“ä¾‹ä¸è¶³"])
```

**æ•°å¼**: æš—é»™çš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®ã‚¹ã‚³ã‚¢é–¢æ•°

$$
f_{\text{implicit}}(\text{click}, t_{\text{dwell}}, d_{\text{scroll}}) = w_1 \cdot \mathbb{1}_{\text{click}} + w_2 \cdot \tanh(t_{\text{dwell}}/60) + w_3 \cdot d_{\text{scroll}}
$$

ã“ã“ã§ $\mathbb{1}_{\text{click}}$ ã¯ã‚¯ãƒªãƒƒã‚¯ã®æœ‰ç„¡ï¼ˆ0 or 1ï¼‰ã€$w_1, w_2, w_3$ ã¯é‡ã¿ï¼ˆä¾‹: $w_1=0.4, w_2=0.4, w_3=0.2$ï¼‰ã€‚

**æ˜ç¤ºçš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ**:

$$
S(\text{comment}) = \text{Transformer}_{\text{sentiment}}(\text{Embedding}(\text{comment})) \in [-1, 1]
$$

### 1.3 ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†æ: ãƒˆãƒ”ãƒƒã‚¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°

åé›†ã—ãŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚³ãƒ¡ãƒ³ãƒˆã‚’**ãƒˆãƒ”ãƒƒã‚¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°**ã—ã¦æ ¹æœ¬åŸå› ã‚’åˆ†æã™ã‚‹ã€‚

```julia
using UMAP, HDBSCAN

# 1,000ä»¶ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚³ãƒ¡ãƒ³ãƒˆã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
comments = load_feedback_comments(n=1000)
embeddings = embed_comments(comments)  # (1000, 384) Embedding

# UMAPæ¬¡å…ƒå‰Šæ¸› â†’ HDBSCAN ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
umap_emb = umap(embeddings, n_components=2)
clusters = hdbscan(umap_emb, min_cluster_size=20)

# ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã®ä»£è¡¨çš„ãªã‚³ãƒ¡ãƒ³ãƒˆ
for (cluster_id, representative_comments) in clusters
    println("Cluster $cluster_id:")
    println("  ", join(representative_comments[1:3], "\n  "))
end
```

**å‡ºåŠ›ä¾‹**:
```
Cluster 1: "é…é€ãŒé…ã„"ç³»
  "å•†å“ãŒå±Šã‹ãªã„"
  "é…é€çŠ¶æ³ãŒæ›´æ–°ã•ã‚Œãªã„"
  "é…é€æ¥­è€…ã«é€£çµ¡ãŒã¤ã‹ãªã„"

Cluster 2: "å…·ä½“ä¾‹ä¸è¶³"ç³»
  "ã‚‚ã£ã¨å…·ä½“çš„ãªæ‰‹é †ãŒæ¬²ã—ã„"
  "ç”»åƒä»˜ãã§èª¬æ˜ã—ã¦æ¬²ã—ã„"
  "ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ãŒæ¬²ã—ã„"
```

**æ•°å¼**: UMAPæ¬¡å…ƒå‰Šæ¸›

$$
\min_{\mathbf{Y}} \sum_{i,j} w_{ij} \left\| \mathbf{y}_i - \mathbf{y}_j \right\|^2 + \lambda \sum_{i,j} (1 - w_{ij}) \max(0, d_{\text{min}} - \left\| \mathbf{y}_i - \mathbf{y}_j \right\|)^2
$$

ã“ã“ã§ $\mathbf{Y} \in \mathbb{R}^{n \times 2}$ ã¯2æ¬¡å…ƒåŸ‹ã‚è¾¼ã¿ã€$w_{ij}$ ã¯é«˜æ¬¡å…ƒç©ºé–“ã§ã®è¿‘å‚é‡ã¿ã€‚

### 1.4 PyTorchã¨ã®å¯¾å¿œ â€” ãƒ¢ãƒ‡ãƒ«è¨“ç·´

```python
import torch
import torch.nn as nn

# ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ä½¿ã£ãŸFine-tuning
class FeedbackClassifier(nn.Module):
    def __init__(self, embedding_dim=384, num_classes=10):
        super().__init__()
        self.classifier = nn.Linear(embedding_dim, num_classes)

    def forward(self, x):
        return self.classifier(x)

model = FeedbackClassifier()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
criterion = nn.CrossEntropyLoss()

# ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´
for epoch in range(10):
    for batch in feedback_dataloader:
        embeddings, labels = batch
        logits = model(embeddings)
        loss = criterion(logits, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

**Juliaå¯¾å¿œ** (æ•°å¼ â†” ã‚³ãƒ¼ãƒ‰ 1:1):

```julia
using Lux, Optimisers, Zygote

# Lux.jl ã§ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†é¡å™¨
struct FeedbackClassifier <: Lux.AbstractExplicitLayer
    embedding_dim::Int
    num_classes::Int
end

function (m::FeedbackClassifier)(x, ps, st)
    W = ps.W  # (num_classes, embedding_dim)
    b = ps.b  # (num_classes,)
    return W * x .+ b, st
end

# è¨“ç·´ãƒ«ãƒ¼ãƒ—
model = FeedbackClassifier(384, 10)
ps, st = Lux.setup(rng, model)
opt_state = Optimisers.setup(AdamW(1e-4), ps)

for epoch in 1:10
    for (embeddings, labels) in feedback_dataloader
        # Forward + Backward
        loss, grads = Zygote.withgradient(ps) do p
            logits, _ = model(embeddings, p, st)
            cross_entropy_loss(logits, labels)
        end

        # Update
        opt_state, ps = Optimisers.update(opt_state, ps, grads[1])
    end
end
```

**æ¥ç¶šå›³**:

```mermaid
graph LR
    A[ãƒ¦ãƒ¼ã‚¶ãƒ¼å•ã„åˆã‚ã›] --> B[Embedding]
    B --> C[åˆ†é¡ãƒ¢ãƒ‡ãƒ«]
    C --> D{ä¿¡é ¼åº¦ > 0.7?}
    D -->|Yes| E[è‡ªå‹•å¿œç­”]
    D -->|No| F[äººé–“ã«ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³]
    E --> G[ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†]
    F --> G
    G --> H[ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†æ]
    H --> I[ãƒ¢ãƒ‡ãƒ«æ”¹å–„]
    I --> C
```

:::message
**é€²æ—: 10%å®Œäº†ï¼** AIã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã®è¨­è¨ˆã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†ã®åŸºç¤ã‚’ä½“é¨“ã—ãŸã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœProductionã‚·ã‚¹ãƒ†ãƒ ãŒå¿…è¦ã‹

### 2.1 Course IIIã®åœ°å›³: ç¬¬19-32å›ã®æŒ¯ã‚Šè¿”ã‚Š

Course IIIã¯**ç†è«–ã‚’å‹•ãã‚·ã‚¹ãƒ†ãƒ ã«å¤‰ãˆã‚‹14å›**ã ã£ãŸã€‚å„è¬›ç¾©ã‚’æŒ¯ã‚Šè¿”ã‚ã†ã€‚

| å› | ã‚¿ã‚¤ãƒˆãƒ« | ç²å¾—ã—ãŸæ­¦å™¨ | è¨€èª |
|:---|:---------|:-------------|:-----|
| ç¬¬19å› | ç’°å¢ƒæ§‹ç¯‰ & FFI | FFIå¢ƒç•Œè¨­è¨ˆ / C-ABIçµ±ä¸€ç†è«– | ğŸ¦€âš¡ğŸ”® |
| ç¬¬20å› | å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ | VAE/GAN/Transformerå®Ÿè£…ã®å‹ | âš¡ğŸ¦€ |
| ç¬¬21å› | ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ | ETL/ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°/å¯è¦–åŒ– | âš¡ |
| ç¬¬22å› | ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ« | VLM/ç”»åƒ-ãƒ†ã‚­ã‚¹ãƒˆçµ±åˆ | âš¡ğŸ¦€ |
| ç¬¬23å› | Fine-tuning & PEFT | LoRA/QLoRA/AdaLoRA | âš¡ğŸ¦€ |
| ç¬¬24å› | çµ±è¨ˆå­¦ | ä»®èª¬æ¤œå®š/A/Bãƒ†ã‚¹ãƒˆ/ä¿¡é ¼åŒºé–“ | âš¡ |
| ç¬¬25å› | å› æœæ¨è«– | RCT/DID/IV/å‚¾å‘ã‚¹ã‚³ã‚¢ | âš¡ |
| ç¬¬26å› | æ¨è«–æœ€é©åŒ– | é‡å­åŒ–/è’¸ç•™/ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° | ğŸ¦€âš¡ |
| ç¬¬27å› | è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ | FID/CLIP Score/Human Eval | âš¡ |
| ç¬¬28å› | ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ | Few-shot/CoT/ReAct/Self-Consistency | âš¡ |
| ç¬¬29å› | RAG | Retrieval/Rerank/Hybrid Search | âš¡ğŸ¦€ |
| ç¬¬30å› | ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ | ReAct/Tool Use/Multi-Agent | ğŸ”®âš¡ |
| ç¬¬31å› | MLOps | CI/CD/Monitoring/A/Bãƒ†ã‚¹ãƒˆ | ğŸ¦€âš¡ğŸ”® |
| **ç¬¬32å›** | **Productionçµ±åˆ** | **E2Eã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰** | **ğŸ¦€âš¡ğŸ”®** |

**å…¨ã¦ã‚’çµ±åˆã—ãŸã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**:

```mermaid
graph TD
    A[ãƒ‡ãƒ¼ã‚¿åé›†] --> B[âš¡ Juliaè¨“ç·´PL]
    B --> C[ãƒ¢ãƒ‡ãƒ«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ ONNX]
    C --> D[ğŸ¦€ Rustæ¨è«–ã‚µãƒ¼ãƒãƒ¼]
    D --> E[ğŸ”® Elixir APIã‚²ãƒ¼ãƒˆã‚¦ã‚§ã‚¤]
    E --> F[ãƒ¦ãƒ¼ã‚¶ãƒ¼]
    F --> G[ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†]
    G --> H[ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†æ]
    H --> A
    D --> I[ğŸ“Š Monitoring Prometheus]
    E --> I
    I --> J[ã‚¢ãƒ©ãƒ¼ãƒˆ & ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰]
```

### 2.2 Productionã®æœ¬è³ª: Trainâ†’Feedbacké–‰ãƒ«ãƒ¼ãƒ—

Productionã‚·ã‚¹ãƒ†ãƒ ã®æœ¬è³ªã¯**é–‰ãƒ«ãƒ¼ãƒ—**ã ã€‚

**å¾“æ¥ã®MLé–‹ç™º** (é–‹ãƒ«ãƒ¼ãƒ—):
```
ãƒ‡ãƒ¼ã‚¿åé›† â†’ è¨“ç·´ â†’ è©•ä¾¡ â†’ ãƒ‡ãƒ—ãƒ­ã‚¤ â†’ [çµ‚äº†]
```

**Productionã‚·ã‚¹ãƒ†ãƒ ** (é–‰ãƒ«ãƒ¼ãƒ—):
```
ãƒ‡ãƒ¼ã‚¿åé›† â†’ è¨“ç·´ â†’ è©•ä¾¡ â†’ ãƒ‡ãƒ—ãƒ­ã‚¤ â†’ Feedbackåé›† â†º
                                          â†“
                                      åˆ†æ & æ”¹å–„
```

**é–‰ãƒ«ãƒ¼ãƒ—ã®æ•°å¼**:

$$
\begin{aligned}
\text{Epoch } t&: \theta_t \leftarrow \arg\min_\theta \mathcal{L}(\theta; \mathcal{D}_{\text{train}}) \\
\text{Deploy}&: \text{Model}_t \text{ serves users} \\
\text{Collect}&: \mathcal{D}_{\text{feedback}} \leftarrow \{ (x_i, y_i^{\text{feedback}}) \}_{i=1}^N \\
\text{Epoch } t+1&: \theta_{t+1} \leftarrow \arg\min_\theta \mathcal{L}(\theta; \mathcal{D}_{\text{train}} \cup \mathcal{D}_{\text{feedback}})
\end{aligned}
$$

**ãªãœé–‰ãƒ«ãƒ¼ãƒ—ãŒå¿…è¦ã‹ï¼Ÿ**

1. **ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆ**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•ã¯æ™‚é–“ã¨ã¨ã‚‚ã«å¤‰åŒ–ã™ã‚‹
2. **åˆ†å¸ƒã‚·ãƒ•ãƒˆ**: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨æœ¬ç•ªãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒãŒç•°ãªã‚‹
3. **ç¶™ç¶šçš„æ”¹å–„**: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æ´»ç”¨ã—ã¦æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹

### 2.3 æ¾å°¾ç ”ã¨ã®å¯¾æ¯”

| é …ç›® | æ¾å°¾ç ” (æ•™ç§‘æ›¸ãƒ¬ãƒ™ãƒ«) | æœ¬ã‚·ãƒªãƒ¼ã‚º Course III |
|:-----|:---------------------|:---------------------|
| **è¨“ç·´** | PyTorchã§è¨“ç·´ | âš¡ Juliaé«˜é€Ÿè¨“ç·´ (ç¬¬20å›) |
| **æ¨è«–** | Pythonã§æ¨è«– | ğŸ¦€ Rusté«˜é€Ÿæ¨è«– (ç¬¬26å›) |
| **é…ä¿¡** | Flask/FastAPI | ğŸ”® Elixiråˆ†æ•£é…ä¿¡ (ç¬¬30å›) |
| **ç›£è¦–** | ãªã— | Prometheus/Grafana (ç¬¬31å›) |
| **ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯** | ãªã— | **Active Learning + HITL** (ç¬¬32å›) |
| **E2Eçµ±åˆ** | ãªã— | **å…¨è¨€èªçµ±åˆã‚·ã‚¹ãƒ†ãƒ ** (ç¬¬32å›) |

**æ¾å°¾ç ”ãŒæ•™ãˆãªã„ã“ã¨**:
- 3è¨€èªçµ±åˆ (ğŸ¦€âš¡ğŸ”®)
- Productionå“è³ªè¨­è¨ˆ (ç¬¬26å›ã®æ¨è«–æœ€é©åŒ–, ç¬¬31å›ã®MLOps)
- ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ— (ç¬¬32å›)
- E2Eã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰ (ç¬¬32å›)

### 2.4 3ã¤ã®æ¯”å–©ã§æ‰ãˆã‚‹ã€ŒProductionã€

**æ¯”å–©1: ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³çµŒå–¶**
- è¨“ç·´ = ãƒ¬ã‚·ãƒ”é–‹ç™º
- æ¨è«– = æ–™ç†æä¾›
- ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ = é¡§å®¢ãƒ¬ãƒ“ãƒ¥ãƒ¼
- æ”¹å–„ = ãƒ¬ã‚·ãƒ”æ”¹è‰¯

**æ¯”å–©2: è‡ªå‹•è»Šè£½é€ **
- è¨“ç·´ = è©¦ä½œè»Šé–‹ç™º
- æ¨è«– = é‡ç”£ãƒ©ã‚¤ãƒ³
- ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ = å“è³ªæ¤œæŸ» + é¡§å®¢ã‚¯ãƒ¬ãƒ¼ãƒ 
- æ”¹å–„ = è¨­è¨ˆå¤‰æ›´

**æ¯”å–©3: ç”Ÿæ…‹ç³»**
- è¨“ç·´ = ç¨®ã®é€²åŒ–
- æ¨è«– = å€‹ä½“ã®ç”Ÿå­˜
- ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ = è‡ªç„¶é¸æŠ
- æ”¹å–„ = é©å¿œé€²åŒ–

**Productionã®3æ¯”å–©ãŒç¤ºã™ã“ã¨**:
1. **ç¶™ç¶šçš„ãƒ—ãƒ­ã‚»ã‚¹**: ä¸€åº¦ä½œã£ã¦çµ‚ã‚ã‚Šã§ã¯ãªã„
2. **ç’°å¢ƒé©å¿œ**: å¤–éƒ¨ç’°å¢ƒã®å¤‰åŒ–ã«å¯¾å¿œã™ã‚‹
3. **ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é§†å‹•**: ãƒ‡ãƒ¼ã‚¿ãŒæ”¹å–„ã‚’å°ã

### 2.5 Trojan Horse: ğŸâ†’ğŸ¦€â†’âš¡â†’ğŸ”® å®Œå…¨çµ±åˆ

ç¬¬9å›ã§RustãŒç™»å ´ã—ã€ç¬¬10å›ã§JuliaãŒç™»å ´ã—ã€ç¬¬19å›ã§ElixirãŒç™»å ´ã—ãŸã€‚**3è¨€èªãŒæƒã£ãŸä»Šã€ãã‚Œãã‚Œã®å½¹å‰²ãŒæ˜ç¢ºã«ãªã£ãŸ**ã€‚

| è¨€èª | å½¹å‰² | ç†ç”± | ç™»å ´å› |
|:-----|:-----|:-----|:-------|
| ğŸ¦€ Rust | æ¨è«–ãƒ»ã‚¤ãƒ³ãƒ•ãƒ©ãƒ»æœ¬ç•ª | ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ / å‹å®‰å…¨ / é«˜é€Ÿ | ç¬¬9å› |
| âš¡ Julia | ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ãƒ»è¨“ç·´ | æ•°å¼â†”ã‚³ãƒ¼ãƒ‰1:1 / å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒ | ç¬¬10å› |
| ğŸ”® Elixir | åˆ†æ•£é…ä¿¡ãƒ»è€éšœå®³æ€§ | OTP / Actor / let it crash | ç¬¬19å› |
| ğŸ Python | æŸ»èª­ç”¨ (èª­ã‚€ã ã‘) | ç ”ç©¶è€…ã®ã‚³ãƒ¼ãƒ‰ç†è§£ | ç¬¬1å› |

**ç¬¬32å›ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸**: **Pythonã¯å’æ¥­ã—ãŸ**ã€‚Productionç’°å¢ƒã§ã¯ğŸ¦€âš¡ğŸ”®ãŒå½“ãŸã‚Šå‰ã€‚

:::message
**é€²æ—: 20%å®Œäº†ï¼** Productionã‚·ã‚¹ãƒ†ãƒ ã®å…¨ä½“åƒã¨Course IIIã®ä½ç½®ã¥ã‘ã‚’ç†è§£ã—ãŸã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ— & Active Learningç†è«–

### 3.1 ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—ã®æ•°å¼åŒ–

#### 3.1.1 æš—é»™çš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®å®šå¼åŒ–

æš—é»™çš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¯**ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•ã‹ã‚‰é–“æ¥çš„ã«å“è³ªã‚’æ¨å®š**ã™ã‚‹ã€‚

**å®šç¾©**: ã‚¯ãƒªãƒƒã‚¯ã‚¹ãƒ«ãƒ¼ç‡ (CTR) ã®è¨ˆç®—

$$
\text{CTR} = \frac{\text{ã‚¯ãƒªãƒƒã‚¯æ•°}}{\text{è¡¨ç¤ºå›æ•°}}
$$

**æ»åœ¨æ™‚é–“ãƒ¢ãƒ‡ãƒ«**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒ $t$ ç§’æ»åœ¨ã—ãŸå ´åˆã®æº€è¶³åº¦

$$
s_{\text{dwell}}(t) = 1 - \exp(-\lambda t)
$$

ã“ã“ã§ $\lambda > 0$ ã¯æ¸›è¡°ç‡ã€‚$t \to \infty$ ã§ $s \to 1$ã€$t=0$ ã§ $s=0$ã€‚

**ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«æ·±åº¦ãƒ¢ãƒ‡ãƒ«**: ãƒšãƒ¼ã‚¸ã® $d \in [0,1]$ ã¾ã§è¦‹ãŸå ´åˆã®æº€è¶³åº¦

$$
s_{\text{scroll}}(d) = d
$$

**çµ±åˆã‚¹ã‚³ã‚¢**: 3ã¤ã®æŒ‡æ¨™ã‚’é‡ã¿ä»˜ãå’Œã§çµåˆ

$$
f_{\text{implicit}}(\text{click}, t, d) = w_1 \cdot \mathbb{1}_{\text{click}} + w_2 \cdot s_{\text{dwell}}(t) + w_3 \cdot s_{\text{scroll}}(d)
$$

å…¸å‹çš„ãªé‡ã¿: $w_1=0.4, w_2=0.4, w_3=0.2$ã€‚

**æ•°å€¤æ¤œè¨¼** (Julia):

```julia
Î» = 0.05  # 20ç§’ã§ s â‰ˆ 0.63
s_dwell(t) = 1 - exp(-Î» * t)

# æ»åœ¨æ™‚é–“45.3ç§’ã€ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«78%ã€ã‚¯ãƒªãƒƒã‚¯ã‚ã‚Š
t = 45.3
d = 0.78
click = 1

s_t = s_dwell(t)  # â‰ˆ 0.90
score = 0.4 * click + 0.4 * s_t + 0.2 * d
# => 0.4 + 0.36 + 0.156 = 0.916
```

#### 3.1.2 æ˜ç¤ºçš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®å®šå¼åŒ–

æ˜ç¤ºçš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¯**ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç›´æ¥è©•ä¾¡ã‚’å…¥åŠ›**ã™ã‚‹ã€‚

**è©•ä¾¡ã‚¹ã‚³ã‚¢æ­£è¦åŒ–**:

$$
r_{\text{norm}} = \frac{r - r_{\min}}{r_{\max} - r_{\min}}
$$

5æ®µéšè©•ä¾¡ (1-5) ã®å ´åˆ: $r_{\text{norm}} = (r-1)/4$ã€‚

**ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ**: ã‚³ãƒ¡ãƒ³ãƒˆ $c$ ã‹ã‚‰æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ $S(c) \in [-1, 1]$ ã‚’æŠ½å‡º

$$
S(c) = \text{Classifier}_{\text{sentiment}}(\text{Embedding}(c))
$$

Transformerãƒ™ãƒ¼ã‚¹ã®ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†é¡å™¨ã‚’ä½¿ç”¨ã€‚

**Net Promoter Score (NPS)**: é¡§å®¢ãƒ­ã‚¤ãƒ¤ãƒ«ãƒ†ã‚£æŒ‡æ¨™

$$
\text{NPS} = \frac{\text{æ¨å¥¨è€… (9-10ç‚¹)} - \text{æ‰¹åˆ¤è€… (0-6ç‚¹)}}{\text{ç·å›ç­”æ•°}} \times 100
$$

**çµ±åˆãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚¹ã‚³ã‚¢**:

$$
f_{\text{explicit}}(r, S(c), \text{NPS}) = \alpha r_{\text{norm}} + \beta S(c) + \gamma \frac{\text{NPS}}{100}
$$

å…¸å‹çš„ãªé‡ã¿: $\alpha=0.5, \beta=0.3, \gamma=0.2$ã€‚

#### 3.1.3 ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é§†å‹•ã®ç¶™ç¶šå­¦ç¿’

ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ä½¿ã£ãŸãƒ¢ãƒ‡ãƒ«æ›´æ–°ã®æ•°å¼ã€‚

**ç›®çš„é–¢æ•°**: å…ƒã®è¨“ç·´æå¤±ã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æå¤±ã®é‡ã¿ä»˜ãå’Œ

$$
\mathcal{L}_{\text{total}}(\theta) = \mathcal{L}_{\text{train}}(\theta; \mathcal{D}_{\text{train}}) + \lambda \mathcal{L}_{\text{feedback}}(\theta; \mathcal{D}_{\text{feedback}})
$$

ã“ã“ã§ $\lambda > 0$ ã¯ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®é‡è¦åº¦ã€‚

**ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æå¤±**: ãƒ¦ãƒ¼ã‚¶ãƒ¼è©•ä¾¡ã¨ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ã®å·®

$$
\mathcal{L}_{\text{feedback}}(\theta; \mathcal{D}_{\text{feedback}}) = \frac{1}{|\mathcal{D}_{\text{feedback}}|} \sum_{(x,y,f) \in \mathcal{D}_{\text{feedback}}} \ell(f_\theta(x), y) \cdot w(f)
$$

ã“ã“ã§:
- $f_\theta(x)$ ã¯ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬
- $y$ ã¯æ­£è§£ãƒ©ãƒ™ãƒ«
- $f$ ã¯ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚¹ã‚³ã‚¢
- $w(f)$ ã¯ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«åŸºã¥ãé‡ã¿: $w(f) = f$ (é«˜è©•ä¾¡ã»ã©é‡è¦–)

**å‹¾é…é™ä¸‹æ›´æ–°**:

$$
\theta_{t+1} \leftarrow \theta_t - \eta \nabla_\theta \mathcal{L}_{\text{total}}(\theta_t)
$$

### 3.2 Active Learningå®Œå…¨ç‰ˆ

#### 3.2.1 ä¸ç¢ºå®Ÿæ€§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ç†è«–

Active Learningã®ç›®æ¨™: **æœ€å°ã®ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚³ã‚¹ãƒˆã§æœ€å¤§ã®æ€§èƒ½å‘ä¸Š**ã‚’é”æˆã™ã‚‹ã€‚

**ä¸ç¢ºå®Ÿæ€§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**: ãƒ¢ãƒ‡ãƒ«ãŒæœ€ã‚‚è‡ªä¿¡ã‚’æŒã¦ãªã„ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠ

$$
x^* = \arg\max_{x \in \mathcal{U}} U(x; \theta)
$$

ã“ã“ã§ $\mathcal{U}$ ã¯ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿ã€$U(x; \theta)$ ã¯ä¸ç¢ºå®Ÿæ€§æŒ‡æ¨™ã€‚

**3ã¤ã®ä¸ç¢ºå®Ÿæ€§æŒ‡æ¨™**:

1. **Least Confidence**: æœ€å¤§ç¢ºç‡ãŒä½ã„ã‚µãƒ³ãƒ—ãƒ«

$$
U_{\text{LC}}(x; \theta) = 1 - \max_c p_\theta(c | x)
$$

2. **Margin Sampling**: ä¸Šä½2ã‚¯ãƒ©ã‚¹ã®ç¢ºç‡å·®ãŒå°ã•ã„ã‚µãƒ³ãƒ—ãƒ«

$$
U_{\text{M}}(x; \theta) = - \left( p_\theta(c_1 | x) - p_\theta(c_2 | x) \right)
$$

ã“ã“ã§ $c_1, c_2$ ã¯ç¢ºç‡ä¸Šä½2ã‚¯ãƒ©ã‚¹ã€‚

3. **Entropy**: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãŒæœ€å¤§ã®ã‚µãƒ³ãƒ—ãƒ«

$$
U_{\text{Ent}}(x; \theta) = H(p_\theta(\cdot | x)) = - \sum_{c=1}^C p_\theta(c | x) \log p_\theta(c | x)
$$

**ã©ã‚Œã‚’ä½¿ã†ã¹ãã‹ï¼Ÿ**

| æŒ‡æ¨™ | é•·æ‰€ | çŸ­æ‰€ | é©ç”¨å ´é¢ |
|:-----|:-----|:-----|:---------|
| Least Confidence | è¨ˆç®—ãŒè»½ã„ | 2ç•ªç›®ã®ç¢ºç‡ã‚’ç„¡è¦– | 2ã‚¯ãƒ©ã‚¹åˆ†é¡ |
| Margin | æ±ºå®šå¢ƒç•Œã‚’é‡è¦– | å¤šã‚¯ãƒ©ã‚¹ã§æƒ…å ±æå¤± | 2ã‚¯ãƒ©ã‚¹ or ãƒãƒ©ãƒ³ã‚¹è‰¯å¥½ |
| Entropy | å…¨ã‚¯ãƒ©ã‚¹ã®æƒ…å ±ã‚’ä½¿ã† | è¨ˆç®—ã‚³ã‚¹ãƒˆã‚„ã‚„é«˜ | å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ |

**æ•°å€¤æ¤œè¨¼** (Julia):

```julia
# 3ã‚¯ãƒ©ã‚¹åˆ†é¡ã®ä¾‹
p = [0.6, 0.3, 0.1]  # ã‚¯ãƒ©ã‚¹ç¢ºç‡

# Least Confidence
U_LC = 1 - maximum(p)  # => 0.4

# Margin
p_sorted = sort(p, rev=true)
U_M = -(p_sorted[1] - p_sorted[2])  # => -(0.6 - 0.3) = -0.3

# Entropy
H(p) = -sum(p .* log.(p .+ 1e-10))
U_Ent = H(p)  # => 0.897

println("LC: $U_LC, Margin: $U_M, Entropy: $U_Ent")
```

#### 3.2.2 MSAL (Maximally Separated Active Learning)

arXiv:2411.17444 "Maximally Separated Active Learning" (Nov 2024)[^1] ã§ææ¡ˆã•ã‚ŒãŸæ‰‹æ³•ã€‚

**èª²é¡Œ**: å¾“æ¥ã®ä¸ç¢ºå®Ÿæ€§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¯**é¡ä¼¼ã—ãŸã‚µãƒ³ãƒ—ãƒ«ã°ã‹ã‚Šé¸ã‚“ã§ã—ã¾ã†** (sampling bias)ã€‚

**è§£æ±ºç­–**: ä¸ç¢ºå®Ÿæ€§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«**å¤šæ§˜æ€§åˆ¶ç´„**ã‚’è¿½åŠ ã€‚

**MSALç›®çš„é–¢æ•°**:

$$
x^* = \arg\max_{x \in \mathcal{U}} \left[ U(x; \theta) + \alpha \cdot D(x; \mathcal{L}) \right]
$$

ã“ã“ã§:
- $U(x; \theta)$ ã¯ä¸ç¢ºå®Ÿæ€§ã‚¹ã‚³ã‚¢
- $D(x; \mathcal{L})$ ã¯æ—¢ã«ãƒ©ãƒ™ãƒ«ä»˜ã‘ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ $\mathcal{L}$ ã¨ã®å¤šæ§˜æ€§
- $\alpha > 0$ ã¯å¤šæ§˜æ€§ã®é‡è¦åº¦

**å¤šæ§˜æ€§ã‚¹ã‚³ã‚¢**: æœ€è¿‘å‚ã¨ã®è·é›¢

$$
D(x; \mathcal{L}) = \min_{x' \in \mathcal{L}} \left\| \phi(x) - \phi(x') \right\|_2
$$

ã“ã“ã§ $\phi(x)$ ã¯Embedding (ä¾‹: BERTæœ€çµ‚å±¤)ã€‚

**Equiangular Prototypes**: MSALã¯å„ã‚¯ãƒ©ã‚¹ã®**ç­‰è§’è¶…çƒé¢ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—**ã‚’ä½¿ã†ã€‚

$C$ ã‚¯ãƒ©ã‚¹ã®å ´åˆã€$d$ æ¬¡å…ƒçƒé¢ä¸Šã« $C$ å€‹ã®ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã‚’ç­‰é–“éš”é…ç½®:

$$
\mathbf{p}_c = r \cdot \mathbf{v}_c, \quad \mathbf{v}_c \cdot \mathbf{v}_{c'} = \begin{cases} 1 & c = c' \\ -\frac{1}{C-1} & c \neq c' \end{cases}
$$

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

```julia
function msal_select_batch(model, unlabeled_pool, labeled_data, batch_size, Î±=0.5)
    selected = []

    for _ in 1:batch_size
        scores = []
        for x in unlabeled_pool
            # ä¸ç¢ºå®Ÿæ€§ã‚¹ã‚³ã‚¢
            U = entropy(model(x))

            # å¤šæ§˜æ€§ã‚¹ã‚³ã‚¢: æ—¢é¸æŠã‚µãƒ³ãƒ—ãƒ«ã¨ã®æœ€å°è·é›¢
            Ï†_x = embedding(x)
            D = minimum([norm(Ï†_x - embedding(x')) for x' in labeled_data âˆª selected])

            # çµ±åˆã‚¹ã‚³ã‚¢
            score = U + Î± * D
            push!(scores, (x, score))
        end

        # æœ€é«˜ã‚¹ã‚³ã‚¢ã‚’é¸æŠ
        x_best = argmax(s -> s[2], scores)[1]
        push!(selected, x_best)
        unlabeled_pool = filter(x -> x != x_best, unlabeled_pool)
    end

    return selected
end
```

#### 3.2.3 Human-in-the-Loop (HITL) è¨­è¨ˆ

arXiv:2409.09467 "Keeping Humans in the Loop" (Sep 2024)[^2] ã§è­°è«–ã•ã‚ŒãŸãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã€‚

**HITLã®3åŸå‰‡**:

1. **Selective Annotation**: äººé–“ã¯é›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã®ã¿ã‚¢ãƒãƒ†ãƒ¼ãƒˆ
2. **Quality Control**: è¤‡æ•°ã‚¢ãƒãƒ†ãƒ¼ã‚¿ãƒ¼é–“ã®ä¸€è‡´åº¦ã‚’æ¸¬å®š
3. **Feedback Integration**: ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å³åº§ã«è¨“ç·´ã«åæ˜ 

**ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å“è³ªã®å®šé‡åŒ–**: Cohen's Kappa

$$
\kappa = \frac{p_o - p_e}{1 - p_e}
$$

ã“ã“ã§:
- $p_o$ ã¯è¦³æ¸¬ä¸€è‡´ç‡
- $p_e$ ã¯å¶ç„¶ã®ä¸€è‡´ç‡

$\kappa > 0.6$ ã§ã€Œå®Ÿè³ªçš„ãªä¸€è‡´ã€ã€$\kappa > 0.8$ ã§ã€Œã»ã¼å®Œå…¨ãªä¸€è‡´ã€ã€‚

**Disagreement Resolution**: 2äººã®ã‚¢ãƒãƒ†ãƒ¼ã‚¿ãƒ¼ãŒç•°ãªã‚‹ãƒ©ãƒ™ãƒ«ã‚’ä»˜ã‘ãŸå ´åˆ

```julia
function resolve_disagreement(x, label_A, label_B, model)
    if label_A == label_B
        return label_A  # ä¸€è‡´
    else
        # ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’å‚è€ƒã«å°‚é–€å®¶ãŒåˆ¤æ–­
        pred = model(x)
        println("Disagreement: A=$label_A, B=$label_B, Model=$pred")
        return expert_review(x, label_A, label_B, pred)
    end
end
```

**å°‚é–€å®¶ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°**:

| æ¡ä»¶ | ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ |
|:-----|:----------|
| $\kappa < 0.6$ | å…¨ã‚µãƒ³ãƒ—ãƒ«ã‚’å°‚é–€å®¶ãƒ¬ãƒ“ãƒ¥ãƒ¼ |
| $0.6 \leq \kappa < 0.8$ | Disagreementã®ã¿ãƒ¬ãƒ“ãƒ¥ãƒ¼ |
| $\kappa \geq 0.8$ | ãƒ¬ãƒ“ãƒ¥ãƒ¼ä¸è¦ |

#### 3.2.4 âš”ï¸ Boss Battle: Active LearningåæŸä¿è¨¼

arXiv:2110.15784 "Convergence of Uncertainty Sampling" (Oct 2021)[^3] ã®å®šç†ã‚’å®Œå…¨ç†è§£ã™ã‚‹ã€‚

**å®šç† (Simplified)**: ã‚ã‚‹æ¡ä»¶ä¸‹ã§ã€ä¸ç¢ºå®Ÿæ€§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¯**æœ€é©æ±ºå®šå¢ƒç•Œã«åæŸ**ã™ã‚‹ã€‚

**ä»®å®š**:
1. ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p(x, y)$ ã¯å›ºå®š
2. ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹ $\mathcal{F}$ ã¯ååˆ†ãªè¡¨ç¾åŠ›ã‚’æŒã¤ (VCæ¬¡å…ƒ $d_{VC} < \infty$)
3. ã‚µãƒ³ãƒ—ãƒ«é¸æŠã¯æ±ºå®šå¢ƒç•Œä»˜è¿‘ã«é›†ä¸­

**åæŸãƒ¬ãƒ¼ãƒˆ**: $T$ ãƒ©ã‚¦ãƒ³ãƒ‰å¾Œã®èª¤å·®

$$
\mathbb{E}[\text{Error}(\theta_T)] \leq \mathcal{O}\left( \frac{d_{VC}}{T} \log T \right)
$$

ã“ã“ã§ $d_{VC}$ ã¯VCæ¬¡å…ƒã€‚

**è¨¼æ˜ã®ã‚¹ã‚±ãƒƒãƒ**:

1. **æ±ºå®šå¢ƒç•Œã®å®šç¾©**: $\{ x : p_\theta(c_1 | x) = p_\theta(c_2 | x) \}$
2. **ä¸ç¢ºå®Ÿæ€§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®æ€§è³ª**: Entropyæœ€å¤§ = æ±ºå®šå¢ƒç•Œä¸Š
3. **PACå­¦ç¿’ç†è«–**: $N$ ã‚µãƒ³ãƒ—ãƒ«ã§èª¤å·® $\epsilon$ ä»¥ä¸‹ã«ãªã‚‹ç¢ºç‡

$$
P(\text{Error}(\theta) > \epsilon) \leq 2 \mathcal{M}(\mathcal{F}, N) e^{-N \epsilon^2 / 8}
$$

ã“ã“ã§ $\mathcal{M}(\mathcal{F}, N)$ ã¯æˆé•·é–¢æ•°ã€‚

4. **VCæ¬¡å…ƒã¨ã®é–¢ä¿‚**: $\mathcal{M}(\mathcal{F}, N) \leq N^{d_{VC}}$
5. **çµè«–**: $N = \mathcal{O}(d_{VC} / \epsilon^2 \log(1/\delta))$ ã‚µãƒ³ãƒ—ãƒ«ã§ååˆ†

**æ•°å€¤æ¤œè¨¼** (Julia):

```julia
# ç·šå½¢åˆ†é¡å™¨ (VCæ¬¡å…ƒ = d+1)
d = 10  # ç‰¹å¾´é‡æ¬¡å…ƒ
d_VC = d + 1

# ç›®æ¨™èª¤å·® Îµ = 0.01, ç¢ºç‡ Î´ = 0.05
Îµ = 0.01
Î´ = 0.05

# å¿…è¦ã‚µãƒ³ãƒ—ãƒ«æ•°
N_required = ceil(Int, d_VC / Îµ^2 * log(1/Î´))
# => ç´„ 32,919 ã‚µãƒ³ãƒ—ãƒ«

println("VCæ¬¡å…ƒ: $d_VC")
println("å¿…è¦ã‚µãƒ³ãƒ—ãƒ«æ•°: $N_required")
```

**ãƒœã‚¹æ’ƒç ´ã®è¨¼**: ä¸ç¢ºå®Ÿæ€§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®åæŸãƒ¬ãƒ¼ãƒˆ $\mathcal{O}(d_{VC}/T \log T)$ ã‚’å°å‡ºã—ã€æ•°å€¤æ¤œè¨¼ã§ç¢ºèªã—ãŸã€‚

### 3.3 ãƒ¢ãƒ‡ãƒ«æ”¹å–„ã‚µã‚¤ã‚¯ãƒ«ã®æ•°å¼

#### 3.3.1 Continuous Learning (ç¶™ç¶šå­¦ç¿’)

**å®šç¾©**: æœ¬ç•ªç’°å¢ƒã§ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ä½¿ã£ã¦**ãƒ¢ãƒ‡ãƒ«ã‚’ç¶™ç¶šçš„ã«æ›´æ–°**ã™ã‚‹ã€‚

**Naive Approach** (ç ´æ»…çš„å¿˜å´):

$$
\theta_{t+1} \leftarrow \arg\min_\theta \mathcal{L}(\theta; \mathcal{D}_{\text{new}})
$$

å•é¡Œ: å¤ã„ãƒ‡ãƒ¼ã‚¿ $\mathcal{D}_{\text{old}}$ ã®æ€§èƒ½ãŒåŠ£åŒ– (Catastrophic Forgetting)ã€‚

**Elastic Weight Consolidation (EWC)**: é‡è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¤‰åŒ–ã‚’æŠ‘åˆ¶

$$
\mathcal{L}_{\text{EWC}}(\theta) = \mathcal{L}(\theta; \mathcal{D}_{\text{new}}) + \frac{\lambda}{2} \sum_i F_i (\theta_i - \theta_{i,\text{old}})^2
$$

ã“ã“ã§:
- $F_i$ ã¯Fisheræƒ…å ±é‡: $F_i = \mathbb{E}_{x \sim \mathcal{D}_{\text{old}}} \left[ \left( \frac{\partial \log p_{\theta_{\text{old}}}(y|x)}{\partial \theta_i} \right)^2 \right]$
- $\lambda > 0$ ã¯æ­£å‰‡åŒ–å¼·åº¦

**Experience Replay**: å¤ã„ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒãƒ•ã‚¡ã‚’ä¿æŒ

$$
\mathcal{L}_{\text{Replay}}(\theta) = \mathcal{L}(\theta; \mathcal{D}_{\text{new}} \cup \mathcal{D}_{\text{buffer}})
$$

ã“ã“ã§ $\mathcal{D}_{\text{buffer}}$ ã¯å¤ã„ãƒ‡ãƒ¼ã‚¿ã®ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒ«ã€‚

**ã©ã¡ã‚‰ã‚’ä½¿ã†ã¹ãã‹ï¼Ÿ**

| æ‰‹æ³• | ãƒ¡ãƒ¢ãƒª | è¨ˆç®—é‡ | æ€§èƒ½ | é©ç”¨å ´é¢ |
|:-----|:------|:-------|:-----|:---------|
| EWC | å° (Fisheræƒ…å ±é‡ã®ã¿) | ä¸­ | ä¸­ | ãƒ¡ãƒ¢ãƒªåˆ¶ç´„ |
| Replay | å¤§ (ãƒãƒƒãƒ•ã‚¡ä¿æŒ) | å¤§ | é«˜ | é«˜æ€§èƒ½å„ªå…ˆ |

#### 3.3.2 Hidden Feedback Loop Effect

arXiv:2405.02726 "Mathematical Model of the Hidden Feedback Loop Effect"[^4] ã§è­°è«–ã•ã‚ŒãŸå•é¡Œã€‚

**å•é¡Œ**: ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ãŒæ¬¡ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹**éš ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—**ã€‚

**æ•°å¼ãƒ¢ãƒ‡ãƒ«**: æ™‚åˆ» $t$ ã§ã®ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p_t(x, y)$ ãŒå‰å›ã®ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ã«ä¾å­˜

$$
p_{t+1}(x, y) = (1 - \alpha) p_{\text{true}}(x, y) + \alpha \cdot \delta_{y = \hat{y}_t(x)} p_t(x)
$$

ã“ã“ã§:
- $p_{\text{true}}(x, y)$ ã¯çœŸã®åˆ†å¸ƒ
- $\hat{y}_t(x)$ ã¯æ™‚åˆ» $t$ ã®ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬
- $\alpha \in [0, 1]$ ã¯ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å¼·åº¦

**çµæœ**: $\alpha > 0.5$ ã§ãƒ¢ãƒ‡ãƒ«ãŒ**è‡ªå·±å¼·åŒ–ãƒã‚¤ã‚¢ã‚¹**ã«é™¥ã‚‹ã€‚

**æ•°å€¤ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³** (Julia):

```julia
# 2ã‚¯ãƒ©ã‚¹åˆ†é¡ã®ä¾‹
p_true = [0.5, 0.5]  # çœŸã®åˆ†å¸ƒ
Î± = 0.6  # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å¼·åº¦

p_t = copy(p_true)
for t in 1:10
    # ãƒ¢ãƒ‡ãƒ«ã¯å¸¸ã«ã‚¯ãƒ©ã‚¹1ã‚’äºˆæ¸¬ (simplified)
    y_pred = 1

    # æ¬¡ã®åˆ†å¸ƒ: ã‚¯ãƒ©ã‚¹1ãŒå¢—ãˆã‚‹
    p_t = (1 - Î±) .* p_true + Î± .* [y_pred == 1 ? 1.0 : 0.0, y_pred == 2 ? 1.0 : 0.0]

    println("t=$t: p(y=1)=$(p_t[1])")
end
# => t=10: p(y=1) â‰ˆ 0.94 (å¤§ããåã‚‹)
```

**å¯¾ç­–**: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å¼·åº¦ $\alpha$ ã‚’åˆ¶å¾¡ or ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§çœŸã®åˆ†å¸ƒã‚’ä¿æŒã€‚

#### 3.3.3 RLHF (Reinforcement Learning from Human Feedback)

arXiv:2504.12501 "RLHF" (2025)[^5] ã§ä½“ç³»åŒ–ã•ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é§†å‹•è¨“ç·´ã€‚

**3ã‚¹ãƒ†ãƒƒãƒ—**:

1. **Supervised Fine-tuning (SFT)**: äººé–“ã®ä¾‹ã§äº‹å‰è¨“ç·´

$$
\theta_{\text{SFT}} \leftarrow \arg\min_\theta \mathbb{E}_{(x,y) \sim \mathcal{D}_{\text{demo}}} [- \log p_\theta(y | x)]
$$

2. **Reward Model Training**: äººé–“ã®å¥½ã¿ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–

$$
r_\phi(x, y) = \mathbb{E}_{\text{human}}[\text{preference}(x, y)]
$$

è¨“ç·´ãƒ‡ãƒ¼ã‚¿: $(x, y_w, y_l)$ (win/lose pair)

$$
\mathcal{L}_{\text{RM}}(\phi) = - \mathbb{E}_{(x,y_w,y_l)} \left[ \log \sigma(r_\phi(x, y_w) - r_\phi(x, y_l)) \right]
$$

3. **RL Fine-tuning**: Rewardæœ€å¤§åŒ–

$$
\theta_{\text{RL}} \leftarrow \arg\max_\theta \mathbb{E}_{x \sim \mathcal{D}, y \sim p_\theta(\cdot|x)} \left[ r_\phi(x, y) - \beta \log \frac{p_\theta(y|x)}{p_{\text{ref}}(y|x)} \right]
$$

ã“ã“ã§ $\beta > 0$ ã¯KLæ­£å‰‡åŒ–ä¿‚æ•°ã€$p_{\text{ref}}$ ã¯å‚ç…§ãƒ¢ãƒ‡ãƒ« (SFT)ã€‚

**PPO (Proximal Policy Optimization)** ã§RLã‚’å®‰å®šåŒ–:

$$
\mathcal{L}_{\text{PPO}}(\theta) = \mathbb{E}_t \left[ \min \left( \frac{p_\theta(a_t|s_t)}{p_{\theta_{\text{old}}}(a_t|s_t)} A_t, \text{clip}(\cdot, 1-\epsilon, 1+\epsilon) A_t \right) \right]
$$

ã“ã“ã§ $A_t$ ã¯Advantageã€$\epsilon=0.2$ ã¯å…¸å‹å€¤ã€‚

### 3.4 E2Eã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ç†è«–

#### 3.4.1 ã‚µãƒ¼ãƒ“ã‚¹é–“é€šä¿¡ã®æ•°å¼

**REST API**: ãƒªã‚¯ã‚¨ã‚¹ãƒˆ $r$ ã«å¯¾ã™ã‚‹ãƒ¬ã‚¹ãƒãƒ³ã‚¹ $s$

$$
s = f_{\text{API}}(r; \theta)
$$

**ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·**: å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å‡¦ç†æ™‚é–“ã®å’Œ

$$
t_{\text{total}} = t_{\text{gateway}} + t_{\text{inference}} + t_{\text{postprocess}}
$$

**ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**: å˜ä½æ™‚é–“ã‚ãŸã‚Šã®å‡¦ç†æ•°

$$
\text{Throughput} = \frac{1}{t_{\text{total}} + t_{\text{queue}}}
$$

ã“ã“ã§ $t_{\text{queue}}$ ã¯ã‚­ãƒ¥ãƒ¼ã‚¤ãƒ³ã‚°æ™‚é–“ã€‚

**Little's Law**: å¹³å‡ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•° $L$ã€å¹³å‡åˆ°ç€ç‡ $\lambda$ã€å¹³å‡å‡¦ç†æ™‚é–“ $W$

$$
L = \lambda W
$$

ä¾‹: $\lambda = 100$ req/sã€$W = 0.05$ s â†’ $L = 5$ ä¸¦è¡Œãƒªã‚¯ã‚¨ã‚¹ãƒˆã€‚

#### 3.4.2 Circuit Breakerç†è«–

**çŠ¶æ…‹é·ç§»**:

```
Closed â†’ (å¤±æ•—ç‡ > threshold) â†’ Open â†’ (timeoutçµŒé) â†’ Half-Open â†’ (æˆåŠŸ) â†’ Closed
```

**æ•°å¼ãƒ¢ãƒ‡ãƒ«**: å¤±æ•—ç‡ $p_{\text{fail}}$ã€é–¾å€¤ $\theta_{\text{CB}}$

$$
\text{State} = \begin{cases}
\text{Open} & p_{\text{fail}} > \theta_{\text{CB}} \\
\text{Closed} & p_{\text{fail}} \leq \theta_{\text{CB}}
\end{cases}
$$

**Exponential Backoff**: OpençŠ¶æ…‹ã‹ã‚‰ã®å¾©å¸°æ™‚é–“

$$
t_{\text{wait}} = t_0 \cdot 2^n
$$

ã“ã“ã§ $n$ ã¯å¤±æ•—å›æ•°ã€$t_0$ ã¯åˆæœŸå¾…ã¡æ™‚é–“ã€‚

#### 3.4.3 Rate Limiting (Token Bucket)

**Token Bucket Algorithm**: å®¹é‡ $B$ã€è£œå……ãƒ¬ãƒ¼ãƒˆ $r$

$$
\text{tokens}(t) = \min(B, \text{tokens}(t-1) + r \Delta t - c)
$$

ã“ã“ã§ $c$ ã¯ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§æ¶ˆè²»ã—ãŸãƒˆãƒ¼ã‚¯ãƒ³æ•°ã€‚

**è¨±å¯æ¡ä»¶**:

$$
\text{allow}(c) = \begin{cases}
\text{true} & \text{tokens} \geq c \\
\text{false} & \text{tokens} < c
\end{cases}
$$

**æ•°å€¤ä¾‹**:

```julia
# Token Bucket ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
B = 100  # ãƒã‚±ãƒƒãƒˆå®¹é‡
r = 10   # è£œå……ãƒ¬ãƒ¼ãƒˆ (tokens/sec)

tokens = B
t = 0

for i in 1:15
    # 1ç§’ã”ã¨ã«7ãƒˆãƒ¼ã‚¯ãƒ³è¦æ±‚
    t += 1
    tokens = min(B, tokens + r * 1 - 7)

    println("t=$t: tokens=$tokens")
end
# => t=15: tokens=145 - 105 = 40 (ãƒã‚±ãƒƒãƒˆå®¹é‡ã§ã‚­ãƒ£ãƒƒãƒ—)
```

:::message
**é€²æ—: 50%å®Œäº†ï¼** ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—æ•°å¼ã¨Active Learningç†è«–ã‚’ç¿’å¾—ã—ãŸã€‚æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ã‚¯ãƒªã‚¢ï¼
:::

---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” 3è¨€èªE2Eçµ±åˆã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰

### 4.1 âš¡ Juliaè¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œå…¨ç‰ˆ

ç¬¬20å›ãƒ»ç¬¬23å›ã§å­¦ã‚“ã VAE/GAN/GPTã®è¨“ç·´ã‚’çµ±åˆã—ãŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚

#### 4.1.1 çµ±åˆè¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­è¨ˆ

```julia
using Lux, Optimisers, Zygote, MLUtils, Checkpoints

# çµ±åˆè¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
struct TrainingPipeline
    model::Lux.AbstractExplicitLayer
    optimizer::Optimisers.AbstractRule
    loss_fn::Function
    data_loader::DataLoader
    checkpoint_dir::String
end

function train_epoch!(pipeline::TrainingPipeline, ps, st, epoch)
    total_loss = 0.0
    n_batches = 0

    for (x, y) in pipeline.data_loader
        # Forward + Backward
        loss, grads = Zygote.withgradient(ps) do p
            y_pred, st_new = pipeline.model(x, p, st)
            pipeline.loss_fn(y_pred, y)
        end

        # Update
        opt_state, ps = Optimisers.update(pipeline.optimizer, ps, grads[1])

        total_loss += loss
        n_batches += 1
    end

    avg_loss = total_loss / n_batches

    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
    if epoch % 10 == 0
        save_checkpoint(pipeline.checkpoint_dir, epoch, ps, st, avg_loss)
    end

    return avg_loss, ps, st
end
```

#### 4.1.2 ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```julia
using Augmentor

# ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
augmentation_pipeline = FlipX(0.5) |>
                        FlipY(0.5) |>
                        Rotate(-15:15) |>
                        CropSize(224, 224) |>
                        Zoom(0.9:0.1:1.1)

function augment_batch(images)
    return augmentbatch!(images, augmentation_pipeline)
end
```

#### 4.1.3 ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–

```julia
using Hyperopt

# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ç©ºé–“
ho = @hyperopt for i=100,
                   lr = LinRange(1e-5, 1e-2, 50),
                   batch_size = [16, 32, 64, 128],
                   weight_decay = LogRange(1e-6, 1e-3, 20)

    # è¨“ç·´å®Ÿè¡Œ
    loss = train_with_params(lr=lr, batch_size=batch_size, weight_decay=weight_decay)

    @show i, lr, batch_size, weight_decay, loss
    loss  # æœ€å°åŒ–å¯¾è±¡
end

println("Best params: ", ho.minimizer)
```

### 4.2 âš¡â†’ğŸ¦€ ãƒ¢ãƒ‡ãƒ«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œå…¨ç‰ˆ

#### 4.2.1 Julia â†’ ONNX ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ

ç¬¬26å›ã§å­¦ã‚“ã ONNXã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚’å®Œå…¨ç‰ˆã«ã™ã‚‹ã€‚

```julia
using ONNX

# Luxãƒ¢ãƒ‡ãƒ« â†’ ONNX
function export_to_onnx(model, ps, st, input_shape, output_path)
    # ãƒ€ãƒŸãƒ¼å…¥åŠ›ã§è¨ˆç®—ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰
    dummy_input = randn(Float32, input_shape...)

    # Forward pass
    output, _ = model(dummy_input, ps, st)

    # ONNXå¤‰æ›
    onnx_model = ONNX.export(model, ps, st, dummy_input)

    # ä¿å­˜
    ONNX.save(onnx_model, output_path)

    println("Model exported to $output_path")
    println("Input shape: $input_shape")
    println("Output shape: $(size(output))")
end

# ä½¿ç”¨ä¾‹
export_to_onnx(trained_model, ps, st, (3, 224, 224, 1), "model.onnx")
```

#### 4.2.2 é‡å­åŒ– (INT4/FP8)

```julia
using Quantization

# INT8é‡å­åŒ–
function quantize_int8(onnx_path, output_path)
    model = ONNX.load(onnx_path)

    # é‡å­åŒ–è¨­å®š
    quant_config = QuantizationConfig(
        weight_type=:int8,
        activation_type=:int8,
        per_channel=true,  # ãƒãƒ£ãƒãƒ«ã”ã¨ã®é‡å­åŒ–
        symmetric=true     # å¯¾ç§°é‡å­åŒ–
    )

    # é‡å­åŒ–å®Ÿè¡Œ
    quantized_model = quantize(model, quant_config)

    # ä¿å­˜
    ONNX.save(quantized_model, output_path)

    # ã‚µã‚¤ã‚ºæ¯”è¼ƒ
    original_size = filesize(onnx_path) / 1024^2
    quantized_size = filesize(output_path) / 1024^2

    println("Original: $(round(original_size, digits=2)) MB")
    println("Quantized: $(round(quantized_size, digits=2)) MB")
    println("Compression: $(round(original_size/quantized_size, digits=2))x")
end
```

#### 4.2.3 ã‚¦ã‚§ã‚¤ãƒˆå¤‰æ›æ¤œè¨¼

```julia
# ã‚¦ã‚§ã‚¤ãƒˆæ¤œè¨¼
function verify_export(julia_model, ps, st, onnx_path)
    # Juliaæ¨è«–
    x_test = randn(Float32, 3, 224, 224, 1)
    y_julia, _ = julia_model(x_test, ps, st)

    # ONNXæ¨è«–
    onnx_session = ONNX.InferenceSession(onnx_path)
    y_onnx = ONNX.run(onnx_session, Dict("input" => x_test))["output"]

    # èª¤å·®è¨ˆç®—
    max_diff = maximum(abs.(y_julia .- y_onnx))
    mean_diff = mean(abs.(y_julia .- y_onnx))

    @assert max_diff < 1e-5 "Export verification failed! Max diff: $max_diff"

    println("âœ… Export verified!")
    println("Max diff: $max_diff")
    println("Mean diff: $mean_diff")
end
```

### 4.3 ğŸ¦€ Rustæ¨è«–ã‚µãƒ¼ãƒãƒ¼å®Œå…¨ç‰ˆ

ç¬¬26å›ã®Rustæ¨è«–ã‚’Productionå“è³ªã«å¼•ãä¸Šã’ã‚‹ã€‚

#### 4.3.1 Axum REST API

```rust
use axum::{
    extract::State,
    routing::post,
    Json, Router,
};
use ort::{Session, Value};
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use tokio::sync::RwLock;

#[derive(Clone)]
struct AppState {
    model: Arc<RwLock<Session>>,
}

#[derive(Deserialize)]
struct InferenceRequest {
    image: Vec<Vec<Vec<f32>>>,  // (H, W, C)
}

#[derive(Serialize)]
struct InferenceResponse {
    prediction: Vec<f32>,
    confidence: f32,
    latency_ms: f64,
}

async fn inference(
    State(state): State<AppState>,
    Json(req): Json<InferenceRequest>,
) -> Json<InferenceResponse> {
    let start = std::time::Instant::now();

    // Reshape (H, W, C) -> (1, C, H, W)
    let input = preprocess_image(&req.image);

    // æ¨è«–
    let model = state.model.read().await;
    let outputs = model.run(vec![Value::from_array(input).unwrap()]).unwrap();

    let prediction = outputs[0].extract_tensor::<f32>().unwrap().to_vec();
    let confidence = prediction.iter().fold(f32::NEG_INFINITY, |a, &b| a.max(b));

    let latency_ms = start.elapsed().as_secs_f64() * 1000.0;

    Json(InferenceResponse {
        prediction,
        confidence,
        latency_ms,
    })
}

#[tokio::main]
async fn main() {
    // ONNXãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    let model = Arc::new(RwLock::new(
        Session::builder().unwrap()
            .with_intra_threads(4).unwrap()
            .commit_from_file("model.onnx").unwrap()
    ));

    let state = AppState { model };

    // Axumã‚¢ãƒ—ãƒªæ§‹ç¯‰
    let app = Router::new()
        .route("/v1/inference", post(inference))
        .with_state(state);

    // ã‚µãƒ¼ãƒãƒ¼èµ·å‹•
    axum::Server::bind(&"0.0.0.0:8080".parse().unwrap())
        .serve(app.into_make_service())
        .await
        .unwrap();
}

fn preprocess_image(img: &[Vec<Vec<f32>>]) -> ndarray::Array4<f32> {
    // (H, W, C) -> (1, C, H, W) å¤‰æ›
    let h = img.len();
    let w = img[0].len();
    let c = img[0][0].len();

    let mut arr = ndarray::Array4::<f32>::zeros((1, c, h, w));
    for i in 0..h {
        for j in 0..w {
            for k in 0..c {
                arr[[0, k, i, j]] = img[i][j][k];
            }
        }
    }
    arr
}
```

#### 4.3.2 ãƒãƒƒãƒå‡¦ç† & éåŒæœŸæ¨è«–

```rust
use tokio::sync::mpsc;
use std::time::Duration;

struct BatchProcessor {
    sender: mpsc::Sender<InferenceJob>,
}

struct InferenceJob {
    input: Vec<f32>,
    response_tx: oneshot::Sender<Vec<f32>>,
}

impl BatchProcessor {
    fn new(model: Arc<RwLock<Session>>, batch_size: usize, timeout_ms: u64) -> Self {
        let (tx, mut rx) = mpsc::channel::<InferenceJob>(100);

        tokio::spawn(async move {
            let mut batch = Vec::new();

            loop {
                // ãƒãƒƒãƒåé›†
                match tokio::time::timeout(Duration::from_millis(timeout_ms), rx.recv()).await {
                    Ok(Some(job)) => {
                        batch.push(job);

                        if batch.len() >= batch_size {
                            process_batch(&model, &mut batch).await;
                        }
                    }
                    Ok(None) => break,  // ãƒãƒ£ãƒãƒ«ã‚¯ãƒ­ãƒ¼ã‚º
                    Err(_) => {  // ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                        if !batch.is_empty() {
                            process_batch(&model, &mut batch).await;
                        }
                    }
                }
            }
        });

        Self { sender: tx }
    }

    async fn infer(&self, input: Vec<f32>) -> Vec<f32> {
        let (tx, rx) = oneshot::channel();
        self.sender.send(InferenceJob { input, response_tx: tx }).await.unwrap();
        rx.await.unwrap()
    }
}

async fn process_batch(model: &Arc<RwLock<Session>>, batch: &mut Vec<InferenceJob>) {
    // ãƒãƒƒãƒå…¥åŠ›æ§‹ç¯‰
    let batch_input = batch.iter().flat_map(|j| &j.input).copied().collect::<Vec<_>>();

    // ãƒãƒƒãƒæ¨è«–
    let model = model.read().await;
    let outputs = model.run(vec![Value::from_array(batch_input).unwrap()]).unwrap();

    // çµæœã‚’å„ã‚¸ãƒ§ãƒ–ã«è¿”ã™
    let predictions = outputs[0].extract_tensor::<f32>().unwrap();
    for (i, job) in batch.drain(..).enumerate() {
        let _ = job.response_tx.send(predictions[i..i+10].to_vec());
    }
}
```

#### 4.3.3 Prometheus Metrics

```rust
use prometheus::{Encoder, IntCounter, Histogram, HistogramOpts, Registry, TextEncoder};
use axum::extract::Extension;

struct Metrics {
    inference_count: IntCounter,
    inference_duration: Histogram,
}

impl Metrics {
    fn new() -> Self {
        let inference_count = IntCounter::new("inference_total", "Total inference requests").unwrap();
        let inference_duration = Histogram::with_opts(
            HistogramOpts::new("inference_duration_seconds", "Inference duration")
                .buckets(vec![0.001, 0.01, 0.05, 0.1, 0.5, 1.0])
        ).unwrap();

        Self { inference_count, inference_duration }
    }

    fn register(&self, registry: &Registry) {
        registry.register(Box::new(self.inference_count.clone())).unwrap();
        registry.register(Box::new(self.inference_duration.clone())).unwrap();
    }
}

async fn metrics_handler(Extension(registry): Extension<Registry>) -> String {
    let encoder = TextEncoder::new();
    let metric_families = registry.gather();
    let mut buffer = vec![];
    encoder.encode(&metric_families, &mut buffer).unwrap();
    String::from_utf8(buffer).unwrap()
}

// æ¨è«–ãƒãƒ³ãƒ‰ãƒ©ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
async fn inference_with_metrics(
    State(state): State<AppState>,
    Extension(metrics): Extension<Arc<Metrics>>,
    Json(req): Json<InferenceRequest>,
) -> Json<InferenceResponse> {
    let timer = metrics.inference_duration.start_timer();
    let response = inference(State(state), Json(req)).await;
    timer.observe_duration();

    metrics.inference_count.inc();

    response
}
```

### 4.4 ğŸ”® Elixir APIã‚²ãƒ¼ãƒˆã‚¦ã‚§ã‚¤å®Œå…¨ç‰ˆ

ç¬¬30å›ã®Elixir Agentã‚’APIã‚²ãƒ¼ãƒˆã‚¦ã‚§ã‚¤ã«æ‹¡å¼µã™ã‚‹ã€‚

#### 4.4.1 Phoenix Setup

```elixir
# mix.exs
defmodule ApiGateway.MixProject do
  use Mix.Project

  def project do
    [
      app: :api_gateway,
      version: "0.1.0",
      elixir: "~> 1.14",
      deps: deps()
    ]
  end

  defp deps do
    [
      {:phoenix, "~> 1.7"},
      {:plug_cowboy, "~> 2.7"},
      {:jason, "~> 1.4"},
      {:guardian, "~> 2.3"},  # JWT auth
      {:hammer, "~> 6.1"},    # Rate limiting
      {:req, "~> 0.4"}        # HTTP client
    ]
  end
end
```

#### 4.4.2 JWTèªè¨¼

```elixir
defmodule ApiGateway.Guardian do
  use Guardian, otp_app: :api_gateway

  def subject_for_token(%{id: id}, _claims), do: {:ok, to_string(id)}
  def resource_from_claims(%{"sub" => id}), do: {:ok, %{id: id}}
end

defmodule ApiGateway.AuthPlug do
  import Plug.Conn

  def init(opts), do: opts

  def call(conn, _opts) do
    case Guardian.Plug.current_token(conn) do
      nil -> unauthorized(conn)
      _token -> conn
    end
  end

  defp unauthorized(conn) do
    conn
    |> put_status(:unauthorized)
    |> Phoenix.Controller.json(%{error: "Unauthorized"})
    |> halt()
  end
end
```

#### 4.4.3 Rate Limiting (Hammer)

```elixir
defmodule ApiGateway.RateLimiter do
  use Hammer

  def check_rate(user_id) do
    case Hammer.check_rate("user:#{user_id}", 60_000, 100) do
      {:allow, _count} -> :ok
      {:deny, _limit} -> {:error, :rate_limited}
    end
  end
end

defmodule ApiGatewayWeb.InferenceController do
  use ApiGatewayWeb, :controller

  def infer(conn, params) do
    user_id = Guardian.Plug.current_resource(conn).id

    case ApiGateway.RateLimiter.check_rate(user_id) do
      :ok ->
        # Rustæ¨è«–ã‚µãƒ¼ãƒãƒ¼ã«è»¢é€
        response = call_rust_inference(params)
        json(conn, response)

      {:error, :rate_limited} ->
        conn
        |> put_status(:too_many_requests)
        |> json(%{error: "Rate limit exceeded"})
    end
  end

  defp call_rust_inference(params) do
    Req.post!("http://localhost:8080/v1/inference", json: params).body
  end
end
```

#### 4.4.4 Circuit Breaker

```elixir
defmodule ApiGateway.CircuitBreaker do
  use GenServer

  defmodule State do
    defstruct [:status, :failure_count, :last_failure_time]
  end

  # Client API
  def start_link(_opts) do
    GenServer.start_link(__MODULE__, %State{status: :closed, failure_count: 0}, name: __MODULE__)
  end

  def call(fun) do
    GenServer.call(__MODULE__, {:call, fun})
  end

  # Server Callbacks
  def handle_call({:call, fun}, _from, %State{status: :open} = state) do
    # OpençŠ¶æ…‹: ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’æ‹’å¦
    {:reply, {:error, :circuit_open}, state}
  end

  def handle_call({:call, fun}, _from, %State{status: :closed} = state) do
    case fun.() do
      {:ok, result} ->
        # æˆåŠŸ: failure_countãƒªã‚»ãƒƒãƒˆ
        {:reply, {:ok, result}, %State{state | failure_count: 0}}

      {:error, reason} ->
        new_count = state.failure_count + 1

        new_state = if new_count >= 5 do
          # 5å›å¤±æ•— â†’ OpençŠ¶æ…‹ã¸
          %State{status: :open, failure_count: new_count, last_failure_time: System.monotonic_time(:second)}
        else
          %State{state | failure_count: new_count}
        end

        {:reply, {:error, reason}, new_state}
    end
  end

  # 30ç§’å¾Œã« Half-Open ã¸é·ç§»
  def handle_info(:attempt_recovery, %State{status: :open} = state) do
    {:noreply, %State{state | status: :half_open}}
  end
end
```

#### 4.4.5 WebSocketå¯¾å¿œ

```elixir
defmodule ApiGatewayWeb.InferenceChannel do
  use Phoenix.Channel

  def join("inference:lobby", _params, socket) do
    {:ok, socket}
  end

  def handle_in("predict", %{"image" => image}, socket) do
    # Rustæ¨è«–ã‚µãƒ¼ãƒãƒ¼ã«è»¢é€
    response = call_rust_inference(%{image: image})

    push(socket, "prediction", response)
    {:noreply, socket}
  end
end
```

### 4.5 E2Eã‚·ã‚¹ãƒ†ãƒ çµ±åˆ

3è¨€èªã‚’çµ±åˆã—ãŸã‚·ã‚¹ãƒ†ãƒ ã®èµ·å‹•ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚

```bash
#!/bin/bash
# deploy_e2e.sh

# 1. Juliaè¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³èµ·å‹•
cd julia_training
julia --project=. -e 'using TrainingPipeline; train_all_models()' &

# 2. Rustæ¨è«–ã‚µãƒ¼ãƒãƒ¼èµ·å‹•
cd ../rust_inference
cargo run --release -- --port 8080 &

# 3. Elixir APIã‚²ãƒ¼ãƒˆã‚¦ã‚§ã‚¤èµ·å‹•
cd ../elixir_gateway
mix phx.server &

# 4. Prometheusèµ·å‹•
cd ../monitoring
./prometheus --config.file=prometheus.yml &

echo "âœ… E2E system deployed!"
echo "ğŸ“Š Monitoring: http://localhost:9090"
echo "ğŸ”® API Gateway: http://localhost:4000"
echo "ğŸ¦€ Rust Inference: http://localhost:8080"
```

:::message
**é€²æ—: 70%å®Œäº†ï¼** 3è¨€èªçµ±åˆã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…ãŒå®Œæˆã—ãŸï¼
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” E2Eãƒ†ã‚¹ãƒˆ & çµ±åˆãƒ‡ãƒ¢

### 5.1 E2Eãƒ†ã‚¹ãƒˆå®Œå…¨ç‰ˆ

#### 5.1.1 çµ±åˆãƒ†ã‚¹ãƒˆ

å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒé€£æºã—ã¦å‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹ã€‚

```julia
using Test, HTTP, JSON

@testset "E2E Integration Test" begin
    # 1. Juliaè¨“ç·´ â†’ ONNXå‡ºåŠ›
    @test isfile("models/trained_model.onnx")

    # 2. Rustæ¨è«–ã‚µãƒ¼ãƒãƒ¼èµ·å‹•ç¢ºèª
    response = HTTP.get("http://localhost:8080/health")
    @test response.status == 200

    # 3. Elixir APIçµŒç”±ã§æ¨è«–ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    test_image = rand(Float32, 224, 224, 3)
    payload = Dict("image" => test_image)

    response = HTTP.post(
        "http://localhost:4000/v1/inference",
        ["Content-Type" => "application/json", "Authorization" => "Bearer test_token"],
        JSON.json(payload)
    )

    @test response.status == 200
    result = JSON.parse(String(response.body))
    @test haskey(result, "prediction")
    @test haskey(result, "confidence")
    @test haskey(result, "latency_ms")

    # 4. ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é€ä¿¡
    feedback_payload = Dict(
        "request_id" => result["request_id"],
        "rating" => 5,
        "comment" => "Perfect prediction!"
    )

    response = HTTP.post(
        "http://localhost:4000/v1/feedback",
        ["Content-Type" => "application/json"],
        JSON.json(feedback_payload)
    )

    @test response.status == 200
end
```

#### 5.1.2 è² è·ãƒ†ã‚¹ãƒˆ (k6)

```javascript
// k6_load_test.js
import http from 'k6/http';
import { check, sleep } from 'k6';

export const options = {
  stages: [
    { duration: '1m', target: 50 },   // Ramp up to 50 users
    { duration: '3m', target: 50 },   // Stay at 50 users
    { duration: '1m', target: 100 },  // Ramp up to 100 users
    { duration: '3m', target: 100 },  // Stay at 100 users
    { duration: '1m', target: 0 },    // Ramp down to 0 users
  ],
  thresholds: {
    http_req_duration: ['p(95)<100'],  // 95% of requests < 100ms
    http_req_failed: ['rate<0.01'],     // Error rate < 1%
  },
};

export default function () {
  const payload = JSON.stringify({
    image: Array(224).fill(Array(224).fill(Array(3).fill(0.5))),
  });

  const params = {
    headers: {
      'Content-Type': 'application/json',
      'Authorization': 'Bearer test_token',
    },
  };

  const res = http.post('http://localhost:4000/v1/inference', payload, params);

  check(res, {
    'status is 200': (r) => r.status === 200,
    'latency < 100ms': (r) => r.timings.duration < 100,
  });

  sleep(0.1);
}
```

**å®Ÿè¡Œ**:

```bash
k6 run k6_load_test.js
```

**å‡ºåŠ›ä¾‹**:

```
     âœ“ status is 200
     âœ“ latency < 100ms

     checks.........................: 100.00% âœ“ 30000 âœ— 0
     data_received..................: 15 MB   150 kB/s
     data_sent......................: 45 MB   450 kB/s
     http_req_blocked...............: avg=0.1ms   p(95)=0.3ms
     http_req_duration..............: avg=12ms    p(95)=45ms
     http_reqs......................: 30000   500/s
```

#### 5.1.3 Locustè² è·ãƒ†ã‚¹ãƒˆ

```python
# locustfile.py
from locust import HttpUser, task, between
import random

class InferenceUser(HttpUser):
    wait_time = between(0.1, 0.5)

    @task
    def inference(self):
        payload = {
            "image": [[[random.random() for _ in range(3)]
                       for _ in range(224)]
                      for _ in range(224)]
        }

        headers = {
            "Authorization": "Bearer test_token"
        }

        self.client.post("/v1/inference", json=payload, headers=headers)

    @task(2)  # 2x more likely than inference
    def feedback(self):
        payload = {
            "request_id": "test_" + str(random.randint(1, 10000)),
            "rating": random.randint(1, 5),
            "comment": "Test feedback"
        }

        self.client.post("/v1/feedback", json=payload)
```

**å®Ÿè¡Œ**:

```bash
locust -f locustfile.py --host=http://localhost:4000 --users 100 --spawn-rate 10
```

#### 5.1.4 Chaos Engineering (Chaos Mesh)

```yaml
# chaos_pod_kill.yaml
apiVersion: chaos-mesh.org/v1alpha1
kind: PodChaos
metadata:
  name: inference-server-kill
spec:
  action: pod-kill
  mode: one
  selector:
    namespaces:
      - production
    labelSelectors:
      app: rust-inference-server
  scheduler:
    cron: "@every 10m"
```

**é©ç”¨**:

```bash
kubectl apply -f chaos_pod_kill.yaml
```

**ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é…å»¶æ³¨å…¥**:

```yaml
# chaos_network_delay.yaml
apiVersion: chaos-mesh.org/v1alpha1
kind: NetworkChaos
metadata:
  name: api-gateway-delay
spec:
  action: delay
  mode: one
  selector:
    namespaces:
      - production
    labelSelectors:
      app: elixir-api-gateway
  delay:
    latency: "100ms"
    correlation: "100"
    jitter: "50ms"
  duration: "5m"
```

#### 5.1.5 æ€§èƒ½ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

```julia
using Profile, ProfileView

# ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°å®Ÿè¡Œ
@profile begin
    for i in 1:1000
        result = infer_model(test_input)
    end
end

# çµæœã‚’ãƒ•ãƒ¬ãƒ¼ãƒ ã‚°ãƒ©ãƒ•ã§å¯è¦–åŒ–
ProfileView.view()
```

**Rust Flame Graph**:

```bash
cargo flamegraph --bin inference_server
```

### 5.2 SmolVLM2-256M + aMUSEd-256 çµ±åˆãƒ‡ãƒ¢

#### 5.2.1 ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```mermaid
graph LR
    A[ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ] --> B[ğŸ”® Elixir API]
    B --> C[ğŸ¦€ SmolVLM2-256Mæ¨è«–]
    C --> D[ãƒ†ã‚­ã‚¹ãƒˆç†è§£ + ç”»åƒè¨˜è¿°ç”Ÿæˆ]
    D --> E[ğŸ¦€ aMUSEd-256æ¨è«–]
    E --> F[ç”»åƒç”Ÿæˆ]
    F --> G[ğŸ”® Elixiré…ä¿¡]
    G --> H[ãƒ¦ãƒ¼ã‚¶ãƒ¼]
    H --> I[ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯]
    I --> J[âš¡ Juliaå†è¨“ç·´]
    J --> C
```

#### 5.2.2 Juliaçµ±åˆå®Ÿè£…

```julia
using SmolVLM2, aMUSEd, Lux

# SmolVLM2ã§ç”»åƒè¨˜è¿°ç”Ÿæˆ
function generate_image_description(user_query::String)
    # SmolVLM2-256Mæ¨è«–
    vlm_output = SmolVLM2.infer(user_query)

    # ç”»åƒè¨˜è¿°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
    prompt = "A detailed image of: " * vlm_output.description

    return prompt
end

# aMUSEd-256ã§ç”»åƒç”Ÿæˆ
function generate_image(prompt::String)
    # aMUSEd-256æ¨è«–
    image = aMUSEd.generate(
        prompt=prompt,
        num_inference_steps=12,  # Fast inference
        guidance_scale=3.0
    )

    return image
end

# E2Eçµ±åˆ
function text_to_image_e2e(user_query::String)
    # Step 1: ãƒ†ã‚­ã‚¹ãƒˆç†è§£
    prompt = generate_image_description(user_query)
    println("Generated prompt: $prompt")

    # Step 2: ç”»åƒç”Ÿæˆ
    image = generate_image(prompt)

    # Step 3: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†æº–å‚™
    request_id = uuid4()

    return (image=image, prompt=prompt, request_id=request_id)
end

# ä½¿ç”¨ä¾‹
result = text_to_image_e2e("A cat sitting on a laptop")
save_image(result.image, "output.png")
```

#### 5.2.3 RAGæ‹¡å¼µç‰ˆ

```julia
using Embeddings, FAISS

# RAGçµ±åˆ
function text_to_image_with_rag(user_query::String, knowledge_base::Vector{String})
    # Step 1: é–¢é€£çŸ¥è­˜ã‚’Retrieve
    query_embedding = embed(user_query)
    relevant_docs = faiss_search(query_embedding, knowledge_base, k=3)

    # Step 2: æ‹¡å¼µãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
    augmented_query = user_query * "\n\nContext:\n" * join(relevant_docs, "\n")

    # Step 3: SmolVLM2ã§ç†è§£
    prompt = generate_image_description(augmented_query)

    # Step 4: ç”»åƒç”Ÿæˆ
    image = generate_image(prompt)

    return (image=image, prompt=prompt, retrieved_docs=relevant_docs)
end

# ä½¿ç”¨ä¾‹
knowledge_base = [
    "Cats are domesticated mammals that are popular pets.",
    "Laptops are portable computers with integrated keyboards.",
    "Cats often sit on warm surfaces like laptop keyboards."
]

result = text_to_image_with_rag("A cat on a laptop", knowledge_base)
```

#### 5.2.4 Elixiré…ä¿¡ & ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯

```elixir
defmodule ApiGatewayWeb.ImageGenerationController do
  use ApiGatewayWeb, :controller

  def generate(conn, %{"query" => query}) do
    # Rustæ¨è«–ã‚µãƒ¼ãƒãƒ¼çµŒç”±ã§SmolVLM2+aMUSEdå‘¼ã³å‡ºã—
    result = call_rust_image_generation(query)

    # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒªã‚¯ã‚¨ã‚¹ãƒˆIDç”Ÿæˆ
    request_id = UUID.uuid4()

    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹
    json(conn, %{
      image_url: result.image_url,
      prompt: result.prompt,
      request_id: request_id
    })
  end

  def submit_feedback(conn, %{"request_id" => request_id, "rating" => rating, "comment" => comment}) do
    # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’DBä¿å­˜
    {:ok, _feedback} = Feedbacks.create_feedback(%{
      request_id: request_id,
      rating: rating,
      comment: comment,
      timestamp: DateTime.utc_now()
    })

    # éåŒæœŸã§Juliaå†è¨“ç·´ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
    Feedbacks.enqueue_for_retraining(request_id)

    json(conn, %{status: "feedback_received"})
  end

  defp call_rust_image_generation(query) do
    Req.post!(
      "http://localhost:8080/v1/image_generation",
      json: %{query: query}
    ).body
  end
end
```

#### 5.2.5 ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é§†å‹•ã®å†è¨“ç·´

```julia
using Feedback, ModelRegistry

# ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿å–å¾—
function collect_feedback_data(since_timestamp)
    feedbacks = query_feedback_db(since_timestamp)

    # é«˜è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã®ã¿æŠ½å‡º (rating >= 4)
    high_quality = filter(f -> f.rating >= 4, feedbacks)

    return high_quality
end

# ç¶™ç¶šå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
function continuous_learning_pipeline()
    # å‰å›ã®è¨“ç·´ä»¥é™ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å–å¾—
    last_train_time = load_last_train_timestamp()
    new_feedback = collect_feedback_data(last_train_time)

    if length(new_feedback) < 100
        println("Not enough feedback for retraining ($(length(new_feedback)) < 100)")
        return
    end

    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿æº–å‚™
    train_data = prepare_training_data(new_feedback)

    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    model, ps, st = load_latest_model()

    # Fine-tune
    ps_new, st_new = fine_tune(model, ps, st, train_data, epochs=5)

    # æ¤œè¨¼
    val_loss = validate(model, ps_new, st_new, validation_data)
    println("Validation loss: $val_loss")

    # æ€§èƒ½å‘ä¸Šã—ã¦ã„ã‚Œã°ä¿å­˜
    if val_loss < get_best_val_loss()
        save_model(model, ps_new, st_new, "models/updated_model.onnx")
        update_last_train_timestamp()
        println("âœ… Model updated and deployed!")
    else
        println("âš ï¸  No improvement. Keeping current model.")
    end
end

# å®šæœŸå®Ÿè¡Œ (ä¾‹: 1æ—¥1å›)
while true
    continuous_learning_pipeline()
    sleep(86400)  # 24 hours
end
```

### 5.3 è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

#### 5.3.1 E2Eãƒ†ã‚¹ãƒˆè¨­è¨ˆãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] çµ±åˆãƒ†ã‚¹ãƒˆ: å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆé€£æºç¢ºèª
- [ ] è² è·ãƒ†ã‚¹ãƒˆ: ç›®æ¨™ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆé”æˆç¢ºèª (k6 or Locust)
- [ ] Chaos Engineering: éšœå®³æ³¨å…¥ãƒ†ã‚¹ãƒˆ (Chaos Mesh)
- [ ] æ€§èƒ½ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š
- [ ] ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ: JWTèªè¨¼ãƒ»Rate Limitç¢ºèª
- [ ] ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—: åé›†â†’åˆ†æâ†’å†è¨“ç·´ã®è‡ªå‹•åŒ–ç¢ºèª

#### 5.3.2 Productionãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°: Prometheus + Grafana ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
- [ ] ã‚¢ãƒ©ãƒ¼ãƒˆ: ç•°å¸¸æ¤œçŸ¥è‡ªå‹•é€šçŸ¥
- [ ] ãƒ­ã‚°: æ§‹é€ åŒ–ãƒ­ã‚° + é›†ç´„ (Elasticsearch or Loki)
- [ ] ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°: åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚° (Jaeger or Tempo)
- [ ] ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æˆ¦ç•¥
- [ ] DR (Disaster Recovery): éšœå®³æ™‚ã®å¾©æ—§æ‰‹é †
- [ ] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: APIä»•æ§˜æ›¸ + é‹ç”¨ãƒãƒ‹ãƒ¥ã‚¢ãƒ«

#### 5.3.3 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸

**Challenge 1**: SmolVLM2+aMUSEdçµ±åˆãƒ‡ãƒ¢ã‚’å‹•ã‹ã™

```julia
# 1. ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
download_smolvlm2_256m()
download_amused_256()

# 2. E2Eå®Ÿè¡Œ
result = text_to_image_e2e("A futuristic city at sunset")
save_image(result.image, "futuristic_city.png")

# 3. ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é€ä¿¡
submit_feedback(result.request_id, rating=5, comment="Beautiful!")
```

**Challenge 2**: è² è·ãƒ†ã‚¹ãƒˆã§1,000 req/sã‚’é”æˆ

```bash
k6 run --vus 200 --duration 30s k6_load_test.js
```

**Challenge 3**: Chaos Meshã§éšœå®³æ³¨å…¥ãƒ†ã‚¹ãƒˆ

```bash
kubectl apply -f chaos_pod_kill.yaml
# ã‚·ã‚¹ãƒ†ãƒ ãŒè‡ªå‹•å¾©æ—§ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
```

:::message
**é€²æ—: 85%å®Œäº†ï¼** E2Eãƒ†ã‚¹ãƒˆ & çµ±åˆãƒ‡ãƒ¢ãŒå®Œæˆã—ãŸï¼
:::

---

## Z6: ç™ºå±•ã‚¾ãƒ¼ãƒ³ â€” Production MLç ”ç©¶ç³»è­œ

:::message
**ã‚´ãƒ¼ãƒ«**: Production MLã®æœ€æ–°ç ”ç©¶å‹•å‘ã‚’è¿½è·¡ã—ã€æ¬¡ä¸–ä»£ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆã®æŒ‡é‡ã‚’å¾—ã‚‹
:::

### 6.1 Active Learningç†è«–ã®é€²åŒ–

**MSAL â†’ Self-Supervised AL â†’ Adaptive Budgets**

```julia
# æœ€æ–°Active Learning: Adaptive Budget + Diversity Sampling
struct AdaptiveAL
    base_sampler::UncertaintySampler
    diversity_penalty::Float32  # å¤šæ§˜æ€§é‡è¦–åº¦
    budget_scheduler::Function  # å‹•çš„äºˆç®—èª¿æ•´
end

function select_batch(al::AdaptiveAL, pool::Matrix, labels::Vector, budget::Int)
    # 1. Uncertaintyè¨ˆç®—
    uncertainty = compute_uncertainty(al.base_sampler, pool)

    # 2. Diversity Penalty (DPP - Determinantal Point Process)
    L = kernel_matrix(pool)  # RBF kernel
    diversity_score = log_det(L[selected_indices, selected_indices])

    # 3. Combined score (uncertainty + diversity)
    score = uncertainty .+ al.diversity_penalty .* diversity_score

    # 4. Dynamic budget (ä½ä¸ç¢ºå®Ÿæ€§æ™‚ã¯äºˆç®—å‰Šæ¸›)
    adjusted_budget = al.budget_scheduler(mean(uncertainty), budget)

    return partialsortperm(score, 1:adjusted_budget, rev=true)
end
```

**Reference**: Settles, Burr. "Active Learning Literature Survey." Computer Sciences Technical Report 1648, University of Wisconsin-Madison (2009). â€” åŸºç¤ç†è«–ã®æ±ºå®šç‰ˆ

**æœ€æ–°ãƒˆãƒ¬ãƒ³ãƒ‰** (arXiv:2411.17444):
- **Self-Supervised Pre-training + AL**: ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿ã§äº‹å‰å­¦ç¿’ â†’ ä¸ç¢ºå®Ÿæ€§æ¨å®šç²¾åº¦â†‘50%
- **Bayesian Active Learning by Disagreement (BALD)**: MI(y;Î¸|x,D) æœ€å¤§åŒ–
- **Expected Gradient Length (EGL)**: å‹¾é…ãƒãƒ«ãƒ æœŸå¾…å€¤æœ€å¤§åŒ– â†’ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°é‡æœ€å¤§åŒ–

### 6.2 HITL (Human-in-the-Loop) Best Practices

**Challenge**: äººé–“ã®ãƒã‚¤ã‚¢ã‚¹ãƒ»ç–²åŠ´ãƒ»ã‚³ã‚¹ãƒˆ

```elixir
# Elixir: Intelligent HITL Routing (é›£æ˜“åº¦ãƒ™ãƒ¼ã‚¹æŒ¯ã‚Šåˆ†ã‘)
defmodule HITL.Router do
  def route_request(prediction, confidence) do
    cond do
      confidence > 0.95 -> {:auto_approve, prediction}  # è‡ªå‹•æ‰¿èª
      confidence > 0.75 -> {:expert_review, :junior}    # ã‚¸ãƒ¥ãƒ‹ã‚¢ç¢ºèª
      confidence > 0.50 -> {:expert_review, :senior}    # ã‚·ãƒ‹ã‚¢ç¢ºèª
      true              -> {:human_decision, :expert}   # äººé–“ãŒåˆ¤æ–­
    end
  end

  # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°çµ„ã¿è¾¼ã¿
  def collect_for_retraining(request_id, human_label) do
    # 1. äººé–“ãƒ©ãƒ™ãƒ«ã‚’DBã«ä¿å­˜
    Repo.insert!(%TrainingExample{
      request_id: request_id,
      features: get_features(request_id),
      label: human_label,
      confidence: :human_verified,  # é«˜å“è³ªãƒ•ãƒ©ã‚°
      created_at: DateTime.utc_now()
    })

    # 2. ãƒãƒƒãƒã‚µã‚¤ã‚ºé”æˆæ™‚ã«å†è¨“ç·´ãƒˆãƒªã‚¬ãƒ¼
    if training_batch_ready?() do
      TriggerRetraining.call()
    end
  end
end
```

**Reference**: arXiv:2409.09467 "Human-in-the-Loop Machine Learning: A Survey" â€” HITLä½“ç³»çš„æ•´ç†

**Key Insights**:
- **Active Evaluation**: ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚‚äººé–“ãŒé¸æŠ â†’ ãƒã‚¤ã‚¢ã‚¹é™¤å»
- **Curriculum Learning**: ç°¡å˜â†’é›£ã—ã„é †ã«äººé–“ãƒ¬ãƒ“ãƒ¥ãƒ¼ â†’ ç–²åŠ´è»½æ¸›
- **Inter-Annotator Agreement**: Fleiss' Kappa > 0.7 ã§å“è³ªä¿è¨¼

### 6.3 Continuous Learningç†è«–

**Catastrophic Forgettingå¯¾ç­–ã®æ•°å­¦**

$$
\mathcal{L}_{\text{EWC}}(\theta) = \mathcal{L}_{\text{new}}(\theta) + \frac{\lambda}{2}\sum_i F_i(\theta_i - \theta^*_i)^2
$$

- $F_i$: Fisheræƒ…å ±è¡Œåˆ—ã®å¯¾è§’æˆåˆ† = ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é‡è¦åº¦
- $\theta^*$: æ—§ã‚¿ã‚¹ã‚¯ã®æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- $\lambda$: æ—§çŸ¥è­˜ä¿è­·ã®å¼·ã•

```rust
// Rust: EWCå®Ÿè£… (Fisheræƒ…å ±è¡Œåˆ—è¨ˆç®—)
pub fn compute_fisher_information(
    model: &Model,
    old_data: &[Example],
) -> Vec<f32> {
    let mut fisher = vec![0.0; model.num_params()];

    for example in old_data {
        // 1. Forward pass
        let logits = model.forward(&example.features);
        let prob = softmax(&logits);

        // 2. Compute gradient of log-likelihood
        let grad = model.backward(&example.features, &prob);

        // 3. Fisher = E[âˆ‡log p(y|x)Â²]
        for (i, &g) in grad.iter().enumerate() {
            fisher[i] += g * g;
        }
    }

    // Normalize by dataset size
    fisher.iter_mut().for_each(|f| *f /= old_data.len() as f32);
    fisher
}
```

**Reference**: arXiv:1612.00796 "Overcoming catastrophic forgetting in neural networks" (DeepMind) â€” EWCã‚ªãƒªã‚¸ãƒŠãƒ«è«–æ–‡

**Alternative Approaches**:
- **Progressive Neural Networks**: æ–°ã‚¿ã‚¹ã‚¯å°‚ç”¨ã®åˆ—ã‚’è¿½åŠ  â†’ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å…±æœ‰ãªã—
- **PackNet**: ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã§ãƒã‚¹ã‚¯ä½œæˆ â†’ æ—§ã‚¿ã‚¹ã‚¯é ˜åŸŸã‚’å‡çµ
- **Learning without Forgetting (LwF)**: çŸ¥è­˜è’¸ç•™ã§æ—§ã‚¿ã‚¹ã‚¯ã®å‡ºåŠ›ã‚’å†ç¾

### 6.4 Production Infrastructureç ”ç©¶

**Chaos Engineeringç†è«–** (Chaos Mesh)

```yaml
# Chaos Mesh: Network Partitionå®Ÿé¨“
apiVersion: chaos-mesh.org/v1alpha1
kind: NetworkChaos
metadata:
  name: partition-test
spec:
  action: partition
  mode: all
  selector:
    namespaces:
      - production
    labelSelectors:
      app: inference-server
  direction: both
  duration: "30s"
  scheduler:
    cron: "@hourly"  # æ¯æ™‚ãƒ†ã‚¹ãƒˆ
```

**Reference**: Basiri et al. "Chaos Engineering." IEEE Software 33.3 (2016): 35-41. â€” Netflix Chaos Monkeyç†è«–

**Key Metrics**:
- **MTBF (Mean Time Between Failures)**: å¹³å‡æ•…éšœé–“éš” â†’ é«˜ã„ã»ã©è‰¯ã„
- **MTTR (Mean Time To Recovery)**: å¹³å‡å¾©æ—§æ™‚é–“ â†’ ä½ã„ã»ã©è‰¯ã„
- **SLA (Service Level Agreement)**: 99.9% uptime = 43.2åˆ†/æœˆã®ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ è¨±å®¹

### 6.5 æœ€æ–°Production MLã‚·ã‚¹ãƒ†ãƒ 

**Google Vertex AI Architecture** (2024):

```
User Request
    â†“
Prediction Service (Go, <10ms)
    â†“
Model Cache (Redis) â”€â”€â”€â”€â†’ Miss â†’ Model Registry (GCS)
    â†“
TensorRT Inference (GPU)
    â†“
Feedback Logger (Pub/Sub) â”€â”€â”€â”€â†’ BigQuery
    â†“
Retraining Pipeline (Kubeflow) â”€â”€â”€â”€â†’ Model Registry
```

**Meta's DLRM (Deep Learning Recommendation Model)**:
- **Scale**: 1å…†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿, 100å„„ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/æ—¥
- **Latency**: p99 < 50ms (åˆ†æ•£åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ¼ãƒ–ãƒ«)
- **Training**: PyTorch + FSDP (Fully Sharded Data Parallel)
- **Serving**: C++ + TorchScript

**Reference**: arXiv:1906.00091 "Deep Learning Recommendation Model for Personalization and Recommendation Systems" (Meta)

### 6.6 æ¬¡ä¸–ä»£ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆæŒ‡é‡

**1. Model-as-Data Paradigm**
- ãƒ¢ãƒ‡ãƒ« = é™çš„ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ â†’ å‹•çš„ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒªãƒ¼ãƒ 
- Git-LFS â†’ DVC (Data Version Control) â†’ Pachyderm

**2. Feature Storeçµ±åˆ**
- Feast, Tecton â†’ ã‚ªãƒ•ãƒ©ã‚¤ãƒ³/ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ç‰¹å¾´é‡ã®çµ±ä¸€ç®¡ç†
- è¨“ç·´/æ¨è«–ã®Feature Skewè§£æ¶ˆ

**3. Federated Learning**
- ãƒ‡ãƒã‚¤ã‚¹ä¸Šå­¦ç¿’ â†’ ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·
- Differential Privacyä¿è¨¼ä»˜ãå‹¾é…é›†ç´„

**4. AutoML in Production**
- Neural Architecture Search (NAS) â†’ è‡ªå‹•ãƒ¢ãƒ‡ãƒ«è¨­è¨ˆ
- Hyperparameter Optimization (Optuna, Ray Tune) â†’ ç¶™ç¶šçš„ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

---

## Z7: æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ â€” Course IIIå®Œå…¨èª­äº†

:::message
**ãŠã‚ã§ã¨ã†ï¼** Course III (å…¨14è¬›: ç¬¬19-32å›) ã‚’å®Œå…¨åˆ¶è¦‡ã—ãŸï¼
:::

### 7.1 Course IIIå­¦ç¿’ãƒãƒƒãƒ—

```mermaid
graph TB
    subgraph "Phase 1: åŸºç¤ç†è«– (ç¬¬19-23å›)"
        L19[ç¬¬19å›: Backpropå®Œå…¨ç‰ˆ]
        L20[ç¬¬20å›: Optimizerç¾¤]
        L21[ç¬¬21å›: Norm & Regularization]
        L22[ç¬¬22å›: CNNå®Œå…¨ç‰ˆ]
        L23[ç¬¬23å›: RNN/LSTM/GRU]
    end

    subgraph "Phase 2: å…ˆé€²ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ (ç¬¬24-27å›)"
        L24[ç¬¬24å›: Transformerå®Œå…¨ç‰ˆ]
        L25[ç¬¬25å›: BERT/GPT/T5]
        L26[ç¬¬26å›: Vision Transformer]
        L27[ç¬¬27å›: Diffusion Models]
    end

    subgraph "Phase 3: Production (ç¬¬28-32å›)"
        L28[ç¬¬28å›: Distributed Training]
        L29[ç¬¬29å›: Quantization & Pruning]
        L30[ç¬¬30å›: ONNX & Deployment]
        L31[ç¬¬31å›: MLOpså®Œå…¨ç‰ˆ]
        L32[ç¬¬32å›: Production & Feedback Loop]
    end

    L19 --> L20 --> L21 --> L22 --> L23
    L23 --> L24 --> L25 --> L26 --> L27
    L27 --> L28 --> L29 --> L30 --> L31 --> L32

    style L32 fill:#ff6b6b,stroke:#c92a2a,stroke-width:3px
```

### 7.2 çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æŒ¯ã‚Šè¿”ã‚Š

**ã‚ãªãŸãŒæ§‹ç¯‰ã—ãŸE2E Production MLã‚·ã‚¹ãƒ†ãƒ **:

| Component | Technology | Role | Key Metrics |
|-----------|-----------|------|-------------|
| **è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³** | Julia + Lux + Reactant | GPU/TPUè¨“ç·´ + ONNXå‡ºåŠ› | Epoch: 3.2s (TPU v5e) |
| **æ¨è«–ã‚µãƒ¼ãƒãƒ¼** | Rust + ort + Axum | ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ¨è«– | p95 < 10ms |
| **APIã‚²ãƒ¼ãƒˆã‚¦ã‚§ã‚¤** | Elixir + Phoenix | Rate Limit + èªè¨¼ | 50K req/s |
| **ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯DB** | PostgreSQL + TimescaleDB | æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ä¿å­˜ | 10M records/day |
| **ç¶™ç¶šå­¦ç¿’** | Kubeflow Pipelines | è‡ªå‹•å†è¨“ç·´ | Daily batch |
| **ç›£è¦–** | Prometheus + Grafana | ãƒ¡ãƒˆãƒªã‚¯ã‚¹å¯è¦–åŒ– | 99.9% uptime |
| **è² è·ãƒ†ã‚¹ãƒˆ** | k6 + Locust | ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¤œè¨¼ | 1K VUs |
| **Chaos Engineering** | Chaos Mesh | éšœå®³æ³¨å…¥ãƒ†ã‚¹ãƒˆ | MTTR < 5min |

### 7.3 æŠ€è¡“çš„æˆé•·ã®è»Œè·¡

**ç¬¬19å› (Backprop)** â†’ **ç¬¬32å› (Production)**ã¾ã§ã®é€²åŒ–:

```julia
# ç¬¬19å›: å˜ç´”ãªBackpropagation
function backward_simple(x, y, Å·)
    dL_dÅ· = 2 * (Å· - y)  # MSE gradient
    return dL_dÅ·
end

# â†“ â†“ â†“

# ç¬¬32å›: Production-ready Backprop with Gradient Clipping & Mixed Precision
function backward_production(
    loss_fn::Function,
    model::Lux.AbstractExplicitLayer,
    ps::NamedTuple,
    st::NamedTuple,
    batch::Tuple,
    scaler::GradScaler
)
    # 1. Mixed Precision Forward (AMP)
    (loss, st), pullback = Zygote.pullback(ps, st) do p, s
        Å·, s_new = model(batch[1], p, s)
        loss_fn(Å·, batch[2]), s_new
    end

    # 2. Scaled Backward
    scaled_loss = scaler.scale * loss
    grads = pullback((scaler.scale, nothing))[1]

    # 3. Gradient Clipping (é˜²æ­¢çˆ†ç™º)
    grads = clip_gradients(grads, max_norm=1.0)

    # 4. Unscale & Check for Inf/NaN
    grads = unscale_gradients(grads, scaler.scale)
    if !all(isfinite, grads)
        @warn "Gradient overflow detected, skipping update"
        return ps, st, loss
    end

    return grads, st, loss
end
```

**Key Takeaways**:
1. **ç†è«– â†’ å®Ÿè·µã®å®Œå…¨ãªæ©‹æ¸¡ã—**: æ•°å¼ â†’ Juliaå®Ÿè£… â†’ Rustæœ€é©åŒ– â†’ Productioné…å‚™
2. **3è¨€èªãƒã‚¹ã‚¿ãƒ¼**: ğŸ¦€ Rust (é€Ÿåº¦), âš¡ Julia (è¡¨ç¾åŠ›), ğŸ”® Elixir (ä¸¦è¡Œæ€§)
3. **End-to-Endã‚·ã‚¹ãƒ†ãƒ æ€è€ƒ**: å˜ä¸€ãƒ¢ãƒ‡ãƒ« â†’ ãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯MLã‚·ã‚¹ãƒ†ãƒ 
4. **å“è³ªä¿è¨¼**: ãƒ†ã‚¹ãƒˆ â†’ è² è·ãƒ†ã‚¹ãƒˆ â†’ Chaos Engineering

### 7.4 æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: Advanced Topics

**ã•ã‚‰ã«æ·±ã‚ã‚‹ãªã‚‰**:

1. **Reinforcement Learning (RL)**
   - DQN, A3C, PPO, SAC
   - OpenAI Gymç’°å¢ƒ
   - AlphaZeroç³»ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

2. **Multimodal Learning**
   - CLIP (Contrastive Language-Image Pre-training)
   - Flamingo (Vision-Language Model)
   - ImageBind (6ãƒ¢ãƒ€ãƒªãƒ†ã‚£çµ±åˆ)

3. **Large Language Models (LLM)**
   - GPT-4, Claude, Gemini architecture
   - Retrieval-Augmented Generation (RAG)
   - Mixture-of-Experts (MoE)

4. **Efficient Deep Learning**
   - Flash Attention, PagedAttention
   - LoRA (Low-Rank Adaptation)
   - Sparse Mixture-of-Experts

---

### 6.X ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

:::message alert
**Critical Question**: MLã‚·ã‚¹ãƒ†ãƒ ã®æœ¬è³ªã¯ã€Œãƒ¢ãƒ‡ãƒ«ã€ã‹ã€Œãƒ‡ãƒ¼ã‚¿ã€ã‹ï¼Ÿ
:::

### å•ã„1: Model-Centric vs Data-Centric AI

**å¾“æ¥ã®MLé–‹ç™º**:
```
å›ºå®šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ â†’ ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ”¹å–„ â†’ ç²¾åº¦å‘ä¸Š
```

**Data-Centric AI (Andrew Ng, 2021)**:
```
å›ºå®šãƒ¢ãƒ‡ãƒ« â†’ ãƒ‡ãƒ¼ã‚¿å“è³ªæ”¹å–„ â†’ ç²¾åº¦å‘ä¸Š
```

**å®Ÿé¨“**:
- ImageNet-1Kã§ ResNet-50ã‚’è¨“ç·´
- Approach A: ãƒ‡ãƒ¼ã‚¿å›ºå®š â†’ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ”¹å–„ (ResNet-50 â†’ EfficientNet-B7) â†’ **+2.3% accuracy**
- Approach B: ãƒ¢ãƒ‡ãƒ«å›ºå®š â†’ ãƒã‚¤ã‚ºãƒ©ãƒ™ãƒ«é™¤å» + Data Augmentation â†’ **+4.1% accuracy**

**çµè«–**: **ãƒ‡ãƒ¼ã‚¿å“è³ª > ãƒ¢ãƒ‡ãƒ«è¤‡é›‘åŒ–** (ä¸€å®šã®é–¾å€¤ä»¥ä¸Šã§ã¯)

### å•ã„2: Training vs Inference â€” ã©ã¡ã‚‰ãŒæœ¬è³ªã‹ï¼Ÿ

**Trainingè¦–ç‚¹**:
- å­¦ç¿’ = çŸ¥è­˜ç²å¾—ã®ãƒ—ãƒ­ã‚»ã‚¹
- Backpropagation = çŸ¥è­˜ã®çµæ™¶åŒ–
- ãƒ¢ãƒ‡ãƒ« = å­¦ç¿’ã®å‰¯ç”£ç‰©

**Inferenceè¦–ç‚¹**:
- æ¨è«– = ä¾¡å€¤æä¾›ã®ç¬é–“
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ = ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã§æ±ºã¾ã‚‹
- ãƒ¢ãƒ‡ãƒ« = æ¨è«–ã®ãŸã‚ã®é“å…·

**Production Reality**:
```
Training: 1å›/æ—¥ (10åˆ†) = 0.7% of time
Inference: 1å„„å›/æ—¥ (10ms each) = 99.3% of time
```

**çµè«–**: **Inferenceæœ€é©åŒ–ãŒãƒ“ã‚¸ãƒã‚¹ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆæœ€å¤§** â†’ Quantization, Pruning, Distillation

### å•ã„3: Human vs Machine â€” èª°ãŒå­¦ç¿’ã™ã¹ãã‹ï¼Ÿ

**HITL (Human-in-the-Loop)**:
- äººé–“ = ãƒ©ãƒ™ãƒ«æä¾›è€…
- æ©Ÿæ¢° = ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’è€…

**Machine Teaching**:
- äººé–“ = æ•™å¸« (ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ è¨­è¨ˆ)
- æ©Ÿæ¢° = ç”Ÿå¾’ (åŠ¹ç‡çš„å­¦ç¿’)

**Active Learning**:
- æ©Ÿæ¢° = è³ªå•è€… (ä¸ç¢ºå®Ÿæ€§æ¤œå‡º)
- äººé–“ = å›ç­”è€… (é›£ã—ã„ã‚±ãƒ¼ã‚¹ã®ã¿)

**æœ€é©è§£**: **Collaborative Intelligence** â€” äººé–“ã¨æ©Ÿæ¢°ã®å¼·ã¿ã‚’çµ„ã¿åˆã‚ã›ã‚‹
- äººé–“: å‰µé€ æ€§, å¸¸è­˜, å€«ç†åˆ¤æ–­
- æ©Ÿæ¢°: ã‚¹ã‚±ãƒ¼ãƒ«, é€Ÿåº¦, ä¸€è²«æ€§

### å•ã„4: Static vs Dynamic â€” ãƒ¢ãƒ‡ãƒ«ã¯å›ºå®šã‹é€²åŒ–ã‹ï¼Ÿ

**Static Deployment**:
- ãƒ¢ãƒ‡ãƒ« = 1å›è¨“ç·´ â†’ æ°¸ç¶šçš„ã«ä½¿ç”¨
- åˆ©ç‚¹: ã‚·ãƒ³ãƒ—ãƒ«, å†ç¾æ€§é«˜ã„
- æ¬ ç‚¹: Concept Driftå¯¾å¿œä¸å¯

**Continuous Learning**:
- ãƒ¢ãƒ‡ãƒ« = å¸¸ã«é€²åŒ–
- åˆ©ç‚¹: æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã«é©å¿œ
- æ¬ ç‚¹: Catastrophic Forgetting, ãƒ‡ãƒãƒƒã‚°å›°é›£

**Production Tradeoff**:
```python
# Googleç¿»è¨³: é€±æ¬¡å†è¨“ç·´ (Static + Periodic Update)
if week_passed():
    retrain_model(new_data)
    A/B_test(old_model, new_model)
    if new_model_better():
        deploy(new_model)

# æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ : ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’ (Dynamic)
on_user_click(item):
    update_embedding(user, item)  # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å‹¾é…æ›´æ–°
    refresh_recommendations()
```

**çµè«–**: **ã‚¿ã‚¹ã‚¯ä¾å­˜** â€” Translation (é€±æ¬¡), Recommendation (ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ), Medical (é™çš„+å³æ ¼æ¤œè¨¼)

### æœ€çµ‚å•ã„: MLã®æœªæ¥ã¯ï¼Ÿ

**äºˆæƒ³ã•ã‚Œã‚‹æŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰ (2025-2030)**:

1. **Foundation Modelsæ™‚ä»£**
   - Pre-trainedå·¨å¤§ãƒ¢ãƒ‡ãƒ« (GPT-5, Gemini Ultra) â†’ Fine-tuningä¸»æµ
   - ã‚¼ãƒ­ã‹ã‚‰è¨“ç·´ â†’ ã»ã¼æ¶ˆæ»…

2. **Agentic AI**
   - Tool Use (é–¢æ•°å‘¼ã³å‡ºã—, APIé€£æº)
   - Multi-Agent Collaboration
   - Self-Improving Systems

3. **Multimodalçµ±åˆ**
   - Text + Image + Audio + Video â†’ çµ±ä¸€ãƒ¢ãƒ‡ãƒ«
   - ä»»æ„ãƒ¢ãƒ€ãƒªãƒ†ã‚£å…¥å‡ºåŠ›

4. **Efficient AI**
   - 1-bit LLMs (BitNet)
   - Mixture-of-Experts (MoE)
   - On-Device AI (ã‚¹ãƒãƒ›, ã‚¨ãƒƒã‚¸)

**ã‚ãªãŸã®å½¹å‰²**:
- **ç†è«–ã‚’å®Ÿè£…ã«è½ã¨ã›ã‚‹**: è«–æ–‡ â†’ Production Code
- **ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã‚’è¨­è¨ˆã§ãã‚‹**: Training â†’ Serving â†’ Monitoring â†’ Feedback
- **å“è³ªã‚’ä¿è¨¼ã§ãã‚‹**: Testing â†’ Load Testing â†’ Chaos Engineering

---

## è¨˜æ³•è¦ç´„

### æ•°å­¦è¨˜æ³•

| è¨˜å· | æ„å‘³ | ä¾‹ |
|------|------|-----|
| $\theta$ | ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | $\theta \in \mathbb{R}^d$ |
| $\mathcal{L}$ | æå¤±é–¢æ•° | $\mathcal{L}(\theta) = \text{MSE}$ |
| $\nabla_\theta$ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«é–¢ã™ã‚‹å‹¾é… | $\nabla_\theta \mathcal{L}$ |
| $\mathbb{E}_{x \sim p}$ | åˆ†å¸ƒ$p$ã«é–¢ã™ã‚‹æœŸå¾…å€¤ | $\mathbb{E}_{x \sim \mathcal{D}}[f(x)]$ |
| $\mathcal{D}_{\text{pool}}$ | ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¼ãƒ« | Active Learningç”¨ |
| $x^{(i)}$ | $i$ç•ªç›®ã®ã‚µãƒ³ãƒ—ãƒ« | $(x^{(1)}, y^{(1)}), \ldots$ |
| $\mathcal{H}$ | ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ | $\mathcal{H}(p) = -\sum p \log p$ |
| $\text{MI}(X;Y)$ | ç›¸äº’æƒ…å ±é‡ | $\text{MI}(y;\theta \mid x, \mathcal{D})$ |

### ã‚³ãƒ¼ãƒ‰è¦ç´„

**Julia**:
```julia
# é–¢æ•°å: snake_case
function train_model(data::Matrix, labels::Vector)
    # ...
end

# å‹å: PascalCase
struct TrainingPipeline
    model::Lux.AbstractExplicitLayer
end

# å®šæ•°: UPPER_CASE
const BATCH_SIZE = 32
```

**Rust**:
```rust
// é–¢æ•°å: snake_case
pub fn run_inference(input: &[f32]) -> Vec<f32> {
    // ...
}

// å‹å: PascalCase
pub struct InferenceEngine {
    session: Session,
}

// å®šæ•°: SCREAMING_SNAKE_CASE
const MAX_BATCH_SIZE: usize = 128;
```

**Elixir**:
```elixir
# é–¢æ•°å: snake_case
def process_request(request) do
  # ...
end

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å: PascalCase
defmodule FeedbackCollector do
  # ...
end

# ã‚¢ãƒˆãƒ : lowercase
:ok, :error, :rate_limited
```

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å›³è¨˜æ³•

```mermaid
graph LR
    A[Component A] -->|REST API| B[Component B]
    B -->|gRPC| C[Component C]
    C -.->|Async| D[(Database)]

    style A fill:#4ecdc4,stroke:#1a535c
    style B fill:#ffe66d,stroke:#ff6b6b
    style C fill:#95e1d3,stroke:#38ada9
    style D fill:#f38181,stroke:#aa4465
```

- **å®Ÿç·š**: åŒæœŸé€šä¿¡ (REST, gRPC)
- **ç‚¹ç·š**: éåŒæœŸé€šä¿¡ (Message Queue, Event)
- **å††æŸ±**: ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆã‚¢ (DB, Cache)
- **è‰²**: è¨€èªåˆ¥ (ğŸ¦€ Rust=é’, âš¡ Julia=é»„, ğŸ”® Elixir=ç·‘)

---

:::message
**ğŸ“ Course IIIå®Œå…¨åˆ¶è¦‡ãŠã‚ã§ã¨ã†ï¼**

ã‚ãªãŸã¯ä»Šã€ä»¥ä¸‹ã®ã‚¹ã‚­ãƒ«ã‚’ç²å¾—ã—ãŸ:
1. âœ… ç†è«–ï¼ˆCourse I-IIï¼‰â†’ å®Ÿè£…ï¼ˆCourse IIIï¼‰ã®å®Œå…¨æ©‹æ¸¡ã—
2. âœ… Julia/Rust/Elixir 3è¨€èªã§ã®Production E2Eã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰åŠ›
3. âœ… è¨“ç·´â†’æ¨è«–â†’é…ä¿¡â†’ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯â†’ç¶™ç¶šå­¦ç¿’ã®å®Ÿè£…
4. âœ… è² è·ãƒ†ã‚¹ãƒˆãƒ»Chaos Engineeringãƒ»MLOpsã®å®Ÿè·µçŸ¥è­˜

**ã“ã“ã‹ã‚‰2ã¤ã®ãƒ«ãƒ¼ãƒˆãŒåˆ†å²ã™ã‚‹**:

**ğŸŒŠ Course IV: æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ç†è«–æ·±åŒ–ï¼ˆç¬¬33-42å›ã€å…¨10å›ï¼‰**
- Normalizing Flows â†’ EBM â†’ Score Matching â†’ DDPM â†’ SDE â†’ Flow Matching â†’ LDM â†’ Consistency Models â†’ World Models â†’ çµ±ä¸€ç†è«–
- ã€Œæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«è«–æ–‡ã®ç†è«–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒå°å‡ºã§ãã‚‹ã€æ•°å­¦åŠ›ã‚’ç²å¾—
- å¯†åº¦ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®è«–ç†çš„ãƒã‚§ãƒ¼ãƒ³ã‚’å®Œå…¨è¸ç ´

**ğŸ¨ Course V: ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–å¿œç”¨ï¼ˆç¬¬43-50å›ã€å…¨8å›ï¼‰**
- Visionãƒ»Audioãƒ»RLãƒ»Proteinãƒ»Moleculeãƒ»Climateãƒ»Robotãƒ»Simulation
- å„ãƒ‰ãƒ¡ã‚¤ãƒ³ã®æœ€æ–°SOTAæŠ€è¡“ã‚’å®Ÿè£…
- å®Ÿä¸–ç•Œå•é¡Œã¸ã®é©ç”¨åŠ›ã‚’é›ãˆã‚‹

**Course IVã¨Vã¯ç‹¬ç«‹** â€” ã©ã¡ã‚‰ã‹ã‚‰å§‹ã‚ã¦ã‚‚è‰¯ã„ã€‚ä¸¡æ–¹å±¥ä¿®ã§å…¨50å›å®Œå…¨åˆ¶è¦‡ã€‚

**æ¬¡å›äºˆå‘Š: ç¬¬33å› Normalizing Flows â€” å¯é€†å¤‰æ›ã§å³å¯†å°¤åº¦ã‚’æ‰‹ã«å…¥ã‚Œã‚‹**
:::

---

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
