---
title: "ç¬¬4å›: ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸ²"
type: "tech"
topics: ["machinelearning", "deeplearning", "probability", "python"]
published: true
---

# ç¬¬4å›: ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦ â€” ä¸ç¢ºå®Ÿæ€§ã‚’æ•°å­¦ã§é£¼ã„ãªã‚‰ã™

> **ç¢ºç‡ã¨ã¯ã€Œã‚ã‹ã‚‰ãªã•ã€ã®è¨€èªã ã€‚ã“ã®è¨€èªã‚’æ“ã‚Œã‚‹è€…ã ã‘ãŒã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æœ¬è³ªã«è§¦ã‚Œã‚‰ã‚Œã‚‹ã€‚**

ç¬¬3å›ã§è¡Œåˆ—ã®åˆ†è§£ã¨å¾®åˆ†ã‚’æ‰‹ã«å…¥ã‚ŒãŸã€‚SVDã§ç©ºé–“ã‚’åˆ†è§£ã—ã€ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã§å¤‰æ›ã®å±€æ‰€çš„æŒ¯ã‚‹èˆã„ã‚’æ‰ãˆã€è‡ªå‹•å¾®åˆ†ã§Backpropagationã®æ•°å­¦çš„åŸºç›¤ã‚’ç†è§£ã—ãŸã€‚ã ãŒã€ã“ã“ã§æ ¹æœ¬çš„ãªå•ã„ãŒç«‹ã¡ã¯ã ã‹ã‚‹ â€” **ãƒ‡ãƒ¼ã‚¿ã«ã¯ã€Œãƒã‚¤ã‚ºã€ãŒã‚ã‚‹ã€‚ä¸ç¢ºå®Ÿæ€§ã‚’ã©ã†æ‰±ã†ã®ã‹ï¼Ÿ**

ç·šå½¢ä»£æ•°ã¯ã€Œç¢ºå®šã—ãŸé‡ã€ã®æ•°å­¦ã ã€‚è¡Œåˆ— $A$ ã‚’ã‹ã‘ã‚Œã°ã€ãƒ™ã‚¯ãƒˆãƒ« $\mathbf{x}$ ã¯ç¢ºå®šçš„ã« $A\mathbf{x}$ ã«å¤‰æ›ã•ã‚Œã‚‹ã€‚ã ãŒç¾å®Ÿã®ãƒ‡ãƒ¼ã‚¿ã¯é•ã†ã€‚åŒã˜å…¥åŠ›ã«å¯¾ã—ã¦å‡ºåŠ›ãŒã°ã‚‰ã¤ãã€‚åŒã˜æ–‡è„ˆã«å¯¾ã—ã¦LLMãŒæ¯å›é•ã†ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆã™ã‚‹ã€‚ã“ã®ã€Œã°ã‚‰ã¤ãã€ã‚’è¨˜è¿°ã™ã‚‹æ•°å­¦ãŒç¢ºç‡è«–ã ã€‚

æœ¬è¬›ç¾©ã§ã¯ã€ç¢ºç‡ç©ºé–“ã®å³å¯†ãªå®šç¾©ã‹ã‚‰å§‹ã‚ã¦ã€ç¢ºç‡åˆ†å¸ƒã®è¨˜è¿°ãƒ»æ“ä½œãƒ»æ¨å®šã‚’å®Œå…¨ã«ç¿’å¾—ã™ã‚‹ã€‚ãã—ã¦ã“ã‚ŒãŒå˜ãªã‚‹æ•°å­¦ã®æ¼”ç¿’ã§ã¯ãªã„ã“ã¨ã‚’ã€LLMã®è‡ªå·±å›å¸°ç”Ÿæˆ $p(x_t \mid x_{<t})$ â€” ã¾ã•ã«æ¡ä»¶ä»˜ãç¢ºç‡ãã®ã‚‚ã® â€” ã‚’é€šã˜ã¦ä½“æ„Ÿã™ã‚‹ã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2025-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚
:::

```mermaid
graph LR
    A["ğŸ² ç¢ºç‡ç©ºé–“<br/>(Î©,F,P)"] --> B["ğŸ“Š ç¢ºç‡å¤‰æ•°<br/>X: Î©â†’â„"]
    B --> C["ğŸ“ˆ ç¢ºç‡åˆ†å¸ƒ<br/>é›¢æ•£ãƒ»é€£ç¶š"]
    C --> D["ğŸ”„ ãƒ™ã‚¤ã‚ºæ¨è«–<br/>äº‹å¾Œâˆå°¤åº¦Ã—äº‹å‰"]
    D --> E["ğŸ“ MLE/MAP<br/>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®š"]
    E --> F["ğŸ¯ Fisheræƒ…å ±é‡<br/>æ¨å®šã®é™ç•Œ"]
    style A fill:#e1f5fe
    style F fill:#c8e6c9
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 6 | æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” ãƒ™ã‚¤ã‚ºã®å®šç†ã‚’3è¡Œã§å‹•ã‹ã™

**ã‚´ãƒ¼ãƒ«**: ç¢ºç‡ã®æ ¸å¿ƒã‚’30ç§’ã§ä½“é¨“ã™ã‚‹ã€‚

```python
import numpy as np

# Bayes' theorem: P(A|B) = P(B|A) * P(A) / P(B)
prior = 0.01            # P(disease) = 1%
sensitivity = 0.95      # P(positive | disease) = 95%
false_positive = 0.05   # P(positive | healthy) = 5%
p_positive = sensitivity * prior + false_positive * (1 - prior)
posterior = sensitivity * prior / p_positive
print(f"Prior:     {prior:.2%}")
print(f"Posterior: {posterior:.2%}")   # 16.1% â€” not 95%!
```

å‡ºåŠ›:
```
Prior:     1.00%
Posterior: 16.10%
```

**ã“ã®3è¡Œã®è£ã«ã‚ã‚‹æ•°å¼**:

$$
P(\text{disease} \mid \text{positive}) = \frac{P(\text{positive} \mid \text{disease}) \cdot P(\text{disease})}{P(\text{positive})}
$$

æ¤œæŸ»ã®æ„Ÿåº¦ãŒ95%ã§ã‚‚ã€äº‹å‰ç¢ºç‡ãŒ1%ãªã‚‰ã€é™½æ€§ã¨å‡ºã¦ã‚‚å®Ÿéš›ã«ç—…æ°—ã§ã‚ã‚‹ç¢ºç‡ã¯**ãŸã£ãŸ16%**ã€‚ç›´æ„Ÿã«åã™ã‚‹ã€‚ã ãŒæ•°å¼ã¯å˜˜ã‚’ã¤ã‹ãªã„ã€‚ã“ã‚ŒãŒãƒ™ã‚¤ã‚ºã®å®šç† [^1] ã®åŠ›ã ã€‚

ã“ã®ã€Œäº‹å‰ã®ä¿¡å¿µã‚’æ–°ã—ã„è¨¼æ‹ ã§æ›´æ–°ã™ã‚‹ã€æ§‹é€ ã¯ã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æ ¹å¹¹ã«ç¾ã‚Œã‚‹ã€‚VAEã®äº‹å¾Œåˆ†å¸ƒ $q_\phi(\mathbf{z} \mid \mathbf{x})$ [^2] ã‚‚ã€ãƒ™ã‚¤ã‚ºã®å®šç†ã®å¤‰åˆ†è¿‘ä¼¼ã«ä»–ãªã‚‰ãªã„ã€‚

:::message
**é€²æ—: 3% å®Œäº†** ãƒ™ã‚¤ã‚ºã®å®šç†ã‚’ã€Œå‹•ã‹ã—ã¦ã€ä½“é¨“ã—ãŸã€‚ç›´æ„Ÿã¨æ•°å­¦ã®ã‚®ãƒ£ãƒƒãƒ— â€” ã“ã‚ŒãŒç¢ºç‡è«–ã‚’å­¦ã¶ç†ç”±ã ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” ç¢ºç‡åˆ†å¸ƒã‚’è§¦ã£ã¦éŠã¶

### 1.1 é›¢æ•£åˆ†å¸ƒ â€” Categoricalåˆ†å¸ƒã¨Softmax

LLMãŒæ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é¸ã¶ã¨ãã€èªå½™å…¨ä½“ã®ç¢ºç‡åˆ†å¸ƒã‚’è¨ˆç®—ã™ã‚‹ã€‚ã“ã‚Œã¯**Categoricalåˆ†å¸ƒ**ãã®ã‚‚ã®ã ã€‚

$$
p(x = k) = \pi_k, \quad \sum_{k=1}^{K} \pi_k = 1, \quad \pi_k \geq 0
$$

SoftmaxãŒè¿”ã™ç¢ºç‡ãƒ™ã‚¯ãƒˆãƒ«ã¯ã€ã¾ã•ã«Categoricalåˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\boldsymbol{\pi} = (\pi_1, \ldots, \pi_K)$ ã ã€‚

```python
import numpy as np

def categorical_sample(logits: np.ndarray, temperature: float = 1.0, n_samples: int = 10000) -> np.ndarray:
    """Sample from categorical distribution via softmax.

    corresponds to: p(x=k) = exp(z_k/T) / Î£_j exp(z_j/T)
    """
    scaled = logits / temperature
    probs = np.exp(scaled - np.max(scaled))
    probs /= probs.sum()
    return np.random.choice(len(logits), size=n_samples, p=probs)

# Simulate LLM next-token prediction
vocab = ["the", "a", "cat", "dog", "sat", "on", "mat", "ran"]
logits = np.array([2.5, 1.0, 3.0, 0.5, 2.0, 1.5, 0.8, 0.3])

print("=== LLM Next-Token Sampling Simulation ===\n")
print(f"{'Token':<8} {'Logit':>6} | {'T=0.5':>8} {'T=1.0':>8} {'T=2.0':>8}")
print("-" * 50)

for T in [0.5, 1.0, 2.0]:
    samples = categorical_sample(logits, T)
    unique, counts = np.unique(samples, return_counts=True)
    freq = np.zeros(len(vocab))
    for u, c in zip(unique, counts):
        freq[u] = c / len(samples)
    if T == 0.5:
        all_freqs = [freq]
    else:
        all_freqs.append(freq)

for i, word in enumerate(vocab):
    row = f"{word:<8} {logits[i]:>6.1f} |"
    for freq in all_freqs:
        row += f" {freq[i]:>7.1%}"
    print(row)
```

| æ¸©åº¦ $T$ | åŠ¹æœ | LLMã§ã®ç”¨é€” |
|:---------|:-----|:-----------|
| $T \to 0$ | æœ€å¤§ç¢ºç‡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿é¸æŠï¼ˆgreedyï¼‰ | ç¿»è¨³ãƒ»è¦ç´„ï¼ˆæ­£ç¢ºã•é‡è¦–ï¼‰ |
| $T = 1.0$ | ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’é€šã‚Šã®åˆ†å¸ƒ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ |
| $T > 1.0$ | åˆ†å¸ƒãŒå¹³å¦åŒ–ï¼ˆå¤šæ§˜æ€§UPï¼‰ | å‰µä½œãƒ»ãƒ–ãƒ¬ã‚¹ãƒˆ |

Hintonã‚‰ã®çŸ¥è­˜è’¸ç•™è«–æ–‡ [^3] ã§ä½“ç³»åŒ–ã•ã‚ŒãŸã“ã®æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€ç¬¬1å›ã§ä½“é¨“ã—ãŸSoftmaxã®æ‹¡å¼µã ã€‚ã“ã“ã§é‡è¦ãªã®ã¯ã€**æ¸©åº¦ã‚’å¤‰ãˆã¦ã‚‚Categoricalåˆ†å¸ƒã§ã‚ã‚‹ã“ã¨è‡ªä½“ã¯å¤‰ã‚ã‚‰ãªã„**ã¨ã„ã†ç‚¹ã€‚åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\boldsymbol{\pi}$ ãŒå¤‰ã‚ã‚‹ã ã‘ã ã€‚

:::message
ã“ã“ã§å¤šãã®äººãŒæ··ä¹±ã™ã‚‹ã®ãŒã€Œlogitã¨ã¯ä½•ã‹ã€ã ã€‚logitã¯Softmaxé©ç”¨**å‰**ã®ç”Ÿã®ã‚¹ã‚³ã‚¢ã€‚ç¢ºç‡ã§ã¯ãªã„ã€‚Softmaxã‚’é€šã—ã¦åˆã‚ã¦Categoricalåˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\pi_k$ ã«ãªã‚‹ã€‚
:::

### 1.2 é€£ç¶šåˆ†å¸ƒ â€” ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®ä¸‡èƒ½æ€§

æ©Ÿæ¢°å­¦ç¿’ã§æœ€ã‚‚é »ç¹ã«ç¾ã‚Œã‚‹é€£ç¶šåˆ†å¸ƒ â€” ã‚¬ã‚¦ã‚¹åˆ†å¸ƒï¼ˆæ­£è¦åˆ†å¸ƒï¼‰ã€‚

$$
\mathcal{N}(x \mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)
$$

```python
import numpy as np

def gaussian_pdf(x: np.ndarray, mu: float, sigma: float) -> np.ndarray:
    """Gaussian probability density function.

    corresponds to: N(x|Î¼,ÏƒÂ²) = (2Ï€ÏƒÂ²)^{-1/2} exp(-(x-Î¼)Â²/(2ÏƒÂ²))
    """
    return (1 / np.sqrt(2 * np.pi * sigma**2)) * np.exp(-((x - mu)**2) / (2 * sigma**2))

x = np.linspace(-6, 6, 1000)
configs = [(0, 1, "Î¼=0,Ïƒ=1 (æ¨™æº–æ­£è¦)"), (0, 0.5, "Î¼=0,Ïƒ=0.5 (é‹­ã„)"),
           (2, 1, "Î¼=2,Ïƒ=1 (å³ã‚·ãƒ•ãƒˆ)"), (0, 2, "Î¼=0,Ïƒ=2 (åºƒã„)")]

print(f"{'Config':<25} {'Peak':>8} {'P(|x|<1)':>10} {'P(|x|<2)':>10}")
print("-" * 55)
for mu, sigma, name in configs:
    pdf = gaussian_pdf(x, mu, sigma)
    peak = gaussian_pdf(np.array([mu]), mu, sigma)[0]
    dx = x[1] - x[0]
    p1 = np.sum(pdf[np.abs(x - mu) < 1]) * dx
    p2 = np.sum(pdf[np.abs(x - mu) < 2]) * dx
    print(f"{name:<25} {peak:>8.4f} {p1:>9.1%} {p2:>9.1%}")
```

å‡ºåŠ›:
```
Config                       Peak   P(|x|<1)   P(|x|<2)
-------------------------------------------------------
Î¼=0,Ïƒ=1 (æ¨™æº–æ­£è¦)         0.3989     68.3%     95.4%
Î¼=0,Ïƒ=0.5 (é‹­ã„)           0.7979     95.4%    100.0%
Î¼=2,Ïƒ=1 (å³ã‚·ãƒ•ãƒˆ)         0.3989     68.3%     95.4%
Î¼=0,Ïƒ=2 (åºƒã„)             0.1995     38.3%     68.3%
```

**68-95-99.7ãƒ«ãƒ¼ãƒ«**: æ¨™æº–æ­£è¦åˆ†å¸ƒã§ã¯ã€$\pm 1\sigma$ ã«68.3%ã€$\pm 2\sigma$ ã«95.4%ã€$\pm 3\sigma$ ã«99.7%ã®ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã‚‹ã€‚

ãªãœã‚¬ã‚¦ã‚¹åˆ†å¸ƒãŒã“ã‚Œã»ã©é‡è¦ã‹ï¼Ÿ ä¸­å¿ƒæ¥µé™å®šç†ï¼ˆCLTï¼‰ãŒãã®ç­”ãˆã  â€” **ç‹¬ç«‹ãªç¢ºç‡å¤‰æ•°ã®å’Œã¯ã€å…ƒã®åˆ†å¸ƒãŒä½•ã§ã‚ã‚Œã€æ­£è¦åˆ†å¸ƒã«è¿‘ã¥ã**ã€‚ã“ã‚Œã¯å¾Œã®Zone 3ã§å³å¯†ã«å°å‡ºã™ã‚‹ã€‚

:::details ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã¨LLM â€” éš ã‚ŒãŸæ¥ç¶š
LLMã®å­¦ç¿’ã§ã¯ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆæœŸåŒ–ã«ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã‚’ä½¿ã†ã€‚Heã®åˆæœŸåŒ– $\mathcal{N}(0, 2/n)$ ã‚„Xavierã®åˆæœŸåŒ– $\mathcal{N}(0, 2/(n_{in}+n_{out}))$ ã¯ã€å‹¾é…ã®åˆ†æ•£ã‚’åˆ¶å¾¡ã™ã‚‹ãŸã‚ã«åˆ†æ•£ã‚’ç²¾å¯†ã«è¨­å®šã—ã¦ã„ã‚‹ã€‚

VAE [^2] ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¯ã€æ½œåœ¨å¤‰æ•°ã®åˆ†å¸ƒã‚’ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ $q_\phi(\mathbf{z} \mid \mathbf{x}) = \mathcal{N}(\boldsymbol{\mu}, \text{diag}(\boldsymbol{\sigma}^2))$ ã§è¿‘ä¼¼ã™ã‚‹ã€‚ã€Œãªãœã‚¬ã‚¦ã‚¹ãªã®ã‹ã€ã¯ã€æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®æ€§è³ªã¨è¨ˆç®—ã®éƒ½åˆã«ã‚ˆã‚‹ â€” ã“ã‚Œã‚‚æœ¬è¬›ç¾©ã§æ‰±ã†ã€‚

æ‹¡æ•£ãƒ¢ãƒ‡ãƒ« [^4] ã®å‰æ–¹éç¨‹ã¯ã€ã‚¬ã‚¦ã‚¹ãƒã‚¤ã‚ºã‚’æ®µéšçš„ã«åŠ ãˆã¦ã„ãæ“ä½œã ã€‚$q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}(\sqrt{1-\beta_t}\mathbf{x}_{t-1}, \beta_t \mathbf{I})$ â€” ã“ã“ã§ã‚‚ã‚¬ã‚¦ã‚¹åˆ†å¸ƒãŒä¸»å½¹ã‚’æ¼”ã˜ã‚‹ã€‚
:::

### 1.3 ãƒ™ãƒ«ãƒŒãƒ¼ã‚¤åˆ†å¸ƒã‹ã‚‰Multinomialã¸ â€” é›¢æ•£åˆ†å¸ƒã®ç³»è­œ

ç¢ºç‡åˆ†å¸ƒã¯å­¤ç«‹ã—ã¦å­˜åœ¨ã™ã‚‹ã®ã§ã¯ãªãã€**æ—ï¼ˆãƒ•ã‚¡ãƒŸãƒªãƒ¼ï¼‰**ã‚’å½¢æˆã™ã‚‹ã€‚

```mermaid
graph TD
    B["Bernoulli<br/>p(x=1)=p<br/>ã‚³ã‚¤ãƒ³1å›"]
    Bin["Binomial<br/>B(n,p)<br/>ã‚³ã‚¤ãƒ³nå›"]
    Cat["Categorical<br/>Cat(Ï€â‚,...,Ï€K)<br/>ã‚µã‚¤ã‚³ãƒ­1å›"]
    Multi["Multinomial<br/>Multi(n,Ï€)<br/>ã‚µã‚¤ã‚³ãƒ­nå›"]
    Poi["Poisson<br/>Poi(Î»)<br/>ç¨€ãªäº‹è±¡ã®ã‚«ã‚¦ãƒ³ãƒˆ"]

    B -->|"nå›ç¹°ã‚Šè¿”ã—"| Bin
    B -->|"Ké¢ã«æ‹¡å¼µ"| Cat
    Cat -->|"nå›ç¹°ã‚Šè¿”ã—"| Multi
    Bin -->|"nâ†’âˆ,pâ†’0<br/>np=Î»"| Poi

    style B fill:#e3f2fd
    style Cat fill:#fff3e0
```

```python
import numpy as np

# Bernoulli: coin flip
p_heads = 0.7
flips = np.random.binomial(1, p_heads, size=10000)
print(f"Bernoulli(p={p_heads}): mean={flips.mean():.3f} (theory={p_heads})")

# Binomial: n coin flips
n, p = 20, 0.3
binom_samples = np.random.binomial(n, p, size=10000)
print(f"Binomial(n={n},p={p}): mean={binom_samples.mean():.2f} (theory={n*p}), var={binom_samples.var():.2f} (theory={n*p*(1-p):.2f})")

# Categorical: dice roll (LLM token selection)
probs = np.array([0.1, 0.3, 0.05, 0.4, 0.15])
labels = ["A", "B", "C", "D", "E"]
cat_samples = np.random.choice(len(probs), size=10000, p=probs)
print(f"\nCategorical samples (10000 draws):")
for i, label in enumerate(labels):
    freq = (cat_samples == i).mean()
    print(f"  {label}: freq={freq:.3f} (theory={probs[i]:.3f})")

# Poisson: rare event counting
lam = 3.0
pois_samples = np.random.poisson(lam, size=10000)
print(f"\nPoisson(Î»={lam}): mean={pois_samples.mean():.2f} (theory={lam}), var={pois_samples.var():.2f} (theory={lam})")
```

| åˆ†å¸ƒ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å¹³å‡ | åˆ†æ•£ | MLå¿œç”¨ |
|:-----|:----------|:-----|:-----|:-------|
| Bernoulli($p$) | $p \in [0,1]$ | $p$ | $p(1-p)$ | äºŒå€¤åˆ†é¡ |
| Binomial($n, p$) | $n \in \mathbb{N}, p \in [0,1]$ | $np$ | $np(1-p)$ | ã‚«ã‚¦ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ |
| Categorical($\boldsymbol{\pi}$) | $\pi_k \geq 0, \sum \pi_k = 1$ | â€” | â€” | **LLMæ¬¡ãƒˆãƒ¼ã‚¯ãƒ³é¸æŠ** |
| Poisson($\lambda$) | $\lambda > 0$ | $\lambda$ | $\lambda$ | ç¨€ãªäº‹è±¡ |
| Gaussian($\mu, \sigma^2$) | $\mu \in \mathbb{R}, \sigma^2 > 0$ | $\mu$ | $\sigma^2$ | **VAEæ½œåœ¨ç©ºé–“** |

### 1.4 Dirichletåˆ†å¸ƒ â€” Categoricalåˆ†å¸ƒã®ãƒ™ã‚¤ã‚ºäº‹å‰åˆ†å¸ƒ

LLMã®å‡ºåŠ›ãŒCategoricalåˆ†å¸ƒãªã‚‰ã€ãã®ãƒ™ã‚¤ã‚ºäº‹å‰åˆ†å¸ƒã¯ä½•ã‹ï¼Ÿ ç­”ãˆã¯**Dirichletåˆ†å¸ƒ**ã ã€‚

$$
\text{Dir}(\boldsymbol{\pi} \mid \boldsymbol{\alpha}) = \frac{\Gamma(\sum_k \alpha_k)}{\prod_k \Gamma(\alpha_k)} \prod_{k=1}^{K} \pi_k^{\alpha_k - 1}
$$

Dirichletåˆ†å¸ƒã¯ã€Œç¢ºç‡ãƒ™ã‚¯ãƒˆãƒ« $\boldsymbol{\pi}$ ã®ä¸Šã®ç¢ºç‡åˆ†å¸ƒã€ã ã€‚$\boldsymbol{\alpha}$ ãŒé›†ä¸­åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã€$\alpha_k$ ãŒå¤§ãã„ã»ã© $\pi_k$ ã«è³ªé‡ãŒé›†ä¸­ã™ã‚‹ã€‚

```python
import numpy as np

def dirichlet_samples(alpha: np.ndarray, n_samples: int = 5) -> np.ndarray:
    """Sample probability vectors from Dirichlet(Î±).

    corresponds to: Ï€ ~ Dir(Î±), Î£Ï€_k = 1, Ï€_k â‰¥ 0
    """
    samples = np.random.dirichlet(alpha, size=n_samples)
    return samples

# Explore Dirichlet with different concentration parameters
K = 5  # 5-class categorical (like a tiny vocabulary)
print("=== Dirichlet Distribution â€” Prior for Categorical ===\n")

configs = [
    (np.ones(K) * 0.1, "Î±=0.1 (sparse â€” few classes dominate)"),
    (np.ones(K) * 1.0, "Î±=1.0 (uniform â€” any Ï€ equally likely)"),
    (np.ones(K) * 10.0, "Î±=10 (concentrated â€” all Ï€_k â‰ˆ 1/K)"),
    (np.array([10, 1, 1, 1, 1]), "Î±=[10,1,1,1,1] (class 0 dominant)"),
]

for alpha, name in configs:
    samples = dirichlet_samples(alpha, n_samples=3)
    print(f"{name}")
    for s in samples:
        print(f"  Ï€ = [{', '.join(f'{p:.3f}' for p in s)}]")
    mean = alpha / alpha.sum()
    print(f"  E[Ï€] = [{', '.join(f'{m:.3f}' for m in mean)}]")
    entropy = -np.mean([np.sum(s * np.log(s + 1e-10)) for s in dirichlet_samples(alpha, 10000)])
    print(f"  E[H(Ï€)] = {entropy:.3f} (average entropy of sampled distributions)")
    print()
```

| $\alpha$ | ç›´æ„Ÿ | LLMæ–‡è„ˆ |
|:---------|:-----|:--------|
| $\alpha_k \ll 1$ | ã‚¹ãƒ‘ãƒ¼ã‚¹ â€” å°‘æ•°ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«é›†ä¸­ | æ±ºå®šçš„ãªå‡ºåŠ› |
| $\alpha_k = 1$ | ä¸€æ§˜ â€” ã‚ã‚‰ã‚†ã‚‹åˆ†å¸ƒãŒç­‰ç¢ºç‡ | ç„¡æƒ…å ±äº‹å‰åˆ†å¸ƒ |
| $\alpha_k \gg 1$ | é›†ä¸­ â€” ä¸€æ§˜åˆ†å¸ƒã«è¿‘ã„ | ä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„å‡ºåŠ› |

**ãƒ™ã‚¤ã‚ºæ›´æ–°**: Categoricalå°¤åº¦ + Dirichletäº‹å‰ â†’ Dirichletäº‹å¾Œï¼ˆå…±å½¹ãƒšã‚¢ï¼‰ã€‚

$$
\boldsymbol{\pi} \sim \text{Dir}(\boldsymbol{\alpha}) \quad \to \quad \boldsymbol{\pi} \mid \text{data} \sim \text{Dir}(\boldsymbol{\alpha} + \text{counts})
$$

ã“ã‚Œã¯LLMã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã‘ã‚‹ã€Œå‡ºåŠ›åˆ†å¸ƒã®äº‹å‰çŸ¥è­˜ã€ã®æ•°å­¦çš„è¡¨ç¾ã ã€‚

### 1.5 åŒæ™‚åˆ†å¸ƒãƒ»å‘¨è¾ºåˆ†å¸ƒãƒ»æ¡ä»¶ä»˜ãåˆ†å¸ƒ

ç¢ºç‡ã®3ã¤ã®é¡”ã‚’ã€ã‚³ãƒ¼ãƒ‰ã§ä½“æ„Ÿã™ã‚‹ã€‚

```python
import numpy as np

# Joint distribution P(X, Y) as a 2D table
# X = weather (0=sunny, 1=rainy), Y = umbrella (0=no, 1=yes)
joint = np.array([
    [0.40, 0.10],  # sunny: no umbrella, umbrella
    [0.05, 0.45],  # rainy: no umbrella, umbrella
])

print("=== Joint Distribution P(X, Y) ===")
print(f"           No Umbrella  Umbrella")
print(f"Sunny:     {joint[0,0]:.2f}         {joint[0,1]:.2f}")
print(f"Rainy:     {joint[1,0]:.2f}         {joint[1,1]:.2f}")

# Marginal: P(X) = Î£_y P(X, y)
p_x = joint.sum(axis=1)
print(f"\n=== Marginal P(X) ===")
print(f"P(Sunny) = {p_x[0]:.2f}, P(Rainy) = {p_x[1]:.2f}")

# Marginal: P(Y) = Î£_x P(x, Y)
p_y = joint.sum(axis=0)
print(f"\n=== Marginal P(Y) ===")
print(f"P(No Umbrella) = {p_y[0]:.2f}, P(Umbrella) = {p_y[1]:.2f}")

# Conditional: P(Y|X) = P(X,Y) / P(X)
p_y_given_x = joint / p_x[:, np.newaxis]
print(f"\n=== Conditional P(Y|X) ===")
print(f"P(No Umbrella | Sunny) = {p_y_given_x[0,0]:.3f}")
print(f"P(Umbrella | Sunny)    = {p_y_given_x[0,1]:.3f}")
print(f"P(No Umbrella | Rainy) = {p_y_given_x[1,0]:.3f}")
print(f"P(Umbrella | Rainy)    = {p_y_given_x[1,1]:.3f}")

# Verify: each row sums to 1
print(f"\nRow sums (must be 1.0): {p_y_given_x.sum(axis=1)}")
```

| æ¦‚å¿µ | æ•°å¼ | ã‚³ãƒ¼ãƒ‰ | ç›´æ„Ÿ |
|:-----|:-----|:-------|:-----|
| åŒæ™‚åˆ†å¸ƒ | $P(X, Y)$ | `joint` | 2ã¤ã®å¤‰æ•°ã®å…¨çµ„ã¿åˆã‚ã› |
| å‘¨è¾ºåˆ†å¸ƒ | $P(X) = \sum_y P(X, y)$ | `joint.sum(axis=1)` | ç‰‡æ–¹ã‚’è¶³ã—ä¸Šã’ã‚‹ |
| æ¡ä»¶ä»˜ãåˆ†å¸ƒ | $P(Y \mid X) = \frac{P(X,Y)}{P(X)}$ | `joint / p_x[:, None]` | çŸ¥ã£ã¦ã„ã‚‹æƒ…å ±ã§çµã‚‹ |

**LLMã¨ã®æ¥ç¶š**: LLMã®è‡ªå·±å›å¸°ç”Ÿæˆã¯æ¡ä»¶ä»˜ãåˆ†å¸ƒã®é€£é–ã ã€‚

$$
p(\mathbf{x}) = p(x_1) \cdot p(x_2 \mid x_1) \cdot p(x_3 \mid x_1, x_2) \cdots = \prod_{t=1}^{T} p(x_t \mid x_{<t})
$$

ã€ŒThe cat sat on theã€ã®æ¬¡ã«æ¥ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®ç¢ºç‡ $p(x_7 \mid x_1, \ldots, x_6)$ â€” ã“ã‚Œã¯æ¡ä»¶ä»˜ãåˆ†å¸ƒä»¥å¤–ã®ä½•ç‰©ã§ã‚‚ãªã„ã€‚Malach (2023) [^5] ã¯ã€ã“ã®ã‚ˆã†ãªè‡ªå·±å›å¸°æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬å™¨ãŒä»»æ„ã®ãƒãƒ¥ãƒ¼ãƒªãƒ³ã‚°ãƒã‚·ãƒ³è¨ˆç®—å¯èƒ½ãªé–¢æ•°ã‚’è¿‘ä¼¼ã§ãã‚‹ã€Œæ™®éå­¦ç¿’å™¨ã€ã§ã‚ã‚‹ã“ã¨ã‚’ç†è«–çš„ã«ç¤ºã—ãŸã€‚

> **Zone 1 ã¾ã¨ã‚**: Categoricalåˆ†å¸ƒï¼ˆLLMã®Softmaxå‡ºåŠ›ï¼‰â†’ Gaussianåˆ†å¸ƒï¼ˆVAEã®æ½œåœ¨ç©ºé–“ï¼‰â†’ æ¡ä»¶ä»˜ãåˆ†å¸ƒï¼ˆè‡ªå·±å›å¸°ç”Ÿæˆ $\prod_t p(x_t \mid x_{<t})$ï¼‰ã€‚ç¢ºç‡åˆ†å¸ƒã¯ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ã€Œè¨€èªã€ã ã€‚

:::message
**é€²æ—: 10% å®Œäº†** é›¢æ•£åˆ†å¸ƒãƒ»é€£ç¶šåˆ†å¸ƒãƒ»åŒæ™‚/å‘¨è¾º/æ¡ä»¶ä»˜ãåˆ†å¸ƒã‚’ã€Œè§¦ã£ã¦ã€ç†è§£ã—ãŸã€‚Zone 0-1 ã‚¯ãƒªã‚¢ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœç¢ºç‡è«–ãŒAIã®å¿ƒè‡“ãªã®ã‹

### 2.1 ç¢ºç‡ã‚’å­¦ã¶ã€Œæœ¬å½“ã®ç†ç”±ã€

å¤šãã®MLå…¥é–€ã¯ã€Œãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’å©ã‘ã°å‹•ãã€ã‹ã‚‰å§‹ã¾ã‚‹ã€‚ã ãŒè«–æ–‡ã‚’èª­ã‚‚ã†ã¨ã—ãŸç¬é–“ã€ç¢ºç‡ã®å£ã«ã¶ã¤ã‹ã‚‹ã€‚

- VAEã®æå¤±é–¢æ•° $\mathcal{L} = \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}[\log p_\theta(\mathbf{x}|\mathbf{z})] - D_{KL}[q_\phi(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z})]$ â€” æœŸå¾…å€¤ã€æ¡ä»¶ä»˜ãåˆ†å¸ƒã€KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®å¡Š
- Diffusionã®å‰æ–¹éç¨‹ $q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}(\sqrt{1-\beta_t}\mathbf{x}_{t-1}, \beta_t \mathbf{I})$ â€” æ¡ä»¶ä»˜ãã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®ãƒãƒ«ã‚³ãƒ•é€£é–
- GANã®ç›®çš„é–¢æ•° $\min_G \max_D \mathbb{E}_{\mathbf{x} \sim p_\text{data}}[\log D(\mathbf{x})] + \mathbb{E}_{\mathbf{z} \sim p(\mathbf{z})}[\log(1 - D(G(\mathbf{z})))]$ â€” æœŸå¾…å€¤ã¨ç¢ºç‡åˆ†å¸ƒã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

**ç¢ºç‡è«–ãªã—ã«ã€ã“ã‚Œã‚‰ã®å¼ã¯1æ–‡å­—ã‚‚èª­ã‚ãªã„ã€‚**

> ã“ã®ç« ã‚’èª­ã‚ã°: ç¢ºç‡è«–ãŒæ®‹ã‚Š36å›ã®å…¨è¬›ç¾©ã§ã©ã†ä½¿ã‚ã‚Œã‚‹ã‹ã€å…¨ä½“åƒãŒè¦‹ãˆã‚‹ã€‚

### 2.2 Course Iï¼ˆæ•°å­¦åŸºç¤ç·¨ï¼‰ã§ã®ä½ç½®ã¥ã‘

```mermaid
graph TD
    L1["ç¬¬1å›: æ¦‚è«–<br/>æ•°å¼ã®èª­ã¿æ–¹"]
    L2["ç¬¬2å›: ç·šå½¢ä»£æ•° I<br/>ãƒ™ã‚¯ãƒˆãƒ«ãƒ»è¡Œåˆ—"]
    L3["ç¬¬3å›: ç·šå½¢ä»£æ•° II<br/>SVDãƒ»è¡Œåˆ—å¾®åˆ†"]
    L4["ç¬¬4å›: ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦<br/>åˆ†å¸ƒãƒ»ãƒ™ã‚¤ã‚ºãƒ»MLE"]
    L5["ç¬¬5å›: æ¸¬åº¦è«–ãƒ»ç¢ºç‡éç¨‹<br/>å³å¯†ãªç¢ºç‡"]
    L6["ç¬¬6å›: æƒ…å ±ç†è«–ãƒ»æœ€é©åŒ–<br/>KLãƒ»SGD"]
    L7["ç¬¬7å›: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«æ¦‚è¦<br/>MLE"]
    L8["ç¬¬8å›: æ½œåœ¨å¤‰æ•° & EM<br/>éš ã‚Œå¤‰æ•°"]

    L1 --> L2 --> L3 --> L4
    L4 -->|"ç¢ºç‡åˆ†å¸ƒãŒã‚ã‹ã£ãŸ<br/>â†’å³å¯†ã«å®šç¾©ã—ãŸã„"| L5
    L5 --> L6 --> L7 --> L8

    style L4 fill:#ffeb3b
```

ç¬¬4å›ã¯**Course Iã®æŠ˜ã‚Šè¿”ã—åœ°ç‚¹**ã ã€‚ç¬¬1-3å›ã§é›ãˆãŸç·šå½¢ä»£æ•°ã®ä¸Šã«ã€ä¸ç¢ºå®Ÿæ€§ã®æ•°å­¦ã‚’ç©ã‚€ã€‚ãã—ã¦ç¬¬5å›ï¼ˆæ¸¬åº¦è«–ï¼‰ã§ç¢ºç‡è«–ã‚’å³å¯†ã«å†å®šç¾©ã—ã€ç¬¬6å›ï¼ˆæƒ…å ±ç†è«–ï¼‰ã§KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¨ã„ã†ã€Œåˆ†å¸ƒé–“ã®è·é›¢ã€ã‚’æ‰‹ã«å…¥ã‚Œã‚‹ã€‚

| å› | ãƒ†ãƒ¼ãƒ | ç¬¬4å›ã¨ã®æ¥ç¶š |
|:---|:------|:-------------|
| ç¬¬2å› | ç·šå½¢ä»£æ•° I | å…±åˆ†æ•£è¡Œåˆ—ã¯**å¯¾ç§°æ­£å®šå€¤è¡Œåˆ—** â€” å›ºæœ‰å€¤åˆ†è§£ã®å¯¾è±¡ |
| ç¬¬3å› | ç·šå½¢ä»£æ•° II | ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ãŒ**ç¢ºç‡å¤‰æ•°ã®å¤‰æ›**ã«å¿…è¦ï¼ˆç¬¬25å› NFï¼‰ |
| **ç¬¬4å›** | **ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦** | **æœ¬è¬›ç¾©** |
| ç¬¬5å› | æ¸¬åº¦è«– | ç¢ºç‡å¯†åº¦é–¢æ•°ã®**å³å¯†ãªå®šç¾©**ï¼ˆRadon-Nikodymï¼‰ |
| ç¬¬6å› | æƒ…å ±ç†è«– | KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ = **åˆ†å¸ƒé–“ã®éå¯¾ç§°è·é›¢** |
| ç¬¬8å› | EMç®—æ³• | **æ½œåœ¨å¤‰æ•°ã®å‘¨è¾ºåŒ–** = æœ¬è¬›ç¾©ã®ç›´æ¥çš„æ‹¡å¼µ |

### 2.3 æ¾å°¾ç ”ã¨ã®å·®åˆ¥åŒ–

| è¦³ç‚¹ | æ¾å°¾ãƒ»å²©æ¾¤ç ” | æœ¬ã‚·ãƒªãƒ¼ã‚º |
|:-----|:------------|:----------|
| ç¢ºç‡è«–ã®æ‰±ã„ | ã€Œå‰æçŸ¥è­˜ã€ã¨ã—ã¦çœç•¥ | **8æ™‚é–“ã‹ã‘ã¦å®Œå…¨ç¿’å¾—** |
| ãƒ™ã‚¤ã‚ºæ¨è«– | æ•°å¼ã®çµæœã ã‘ | **äº‹å‰â†’å°¤åº¦â†’äº‹å¾Œã®å…¨ã‚¹ãƒ†ãƒƒãƒ—å°å‡º** |
| MLE | å®šç¾©ã®ã¿ | **æ­£å‰‡æ¡ä»¶ãƒ»æ¼¸è¿‘æ­£è¦æ€§ãƒ»Fisheræƒ…å ±é‡ã¾ã§** |
| æŒ‡æ•°å‹åˆ†å¸ƒæ— | è¨€åŠãªã— | **çµ±ä¸€çš„ç†è§£ â†’ VAE/EBMã®ç†è«–åŸºç›¤** |
| å®Ÿè£… | ãªã— | **NumPyã§å…¨åˆ†å¸ƒã‚’ã‚¹ã‚¯ãƒ©ãƒƒãƒå®Ÿè£…** |

ã€Œå‰æçŸ¥è­˜ã§ç‰‡ä»˜ã‘ã‚‹ã€ã¨ã¯ã€ã¤ã¾ã‚Šã€Œã‚ã‹ã‚‰ãªãã¦ã‚‚å…ˆã«é€²ã‚ã€ã¨ã„ã†ã“ã¨ã ã€‚æœ¬ã‚·ãƒªãƒ¼ã‚ºã¯ãã‚Œã‚’è¨±ã•ãªã„ã€‚ç¢ºç‡è«–ã®åœŸå°ãŒè„†ã„ã¨ã€ç¬¬9å›ï¼ˆå¤‰åˆ†æ¨è«–ï¼‰ä»¥é™ã§å¿…ãšå´©å£Šã™ã‚‹ã€‚

### 2.4 3ã¤ã®å­¦ç¿’ãƒ¡ã‚¿ãƒ•ã‚¡ â€” ç¢ºç‡è«–ã‚’ã€Œä½“ã§è¦šãˆã‚‹ã€

ç¢ºç‡è«–ã¯æŠ½è±¡åº¦ãŒé«˜ã„ã€‚3ã¤ã®ãƒ¡ã‚¿ãƒ•ã‚¡ã§ç›´æ„Ÿã‚’æ´ã‚‚ã†ã€‚

**ãƒ¡ã‚¿ãƒ•ã‚¡1: ç¢ºç‡ç©ºé–“ = RPGã®ä¸–ç•Œè¨­å®š**

$(\Omega, \mathcal{F}, P)$ ã¯ã‚²ãƒ¼ãƒ ã®ä¸–ç•Œè¨­å®šã ã€‚$\Omega$ ã¯å…¨ã¦ã®å¯èƒ½ãªã‚¤ãƒ™ãƒ³ãƒˆï¼ˆãƒãƒƒãƒ—ï¼‰ã€$\mathcal{F}$ ã¯ã€Œè¦³æ¸¬å¯èƒ½ãªã‚¤ãƒ™ãƒ³ãƒˆã®é›†åˆã€ï¼ˆã‚¯ã‚¨ã‚¹ãƒˆä¸€è¦§ï¼‰ã€$P$ ã¯å„ã‚¤ãƒ™ãƒ³ãƒˆã®ç™ºç”Ÿç¢ºç‡ï¼ˆãƒ¬ã‚¢ãƒ‰ãƒ­ãƒƒãƒ—ç‡ï¼‰ã€‚ç¢ºç‡å¤‰æ•° $X$ ã¯ã€Œãƒ€ãƒ³ã‚¸ãƒ§ãƒ³ã®å ±é…¬ã€â€” åŒã˜ãƒ€ãƒ³ã‚¸ãƒ§ãƒ³ã§ã‚‚æ¯å›é•ã†å ±é…¬ï¼ˆç¢ºç‡çš„ï¼‰ã€‚

**ãƒ¡ã‚¿ãƒ•ã‚¡2: ãƒ™ã‚¤ã‚ºæ›´æ–° = æ¢åµã®æ¨ç†**

æ®ºäººäº‹ä»¶ã€‚å®¹ç–‘è€…ãŒ3äººã€‚æœ€åˆã¯å…¨å“¡ç­‰ã—ãç–‘ã‚ã—ã„ï¼ˆäº‹å‰åˆ†å¸ƒ: ä¸€æ§˜ï¼‰ã€‚å‡¶å™¨ã«Aã®æŒ‡ç´‹ãŒè¦‹ã¤ã‹ã£ãŸï¼ˆãƒ‡ãƒ¼ã‚¿: å°¤åº¦ãŒé«˜ã„ï¼‰ã€‚Aã®äº‹å¾Œç¢ºç‡ãŒä¸ŠãŒã‚‹ã€‚ã ãŒAã«ã¯é‰„å£ã®ã‚¢ãƒªãƒã‚¤ãŒå‡ºã¦ããŸï¼ˆæ–°ãŸãªãƒ‡ãƒ¼ã‚¿ï¼‰ã€‚äº‹å¾Œåˆ†å¸ƒãŒã¾ãŸæ›´æ–°ã•ã‚Œã‚‹ã€‚**ãƒ™ã‚¤ã‚ºæ¨è«– = è¨¼æ‹ ã‚’ç©ã¿é‡ã­ã¦ä¿¡å¿µã‚’æ›´æ–°ã™ã‚‹éç¨‹**ã€‚

**ãƒ¡ã‚¿ãƒ•ã‚¡3: MLE = çŠ¯äººå½“ã¦ã‚²ãƒ¼ãƒ ã®æœ€é©æˆ¦ç•¥**

ç®±ã®ä¸­ã«ã‚³ã‚¤ãƒ³ãŒã‚ã‚‹ã€‚è¡¨ã®å‡ºã‚‹ç¢ºç‡ $p$ ã¯ä¸æ˜ã€‚10å›æŠ•ã’ã¦8å›è¡¨ãŒå‡ºãŸã€‚$p$ ã¯ã„ãã¤ã ã¨ã€Œæœ€ã‚‚ã‚ã‚Šãã†ã€ã‹ï¼Ÿ $p = 0.8$ ãŒå°¤åº¦ã‚’æœ€å¤§åŒ–ã™ã‚‹ â€” ã“ã‚ŒãŒMLEã€‚ã€Œãƒ‡ãƒ¼ã‚¿ãŒæœ€ã‚‚èµ·ã“ã‚Šã‚„ã™ã‹ã£ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ã‚’é¸ã¶ã€‚

### 2.5 LLMã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚° â€” æ¡ä»¶ä»˜ãç¢ºç‡ã¨è‡ªå·±å›å¸°

ç¬¬4å›ã®LLMæ¥ç¶šã¯**æ¡ä»¶ä»˜ãç¢ºç‡**ã¨**è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«** $p(x_t \mid x_{<t})$ ã ã€‚

LLMã®ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã¯ã€ä»¥ä¸‹ã®ç¢ºç‡ã®é€£é–è¦å‰‡ï¼ˆchain ruleï¼‰ã«æ”¯é…ã•ã‚Œã‚‹:

$$
p(x_1, x_2, \ldots, x_T) = \prod_{t=1}^{T} p(x_t \mid x_1, \ldots, x_{t-1})
$$

ã“ã‚Œã¯ç¢ºç‡ã®å…¬ç†ã‹ã‚‰å°ã‹ã‚Œã‚‹æ’ç­‰å¼ã ã€‚ä½•ã®ä»®å®šã‚‚è¿‘ä¼¼ã‚‚å…¥ã£ã¦ã„ãªã„ã€‚LLMã¯ã“ã®å„æ¡ä»¶ä»˜ãç¢ºç‡ $p(x_t \mid x_{<t})$ ã‚’Transformerã§è¿‘ä¼¼ã—ã€Softmaxå‡ºåŠ›ã§Categoricalåˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¦ã„ã‚‹ã€‚

```mermaid
graph LR
    X1["xâ‚='The'"] --> P2["p(xâ‚‚|xâ‚)"]
    P2 --> X2["xâ‚‚='cat'"]
    X2 --> P3["p(xâ‚ƒ|xâ‚,xâ‚‚)"]
    P3 --> X3["xâ‚ƒ='sat'"]
    X3 --> P4["p(xâ‚„|xâ‚,...,xâ‚ƒ)"]
    P4 --> X4["xâ‚„='on'"]

    style P2 fill:#fff3e0
    style P3 fill:#fff3e0
    style P4 fill:#fff3e0
```

**è¦šãˆã¦ãŠã„ã¦ã»ã—ã„**: ã“ã®è‡ªå·±å›å¸°æ§‹é€ ã¯ç¬¬15å›ï¼ˆè‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ï¼‰ã§æœ¬æ ¼çš„ã«æ‰±ã†ã€‚ä»Šã¯ã€ŒLLMã®ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ = æ¡ä»¶ä»˜ãç¢ºç‡ã®é€£é–ã€ã¨ã„ã†æ¥ç¶šã‚’æŠŠæ¡ã—ã¦ãŠã‘ã°ååˆ†ã ã€‚

:::details ãƒˆãƒ­ã‚¤ã®æœ¨é¦¬ â€” Pythonã®çµ‚ã‚ã‚Šã®å§‹ã¾ã‚Š
æœ¬è¬›ç¾©ã¯Python 100%ã ã€‚NumPyã§å…¨ã¦ã®åˆ†å¸ƒã‚’ã‚¹ã‚¯ãƒ©ãƒƒãƒå®Ÿè£…ã™ã‚‹ã€‚ã€ŒPythonã¯ä¾¿åˆ©ã ã€ã¨å®‰å¿ƒã—ã¦ã»ã—ã„ã€‚

......ãŸã ã—ã€ç¬¬5å›ã‹ã‚‰ `%timeit` ãŒç™»å ´ã™ã‚‹ã€‚Monte Carloç©åˆ†ã®è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’æ¸¬ã‚Šå§‹ã‚ã‚‹ã¨ã€Pythonã®ã€Œé…ã•ã€ãŒå°‘ã—ãšã¤è¦‹ãˆã¦ãã‚‹ã€‚ç¬¬9å›ã§JuliaãŒåˆç™»å ´ã—ãŸã¨ãã€ELBOè¨ˆç®—ãŒ50å€é€Ÿããªã‚‹è¡æ’ƒãŒå¾…ã£ã¦ã„ã‚‹ã€‚

ä»Šã¯Pythonã‚’ä¿¡ã˜ã¦ã€ç¢ºç‡è«–ã«é›†ä¸­ã—ã‚ˆã†ã€‚
:::

:::message
**é€²æ—: 20% å®Œäº†** ç¢ºç‡è«–ãŒã‚·ãƒªãƒ¼ã‚ºå…¨ä½“ã§ã©ã†ä½¿ã‚ã‚Œã‚‹ã‹ã€å…¨ä½“åƒã‚’æŠŠæ¡ã—ãŸã€‚Zone 2 ã‚¯ãƒªã‚¢ã€‚ã„ã‚ˆã„ã‚ˆæ•°å¼ä¿®è¡Œã®æœ¬ç•ªã«å…¥ã‚‹ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” ç¢ºç‡è«–ã®å…¨æ­¦è£…

ã“ã“ã‹ã‚‰ãŒæœ¬ç•ªã ã€‚ç¢ºç‡ç©ºé–“ã®å³å¯†ãªå®šç¾©ã‹ã‚‰ã€ãƒ™ã‚¤ã‚ºã®å®šç†ã€æŒ‡æ•°å‹åˆ†å¸ƒæ—ã€MLEã€Fisheræƒ…å ±é‡ã¾ã§ â€” ç¢ºç‡è«–ã®æ­¦å™¨åº«ã‚’ä¸€æ°—ã«åŸ‹ã‚ã‚‹ã€‚

```mermaid
graph TD
    A["3.1 ç¢ºç‡ç©ºé–“ (Î©,F,P)"] --> B["3.2 ç¢ºç‡å¤‰æ•°ã¨æœŸå¾…å€¤"]
    B --> C["3.3 ãƒ™ã‚¤ã‚ºã®å®šç†"]
    C --> D["3.4 ä¸»è¦ãªç¢ºç‡åˆ†å¸ƒ"]
    D --> E["3.5 å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒ"]
    E --> F["3.6 æŒ‡æ•°å‹åˆ†å¸ƒæ—"]
    F --> G["3.7 MLEå®Œå…¨ç‰ˆ"]
    G --> H["3.8 Fisheræƒ…å ±é‡"]
    H --> I["3.9 å¤§æ•°ã®æ³•å‰‡ã¨CLT"]
    I --> J["âš”ï¸ Boss Battle"]

    style J fill:#ff5252,color:#fff
```

### 3.1 ç¢ºç‡ç©ºé–“ã®å®šç¾© â€” (Î©, F, P)

å…¨ã¦ã®ç¢ºç‡ã®è­°è«–ã¯ã€3ã¤çµ„ $(\Omega, \mathcal{F}, P)$ ã‹ã‚‰å§‹ã¾ã‚‹ã€‚Kolmogorov [^6] ãŒ1933å¹´ã«ç¢ºç«‹ã—ãŸã“ã®å…¬ç†ç³»ãŒã€ç¢ºç‡è«–ã®åœŸå°ã ã€‚

**å®šç¾©ï¼ˆç¢ºç‡ç©ºé–“ï¼‰**: ç¢ºç‡ç©ºé–“ã¨ã¯ä¸‰ã¤çµ„ $(\Omega, \mathcal{F}, P)$ ã§ã‚ã‚Š:

1. **æ¨™æœ¬ç©ºé–“** $\Omega$ â€” èµ·ã“ã‚Šã†ã‚‹å…¨ã¦ã®çµæœã®é›†åˆ
2. **äº‹è±¡ã®Ïƒ-åŠ æ³•æ—** $\mathcal{F}$ â€” $\Omega$ ã®éƒ¨åˆ†é›†åˆã®æ—ã§ã€ä»¥ä¸‹ã‚’æº€ãŸã™:
   - $\Omega \in \mathcal{F}$
   - $A \in \mathcal{F} \Rightarrow A^c \in \mathcal{F}$ï¼ˆè£œé›†åˆã§é–‰ã˜ã‚‹ï¼‰
   - $A_1, A_2, \ldots \in \mathcal{F} \Rightarrow \bigcup_{n=1}^{\infty} A_n \in \mathcal{F}$ï¼ˆå¯ç®—åˆä½µã§é–‰ã˜ã‚‹ï¼‰
3. **ç¢ºç‡æ¸¬åº¦** $P: \mathcal{F} \to [0,1]$ â€” ä»¥ä¸‹ã‚’æº€ãŸã™é–¢æ•°:
   - $P(\Omega) = 1$ï¼ˆæ­£è¦åŒ–ï¼‰
   - äº’ã„ã«ç´ ãª $A_1, A_2, \ldots \in \mathcal{F}$ ã«å¯¾ã—ã¦ $P\left(\bigcup_{n=1}^{\infty} A_n\right) = \sum_{n=1}^{\infty} P(A_n)$ï¼ˆÏƒ-åŠ æ³•æ€§ï¼‰

:::message
ã“ã“ã§å¤šãã®äººãŒæ··ä¹±ã™ã‚‹ã®ãŒã€ŒãªãœÏƒ-åŠ æ³•æ—ãŒå¿…è¦ãªã®ã‹ã€ã ã€‚Î© ã®ã‚ã‚‰ã‚†ã‚‹éƒ¨åˆ†é›†åˆã«ç¢ºç‡ã‚’å‰²ã‚Šå½“ã¦ã‚ˆã†ã¨ã™ã‚‹ã¨ã€æ•°å­¦çš„ã«çŸ›ç›¾ãŒç”Ÿã˜ã‚‹ï¼ˆVitaliã®éå¯æ¸¬é›†åˆï¼‰ã€‚Ïƒ-åŠ æ³•æ—ã¯ã€Œç¢ºç‡ã‚’å®šç¾©ã§ãã‚‹éƒ¨åˆ†é›†åˆã€ã‚’åˆ¶é™ã™ã‚‹ â€” ç¬¬5å›ï¼ˆæ¸¬åº¦è«–ï¼‰ã§è©³ã—ãæ‰±ã†ã€‚ä»Šã¯ã€Œç¢ºç‡ã‚’å‰²ã‚Šå½“ã¦ã‚‹å¯¾è±¡ã‚’åˆ¶é™ã™ã‚‹ä»•çµ„ã¿ã€ã¨ç†è§£ã™ã‚Œã°ååˆ†ã ã€‚
:::

```python
import numpy as np

# Example: probability space for a fair die
# Î© = {1, 2, 3, 4, 5, 6}
omega = {1, 2, 3, 4, 5, 6}

# F = power set of Î© (all subsets) â€” 2^6 = 64 events
# P: uniform probability
def P(event: set) -> float:
    """Probability measure for a fair die.

    corresponds to: P(A) = |A| / |Î©| for uniform distribution
    """
    return len(event) / len(omega)

# Verify axioms
print("=== Kolmogorov Axioms Verification ===")
print(f"1. P(Î©) = {P(omega)} (must be 1)")
print(f"2. P(âˆ…) = {P(set())} (must be 0)")

A = {1, 3, 5}  # odd numbers
B = {2, 4, 6}  # even numbers
print(f"3. P(odd) = {P(A):.4f}")
print(f"   P(even) = {P(B):.4f}")
print(f"   P(odd âˆª even) = {P(A | B):.4f}")
print(f"   P(odd) + P(even) = {P(A) + P(B):.4f} (Ïƒ-additivity)")

# Non-trivial example: conditional probability
C = {1, 2, 3}  # â‰¤ 3
D = {2, 4, 6}  # even
# P(D|C) = P(Dâˆ©C) / P(C)
p_d_given_c = P(D & C) / P(C)
print(f"\nP(even | â‰¤3) = P({{2}}) / P({{1,2,3}}) = {P(D & C):.4f} / {P(C):.4f} = {p_d_given_c:.4f}")
```

**Ïƒ-åŠ æ³•æ—ãŒã€Œãªãœã€å¿…è¦ã‹ã®ç›´æ„Ÿ**: ã‚µã‚¤ã‚³ãƒ­ã®6é¢ãªã‚‰ã€å…¨éƒ¨åˆ†é›†åˆï¼ˆå†ªé›†åˆï¼‰ã«ç¢ºç‡ã‚’å®šç¾©ã§ãã‚‹ã€‚ã ãŒ $\Omega = \mathbb{R}$ï¼ˆé€£ç¶šç©ºé–“ï¼‰ã§ã¯ã€å†ªé›†åˆã®ã™ã¹ã¦ã«ã€Œé•·ã•ã€ã‚’å®šç¾©ã™ã‚‹ã“ã¨ãŒä¸å¯èƒ½ã§ã‚ã‚‹ã“ã¨ãŒBanach-Tarskiã®ãƒ‘ãƒ©ãƒ‰ãƒƒã‚¯ã‚¹ã‹ã‚‰å¸°çµã•ã‚Œã‚‹ã€‚Ïƒ-åŠ æ³•æ—ã¯ã“ã®å•é¡Œã‚’å›é¿ã™ã‚‹ãŸã‚ã®æ•°å­¦çš„è£…ç½®ã ã€‚ç¬¬5å›ã§Lebesgueæ¸¬åº¦ã¨Borelé›†åˆã¨ã—ã¦å³å¯†ã«æ‰±ã†ã€‚

### 3.2 ç¢ºç‡å¤‰æ•°ã¨æœŸå¾…å€¤ãƒ»åˆ†æ•£

**å®šç¾©ï¼ˆç¢ºç‡å¤‰æ•°ï¼‰**: ç¢ºç‡ç©ºé–“ $(\Omega, \mathcal{F}, P)$ ä¸Šã®ç¢ºç‡å¤‰æ•° $X$ ã¨ã¯ã€å¯æ¸¬é–¢æ•° $X: \Omega \to \mathbb{R}$ ã§ã‚ã‚‹ã€‚

ã€Œå¯æ¸¬ã€ã¨ã¯ã€ä»»æ„ã®Borelé›†åˆ $B \subseteq \mathbb{R}$ ã«å¯¾ã—ã¦ $X^{-1}(B) = \{\omega \in \Omega : X(\omega) \in B\} \in \mathcal{F}$ ãŒæˆã‚Šç«‹ã¤ã“ã¨ã€‚ç›´æ„Ÿçš„ã«ã¯ã€Œ$X$ ã®å€¤ã«é–¢ã™ã‚‹ä»»æ„ã®å•ã„ï¼ˆ$X \leq a$ ãªã©ï¼‰ã«ç¢ºç‡ã‚’å‰²ã‚Šå½“ã¦ã‚‰ã‚Œã‚‹ã€ã¨ã„ã†ã“ã¨ã ã€‚

**æœŸå¾…å€¤**ï¼ˆé›¢æ•£ã®å ´åˆï¼‰:

$$
\mathbb{E}[X] = \sum_{x} x \cdot P(X = x)
$$

**æœŸå¾…å€¤**ï¼ˆé€£ç¶šã®å ´åˆï¼‰:

$$
\mathbb{E}[X] = \int_{-\infty}^{\infty} x \cdot f_X(x) \, dx
$$

ã“ã“ã§ $f_X(x)$ ã¯ç¢ºç‡å¯†åº¦é–¢æ•°ï¼ˆPDFï¼‰ã€‚

**åˆ†æ•£**:

$$
\text{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2
$$

**å…±åˆ†æ•£**:

$$
\text{Cov}(X, Y) = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])] = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]
$$

```python
import numpy as np

# Expectation, Variance, Covariance from first principles
np.random.seed(42)
n = 100000

# Discrete: roll two dice
X = np.random.randint(1, 7, size=n)  # die 1
Y = np.random.randint(1, 7, size=n)  # die 2
Z = X + Y  # sum

print("=== Expectation & Variance ===")
print(f"E[X] = {X.mean():.4f} (theory = 3.5)")
print(f"E[Y] = {Y.mean():.4f} (theory = 3.5)")
print(f"E[Z] = E[X+Y] = {Z.mean():.4f} (theory = 7.0)")
print(f"Var(X) = {X.var():.4f} (theory = {(6**2-1)/12:.4f})")
print(f"Var(Z) = {Z.var():.4f} (theory = {2*(6**2-1)/12:.4f})")

# Linearity of expectation: E[aX+b] = aE[X]+b
a, b = 3, -2
print(f"\nE[{a}X+({b})] = {(a*X+b).mean():.4f} (theory = {a*3.5+b:.4f})")
print(f"Var({a}X+({b})) = {(a*X+b).var():.4f} (theory = {a**2*(6**2-1)/12:.4f})")

# Covariance and independence
print(f"\nCov(X, Y) = {np.cov(X, Y, ddof=0)[0,1]:.4f} (theory â‰ˆ 0, independent)")
print(f"Cov(X, Z) = {np.cov(X, Z, ddof=0)[0,1]:.4f} (theory = Var(X) = {(6**2-1)/12:.4f})")
```

| æ€§è³ª | æ•°å¼ | åå‰ |
|:-----|:-----|:-----|
| ç·šå½¢æ€§ | $\mathbb{E}[aX + b] = a\mathbb{E}[X] + b$ | æœŸå¾…å€¤ã®ç·šå½¢æ€§ |
| ç‹¬ç«‹ãªã‚‰ | $\mathbb{E}[XY] = \mathbb{E}[X]\mathbb{E}[Y]$ | ç‹¬ç«‹ã®å¸°çµ |
| åˆ†æ•£ã®å¤‰æ› | $\text{Var}(aX+b) = a^2 \text{Var}(X)$ | åˆ†æ•£ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° |
| ç‹¬ç«‹ã®å’Œ | $\text{Var}(X+Y) = \text{Var}(X) + \text{Var}(Y)$ | ç‹¬ç«‹ãªã‚‰åˆ†æ•£ã‚‚åŠ æ³•çš„ |

**ç‹¬ç«‹æ€§ã®å®šç¾©**: ç¢ºç‡å¤‰æ•° $X, Y$ ãŒç‹¬ç«‹ $\iff$ $P(X \in A, Y \in B) = P(X \in A) \cdot P(Y \in B)$ ãŒå…¨ã¦ã®äº‹è±¡ $A, B$ ã§æˆç«‹ã€‚

ç‹¬ç«‹æ€§ã¯æ©Ÿæ¢°å­¦ç¿’ã§é »ç¹ã«ä»®å®šã•ã‚Œã‚‹ã€‚ãƒ‡ãƒ¼ã‚¿ãŒ**ç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆi.i.d.ï¼‰**ã§ã‚ã‚‹ã¨ã„ã†ä»®å®š â€” $\mathbf{x}_1, \ldots, \mathbf{x}_N \overset{\text{i.i.d.}}{\sim} p(\mathbf{x}; \theta)$ â€” ã¯MLEï¼ˆ3.7ç¯€ï¼‰ã®å‡ºç™ºç‚¹ã ã€‚

### 3.3 ãƒ™ã‚¤ã‚ºã®å®šç† â€” äº‹å¾Œæ¨è«–ã®å‡ºç™ºç‚¹

**å®šç†ï¼ˆãƒ™ã‚¤ã‚ºã®å®šç†ï¼‰**: äº‹è±¡ $A, B$ ã«å¯¾ã—ã¦ $P(B) > 0$ ã®ã¨ã:

$$
P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}
$$

**è¨¼æ˜**: æ¡ä»¶ä»˜ãç¢ºç‡ã®å®šç¾© $P(A \mid B) = P(A \cap B) / P(B)$ ã¨ $P(B \mid A) = P(A \cap B) / P(A)$ ã‹ã‚‰ã€$P(A \cap B) = P(B \mid A) P(A)$ ã‚’ä»£å…¥ã™ã‚Œã°ç›´ã¡ã«å¾—ã‚‰ã‚Œã‚‹ã€‚$\square$

é€£ç¶šç¢ºç‡å¤‰æ•°ã®å ´åˆã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã®ãƒ™ã‚¤ã‚ºæ¨è«–ã¯:

$$
\underbrace{p(\theta \mid \mathcal{D})}_{\text{äº‹å¾Œåˆ†å¸ƒ}} = \frac{\overbrace{p(\mathcal{D} \mid \theta)}^{\text{å°¤åº¦}} \cdot \overbrace{p(\theta)}^{\text{äº‹å‰åˆ†å¸ƒ}}}{\underbrace{p(\mathcal{D})}_{\text{ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹}}}
$$

ã“ã“ã§ $\mathcal{D} = \{x_1, \ldots, x_N\}$ ã¯ãƒ‡ãƒ¼ã‚¿ã€$p(\mathcal{D}) = \int p(\mathcal{D} \mid \theta) p(\theta) d\theta$ ã¯ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ï¼ˆå‘¨è¾ºå°¤åº¦ï¼‰ã€‚

```python
import numpy as np

def bayesian_coin_update(prior_a: float, prior_b: float, heads: int, tails: int):
    """Bayesian updating with Beta-Bernoulli conjugate pair.

    Prior:     Beta(a, b)
    Likelihood: Bernoulli(Î¸)
    Posterior:  Beta(a + heads, b + tails)

    corresponds to: p(Î¸|D) âˆ Î¸^(a+h-1) (1-Î¸)^(b+t-1)
    """
    post_a = prior_a + heads
    post_b = prior_b + tails
    post_mean = post_a / (post_a + post_b)
    post_var = (post_a * post_b) / ((post_a + post_b)**2 * (post_a + post_b + 1))
    return post_a, post_b, post_mean, post_var

# Start with uniform prior Beta(1,1) = Uniform(0,1)
a, b = 1.0, 1.0
print("=== Sequential Bayesian Updating ===")
print(f"Prior: Beta({a:.0f},{b:.0f}), E[Î¸]={a/(a+b):.3f}\n")

# Observe coins sequentially
observations = [1, 1, 0, 1, 1, 1, 0, 1, 1, 1,  # 8H, 2T
                1, 1, 0, 1, 1, 1, 0, 1, 1, 1]  # 8H, 2T total: 16H, 4T
for i, obs in enumerate(observations, 1):
    a, b, mean, var = bayesian_coin_update(a, b, obs, 1 - obs)
    if i in [1, 5, 10, 20]:
        print(f"After {i:2d} obs: Beta({a:.0f},{b:.0f}), E[Î¸]={mean:.3f}, Std={np.sqrt(var):.3f}")

print(f"\nTrue Î¸: 0.800")
print(f"MLE:    {16/20:.3f}")
print(f"Bayes:  {a/(a+b):.3f} (Beta posterior mean)")
```

:::message
ã“ã“ãŒæœ€åˆã®ã¤ã¾ãšããƒã‚¤ãƒ³ãƒˆã  â€” **ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ $p(\mathcal{D})$ ã®è¨ˆç®—ãŒå›°é›£**ã€‚ã“ã‚Œã¯ $\theta$ ã«é–¢ã™ã‚‹ç©åˆ† $\int p(\mathcal{D} \mid \theta) p(\theta) d\theta$ ã§ã‚ã‚Šã€å¤šãã®å ´åˆè§£æçš„ã«è¨ˆç®—ã§ããªã„ã€‚ã“ã®å›°é›£ãŒç¬¬8å›ï¼ˆEMç®—æ³•ï¼‰ã¨ç¬¬9å›ï¼ˆå¤‰åˆ†æ¨è«–ï¼‰ã®å‹•æ©Ÿã«ãªã‚‹ã€‚VAEã¯ã“ã®ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ã®ä¸‹ç•Œï¼ˆELBOï¼‰ã‚’æœ€å¤§åŒ–ã™ã‚‹æ‰‹æ³•ã ã€‚
:::

**å…±å½¹äº‹å‰åˆ†å¸ƒ**: äº‹å‰åˆ†å¸ƒã¨äº‹å¾Œåˆ†å¸ƒãŒåŒã˜åˆ†å¸ƒæ—ã«å±ã™ã‚‹ã¨ãã€ãã®äº‹å‰åˆ†å¸ƒã‚’**å…±å½¹äº‹å‰åˆ†å¸ƒ**ã¨ã„ã†ã€‚

| å°¤åº¦ | å…±å½¹äº‹å‰åˆ†å¸ƒ | äº‹å¾Œåˆ†å¸ƒ |
|:-----|:-----------|:---------|
| Bernoulli($\theta$) | Beta($a, b$) | Beta($a + h, b + t$) |
| Gaussian($\mu$, æ—¢çŸ¥$\sigma^2$) | Gaussian($\mu_0, \sigma_0^2$) | Gaussian($\mu_N, \sigma_N^2$) |
| Poisson($\lambda$) | Gamma($\alpha, \beta$) | Gamma($\alpha + \sum x_i, \beta + N$) |
| Categorical($\boldsymbol{\pi}$) | Dirichlet($\boldsymbol{\alpha}$) | Dirichlet($\boldsymbol{\alpha} + \text{counts}$) |

ã“ã“ã§ã€Gaussianè¡Œã®äº‹å¾Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯:

$$
\sigma_N^2 = \left(\frac{1}{\sigma_0^2} + \frac{N}{\sigma^2}\right)^{-1}, \quad \mu_N = \sigma_N^2 \left(\frac{\mu_0}{\sigma_0^2} + \frac{N \bar{x}}{\sigma^2}\right)
$$

å…±å½¹äº‹å‰åˆ†å¸ƒãŒä¾¿åˆ©ãªç†ç”±ã¯ã€äº‹å¾Œåˆ†å¸ƒã®è¨ˆç®—ãŒ**é–‰ã˜ãŸå½¢**ã§å¾—ã‚‰ã‚Œã‚‹ã“ã¨ã ã€‚æŒ‡æ•°å‹åˆ†å¸ƒæ—ï¼ˆ3.6ç¯€ï¼‰ã¨ã®æ·±ã„é–¢ä¿‚ãŒã‚ã‚‹ã€‚

:::details Jeffreysäº‹å‰åˆ†å¸ƒ â€” å®¢è¦³ãƒ™ã‚¤ã‚ºã®è©¦ã¿
äº‹å‰åˆ†å¸ƒã‚’ã©ã†é¸ã¶ã‹ã¯ä¸»è¦³çš„ãªåˆ¤æ–­ã ã€‚Harold Jeffreysã¯ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¤‰æ›ã«å¯¾ã—ã¦ä¸å¤‰ãªã€Œå®¢è¦³çš„ã€äº‹å‰åˆ†å¸ƒã‚’ææ¡ˆã—ãŸ:

$$
p_J(\theta) \propto \sqrt{\det I(\theta)}
$$

ã“ã“ã§ $I(\theta)$ ã¯Fisheræƒ…å ±è¡Œåˆ—ï¼ˆ3.8ç¯€ï¼‰ã€‚Bernoulliå°¤åº¦ã«å¯¾ã™ã‚‹Jeffreysäº‹å‰åˆ†å¸ƒã¯ $\text{Beta}(1/2, 1/2)$ â€” Uå­—å‹ã®åˆ†å¸ƒã§ã€0ã¨1ã®è¿‘ãã«è³ªé‡ã‚’é›†ä¸­ã•ã›ã‚‹ã€‚

Jeffreysäº‹å‰åˆ†å¸ƒã¯ä½æ¬¡å…ƒã§ã¯æœ‰ç”¨ã ãŒã€é«˜æ¬¡å…ƒã§ã¯ã€Œæ‹¡æ•£ã—ã™ãã‚‹ã‹é›†ä¸­ã—ã™ãã‚‹ã€å•é¡ŒãŒçŸ¥ã‚‰ã‚Œã¦ã„ã‚‹ã€‚å®Ÿç”¨ä¸Šã¯ã€å¼±æƒ…å ±äº‹å‰åˆ†å¸ƒï¼ˆweakly informative priorï¼‰ã‚’æ¨å¥¨ã™ã‚‹å£°ãŒå¤šã„ã€‚
:::

### 3.4 ä¸»è¦ãªç¢ºç‡åˆ†å¸ƒ â€” é›¢æ•£ã¨é€£ç¶š

#### 3.4.1 é›¢æ•£åˆ†å¸ƒã®è©³ç´°

**Bernoulliåˆ†å¸ƒ**: $X \sim \text{Bernoulli}(p)$

$$
P(X = x) = p^x (1-p)^{1-x}, \quad x \in \{0, 1\}
$$

$$
\mathbb{E}[X] = p, \quad \text{Var}(X) = p(1-p)
$$

**Binomialåˆ†å¸ƒ**: $X \sim \text{Binomial}(n, p)$ â€” $n$ å›ã®ç‹¬ç«‹ãªBernoulliè©¦è¡Œã®æˆåŠŸå›æ•°

$$
P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}, \quad k \in \{0, 1, \ldots, n\}
$$

$$
\mathbb{E}[X] = np, \quad \text{Var}(X) = np(1-p)
$$

**Poissonåˆ†å¸ƒ**: $X \sim \text{Poisson}(\lambda)$ â€” å˜ä½æ™‚é–“ã‚ãŸã‚Šã®äº‹è±¡ç™ºç”Ÿå›æ•°

$$
P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}, \quad k \in \{0, 1, 2, \ldots\}
$$

$$
\mathbb{E}[X] = \lambda, \quad \text{Var}(X) = \lambda
$$

Poissonåˆ†å¸ƒã¯Binomialåˆ†å¸ƒã®æ¥µé™ã¨ã—ã¦å¾—ã‚‰ã‚Œã‚‹: $n \to \infty$, $p \to 0$, $np = \lambda$ ã®ã¨ã $\text{Binomial}(n,p) \to \text{Poisson}(\lambda)$ã€‚

```python
import numpy as np
from math import comb, factorial

def binomial_pmf(k: int, n: int, p: float) -> float:
    """P(X=k) = C(n,k) p^k (1-p)^(n-k)"""
    return comb(n, k) * p**k * (1 - p)**(n - k)

def poisson_pmf(k: int, lam: float) -> float:
    """P(X=k) = Î»^k e^{-Î»} / k!"""
    return lam**k * np.exp(-lam) / factorial(k)

# Poisson as limit of Binomial
lam = 5.0
print(f"Poisson limit theorem: Binomial(n, Î»/n) â†’ Poisson(Î»={lam})")
print(f"{'k':<4} {'Poisson':>10} {'Bin(10)':>10} {'Bin(100)':>10} {'Bin(1000)':>10}")
print("-" * 48)
for k in range(11):
    pois = poisson_pmf(k, lam)
    b10 = binomial_pmf(k, 10, lam/10)
    b100 = binomial_pmf(k, 100, lam/100)
    b1000 = binomial_pmf(k, 1000, lam/1000)
    print(f"{k:<4} {pois:>10.6f} {b10:>10.6f} {b100:>10.6f} {b1000:>10.6f}")
```

#### 3.4.2 é€£ç¶šåˆ†å¸ƒã®è©³ç´°

**Gaussianåˆ†å¸ƒ** (1.2ç¯€ã§å°å…¥æ¸ˆã¿): $X \sim \mathcal{N}(\mu, \sigma^2)$

$$
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
$$

**Gammaåˆ†å¸ƒ**: $X \sim \text{Gamma}(\alpha, \beta)$ï¼ˆshape $\alpha$, rate $\beta$ï¼‰

$$
f(x) = \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-1} e^{-\beta x}, \quad x > 0
$$

$$
\mathbb{E}[X] = \frac{\alpha}{\beta}, \quad \text{Var}(X) = \frac{\alpha}{\beta^2}
$$

**Betaåˆ†å¸ƒ**: $X \sim \text{Beta}(\alpha, \beta)$

$$
f(x) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} x^{\alpha-1} (1-x)^{\beta-1}, \quad x \in (0, 1)
$$

$$
\mathbb{E}[X] = \frac{\alpha}{\alpha+\beta}, \quad \text{Var}(X) = \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}
$$

Betaåˆ†å¸ƒã¯Bernoulli/Binomialã®å…±å½¹äº‹å‰åˆ†å¸ƒã€‚$\alpha = \beta = 1$ ã§ä¸€æ§˜åˆ†å¸ƒã«ä¸€è‡´ã™ã‚‹ã€‚

```python
import numpy as np

def gamma_pdf(x: np.ndarray, alpha: float, beta: float) -> np.ndarray:
    """Gamma PDF: f(x) = Î²^Î± / Î“(Î±) x^{Î±-1} exp(-Î²x)"""
    from math import gamma as gamma_fn
    return (beta**alpha / gamma_fn(alpha)) * x**(alpha - 1) * np.exp(-beta * x)

def beta_pdf(x: np.ndarray, a: float, b: float) -> np.ndarray:
    """Beta PDF: f(x) = Î“(a+b)/(Î“(a)Î“(b)) x^{a-1} (1-x)^{b-1}"""
    from math import gamma as gamma_fn
    B = gamma_fn(a) * gamma_fn(b) / gamma_fn(a + b)
    return x**(a - 1) * (1 - x)**(b - 1) / B

# Gamma distribution properties
x_gamma = np.linspace(0.01, 15, 1000)
for alpha, beta in [(1, 1), (2, 1), (3, 0.5), (5, 1)]:
    pdf = gamma_pdf(x_gamma, alpha, beta)
    mean_theory = alpha / beta
    var_theory = alpha / beta**2
    # Numerical verification via trapezoidal rule
    dx = x_gamma[1] - x_gamma[0]
    mean_num = np.sum(x_gamma * pdf) * dx
    var_num = np.sum((x_gamma - mean_num)**2 * pdf) * dx
    print(f"Gamma(Î±={alpha},Î²={beta}): E[X]={mean_theory:.2f} (num:{mean_num:.2f}), "
          f"Var={var_theory:.2f} (num:{var_num:.2f})")

print()
# Beta distribution â€” conjugate prior for Bernoulli
x_beta = np.linspace(0.001, 0.999, 1000)
for a, b in [(1, 1), (2, 5), (5, 2), (0.5, 0.5)]:
    pdf = beta_pdf(x_beta, a, b)
    mean_theory = a / (a + b)
    name = "Uniform" if (a == 1 and b == 1) else f"Beta({a},{b})"
    if a == 0.5 and b == 0.5:
        name = "Jeffreys"
    dx = x_beta[1] - x_beta[0]
    mode_x = x_beta[np.argmax(pdf)]
    print(f"{name:>12}: E[Î¸]={mean_theory:.3f}, modeâ‰ˆ{mode_x:.3f}")
```

### 3.5 å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒ â€” ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®åŸºç¤è¨€èª

$d$ æ¬¡å…ƒã®å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã¯:

$$
\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \frac{1}{(2\pi)^{d/2} |\boldsymbol{\Sigma}|^{1/2}} \exp\left(-\frac{1}{2}(\mathbf{x} - \boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu})\right)
$$

ã“ã“ã§ $\boldsymbol{\mu} \in \mathbb{R}^d$ ã¯å¹³å‡ãƒ™ã‚¯ãƒˆãƒ«ã€$\boldsymbol{\Sigma} \in \mathbb{R}^{d \times d}$ ã¯å…±åˆ†æ•£è¡Œåˆ—ï¼ˆå¯¾ç§°æ­£å®šå€¤ï¼‰ã€‚

**ç²¾åº¦è¡Œåˆ—**: $\boldsymbol{\Lambda} = \boldsymbol{\Sigma}^{-1}$ ã‚’ç²¾åº¦è¡Œåˆ—ã¨å‘¼ã¶ã€‚å¯†åº¦é–¢æ•°ã¯ç²¾åº¦è¡Œåˆ—ã‚’ä½¿ã†ã¨:

$$
\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \boldsymbol{\Lambda}^{-1}) \propto \exp\left(-\frac{1}{2}(\mathbf{x} - \boldsymbol{\mu})^\top \boldsymbol{\Lambda} (\mathbf{x} - \boldsymbol{\mu})\right)
$$

ç²¾åº¦è¡Œåˆ—ã® $(i,j)$ æˆåˆ†ãŒ0ã§ã‚ã‚‹ã“ã¨ã¯ã€$X_i$ ã¨ $X_j$ ãŒä»–ã®å¤‰æ•°ã‚’æ¡ä»¶ä»˜ã‘ãŸã¨ãæ¡ä»¶ä»˜ãç‹¬ç«‹ã§ã‚ã‚‹ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã€‚ã“ã‚ŒãŒã‚¬ã‚¦ã‚¹ã‚°ãƒ©ãƒ•ã‚£ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼ˆGaussian Graphical Modelï¼‰ã®åŸºç›¤ã ã€‚

**æ¡ä»¶ä»˜ãåˆ†å¸ƒã¨å‘¨è¾ºåˆ†å¸ƒ**:

$\mathbf{x} = (\mathbf{x}_1, \mathbf{x}_2)^\top$ ã¨åˆ†å‰²ã—ã€å¯¾å¿œã™ã‚‹å¹³å‡ã¨å…±åˆ†æ•£ã‚‚åˆ†å‰²ã™ã‚‹:

$$
\boldsymbol{\mu} = \begin{pmatrix} \boldsymbol{\mu}_1 \\ \boldsymbol{\mu}_2 \end{pmatrix}, \quad \boldsymbol{\Sigma} = \begin{pmatrix} \boldsymbol{\Sigma}_{11} & \boldsymbol{\Sigma}_{12} \\ \boldsymbol{\Sigma}_{21} & \boldsymbol{\Sigma}_{22} \end{pmatrix}
$$

ã“ã®ã¨ã:

**å‘¨è¾ºåˆ†å¸ƒ**: $\mathbf{x}_1 \sim \mathcal{N}(\boldsymbol{\mu}_1, \boldsymbol{\Sigma}_{11})$ â€” å˜ã«å¯¾å¿œã™ã‚‹éƒ¨åˆ†ã‚’æŠœãå‡ºã™ã ã‘

**æ¡ä»¶ä»˜ãåˆ†å¸ƒ**:

$$
\mathbf{x}_1 \mid \mathbf{x}_2 \sim \mathcal{N}\left(\boldsymbol{\mu}_1 + \boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}(\mathbf{x}_2 - \boldsymbol{\mu}_2), \; \boldsymbol{\Sigma}_{11} - \boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}\boldsymbol{\Sigma}_{21}\right)
$$

æ¡ä»¶ä»˜ãå…±åˆ†æ•£ $\boldsymbol{\Sigma}_{11} - \boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}\boldsymbol{\Sigma}_{21}$ ã¯ç·šå½¢ä»£æ•°ã®**Schurè£œè¡Œåˆ—**ãã®ã‚‚ã®ã€‚ç¬¬2å›ã§å­¦ã‚“ã é“å…·ãŒã“ã“ã§å†ç™»å ´ã™ã‚‹ã€‚

```python
import numpy as np

# 2D Gaussian: conditional and marginal
mu = np.array([1.0, 2.0])
Sigma = np.array([[1.0, 0.8],
                   [0.8, 1.5]])

# Marginal of x1
print(f"Marginal xâ‚: N({mu[0]}, {Sigma[0,0]})")
print(f"Marginal xâ‚‚: N({mu[1]}, {Sigma[1,1]})")

# Conditional x1 | x2 = 3.0
x2_obs = 3.0
# Î¼_{1|2} = Î¼â‚ + Î£â‚â‚‚ Î£â‚‚â‚‚â»Â¹ (xâ‚‚ - Î¼â‚‚)
mu_cond = mu[0] + Sigma[0, 1] / Sigma[1, 1] * (x2_obs - mu[1])
# Î£_{1|2} = Î£â‚â‚ - Î£â‚â‚‚ Î£â‚‚â‚‚â»Â¹ Î£â‚‚â‚  (Schur complement)
sigma_cond = Sigma[0, 0] - Sigma[0, 1]**2 / Sigma[1, 1]
print(f"\nConditional xâ‚ | xâ‚‚={x2_obs}:")
print(f"  Î¼_{'{1|2}'} = {mu[0]} + {Sigma[0,1]}/{Sigma[1,1]} Ã— ({x2_obs} - {mu[1]}) = {mu_cond:.4f}")
print(f"  ÏƒÂ²_{'{1|2}'} = {Sigma[0,0]} - {Sigma[0,1]}Â²/{Sigma[1,1]} = {sigma_cond:.4f}")

# Verify by sampling
samples = np.random.multivariate_normal(mu, Sigma, size=100000)
mask = np.abs(samples[:, 1] - x2_obs) < 0.1
cond_samples = samples[mask, 0]
print(f"\nNumerical verification (samples near xâ‚‚={x2_obs}):")
print(f"  Mean: {cond_samples.mean():.4f} (theory: {mu_cond:.4f})")
print(f"  Var:  {cond_samples.var():.4f} (theory: {sigma_cond:.4f})")
```

**Mahalanobisè·é›¢**: å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã®æŒ‡æ•°éƒ¨ã«ç¾ã‚Œã‚‹ $(\mathbf{x} - \boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1}(\mathbf{x} - \boldsymbol{\mu})$ ã¯Mahalanobisè·é›¢ã®äºŒä¹—ã ã€‚ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã¨ç•°ãªã‚Šã€å…±åˆ†æ•£æ§‹é€ ã‚’è€ƒæ…®ã—ãŸã€Œå½¢ã«åˆã£ãŸè·é›¢ã€ã‚’æ¸¬ã‚‹ã€‚ç­‰Mahalanobisè·é›¢ã®æ›²é¢ã¯æ¥•å††ä½“ã‚’å½¢æˆã—ã€ãã®ä¸»è»¸ã¯å…±åˆ†æ•£è¡Œåˆ—ã®å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã€è»¸ã®é•·ã•ã¯å›ºæœ‰å€¤ã®å¹³æ–¹æ ¹ã«æ¯”ä¾‹ã™ã‚‹ã€‚

:::details VAEã®æ½œåœ¨ç©ºé–“ â€” å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã®ç›´æ¥å¿œç”¨
VAE [^2] ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¯å…¥åŠ› $\mathbf{x}$ ã«å¯¾ã—ã¦:

$$
q_\phi(\mathbf{z} \mid \mathbf{x}) = \mathcal{N}(\mathbf{z} \mid \boldsymbol{\mu}_\phi(\mathbf{x}), \text{diag}(\boldsymbol{\sigma}_\phi^2(\mathbf{x})))
$$

ã‚’å‡ºåŠ›ã™ã‚‹ã€‚ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒ $\boldsymbol{\mu}$ ã¨ $\boldsymbol{\sigma}^2$ ã‚’äºˆæ¸¬ã—ã€æ½œåœ¨å¤‰æ•° $\mathbf{z}$ ã‚’å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã€‚å¯¾è§’å…±åˆ†æ•£ã‚’ä»®å®šã™ã‚‹ã®ã¯è¨ˆç®—åŠ¹ç‡ã®ãŸã‚ â€” ç¬¬10å›ï¼ˆVAEåŸºç¤ï¼‰ã§ã€ã“ã®ä»®å®šã®å¸°çµã¨æ”¹å–„ç­–ã‚’è©³ã—ãè­°è«–ã™ã‚‹ã€‚
:::

### 3.6 æŒ‡æ•°å‹åˆ†å¸ƒæ— â€” çµ±ä¸€çš„ç†è§£

å¤šãã®é‡è¦ãªåˆ†å¸ƒã¯ã€**æŒ‡æ•°å‹åˆ†å¸ƒæ—**ï¼ˆExponential Familyï¼‰ã¨ã„ã†1ã¤ã®æ çµ„ã¿ã§çµ±ä¸€çš„ã«è¨˜è¿°ã§ãã‚‹ã€‚

**å®šç¾©**: ç¢ºç‡åˆ†å¸ƒãŒä»¥ä¸‹ã®å½¢ã§æ›¸ã‘ã‚‹ã¨ãã€æŒ‡æ•°å‹åˆ†å¸ƒæ—ã«å±ã™ã‚‹:

$$
p(\mathbf{x} \mid \boldsymbol{\eta}) = h(\mathbf{x}) \exp\left(\boldsymbol{\eta}^\top \mathbf{T}(\mathbf{x}) - A(\boldsymbol{\eta})\right)
$$

| è¦ç´  | è¨˜å· | æ„å‘³ |
|:-----|:-----|:-----|
| è‡ªç„¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | $\boldsymbol{\eta}$ | åˆ†å¸ƒã‚’æŒ‡å®šã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ |
| ååˆ†çµ±è¨ˆé‡ | $\mathbf{T}(\mathbf{x})$ | ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æŠ½å‡ºã™ã‚‹çµ±è¨ˆé‡ |
| å¯¾æ•°æ­£è¦åŒ–å®šæ•° | $A(\boldsymbol{\eta})$ | $\int h(\mathbf{x}) \exp(\boldsymbol{\eta}^\top \mathbf{T}(\mathbf{x})) d\mathbf{x} = \exp(A(\boldsymbol{\eta}))$ |
| åŸºåº•æ¸¬åº¦ | $h(\mathbf{x})$ | $\boldsymbol{\eta}$ ã«ä¾å­˜ã—ãªã„å› å­ |

**æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®é©šãã¹ãæ€§è³ª**:

1. $\nabla_{\boldsymbol{\eta}} A(\boldsymbol{\eta}) = \mathbb{E}[\mathbf{T}(\mathbf{x})]$ â€” å¯¾æ•°æ­£è¦åŒ–å®šæ•°ã®å‹¾é…ãŒååˆ†çµ±è¨ˆé‡ã®æœŸå¾…å€¤
2. $\nabla_{\boldsymbol{\eta}}^2 A(\boldsymbol{\eta}) = \text{Cov}[\mathbf{T}(\mathbf{x})]$ â€” ãƒ˜ã‚·ã‚¢ãƒ³ãŒå…±åˆ†æ•£
3. $A(\boldsymbol{\eta})$ ã¯å‡¸é–¢æ•° â€” æœ€é©åŒ–ã«éƒ½åˆãŒã‚ˆã„
4. MLEã¯ $\mathbb{E}[\mathbf{T}(\mathbf{x})] = \frac{1}{N}\sum_{i=1}^{N} \mathbf{T}(\mathbf{x}_i)$ ã§é–‰ã˜ãŸå½¢

```python
import numpy as np

# Gaussian as exponential family
# N(x|Î¼,ÏƒÂ²) = (2Ï€ÏƒÂ²)^{-1/2} exp(-(x-Î¼)Â²/(2ÏƒÂ²))
# = (2Ï€)^{-1/2} exp(Î¼x/ÏƒÂ² - xÂ²/(2ÏƒÂ²) - Î¼Â²/(2ÏƒÂ²) - log Ïƒ)
# Î· = (Î¼/ÏƒÂ², -1/(2ÏƒÂ²))
# T(x) = (x, xÂ²)
# A(Î·) = -Î·â‚Â²/(4Î·â‚‚) - 1/2 log(-2Î·â‚‚) + 1/2 log(2Ï€)?
# Simpler: verify E[T(x)] = âˆ‡A(Î·)

mu, sigma = 2.0, 1.5

# Natural parameters
eta1 = mu / sigma**2
eta2 = -1 / (2 * sigma**2)
print(f"Gaussian N({mu}, {sigma**2})")
print(f"Natural parameters: Î·â‚ = Î¼/ÏƒÂ² = {eta1:.4f}, Î·â‚‚ = -1/(2ÏƒÂ²) = {eta2:.4f}")

# Sufficient statistics
samples = np.random.normal(mu, sigma, size=100000)
T1 = samples.mean()       # E[Tâ‚(x)] = E[x] = Î¼
T2 = (samples**2).mean()  # E[Tâ‚‚(x)] = E[xÂ²] = Î¼Â² + ÏƒÂ²
print(f"\nSufficient statistics (empirical):")
print(f"  E[Tâ‚(x)] = E[x] = {T1:.4f} (theory: {mu})")
print(f"  E[Tâ‚‚(x)] = E[xÂ²] = {T2:.4f} (theory: {mu**2 + sigma**2:.4f})")

# Show major distributions as exponential family
print("\n=== Distributions as Exponential Family ===")
print(f"{'Distribution':<20} {'Î· (natural param)':<25} {'T(x) (suff. stat.)':<20}")
print("-" * 65)
print(f"{'Bernoulli(p)':<20} {'log(p/(1-p))':<25} {'x':<20}")
print(f"{'Poisson(Î»)':<20} {'log(Î»)':<25} {'x':<20}")
print(f"{'Gaussian(Î¼,ÏƒÂ²)':<20} {'(Î¼/ÏƒÂ², -1/(2ÏƒÂ²))':<25} {'(x, xÂ²)':<20}")
print(f"{'Gamma(Î±,Î²)':<20} {'(Î±-1, -Î²)':<25} {'(log x, x)':<20}")
print(f"{'Beta(Î±,Î²)':<20} {'(Î±-1, Î²-1)':<25} {'(log x, log(1-x))':<20}")
```

**æ•°å€¤æ¤œè¨¼ â€” æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®æ€§è³ª $\nabla A = \mathbb{E}[T(x)]$**:

```python
import numpy as np

# Verify: âˆ‚A/âˆ‚Î· = E[T(x)] for Bernoulli
# Bernoulli as exp family: p(x|Î·) = exp(Î·x - log(1+exp(Î·)))
# Î· = log(p/(1-p)), A(Î·) = log(1+exp(Î·)), T(x) = x

def bernoulli_A(eta: float) -> float:
    """Log-normalizer A(Î·) = log(1 + exp(Î·))"""
    return np.log(1 + np.exp(eta))

def bernoulli_dA(eta: float, dt: float = 1e-6) -> float:
    """Numerical derivative of A"""
    return (bernoulli_A(eta + dt) - bernoulli_A(eta - dt)) / (2 * dt)

def bernoulli_d2A(eta: float, dt: float = 1e-5) -> float:
    """Numerical second derivative of A"""
    return (bernoulli_A(eta + dt) - 2 * bernoulli_A(eta) + bernoulli_A(eta - dt)) / dt**2

print("=== Exponential Family Property Verification ===\n")
print(f"{'p':>6} {'Î·':>8} {'âˆ‚A/âˆ‚Î·':>10} {'E[T(x)]':>10} {'âˆ‚Â²A/âˆ‚Î·Â²':>10} {'Var[T(x)]':>10}")
print("-" * 58)

for p in [0.1, 0.3, 0.5, 0.7, 0.9]:
    eta = np.log(p / (1 - p))  # natural parameter
    dA = bernoulli_dA(eta)      # should equal E[x] = p
    d2A = bernoulli_d2A(eta)    # should equal Var[x] = p(1-p)

    # Verify by sampling
    samples = np.random.binomial(1, p, 100000).astype(float)
    E_T = samples.mean()
    V_T = samples.var()

    print(f"{p:>6.1f} {eta:>8.4f} {dA:>10.6f} {E_T:>10.6f} {d2A:>10.6f} {V_T:>10.6f}")

print("\nâ†’ âˆ‚A/âˆ‚Î· = E[T(x)] and âˆ‚Â²A/âˆ‚Î·Â² = Var[T(x)] confirmed!")
print("â†’ This is WHY exponential families are mathematically elegant.")
```

**Softmaxé–¢æ•°ã¯æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®æ­£è¦åŒ–**: Categoricalåˆ†å¸ƒã‚’æŒ‡æ•°å‹åˆ†å¸ƒæ—ã¨ã—ã¦æ›¸ãã¨ $p(x=k \mid \boldsymbol{\eta}) = \exp(\eta_k - A(\boldsymbol{\eta}))$ ã§ã€$A(\boldsymbol{\eta}) = \log \sum_k \exp(\eta_k)$ â€” ã“ã‚Œã¯ã¾ã•ã«log-sum-expã€ã¤ã¾ã‚ŠSoftmaxã®æ­£è¦åŒ–å®šæ•°ã ã€‚LLMã®å‡ºåŠ›å±¤ã¯æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®è‡ªç„¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨ˆç®—ã—ã¦ã„ã‚‹ã€‚

:::message
æŒ‡æ•°å‹åˆ†å¸ƒæ—ãŒé‡è¦ãªç†ç”±ã¯3ã¤ã‚ã‚‹:

1. **å…±å½¹äº‹å‰åˆ†å¸ƒãŒè‡ªå‹•çš„ã«å­˜åœ¨ã™ã‚‹** â€” ãƒ™ã‚¤ã‚ºæ¨è«–ãŒç°¡æ½”
2. **MLEãŒååˆ†çµ±è¨ˆé‡ã®ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆãƒãƒƒãƒãƒ³ã‚°ã«å¸°ç€** â€” åŠ¹ç‡çš„ãªæ¨å®š
3. **Fisheræƒ…å ±é‡ãŒå¯¾æ•°æ­£è¦åŒ–å®šæ•°ã®ãƒ˜ã‚·ã‚¢ãƒ³ã«ä¸€è‡´** â€” æ¨å®šã®æœ€é©æ€§ç†è«–ã«ç›´çµ

EBMï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼‰[^7] ã® $p(\mathbf{x}) = \frac{1}{Z(\theta)} \exp(-E_\theta(\mathbf{x}))$ ã¯æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®ä¸€èˆ¬åŒ–ã§ã‚ã‚Šã€ç¬¬27å›ã§è©³ã—ãæ‰±ã†ã€‚
:::

### 3.7 æœ€å°¤æ¨å®šï¼ˆMLEï¼‰å®Œå…¨ç‰ˆ

**å•é¡Œè¨­å®š**: ãƒ‡ãƒ¼ã‚¿ $\mathcal{D} = \{\mathbf{x}_1, \ldots, \mathbf{x}_N\} \overset{\text{i.i.d.}}{\sim} p(\mathbf{x}; \theta)$ ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã‚’æ¨å®šã—ãŸã„ã€‚

**å®šç¾©ï¼ˆæœ€å°¤æ¨å®šé‡ï¼‰**:

$$
\hat{\theta}_{\text{MLE}} = \arg\max_\theta \prod_{i=1}^{N} p(\mathbf{x}_i; \theta) = \arg\max_\theta \sum_{i=1}^{N} \log p(\mathbf{x}_i; \theta)
$$

ç©ã‚’å¯¾æ•°ã§å’Œã«å¤‰æ›ã™ã‚‹ã®ã¯ã€æ•°å€¤çš„å®‰å®šæ€§ã¨å¾®åˆ†ã®å®¹æ˜“ã•ã®ãŸã‚ã€‚

**å°¤åº¦æ–¹ç¨‹å¼**: $\hat{\theta}_{\text{MLE}}$ ã¯ä»¥ä¸‹ã‚’æº€ãŸã™:

$$
\frac{\partial}{\partial \theta} \sum_{i=1}^{N} \log p(\mathbf{x}_i; \theta) = \mathbf{0}
$$

**ä¾‹: ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®MLE**

$x_1, \ldots, x_N \overset{\text{i.i.d.}}{\sim} \mathcal{N}(\mu, \sigma^2)$ ã®ã¨ã:

$$
\ell(\mu, \sigma^2) = \sum_{i=1}^{N} \log \mathcal{N}(x_i \mid \mu, \sigma^2) = -\frac{N}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^{N}(x_i - \mu)^2
$$

$\mu$ ã§åå¾®åˆ†ã—ã¦0ã¨ãŠã:

$$
\frac{\partial \ell}{\partial \mu} = \frac{1}{\sigma^2}\sum_{i=1}^{N}(x_i - \mu) = 0 \implies \hat{\mu}_{\text{MLE}} = \frac{1}{N}\sum_{i=1}^{N}x_i = \bar{x}
$$

$\sigma^2$ ã§åå¾®åˆ†ã—ã¦0ã¨ãŠã:

$$
\frac{\partial \ell}{\partial \sigma^2} = -\frac{N}{2\sigma^2} + \frac{1}{2(\sigma^2)^2}\sum_{i=1}^{N}(x_i - \bar{x})^2 = 0 \implies \hat{\sigma}^2_{\text{MLE}} = \frac{1}{N}\sum_{i=1}^{N}(x_i - \bar{x})^2
$$

åˆ†æ¯ãŒ $N$ ã§ã‚ã£ã¦ $N-1$ ã§ã¯ãªã„ã“ã¨ã«æ³¨æ„ã€‚MLEã®åˆ†æ•£æ¨å®šé‡ã¯**ãƒã‚¤ã‚¢ã‚¹ãŒã‚ã‚‹**ï¼ˆéå°è©•ä¾¡ã™ã‚‹ï¼‰ã€‚ä¸åæ¨å®šé‡ã¯ $N-1$ ã§å‰²ã‚‹ã€‚

```python
import numpy as np

def gaussian_mle(data: np.ndarray):
    """MLE for Gaussian parameters.

    corresponds to: Î¼_MLE = (1/N)Î£xáµ¢, ÏƒÂ²_MLE = (1/N)Î£(xáµ¢-Î¼)Â²
    """
    mu_mle = data.mean()
    sigma2_mle = ((data - mu_mle)**2).mean()  # biased
    sigma2_unbiased = ((data - mu_mle)**2).sum() / (len(data) - 1)  # unbiased
    return mu_mle, sigma2_mle, sigma2_unbiased

# Ground truth
true_mu, true_sigma = 3.0, 2.0
np.random.seed(42)

print(f"True parameters: Î¼={true_mu}, ÏƒÂ²={true_sigma**2}")
print(f"\n{'N':>6} {'Î¼_MLE':>8} {'ÏƒÂ²_MLE':>10} {'ÏƒÂ²_unbiased':>12} {'|Î¼-Î¼Ì‚|':>8}")
print("-" * 50)
for N in [5, 10, 50, 100, 1000, 10000]:
    data = np.random.normal(true_mu, true_sigma, size=N)
    mu_hat, s2_mle, s2_unb = gaussian_mle(data)
    print(f"{N:>6} {mu_hat:>8.4f} {s2_mle:>10.4f} {s2_unb:>12.4f} {abs(mu_hat-true_mu):>8.4f}")
```

**MLEã®æ¼¸è¿‘çš„æ€§è³ª**ï¼ˆæ­£å‰‡æ¡ä»¶ã®ä¸‹ã§ï¼‰:

1. **ä¸€è‡´æ€§**: $\hat{\theta}_{\text{MLE}} \xrightarrow{P} \theta^*$ï¼ˆ$N \to \infty$ ã§çœŸã®å€¤ã«ç¢ºç‡åæŸï¼‰
2. **æ¼¸è¿‘æ­£è¦æ€§**: $\sqrt{N}(\hat{\theta}_{\text{MLE}} - \theta^*) \xrightarrow{d} \mathcal{N}(0, I(\theta^*)^{-1})$
3. **æ¼¸è¿‘æœ‰åŠ¹æ€§**: æ¼¸è¿‘åˆ†æ•£ãŒCramÃ©r-Raoä¸‹ç•Œã«åˆ°é”

ã“ã“ã§ $I(\theta^*)$ ã¯Fisheræƒ…å ±é‡ï¼ˆæ¬¡ç¯€ï¼‰ã€‚MLEãŒã€Œæœ€ã‚‚åŠ¹ç‡çš„ãªæ¨å®šé‡ã€ã§ã‚ã‚‹ç†ç”±ãŒã“ã“ã«ã‚ã‚‹ã€‚

:::message alert
MLEã®æ­£å‰‡æ¡ä»¶ï¼ˆæš—é»™ã®ä»®å®šï¼‰:
- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ãŒã‚³ãƒ³ãƒ‘ã‚¯ãƒˆã§ã‚ã‚‹ã‹ã€å°¤åº¦ãŒæœ‰ç•Œ
- çœŸã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta^*$ ãŒå†…ç‚¹
- $p(\mathbf{x}; \theta)$ ãŒ $\theta$ ã«ã¤ã„ã¦3å›å¾®åˆ†å¯èƒ½
- Fisheræƒ…å ±é‡ $I(\theta)$ ãŒæ­£å®šå€¤

ã“ã‚Œã‚‰ãŒç ´ã‚Œã‚‹ã¨ã€MLEã¯ä¸€è‡´æ€§ã™ã‚‰ä¿è¨¼ã•ã‚Œãªã„ã€‚æ··åˆãƒ¢ãƒ‡ãƒ«ã®ç‰¹ç•°æ€§ï¼ˆç¬¬8å›EMï¼‰ã¯ãã®å…¸å‹ä¾‹ã ã€‚
:::

**ä¾‹: Bernoulliåˆ†å¸ƒã®MLE**

$x_1, \ldots, x_N \overset{\text{i.i.d.}}{\sim} \text{Bernoulli}(p)$ ã®ã¨ã:

$$
\ell(p) = \sum_{i=1}^{N} [x_i \log p + (1 - x_i) \log(1-p)] = h \log p + (N - h) \log(1-p)
$$

ã“ã“ã§ $h = \sum_{i=1}^{N} x_i$ ã¯æˆåŠŸå›æ•°ã€‚å¾®åˆ†ã—ã¦0ã¨ãŠã:

$$
\frac{\partial \ell}{\partial p} = \frac{h}{p} - \frac{N - h}{1 - p} = 0 \implies \hat{p}_{\text{MLE}} = \frac{h}{N}
$$

ç›´æ„Ÿé€šã‚Šã€ŒæˆåŠŸå›æ•° / è©¦è¡Œå›æ•°ã€ãŒMLEã€‚

**ä¾‹: Poissonåˆ†å¸ƒã®MLE**

$x_1, \ldots, x_N \overset{\text{i.i.d.}}{\sim} \text{Poisson}(\lambda)$ ã®ã¨ã:

$$
\ell(\lambda) = \sum_{i=1}^{N} [x_i \log \lambda - \lambda - \log(x_i!)]
$$

$$
\frac{\partial \ell}{\partial \lambda} = \frac{\sum x_i}{\lambda} - N = 0 \implies \hat{\lambda}_{\text{MLE}} = \bar{x}
$$

æ¨™æœ¬å¹³å‡ãŒMLEã«ãªã‚‹ã€‚æŒ‡æ•°å‹åˆ†å¸ƒæ—ã§ã¯ã€MLEã¯å¸¸ã«ååˆ†çµ±è¨ˆé‡ã®ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆãƒãƒƒãƒãƒ³ã‚°ã«å¸°ç€ã™ã‚‹ã€‚

```python
import numpy as np

# MLE comparison across distributions
np.random.seed(42)
N = 1000

# Bernoulli MLE
p_true = 0.65
bern_data = np.random.binomial(1, p_true, N)
p_mle = bern_data.mean()
print(f"Bernoulli(p={p_true}): pÌ‚_MLE = {p_mle:.4f}")

# Poisson MLE
lam_true = 4.2
pois_data = np.random.poisson(lam_true, N)
lam_mle = pois_data.mean()
print(f"Poisson(Î»={lam_true}): Î»Ì‚_MLE = {lam_mle:.4f}")

# Exponential MLE: Î»Ì‚ = 1/xÌ„
rate_true = 2.5
exp_data = np.random.exponential(1/rate_true, N)
rate_mle = 1 / exp_data.mean()
print(f"Exponential(Î»={rate_true}): Î»Ì‚_MLE = {rate_mle:.4f}")

# Categorical MLE: Ï€Ì‚_k = (count of k) / N
probs_true = np.array([0.1, 0.3, 0.15, 0.35, 0.1])
cat_data = np.random.choice(5, N, p=probs_true)
probs_mle = np.bincount(cat_data, minlength=5) / N
print(f"\nCategorical MLE:")
print(f"  True:  {probs_true}")
print(f"  MLE:   {probs_mle}")
print(f"  |diff|: {np.abs(probs_true - probs_mle).max():.4f}")
```

**MAPæ¨å®š**: MLE + äº‹å‰åˆ†å¸ƒ = MAPï¼ˆMaximum A Posterioriï¼‰:

$$
\hat{\theta}_{\text{MAP}} = \arg\max_\theta \left[\sum_{i=1}^{N} \log p(\mathbf{x}_i; \theta) + \log p(\theta)\right]
$$

ã‚¬ã‚¦ã‚¹äº‹å‰åˆ†å¸ƒ $p(\theta) = \mathcal{N}(0, \tau^2)$ ã‚’ã‹ã‘ã‚‹ã¨ã€$\log p(\theta) = -\theta^2/(2\tau^2) + \text{const}$ ãªã®ã§ã€MLEã«**L2æ­£å‰‡åŒ–**ãŒåŠ ã‚ã‚‹ã€‚ã¤ã¾ã‚ŠRidgeå›å¸°ã®ãƒ™ã‚¤ã‚ºçš„è§£é‡ˆã¯MAPæ¨å®šã ã€‚

**æ­£å‰‡åŒ–ã¨ãƒ™ã‚¤ã‚ºã®å¯¾å¿œ**:

| æ­£å‰‡åŒ– | äº‹å‰åˆ†å¸ƒ | MAPæ¨å®š |
|:-------|:--------|:-------|
| L2ï¼ˆRidgeï¼‰ | $\mathcal{N}(0, \tau^2)$ | $\hat{\theta}_{\text{MAP}} = \arg\max [\ell(\theta) - \frac{\lambda}{2}\|\theta\|_2^2]$ |
| L1ï¼ˆLassoï¼‰ | Laplace$(0, b)$ | $\hat{\theta}_{\text{MAP}} = \arg\max [\ell(\theta) - \lambda\|\theta\|_1]$ |
| Dropout | Spike-and-slab | å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒç¢ºç‡çš„ã«0ã«ãªã‚‹äº‹å‰åˆ†å¸ƒ |

```python
import numpy as np

def map_vs_mle_bernoulli(data: np.ndarray, prior_a: float = 1, prior_b: float = 1):
    """Compare MLE and MAP for Bernoulli with Beta prior.

    MLE: pÌ‚ = h/N
    MAP: pÌ‚ = (h + a - 1) / (N + a + b - 2) for a,b > 1
    """
    N = len(data)
    h = data.sum()
    t = N - h

    mle = h / N if N > 0 else 0.5
    # MAP for Beta prior: mode of Beta(a+h, b+t)
    post_a = prior_a + h
    post_b = prior_b + t
    if post_a > 1 and post_b > 1:
        map_est = (post_a - 1) / (post_a + post_b - 2)
    else:
        map_est = float('nan')
    bayes_mean = post_a / (post_a + post_b)

    return mle, map_est, bayes_mean

# Edge case: N=0 (no data)
np.random.seed(42)
print("=== MLE vs MAP vs Bayes Mean ===\n")
print(f"{'N':>4} {'h':>3} {'MLE':>8} {'MAP':>8} {'Bayes':>8}")
print("-" * 35)

for N in [0, 1, 3, 10, 100]:
    if N == 0:
        data = np.array([])
    else:
        data = np.random.binomial(1, 0.7, N)
    h = data.sum() if N > 0 else 0
    mle, map_est, bayes = map_vs_mle_bernoulli(data, 2, 2)
    mle_str = f"{h/N:.4f}" if N > 0 else "undef"
    print(f"{N:>4} {int(h):>3} {mle_str:>8} {map_est:>8.4f} {bayes:>8.4f}")

print("\nKey: With Beta(2,2) prior, MAP/Bayes shrink toward 0.5 when data is scarce")
print("As Nâ†’âˆ, all three converge to the true parameter")
```

ã“ã®ã€ŒMLEã¨MAPã®é–¢ä¿‚ã€ã¯ã€LLMã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã‚‚é‡è¦ã ã€‚äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã€Œäº‹å‰åˆ†å¸ƒã€ã¨è¦‹ãªã—ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®å°¤åº¦ã§MAPæ›´æ–°ã™ã‚‹ â€” ã“ã‚ŒãŒLoRA [^12] ã‚„adapteræ‰‹æ³•ã®ç¢ºç‡è«–çš„è§£é‡ˆã ã€‚

### 3.8 Fisheræƒ…å ±é‡ã¨CramÃ©r-Raoä¸‹ç•Œ

**å®šç¾©ï¼ˆFisheræƒ…å ±é‡ï¼‰**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã«é–¢ã™ã‚‹ã‚¹ã‚³ã‚¢é–¢æ•°ã‚’ $s(\mathbf{x}; \theta) = \nabla_\theta \log p(\mathbf{x}; \theta)$ ã¨ã™ã‚‹ã¨ã:

$$
I(\theta) = \mathbb{E}\left[s(\mathbf{x}; \theta) s(\mathbf{x}; \theta)^\top\right] = -\mathbb{E}\left[\nabla_\theta^2 \log p(\mathbf{x}; \theta)\right]
$$

2ã¤ç›®ã®ç­‰å·ã¯æ­£å‰‡æ¡ä»¶ã®ä¸‹ã§æˆã‚Šç«‹ã¤ã€‚ç›´æ„Ÿçš„ã«ã¯ã€Fisheræƒ…å ±é‡ã¯ã€Œå¯¾æ•°å°¤åº¦ã®æ›²ç‡ã€ã‚’æ¸¬ã£ã¦ã„ã‚‹ã€‚æ›²ç‡ãŒå¤§ãã„ã»ã©ã€ãƒ‡ãƒ¼ã‚¿ãŒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã¤ã„ã¦å¤šãã®æƒ…å ±ã‚’æŒã¤ã€‚

**å®šç†ï¼ˆCramÃ©r-Raoä¸‹ç•Œï¼‰**: ä¸åæ¨å®šé‡ $\hat{\theta}$ ã®åˆ†æ•£ã¯ä»¥ä¸‹ã§ä¸‹ã‹ã‚‰æŠ¼ã•ãˆã‚‰ã‚Œã‚‹ [^8]:

$$
\text{Var}(\hat{\theta}) \geq \frac{1}{N \cdot I(\theta)}
$$

**ã“ã®ä¸‹ç•Œã«åˆ°é”ã™ã‚‹æ¨å®šé‡ã‚’æœ‰åŠ¹æ¨å®šé‡ã¨å‘¼ã¶ã€‚** MLEã¯æ¼¸è¿‘çš„ã«æœ‰åŠ¹ã§ã‚ã‚‹ã€‚

```python
import numpy as np

def fisher_information_bernoulli(p: float) -> float:
    """Fisher information for Bernoulli(p).

    I(p) = 1/(p(1-p))

    Derivation:
    log P(x|p) = x log p + (1-x) log(1-p)
    d/dp log P = x/p - (1-x)/(1-p)
    dÂ²/dpÂ² log P = -x/pÂ² - (1-x)/(1-p)Â²
    E[-dÂ²/dpÂ²] = 1/p + 0/(1-p)? No:
    E[-dÂ²/dpÂ²] = p/pÂ² + (1-p)/(1-p)Â² = 1/p + 1/(1-p) = 1/(p(1-p))
    """
    return 1 / (p * (1 - p))

def fisher_information_gaussian(sigma: float) -> np.ndarray:
    """Fisher information matrix for N(Î¼, ÏƒÂ²).

    I(Î¼,ÏƒÂ²) = diag(1/ÏƒÂ², 1/(2Ïƒâ´))
    """
    return np.diag([1/sigma**2, 1/(2*sigma**4)])

# CramÃ©r-Rao bound for Bernoulli
print("=== CramÃ©r-Rao Bound: Bernoulli(p) ===\n")
for p in [0.1, 0.3, 0.5, 0.7, 0.9]:
    I_p = fisher_information_bernoulli(p)
    for N in [10, 100, 1000]:
        cr_bound = 1 / (N * I_p)
        # MLE estimator: pÌ‚ = (# heads)/N, Var(pÌ‚) = p(1-p)/N
        mle_var = p * (1 - p) / N
        print(f"p={p}, N={N:4d}: CR bound={cr_bound:.6f}, Var(pÌ‚_MLE)={mle_var:.6f}, "
              f"efficient={'YES' if np.isclose(cr_bound, mle_var) else 'NO'}")
    print()

# Fisher information for Gaussian
print("=== Fisher Information: Gaussian ===")
sigma = 2.0
I_gauss = fisher_information_gaussian(sigma)
print(f"N(Î¼, ÏƒÂ²={sigma**2}):")
print(f"  I(Î¼,ÏƒÂ²) = diag({I_gauss[0,0]:.4f}, {I_gauss[1,1]:.6f})")
print(f"  CR bound on Var(Î¼Ì‚) = ÏƒÂ²/N = {sigma**2:.4f}/N")
print(f"  CR bound on Var(ÏƒÌ‚Â²) = 2Ïƒâ´/N = {2*sigma**4:.4f}/N")
```

**ã‚¹ã‚³ã‚¢é–¢æ•°ã¨Fisheræƒ…å ±é‡ã®æ·±ã„æ„å‘³**: ã‚¹ã‚³ã‚¢é–¢æ•° $s(\mathbf{x}; \theta) = \nabla_\theta \log p(\mathbf{x}; \theta)$ ã®ã€Œã‚¹ã‚³ã‚¢ã€ã¨ã„ã†åå‰ã¯ã€ãƒ‡ãƒ¼ã‚¿ $\mathbf{x}$ ãŒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã‚’ã©ã®æ–¹å‘ã«ã€ŒæŠ¼ã™ã€ã‹ã‚’ç¤ºã™ã“ã¨ã«ç”±æ¥ã™ã‚‹ã€‚Fisheræƒ…å ±é‡ã¯ã“ã®ã€ŒæŠ¼ã™åŠ›ã€ã®åˆ†æ•£ â€” ã¤ã¾ã‚Šãƒ‡ãƒ¼ã‚¿ãŒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã¤ã„ã¦ã©ã‚Œã ã‘ã€Œæƒ…å ±ã‚’æŒã£ã¦ã„ã‚‹ã‹ã€ã‚’å®šé‡åŒ–ã™ã‚‹ã€‚

:::details Score Matchingã¨ã®æ¥ç¶š â€” ç¬¬28å›ã¸ã®ä¼ç·š
Score Matchingã¯ç¢ºç‡åˆ†å¸ƒ $p(\mathbf{x})$ ã®ã‚¹ã‚³ã‚¢ $\nabla_\mathbf{x} \log p(\mathbf{x})$ ã‚’å­¦ç¿’ã™ã‚‹æ‰‹æ³•ã§ã€HyvÃ¤rinen (2005) [^9] ãŒææ¡ˆã—ãŸã€‚

æ³¨æ„: ã“ã“ã§ã€Œã‚¹ã‚³ã‚¢ã€ã¯**ãƒ‡ãƒ¼ã‚¿ç©ºé–“**ã§ã®å‹¾é…ã€‚Fisheræƒ…å ±é‡ã®ã‚¹ã‚³ã‚¢é–¢æ•°ã¯**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“**ã§ã®å‹¾é…ã€‚åå‰ã¯åŒã˜ã ãŒå¯¾è±¡ãŒé•ã†ã€‚

- Fisher score: $\nabla_\theta \log p(\mathbf{x}; \theta)$ â€” ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¨å®šã«ä½¿ã†
- Stein score: $\nabla_\mathbf{x} \log p(\mathbf{x})$ â€” ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ»ç”Ÿæˆã«ä½¿ã†

å¾Œè€…ãŒæ‹¡æ•£ãƒ¢ãƒ‡ãƒ« [^4] ã®ç†è«–çš„åŸºç›¤ã§ã‚ã‚Šã€Song & Ermon (2020) [^10] ãŒSDEæ çµ„ã¿ã§çµ±ä¸€ã—ãŸã€‚ç¬¬28å›ã§æœ¬æ ¼çš„ã«æ‰±ã†ã€‚
:::

### 3.9 ç¢ºç‡å¤‰æ•°ã®å¤‰æ› â€” Change of Variables

ç¢ºç‡å¤‰æ•° $X$ ã«é–¢æ•° $g$ ã‚’é©ç”¨ã—ã¦ $Y = g(X)$ ã‚’å¾—ã‚‹ã¨ãã€$Y$ ã®åˆ†å¸ƒã¯ã©ã†ãªã‚‹ã‹ã€‚

**é›¢æ•£ã®å ´åˆ**: $P(Y = y) = \sum_{\{x : g(x) = y\}} P(X = x)$ï¼ˆé€†åƒã®ç¢ºç‡ã‚’è¶³ã™ï¼‰

**é€£ç¶šã®å ´åˆï¼ˆ1æ¬¡å…ƒã€$g$ ãŒå˜èª¿ï¼‰**:

$$
f_Y(y) = f_X(g^{-1}(y)) \cdot \left|\frac{dg^{-1}}{dy}\right|
$$

**å¤šæ¬¡å…ƒï¼ˆãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ï¼‰**: $\mathbf{Y} = g(\mathbf{X})$ ã®ã¨ã:

$$
f_\mathbf{Y}(\mathbf{y}) = f_\mathbf{X}(g^{-1}(\mathbf{y})) \cdot \left|\det \frac{\partial g^{-1}}{\partial \mathbf{y}}\right|
$$

ç¬¬3å›ã§å­¦ã‚“ã ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ãŒã“ã“ã«ç™»å ´ã™ã‚‹ã€‚ã“ã®å¤‰æ›å…¬å¼ã¯ç¬¬25å›ï¼ˆNormalizing Flowsï¼‰ã®æ ¸å¿ƒã ã€‚Flowãƒ¢ãƒ‡ãƒ«ã¯å¯é€†å¤‰æ› $g$ ã‚’ç¹°ã‚Šè¿”ã—é©ç”¨ã—ã€å˜ç´”ãªåˆ†å¸ƒï¼ˆã‚¬ã‚¦ã‚¹ï¼‰ã‚’è¤‡é›‘ãªãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã«å¤‰æ›ã™ã‚‹ã€‚

```python
import numpy as np

# Example: log-normal distribution via change of variables
# If X ~ N(Î¼, ÏƒÂ²), then Y = exp(X) ~ LogNormal(Î¼, ÏƒÂ²)
np.random.seed(42)
mu, sigma = 1.0, 0.5
N = 100000

X = np.random.normal(mu, sigma, N)
Y = np.exp(X)  # change of variables: g(x) = exp(x)

# Theory: E[Y] = exp(Î¼ + ÏƒÂ²/2), Var(Y) = (exp(ÏƒÂ²)-1)exp(2Î¼+ÏƒÂ²)
E_Y_theory = np.exp(mu + sigma**2 / 2)
V_Y_theory = (np.exp(sigma**2) - 1) * np.exp(2 * mu + sigma**2)

print("=== Change of Variables: X~N(Î¼,ÏƒÂ²) â†’ Y=exp(X)~LogNormal ===")
print(f"E[Y] empirical: {Y.mean():.4f}, theory: {E_Y_theory:.4f}")
print(f"Var(Y) empirical: {Y.var():.4f}, theory: {V_Y_theory:.4f}")

# Verify density via change of variables formula
# f_Y(y) = f_X(log(y)) * |d(log y)/dy| = f_X(log(y)) * 1/y
y_grid = np.linspace(0.01, 15, 1000)
f_Y_formula = (1 / (y_grid * sigma * np.sqrt(2 * np.pi))) * np.exp(-(np.log(y_grid) - mu)**2 / (2 * sigma**2))
print(f"\nDensity check: âˆ«f_Y(y)dy â‰ˆ {np.trapz(f_Y_formula, y_grid):.6f} (should be 1.0)")
```

:::message
å¤‰æ›å…¬å¼ã®ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ $|\det J|$ ã¯ã€Œä½“ç©ã®ä¼¸ç¸®ç‡ã€ã ã€‚å¤‰æ› $g$ ãŒç©ºé–“ã‚’å¼•ãä¼¸ã°ã›ã°ç¢ºç‡å¯†åº¦ã¯è–„ããªã‚Šã€åœ§ç¸®ã™ã‚Œã°æ¿ƒããªã‚‹ã€‚Normalizing Flow [^11] ã¯ã“ã®æ€§è³ªã‚’åˆ©ç”¨ã—ã¦ã€è¤‡é›‘ãªåˆ†å¸ƒã®æ­£ç¢ºãªå°¤åº¦ã‚’è¨ˆç®—ã™ã‚‹ã€‚ç¬¬25å›ã§è©³ã—ãæ‰±ã†ã€‚
:::

### 3.10 ç¢ºç‡çš„ã‚°ãƒ©ãƒ•ã‚£ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«å…¥é–€

ç¢ºç‡å¤‰æ•°é–“ã®ä¾å­˜é–¢ä¿‚ã‚’ã‚°ãƒ©ãƒ•ã§è¡¨ç¾ã™ã‚‹æ çµ„ã¿ã€‚

**ãƒ™ã‚¤ã‚¸ã‚¢ãƒ³ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯**ï¼ˆæœ‰å‘ã‚°ãƒ©ãƒ•ï¼‰:

```mermaid
graph TD
    R["Rain<br/>P(R)"] --> W["Wet Grass<br/>P(W|R,S)"]
    S["Sprinkler<br/>P(S|R)"] --> W

    style R fill:#e3f2fd
    style S fill:#fff3e0
    style W fill:#c8e6c9
```

åŒæ™‚åˆ†å¸ƒãŒã‚°ãƒ©ãƒ•ã®æ§‹é€ ã«æ²¿ã£ã¦å› æ•°åˆ†è§£ã•ã‚Œã‚‹:

$$
P(R, S, W) = P(R) \cdot P(S \mid R) \cdot P(W \mid R, S)
$$

**d-separation**: ã‚°ãƒ©ãƒ•ã®æ§‹é€ ã‹ã‚‰æ¡ä»¶ä»˜ãç‹¬ç«‹æ€§ã‚’èª­ã¿å–ã‚Œã‚‹ã€‚ä¸Šã®ä¾‹ã§ã€$R$ ãŒè¦³æ¸¬ã•ã‚Œã‚‹ã¨ $S$ ã¨ $W$ ã¯æ¡ä»¶ä»˜ãç‹¬ç«‹ã§ã¯ãªã„ï¼ˆexplaining awayï¼‰ã€‚

**ãƒãƒ«ã‚³ãƒ•ç¢ºç‡å ´ï¼ˆMRFï¼‰**ï¼ˆç„¡å‘ã‚°ãƒ©ãƒ•ï¼‰:

$$
P(\mathbf{x}) = \frac{1}{Z} \prod_{c \in \mathcal{C}} \psi_c(\mathbf{x}_c)
$$

ã“ã“ã§ $\psi_c$ ã¯ã‚¯ãƒªãƒ¼ã‚¯ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã€$Z$ ã¯åˆ†é…é–¢æ•°ã€‚EBM [^7] ã¯MRFã®ä¸€èˆ¬åŒ–ã¨è¦‹ãªã›ã‚‹ã€‚

```python
import numpy as np

# Simple Bayesian Network: Rain â†’ Sprinkler, Rain â†’ WetGrass, Sprinkler â†’ WetGrass
# P(R), P(S|R), P(W|R,S)
P_R = np.array([0.8, 0.2])  # [no rain, rain]
P_S_given_R = np.array([[0.6, 0.4],   # S|no rain
                          [0.01, 0.99]]) # S|rain  (sprinkler OFF when raining)
P_W_given_RS = np.array([
    [[0.99, 0.01],  # W|no rain, no sprinkler
     [0.1, 0.9]],   # W|no rain, sprinkler ON
    [[0.2, 0.8],    # W|rain, no sprinkler
     [0.01, 0.99]]  # W|rain, sprinkler ON
])

# Compute joint P(R, S, W)
joint = np.zeros((2, 2, 2))
for r in range(2):
    for s in range(2):
        for w in range(2):
            joint[r, s, w] = P_R[r] * P_S_given_R[r, s] * P_W_given_RS[r, s, w]

print(f"Sum of joint (must be 1.0): {joint.sum():.6f}")

# Query: P(Rain | WetGrass=1) via Bayes
p_w1 = joint[:, :, 1].sum()  # P(W=1)
p_r_given_w1 = joint[:, :, 1].sum(axis=1) / p_w1  # P(R|W=1)
print(f"\nP(Rain | Wet Grass) = {p_r_given_w1[1]:.4f}")
print(f"P(No Rain | Wet Grass) = {p_r_given_w1[0]:.4f}")

# Explaining away: P(Rain | WetGrass=1, Sprinkler=1) vs P(Rain | WetGrass=1, Sprinkler=0)
p_r_given_w1_s1 = joint[:, 1, 1] / joint[:, 1, 1].sum()
p_r_given_w1_s0 = joint[:, 0, 1] / joint[:, 0, 1].sum()
print(f"\nExplaining away:")
print(f"P(Rain | Wet, Sprinkler ON)  = {p_r_given_w1_s1[1]:.4f}")
print(f"P(Rain | Wet, Sprinkler OFF) = {p_r_given_w1_s0[1]:.4f}")
print("â†’ If sprinkler explains wet grass, rain becomes less likely (explaining away)")
```

ç¢ºç‡çš„ã‚°ãƒ©ãƒ•ã‚£ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã¯ã€VAEã®ç”Ÿæˆéç¨‹ ($\mathbf{z} \to \mathbf{x}$) ã‚„HMMã€Diffusionã®ãƒãƒ«ã‚³ãƒ•é€£é–ã‚’çµ±ä¸€çš„ã«è¨˜è¿°ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã ã€‚ç¬¬8å›ï¼ˆEMç®—æ³•ï¼‰ã§GMMã®æ½œåœ¨å¤‰æ•°ã‚’æ‰±ã†ã¨ãã€ã“ã®æ çµ„ã¿ãŒæ´»ãã¦ãã‚‹ã€‚

### 3.11 å¤§æ•°ã®æ³•å‰‡ã¨ä¸­å¿ƒæ¥µé™å®šç†

#### å¤§æ•°ã®æ³•å‰‡ï¼ˆLLNï¼‰

**å®šç†ï¼ˆå¼±å¤§æ•°ã®æ³•å‰‡ï¼‰**: i.i.d.ç¢ºç‡å¤‰æ•° $X_1, X_2, \ldots$ ãŒ $\mathbb{E}[|X_1|] < \infty$ ã‚’æº€ãŸã™ã¨ã:

$$
\bar{X}_N = \frac{1}{N}\sum_{i=1}^{N} X_i \xrightarrow{P} \mathbb{E}[X_1] \quad (N \to \infty)
$$

**å®šç†ï¼ˆå¼·å¤§æ•°ã®æ³•å‰‡ï¼‰**: åŒã˜æ¡ä»¶ã®ä¸‹ã§:

$$
\bar{X}_N \xrightarrow{\text{a.s.}} \mathbb{E}[X_1] \quad (N \to \infty)
$$

ã€Œa.s.ã€ã¯ã€Œæ¦‚åæŸï¼ˆalmost surelyï¼‰ã€â€” ç¢ºç‡1ã§åæŸã™ã‚‹ã€‚ç¬¬5å›ã§æ¸¬åº¦è«–çš„ã«å³å¯†åŒ–ã™ã‚‹ã€‚

#### ä¸­å¿ƒæ¥µé™å®šç†ï¼ˆCLTï¼‰

**å®šç†ï¼ˆä¸­å¿ƒæ¥µé™å®šç†ï¼‰**: i.i.d.ç¢ºç‡å¤‰æ•° $X_1, X_2, \ldots$ ãŒ $\mathbb{E}[X_1] = \mu$, $\text{Var}(X_1) = \sigma^2 < \infty$ ã‚’æº€ãŸã™ã¨ã:

$$
\frac{\bar{X}_N - \mu}{\sigma / \sqrt{N}} \xrightarrow{d} \mathcal{N}(0, 1) \quad (N \to \infty)
$$

ç­‰ä¾¡çš„ã«æ›¸ãã¨: $\sqrt{N}(\bar{X}_N - \mu) \xrightarrow{d} \mathcal{N}(0, \sigma^2)$ã€‚åæŸé€Ÿåº¦ã¯ $O(1/\sqrt{N})$ â€” å®Ÿç”¨ä¸Šã€$N \geq 30$ ã§è¿‘ä¼¼ãŒååˆ†ã«åŠ¹ãï¼ˆBerry-Esseené™ç•Œã«ã‚ˆã‚‹å®šé‡çš„ä¿è¨¼ï¼‰ã€‚

:::details ğŸ“– Berry-Esseené™ç•Œã®è©³ç´°ï¼ˆç™ºå±•ï¼‰
CLTã®åæŸé€Ÿåº¦ã‚’å®šé‡åŒ–ã™ã‚‹Berry-Esseenä¸ç­‰å¼:

$$
\sup_x \left|P\left(\frac{\bar{X}_N - \mu}{\sigma/\sqrt{N}} \leq x\right) - \Phi(x)\right| \leq \frac{C \rho}{\sigma^3 \sqrt{N}}
$$

ã“ã“ã§ $\rho = \mathbb{E}[|X - \mu|^3]$ ã¯3æ¬¡çµ¶å¯¾ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆã€$C$ ã¯çµ¶å¯¾å®šæ•°ï¼ˆ$C \leq 0.4748$ï¼‰ã€‚CLTãŒã€ŒåæŸã™ã‚‹ã€ã¨ã—ã‹è¨€ã‚ãªã„ã®ã«å¯¾ã—ã€Berry-Esseené™ç•Œã¯ã€Œã©ã‚Œã ã‘é€ŸãåæŸã™ã‚‹ã‹ã€ã‚’ $O(1/\sqrt{N})$ ã§å®šé‡åŒ–ã™ã‚‹ã€‚æœ¬ã‚·ãƒªãƒ¼ã‚ºã§ã¯CLTã®çµè«–ã§ååˆ†ã ãŒã€åæŸã®é€Ÿã•ã‚’è­°è«–ã™ã‚‹å ´é¢ã§å‚ç…§ã•ã‚ŒãŸã„ã€‚
:::

```python
import numpy as np

def demonstrate_clt(dist_name: str, sampler, true_mean: float, true_var: float, N_values: list):
    """Demonstrate CLT for various distributions."""
    print(f"\n=== CLT for {dist_name} ===")
    print(f"True mean={true_mean:.2f}, True var={true_var:.2f}")
    print(f"{'N':>8} {'Sample mean':>12} {'Std of mean':>12} {'Ïƒ/âˆšN (theory)':>14} {'Normalized':>10}")
    print("-" * 60)

    n_experiments = 10000
    for N in N_values:
        means = np.array([sampler(N).mean() for _ in range(n_experiments)])
        sample_std = means.std()
        theory_std = np.sqrt(true_var / N)
        # Test normality of standardized means
        standardized = (means - true_mean) / theory_std
        print(f"{N:>8} {means.mean():>12.4f} {sample_std:>12.4f} {theory_std:>14.4f} "
              f"std={standardized.std():>5.3f}")

# Uniform[0,1]: Î¼=0.5, ÏƒÂ²=1/12
demonstrate_clt("Uniform[0,1]",
                lambda n: np.random.uniform(0, 1, n),
                0.5, 1/12, [1, 5, 30, 100, 1000])

# Exponential(Î»=2): Î¼=0.5, ÏƒÂ²=0.25
demonstrate_clt("Exponential(Î»=2)",
                lambda n: np.random.exponential(0.5, n),
                0.5, 0.25, [1, 5, 30, 100, 1000])

# Bernoulli(p=0.3): Î¼=0.3, ÏƒÂ²=0.21
demonstrate_clt("Bernoulli(p=0.3)",
                lambda n: np.random.binomial(1, 0.3, n).astype(float),
                0.3, 0.21, [1, 5, 30, 100, 1000])
```

**CLTãŒé‡è¦ãªç†ç”±**:

1. **MLEã®æ¼¸è¿‘æ­£è¦æ€§ã®æ ¹æ‹ **: MLEæ¨å®šé‡ã¯ååˆ†çµ±è¨ˆé‡ã®å¹³å‡ â†’ CLTã«ã‚ˆã‚Šæ­£è¦åˆ†å¸ƒã«è¿‘ã¥ã
2. **ã‚¬ã‚¦ã‚¹åˆ†å¸ƒãŒé »å‡ºã™ã‚‹ç†ç”±**: å¤šæ•°ã®å°ã•ãªåŠ¹æœã®å’Œ â†’ CLTã«ã‚ˆã‚Šè¿‘ä¼¼çš„ã«ã‚¬ã‚¦ã‚¹
3. **ä¿¡é ¼åŒºé–“ã®æ§‹æˆ**: $\bar{X}_N \pm z_{\alpha/2} \cdot \sigma/\sqrt{N}$ â€” CLTãŒæ­£å½“åŒ–

```mermaid
graph TD
    LLN["å¤§æ•°ã®æ³•å‰‡<br/>XÌ„_N â†’ Î¼<br/>(ä¸€è‡´æ€§)"]
    CLT["ä¸­å¿ƒæ¥µé™å®šç†<br/>âˆšN(XÌ„_N-Î¼) â†’ N(0,ÏƒÂ²)<br/>(åˆ†å¸ƒåæŸ)"]
    BE["Berry-Esseen<br/>åæŸé€Ÿåº¦ O(1/âˆšN)<br/>(å®šé‡çš„ä¿è¨¼)"]
    MLE["MLEæ¼¸è¿‘æ­£è¦æ€§<br/>âˆšN(Î¸Ì‚-Î¸*) â†’ N(0,Iâ»Â¹)<br/>(æ¨å®šã®ç²¾åº¦)"]
    CR["CramÃ©r-Raoä¸‹ç•Œ<br/>Var â‰¥ 1/(NI)<br/>(æ¨å®šã®é™ç•Œ)"]

    LLN --> CLT --> BE
    CLT --> MLE
    MLE --> CR
```

### 3.12 âš”ï¸ Boss Battle â€” è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã®å°¤åº¦ã‚’å®Œå…¨åˆ†è§£ã›ã‚ˆ

æº–å‚™ã¯ã„ã„ã‹ã€‚ã“ã“ã¾ã§ã®å…¨æ­¦å™¨ã‚’ä½¿ã£ã¦ã€LLMã®å­¦ç¿’ç›®æ¨™ã‚’ç¢ºç‡è«–ã®è¨€è‘‰ã§å®Œå…¨ã«è¨˜è¿°ã™ã‚‹ã€‚

**å•é¡Œ**: ãƒ†ã‚­ã‚¹ãƒˆ $\mathbf{x} = (x_1, x_2, \ldots, x_T)$ ã«å¯¾ã—ã¦ã€è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã®å¯¾æ•°å°¤åº¦ã‚’å°å‡ºã—ã€å„ã‚¹ãƒ†ãƒƒãƒ—ãŒCategoricalåˆ†å¸ƒã®MLEã«å¸°ç€ã™ã‚‹ã“ã¨ã‚’ç¤ºã›ã€‚

**è§£ç­”**:

**Step 1**: ç¢ºç‡ã®é€£é–è¦å‰‡ï¼ˆchain ruleï¼‰ã«ã‚ˆã‚Š:

$$
p_\theta(\mathbf{x}) = \prod_{t=1}^{T} p_\theta(x_t \mid x_{<t})
$$

ã“ã‚Œã¯ä½•ã®è¿‘ä¼¼ã‚‚ä»®å®šã‚‚ãªã„æ’ç­‰å¼ï¼ˆç¢ºç‡ã®å…¬ç†ã‹ã‚‰å°å‡ºå¯èƒ½ï¼‰ã€‚

**Step 2**: å¯¾æ•°ã‚’å–ã‚‹ã¨:

$$
\log p_\theta(\mathbf{x}) = \sum_{t=1}^{T} \log p_\theta(x_t \mid x_{<t})
$$

**Step 3**: å„ã‚¹ãƒ†ãƒƒãƒ—ã§ãƒ¢ãƒ‡ãƒ«ã¯Softmaxå‡ºåŠ› $\hat{\boldsymbol{\pi}}_t = \text{softmax}(f_\theta(x_{<t}))$ ã‚’è¨ˆç®—ã€‚$f_\theta$ ã¯Transformerãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€‚ã“ã‚Œã¯Categoricalåˆ†å¸ƒ:

$$
p_\theta(x_t = k \mid x_{<t}) = [\hat{\boldsymbol{\pi}}_t]_k = \frac{\exp([f_\theta(x_{<t})]_k)}{\sum_{j=1}^{V} \exp([f_\theta(x_{<t})]_j)}
$$

ã“ã“ã§ $V$ ã¯èªå½™ã‚µã‚¤ã‚ºã€‚

**Step 4**: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ $\mathcal{D} = \{\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(N)}\}$ ã«å¯¾ã™ã‚‹å¯¾æ•°å°¤åº¦:

$$
\mathcal{L}(\theta) = \frac{1}{N}\sum_{n=1}^{N} \sum_{t=1}^{T_n} \log p_\theta(x_t^{(n)} \mid x_{<t}^{(n)})
$$

**Step 5**: $-\mathcal{L}(\theta)$ ã‚’æœ€å°åŒ–ã™ã‚‹ã®ãŒCross-Entropy Lossï¼ˆç¬¬1å›ã§ä½“é¨“ã—ãŸå¼ï¼‰ã€‚

$$
\text{Cross-Entropy Loss} = -\frac{1}{N}\sum_{n=1}^{N} \frac{1}{T_n}\sum_{t=1}^{T_n} \log p_\theta(x_t^{(n)} \mid x_{<t}^{(n)})
$$

**ã“ã‚Œã¯ã¾ã•ã«ã€å„æ™‚åˆ» $t$ ã§ã®Categoricalåˆ†å¸ƒã«å¯¾ã™ã‚‹MLEã®è² ã®å¯¾æ•°å°¤åº¦ã«ä»–ãªã‚‰ãªã„ã€‚**

```python
import numpy as np

def autoregressive_nll(logits_sequence: list, targets: list) -> float:
    """Compute negative log-likelihood for autoregressive model.

    corresponds to: -L(Î¸) = -(1/T) Î£_t log p_Î¸(x_t | x_{<t})

    Each logits_sequence[t] is a vector of size V (vocabulary)
    Each targets[t] is the true token index
    """
    total_nll = 0.0
    T = len(targets)
    for t in range(T):
        logits = logits_sequence[t]
        # softmax â†’ Categorical distribution
        exp_logits = np.exp(logits - np.max(logits))
        probs = exp_logits / exp_logits.sum()
        # negative log probability of correct token
        total_nll -= np.log(probs[targets[t]] + 1e-10)
    return total_nll / T

# Simulate a tiny autoregressive model
np.random.seed(42)
V = 10  # vocabulary size
T = 5   # sequence length

# Random logits (simulating Transformer output at each step)
logits_seq = [np.random.randn(V) for _ in range(T)]
targets = [3, 7, 1, 5, 2]

nll = autoregressive_nll(logits_seq, targets)
perplexity = np.exp(nll)
print(f"Autoregressive NLL: {nll:.4f}")
print(f"Perplexity:         {perplexity:.2f}")
print(f"Random baseline:    NLL={np.log(V):.4f}, PPL={V}")
print(f"\nInterpretation: The model is choosing from ~{perplexity:.0f} equally likely tokens")
print(f"(perfect model: PPL=1, random guess: PPL={V})")
```

> **Boss Battle æ”»ç•¥å®Œäº†**: è‡ªå·±å›å¸°LLMã®æå¤±é–¢æ•°ã¯ã€æ¡ä»¶ä»˜ãç¢ºç‡ã®é€£é–è¦å‰‡ + å„ã‚¹ãƒ†ãƒƒãƒ—ã®Categoricalåˆ†å¸ƒã®MLE + å¯¾æ•°å¤‰æ› â€” å…¨ã¦æœ¬è¬›ç¾©ã§å­¦ã‚“ã é“å…·ã ã‘ã§æ§‹æˆã•ã‚Œã¦ã„ã‚‹ã€‚

:::message
**é€²æ—: 50% å®Œäº†** ç¢ºç‡ç©ºé–“ã®å…¬ç†ã‹ã‚‰å§‹ã¾ã‚Šã€ãƒ™ã‚¤ã‚ºã®å®šç†ã€ä¸»è¦åˆ†å¸ƒã€å¤šå¤‰é‡æ­£è¦ã€æŒ‡æ•°å‹åˆ†å¸ƒæ—ã€MLEã€Fisheræƒ…å ±é‡ã€CLTã¾ã§ä¸€æ°—ã«é§†ã‘æŠœã‘ãŸã€‚Boss Battleã§ã¯è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã®å°¤åº¦ã‚’å®Œå…¨ã«åˆ†è§£ã—ãŸã€‚Zone 3 ã‚¯ãƒªã‚¢ã€‚
:::

---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” ç¢ºç‡è«–ã‚’ã‚³ãƒ¼ãƒ‰ã«ç„¼ãã¤ã‘ã‚‹

### 4.1 ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# Python 3.10+ recommended
pip install numpy scipy matplotlib
```

æœ¬è¬›ç¾©ã¯Python 100%ã€‚NumPyã¨SciPyã®ã¿ä½¿ç”¨ã™ã‚‹ã€‚PyTorchã¯ä¸è¦ã ã€‚

### 4.2 ç¢ºç‡åˆ†å¸ƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³

NumPyã¨SciPyã®ç¢ºç‡åˆ†å¸ƒé–¢æ•°ã‚’ä½“ç³»çš„ã«æ•´ç†ã™ã‚‹ã€‚

```python
import numpy as np
from scipy import stats

# Sampling, PDF/PMF, CDF, Quantile (PPF) for major distributions
distributions = {
    "Bernoulli(0.7)":     (stats.bernoulli(0.7), "discrete"),
    "Binomial(20,0.3)":   (stats.binom(20, 0.3), "discrete"),
    "Poisson(5)":         (stats.poisson(5), "discrete"),
    "Normal(0,1)":        (stats.norm(0, 1), "continuous"),
    "Gamma(3,2)":         (stats.gamma(3, scale=0.5), "continuous"),
    "Beta(2,5)":          (stats.beta(2, 5), "continuous"),
    "Exponential(2)":     (stats.expon(scale=0.5), "continuous"),
}

print(f"{'Distribution':<22} {'Mean':>8} {'Var':>8} {'Median':>8} {'Entropy':>8}")
print("-" * 58)
for name, (dist, dtype) in distributions.items():
    mean = dist.mean()
    var = dist.var()
    median = dist.median()
    entropy = dist.entropy()
    print(f"{name:<22} {mean:>8.3f} {var:>8.3f} {median:>8.3f} {entropy:>8.3f}")

# Important: scipy vs numpy interface
print("\n=== Sampling Interface Comparison ===")
print("NumPy:  np.random.normal(mu, sigma, N)  â†’ array of samples")
print("SciPy:  stats.norm(mu, sigma).rvs(N)     â†’ array of samples")
print("SciPy:  stats.norm(mu, sigma).pdf(x)     â†’ density at x")
print("SciPy:  stats.norm(mu, sigma).cdf(x)     â†’ P(X â‰¤ x)")
print("SciPy:  stats.norm(mu, sigma).ppf(q)     â†’ quantile (inverse CDF)")
print("SciPy:  stats.norm(mu, sigma).logpdf(x)  â†’ log density (MLEç”¨)")
```

### 4.3 LaTeXç¢ºç‡è¨˜æ³•ãƒãƒ¼ãƒˆã‚·ãƒ¼ãƒˆ

æ•°å¼ã§æ›¸ãâ†”è«–æ–‡ã§èª­ã‚€ã€ã‚’é«˜é€Ÿã«åˆ‡ã‚Šæ›¿ãˆã‚‹ãŸã‚ã®å¯¾å¿œè¡¨ã€‚

| æ•°å¼ | LaTeX | èª­ã¿ | Python |
|:-----|:------|:-----|:-------|
| $P(A)$ | `P(A)` | ãƒ”ãƒ¼ ã‚¨ãƒ¼ | `p_a` |
| $P(A \mid B)$ | `P(A \mid B)` | ãƒ”ãƒ¼ ã‚¨ãƒ¼ ã‚®ãƒ–ãƒ³ ãƒ“ãƒ¼ | `p_a_given_b` |
| $\mathbb{E}[X]$ | `\mathbb{E}[X]` | ã‚¤ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ | `x.mean()` |
| $\text{Var}(X)$ | `\text{Var}(X)` | ãƒãƒªã‚¢ãƒ³ã‚¹ ã‚¨ãƒƒã‚¯ã‚¹ | `x.var()` |
| $\mathcal{N}(\mu, \sigma^2)$ | `\mathcal{N}(\mu, \sigma^2)` | ãƒãƒ¼ãƒãƒ« ãƒŸãƒ¥ãƒ¼ ã‚·ã‚°ãƒäºŒä¹— | `np.random.normal(mu, sigma)` |
| $\sim$ | `\sim` | ã—ãŸãŒã† / åˆ†å¸ƒã™ã‚‹ | sampling |
| $\overset{\text{i.i.d.}}{\sim}$ | `\overset{\text{i.i.d.}}{\sim}` | ç‹¬ç«‹åŒåˆ†å¸ƒã«ã—ãŸãŒã† | `for` loop sampling |
| $\propto$ | `\propto` | æ¯”ä¾‹ã™ã‚‹ | unnormalized |
| $\prod_{i=1}^{N}$ | `\prod_{i=1}^{N}` | ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ | `np.prod()` |
| $\arg\max_\theta$ | `\arg\max_\theta` | ã‚¢ãƒ¼ã‚°ãƒãƒƒã‚¯ã‚¹ ã‚·ãƒ¼ã‚¿ | `theta[np.argmax(...)]` |

### 4.4 è«–æ–‡èª­è§£ã®å®Ÿè·µ â€” 3ãƒ‘ã‚¹ãƒªãƒ¼ãƒ‡ã‚£ãƒ³ã‚°

ç¢ºç‡è«–ã®è«–æ–‡ã‚’èª­ã‚€ãŸã‚ã®ä½“ç³»çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã€‚

```mermaid
graph TD
    P1["Pass 1: æ¦‚è¦æŠŠæ¡ (5åˆ†)<br/>Abstractâ†’Conclusionâ†’å›³è¡¨"]
    P2["Pass 2: æ§‹é€ ç†è§£ (30åˆ†)<br/>å®šç†ã®ä¸»å¼µâ†’ä»®å®šâ†’å¸°çµ"]
    P3["Pass 3: å†ç¾ (2-3æ™‚é–“)<br/>å°å‡ºã‚’ç´™ã§è¿½ã†â†’ã‚³ãƒ¼ãƒ‰å®Ÿè£…"]

    P1 -->|"èª­ã‚€ä¾¡å€¤ã‚ã‚Š?"| P2
    P2 -->|"æ·±ãç†è§£ã—ãŸã„?"| P3
```

**Pass 1 ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ** â€” ç¢ºç‡è«–ã®è«–æ–‡ã«ç‰¹åŒ–:

```python
paper_pass1 = {
    "title": "",
    "authors": "",
    "year": "",
    "venue": "",
    # Probability-specific fields
    "distributions_used": [],       # e.g., ["Gaussian", "Categorical", "Dirichlet"]
    "key_assumptions": [],          # e.g., ["i.i.d.", "compact support", "finite variance"]
    "estimation_method": "",        # e.g., "MLE", "Bayesian", "Variational"
    "main_theorem": "",             # one-sentence statement
    "convergence_rate": "",         # e.g., "O(1/âˆšN)", "exponential"
    "experiments": "",
    "relevance_to_generative": "",  # connection to VAE/GAN/Diffusion
    "read_further": True,           # proceed to Pass 2?
}
```

### 4.5 æ•°å¼â†’ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ‘ã‚¿ãƒ¼ãƒ³

ç¢ºç‡è«–ã«ç‰¹åŒ–ã—ãŸ7ã¤ã®ç¿»è¨³ãƒ‘ã‚¿ãƒ¼ãƒ³ã€‚

**ãƒ‘ã‚¿ãƒ¼ãƒ³1: ç¢ºç‡å¯†åº¦é–¢æ•°ï¼ˆPDFï¼‰**

$$
f(x; \theta) = \text{formula}
$$

```python
def pdf(x: np.ndarray, theta: float) -> np.ndarray:
    """Direct translation of mathematical formula."""
    return formula(x, theta)
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³2: æœŸå¾…å€¤ã®Monte Carloè¿‘ä¼¼**

$$
\mathbb{E}_{p(x)}[g(x)] = \int g(x) p(x) dx \approx \frac{1}{N}\sum_{i=1}^{N} g(x_i), \quad x_i \sim p(x)
$$

```python
samples = np.random.distribution(params, size=N)  # x_i ~ p(x)
expectation = np.mean(g(samples))                  # (1/N) Î£ g(x_i)
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³3: å°¤åº¦ã¨å¯¾æ•°å°¤åº¦**

$$
\ell(\theta) = \sum_{i=1}^{N} \log p(x_i; \theta)
$$

```python
def log_likelihood(data: np.ndarray, theta: float) -> float:
    return np.sum(np.log(pdf(data, theta) + 1e-10))  # +Îµ for numerical stability
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³4: ãƒ™ã‚¤ã‚ºæ›´æ–°**

$$
p(\theta \mid \mathcal{D}) \propto p(\mathcal{D} \mid \theta) \cdot p(\theta)
$$

```python
# Grid approximation
theta_grid = np.linspace(0, 1, 1000)
prior = prior_pdf(theta_grid)
likelihood = np.prod([pdf(x, theta_grid) for x in data], axis=0)
posterior = likelihood * prior
posterior /= np.trapz(posterior, theta_grid)  # normalize
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³5: MLE via æ•°å€¤æœ€é©åŒ–**

$$
\hat{\theta} = \arg\max_\theta \ell(\theta)
$$

```python
from scipy.optimize import minimize_scalar
result = minimize_scalar(lambda t: -log_likelihood(data, t), bounds=(0, 10), method='bounded')
theta_mle = result.x
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³6: æ¡ä»¶ä»˜ãåˆ†å¸ƒã®è¨ˆç®—**

$$
p(y \mid x) = \frac{p(x, y)}{p(x)} = \frac{p(x, y)}{\sum_y p(x, y)}
$$

```python
joint = compute_joint(x, y)           # P(X, Y)
marginal_x = joint.sum(axis=1)        # P(X) = Î£_y P(X,y)
conditional = joint / marginal_x[:, None]  # P(Y|X)
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³7: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨çµŒé¨“åˆ†å¸ƒ**

$$
\hat{p}(x) = \frac{1}{N}\sum_{i=1}^{N} \delta(x - x_i)
$$

```python
samples = np.random.distribution(params, size=N)
# Empirical distribution via histogram
counts, bin_edges = np.histogram(samples, bins=50, density=True)
```

:::details å…¨ç¿»è¨³ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œè¡¨
| æ•°å­¦çš„æ¦‚å¿µ | æ•°å¼ | NumPyã‚³ãƒ¼ãƒ‰ |
|:----------|:-----|:-----------|
| æœŸå¾…å€¤ | $\mathbb{E}[X]$ | `samples.mean()` |
| åˆ†æ•£ | $\text{Var}(X)$ | `samples.var()` |
| å…±åˆ†æ•£è¡Œåˆ— | $\boldsymbol{\Sigma}$ | `np.cov(data.T)` |
| ç²¾åº¦è¡Œåˆ— | $\boldsymbol{\Lambda} = \boldsymbol{\Sigma}^{-1}$ | `np.linalg.inv(cov)` |
| Mahalanobisè·é›¢ | $(\mathbf{x}-\boldsymbol{\mu})^\top\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})$ | `(x-mu) @ inv_cov @ (x-mu)` |
| å¯¾æ•°å°¤åº¦ | $\sum_i \log p(x_i;\theta)$ | `np.sum(np.log(pdf(data, theta)))` |
| Softmax | $\frac{e^{x_i}}{\sum_j e^{x_j}}$ | `np.exp(x-x.max()) / np.exp(x-x.max()).sum()` |
| KL divergence | $\sum_i p_i \log(p_i/q_i)$ | `np.sum(p * np.log(p/q))` |
| ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° | $x \sim \mathcal{N}(\mu,\sigma^2)$ | `np.random.normal(mu, sigma, N)` |
| æ¡ä»¶ä»˜ãç¢ºç‡ | $P(A \mid B)$ | `p_ab / p_b` |
:::

### 4.6 å®Ÿè£…æ¼”ç¿’: ã‚¬ã‚¦ã‚¹æ··åˆãƒ¢ãƒ‡ãƒ«ï¼ˆGMMï¼‰ã®MLE

ç¬¬8å›ï¼ˆEMç®—æ³•ï¼‰ã¸ã®æ©‹æ¸¡ã—ã¨ã—ã¦ã€2æˆåˆ†GMMã®ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã‚’å®Ÿè£…ã™ã‚‹ã€‚ã“ã“ã§ã¯EMç®—æ³•ã®å‰æ®µéšã¨ã—ã¦ã€å˜ä¸€ã‚¬ã‚¦ã‚¹ã®MLEã‚’æ‹¡å¼µã™ã‚‹å½¢ã§å•é¡Œã®å›°é›£ã•ã‚’ä½“æ„Ÿã™ã‚‹ã€‚

```python
import numpy as np

# Generate data from a 2-component Gaussian mixture
np.random.seed(42)
N = 1000
# True parameters
pi_true = 0.4  # mixing weight
mu1_true, sigma1_true = -2.0, 0.8
mu2_true, sigma2_true = 3.0, 1.2

# Sample
component = np.random.binomial(1, 1 - pi_true, N)
data = np.where(component == 0,
                np.random.normal(mu1_true, sigma1_true, N),
                np.random.normal(mu2_true, sigma2_true, N))

print(f"Generated {N} samples from GMM")
print(f"True: Ï€={pi_true}, Î¼â‚={mu1_true}, Ïƒâ‚={sigma1_true}, Î¼â‚‚={mu2_true}, Ïƒâ‚‚={sigma2_true}")

# Single Gaussian MLE (wrong model)
mu_single = data.mean()
sigma_single = data.std()
print(f"\nSingle Gaussian MLE: Î¼={mu_single:.3f}, Ïƒ={sigma_single:.3f}")
print("â†’ Clearly wrong! The data has two modes.")

# GMM log-likelihood (for given parameters)
def gmm_log_likelihood(data, pi, mu1, sig1, mu2, sig2):
    """Log-likelihood of 2-component GMM.

    corresponds to: L = Î£áµ¢ log[Ï€ N(xáµ¢|Î¼â‚,Ïƒâ‚Â²) + (1-Ï€) N(xáµ¢|Î¼â‚‚,Ïƒâ‚‚Â²)]
    """
    from scipy.stats import norm
    ll = np.sum(np.log(
        pi * norm.pdf(data, mu1, sig1) +
        (1 - pi) * norm.pdf(data, mu2, sig2) + 1e-10
    ))
    return ll

# Evaluate at true parameters vs single Gaussian
from scipy.stats import norm
ll_true = gmm_log_likelihood(data, pi_true, mu1_true, sigma1_true, mu2_true, sigma2_true)
ll_single = np.sum(np.log(norm.pdf(data, mu_single, sigma_single) + 1e-10))
print(f"\nLog-likelihood (true GMM params):  {ll_true:.2f}")
print(f"Log-likelihood (single Gaussian):  {ll_single:.2f}")
print(f"Difference: {ll_true - ll_single:.2f} (GMM is much better)")

# The challenge: MLE for GMM has no closed-form solution
# âˆ‚L/âˆ‚Î¼â‚ involves the "responsibility" Î³ which depends on all parameters
# â†’ EM algorithm (Lecture 8) solves this iteratively
print("\nâ†’ GMM MLE has no closed-form solution.")
print("â†’ The EM algorithm (ç¬¬8å›) iteratively maximizes the likelihood.")
print("â†’ Each E-step computes 'responsibilities', each M-step updates parameters.")
```

**ãªãœGMMã®MLEã¯é–‰ã˜ãŸå½¢ã§è§£ã‘ãªã„ã®ã‹**: å¯¾æ•°å°¤åº¦ã®ä¸­ã«**å’Œã®å¯¾æ•°** $\log[\pi \mathcal{N}(x \mid \mu_1, \sigma_1^2) + (1-\pi)\mathcal{N}(x \mid \mu_2, \sigma_2^2)]$ ãŒç¾ã‚Œã‚‹ã€‚å¯¾æ•°ã¨å’Œã®é †åºã‚’å…¥ã‚Œæ›¿ãˆã‚‰ã‚Œãªã„ãŸã‚ã€å¾®åˆ†ã—ã¦ã‚‚å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒäº’ã„ã«çµ¡ã¿åˆã†ã€‚ã“ã®å›°é›£ãŒç¬¬8å›ã®EMç®—æ³•ã®å‹•æ©Ÿã ã€‚

### 4.7 å®Ÿè£…æ¼”ç¿’: ãƒ™ã‚¤ã‚ºæ¨è«–ã®ã‚°ãƒªãƒƒãƒ‰è¿‘ä¼¼

```python
import numpy as np

def bayesian_grid_inference(data: np.ndarray, prior_a: float = 1.0, prior_b: float = 1.0,
                             n_grid: int = 10000):
    """Bayesian inference for Bernoulli parameter using grid approximation.

    Prior: Beta(a, b)
    Likelihood: Bernoulli(Î¸)
    Posterior âˆ Î¸^(a+h-1) (1-Î¸)^(b+t-1)

    Also computes posterior analytically for comparison.
    """
    theta_grid = np.linspace(0.001, 0.999, n_grid)
    heads = data.sum()
    tails = len(data) - heads

    # Prior: Beta(a, b)
    from math import gamma as gamma_fn
    B = gamma_fn(prior_a) * gamma_fn(prior_b) / gamma_fn(prior_a + prior_b)
    prior = theta_grid**(prior_a - 1) * (1 - theta_grid)**(prior_b - 1) / B

    # Likelihood: Bernoulli
    log_lik = heads * np.log(theta_grid) + tails * np.log(1 - theta_grid)
    likelihood = np.exp(log_lik - log_lik.max())  # numerical stability

    # Posterior âˆ Likelihood Ã— Prior
    posterior_unnorm = likelihood * prior
    posterior = posterior_unnorm / np.trapz(posterior_unnorm, theta_grid)

    # Analytical posterior: Beta(a+h, b+t)
    post_a = prior_a + heads
    post_b = prior_b + tails
    B_post = gamma_fn(post_a) * gamma_fn(post_b) / gamma_fn(post_a + post_b)
    posterior_analytic = theta_grid**(post_a - 1) * (1 - theta_grid)**(post_b - 1) / B_post

    # Summary statistics
    dx = theta_grid[1] - theta_grid[0]
    mean_grid = np.sum(theta_grid * posterior) * dx
    mean_analytic = post_a / (post_a + post_b)
    mle = heads / len(data) if len(data) > 0 else 0.5

    print(f"Data: {int(heads)}H / {int(tails)}T (N={len(data)})")
    print(f"Prior: Beta({prior_a}, {prior_b})")
    print(f"Posterior: Beta({post_a}, {post_b})")
    print(f"  Grid mean:      {mean_grid:.4f}")
    print(f"  Analytic mean:  {mean_analytic:.4f}")
    print(f"  MLE:            {mle:.4f}")
    print(f"  MAP:            {(post_a-1)/(post_a+post_b-2):.4f}" if post_a > 1 and post_b > 1 else "")

# Experiment with different data sizes and priors
np.random.seed(42)
true_theta = 0.7

print("=== Effect of Data Size ===\n")
for N in [5, 20, 100]:
    data = np.random.binomial(1, true_theta, N)
    bayesian_grid_inference(data)
    print()

print("=== Effect of Prior ===\n")
data_small = np.random.binomial(1, true_theta, 10)
for a, b, name in [(1, 1, "Uniform"), (0.5, 0.5, "Jeffreys"), (10, 3, "Strong prior Î¸â‰ˆ0.77"), (1, 10, "Wrong prior Î¸â‰ˆ0.09")]:
    print(f"--- {name} prior ---")
    bayesian_grid_inference(data_small, a, b)
    print()
```

:::message
**å®Ÿè£…ã®æ•™è¨“**: ãƒ‡ãƒ¼ã‚¿ãŒå¢—ãˆã‚‹ã»ã©ã€äº‹å‰åˆ†å¸ƒã®å½±éŸ¿ã¯è–„ã‚Œã€ãƒ™ã‚¤ã‚ºæ¨å®šã¯MLEã«è¿‘ã¥ãã€‚ã“ã‚Œã¯äº‹å¾Œåˆ†å¸ƒãŒã€Œå°¤åº¦ã«æ”¯é…ã•ã‚Œã‚‹ã€ãŸã‚ã€‚é€†ã«ã€ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„ã¨ãã¯äº‹å‰åˆ†å¸ƒãŒçµæœã‚’å¤§ããå·¦å³ã™ã‚‹ã€‚

ã“ã®ç¾è±¡ã‚’ã€Œäº‹å¾Œä¸€è‡´æ€§ï¼ˆposterior consistencyï¼‰ã€ã¨å‘¼ã¶ã€‚$N \to \infty$ ã§äº‹å¾Œåˆ†å¸ƒã¯çœŸã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«é›†ä¸­ã™ã‚‹ â€” å¤§æ•°ã®æ³•å‰‡ã®ãƒ™ã‚¤ã‚ºç‰ˆã ã€‚
:::

### 4.8 ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæ¯é–¢æ•°ã¨ç‰¹æ€§é–¢æ•°

**ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæ¯é–¢æ•°ï¼ˆMGFï¼‰**: $M_X(t) = \mathbb{E}[e^{tX}]$

MGFã® $k$ æ¬¡å¾®åˆ†ã¯ $k$ æ¬¡ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆã‚’ä¸ãˆã‚‹: $M_X^{(k)}(0) = \mathbb{E}[X^k]$

```python
import numpy as np

# MGF of Gaussian N(Î¼, ÏƒÂ²): M(t) = exp(Î¼t + ÏƒÂ²tÂ²/2)
mu, sigma = 2.0, 1.5

def gaussian_mgf(t: float, mu: float, sigma: float) -> float:
    """M(t) = exp(Î¼t + ÏƒÂ²tÂ²/2)"""
    return np.exp(mu * t + sigma**2 * t**2 / 2)

# Verify moments via numerical differentiation
dt = 1e-5
M0 = gaussian_mgf(0, mu, sigma)
M1 = (gaussian_mgf(dt, mu, sigma) - gaussian_mgf(-dt, mu, sigma)) / (2 * dt)
M2 = (gaussian_mgf(dt, mu, sigma) - 2*M0 + gaussian_mgf(-dt, mu, sigma)) / dt**2

print(f"Gaussian N({mu}, {sigma**2})")
print(f"E[X]  = M'(0) = {M1:.4f}  (theory: {mu})")
print(f"E[XÂ²] = M''(0) = {M2:.4f} (theory: {mu**2 + sigma**2:.4f})")
print(f"Var(X) = E[XÂ²] - E[X]Â² = {M2 - M1**2:.4f} (theory: {sigma**2:.4f})")

# Monte Carlo verification
samples = np.random.normal(mu, sigma, 100000)
print(f"\nMonte Carlo: E[X]={samples.mean():.4f}, E[XÂ²]={np.mean(samples**2):.4f}, Var={samples.var():.4f}")
```

MGFãŒå­˜åœ¨ã—ãªã„åˆ†å¸ƒã‚‚ã‚ã‚‹ï¼ˆCauchyåˆ†å¸ƒãªã©ï¼‰ã€‚ãã®å ´åˆã¯**ç‰¹æ€§é–¢æ•°** $\varphi_X(t) = \mathbb{E}[e^{itX}]$ ã‚’ä½¿ã†ã€‚ç‰¹æ€§é–¢æ•°ã¯å¸¸ã«å­˜åœ¨ã—ã€åˆ†å¸ƒã‚’ä¸€æ„ã«æ±ºå®šã™ã‚‹ã€‚CLTã®è¨¼æ˜ã¯ã—ã°ã—ã°ç‰¹æ€§é–¢æ•°ã‚’ç”¨ã„ã¦è¡Œã‚ã‚Œã‚‹ã€‚

:::message
**é€²æ—: 70% å®Œäº†** ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã€LaTeXè¨˜æ³•ã€è«–æ–‡èª­è§£ã®3ãƒ‘ã‚¹ã€æ•°å¼â†’ã‚³ãƒ¼ãƒ‰ç¿»è¨³7ãƒ‘ã‚¿ãƒ¼ãƒ³ã€GMMãƒ»ãƒ™ã‚¤ã‚ºæ¨è«–ã®å®Ÿè£…ã€MGFã¾ã§å®Œäº†ã€‚Zone 4 ã‚¯ãƒªã‚¢ã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

### 5.1 è¨˜å·èª­è§£ãƒ†ã‚¹ãƒˆ

ä»¥ä¸‹ã®æ•°å¼ã‚’æ—¥æœ¬èªã§èª­ã¿ä¸Šã’ã€å„è¨˜å·ã®æ„å‘³ã‚’èª¬æ˜ã›ã‚ˆã€‚

:::details Q1: $X \sim \mathcal{N}(\mu, \sigma^2)$
**èª­ã¿**: ã€Œç¢ºç‡å¤‰æ•°ã‚¨ãƒƒã‚¯ã‚¹ã¯æ­£è¦åˆ†å¸ƒãƒŸãƒ¥ãƒ¼ ã‚·ã‚°ãƒäºŒä¹—ã«ã—ãŸãŒã†ã€
- $X$: ç¢ºç‡å¤‰æ•°
- $\sim$: ã€Œã«ã—ãŸãŒã†ã€â€” ç¢ºç‡åˆ†å¸ƒã«å¾“ã†ã“ã¨ã‚’ç¤ºã™è¨˜å·
- $\mathcal{N}$: æ­£è¦åˆ†å¸ƒï¼ˆã‚¬ã‚¦ã‚¹åˆ†å¸ƒï¼‰
- $\mu$: å¹³å‡ï¼ˆãƒŸãƒ¥ãƒ¼ï¼‰
- $\sigma^2$: åˆ†æ•£ï¼ˆã‚·ã‚°ãƒã®äºŒä¹—ï¼‰
:::

:::details Q2: $P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)}$
**èª­ã¿**: ã€Œãƒ”ãƒ¼ ã‚¨ãƒ¼ ã‚®ãƒ–ãƒ³ ãƒ“ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ãƒ”ãƒ¼ ãƒ“ãƒ¼ ã‚®ãƒ–ãƒ³ ã‚¨ãƒ¼ ã‹ã‘ã‚‹ ãƒ”ãƒ¼ ã‚¨ãƒ¼ ã‚ã‚‹ ãƒ”ãƒ¼ ãƒ“ãƒ¼ã€
- ã“ã‚Œã¯ãƒ™ã‚¤ã‚ºã®å®šç† [^1]
- $P(A \mid B)$: äº‹å¾Œç¢ºç‡ â€” $B$ ãŒèµ·ããŸä¸‹ã§ã® $A$ ã®ç¢ºç‡
- $P(B \mid A)$: å°¤åº¦ â€” $A$ ãŒçœŸã®ã¨ãã« $B$ ãŒè¦³æ¸¬ã•ã‚Œã‚‹ç¢ºç‡
- $P(A)$: äº‹å‰ç¢ºç‡ â€” $B$ ã‚’è¦‹ã‚‹å‰ã® $A$ ã®ç¢ºç‡
- $P(B)$: ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ â€” $B$ ã®å‘¨è¾ºç¢ºç‡
:::

:::details Q3: $\hat{\theta}_{\text{MLE}} = \arg\max_\theta \sum_{i=1}^{N} \log p(x_i; \theta)$
**èª­ã¿**: ã€Œã‚·ãƒ¼ã‚¿ãƒãƒƒãƒˆMLE ã‚¤ã‚³ãƒ¼ãƒ« ã‚¢ãƒ¼ã‚°ãƒãƒƒã‚¯ã‚¹ ã‚·ãƒ¼ã‚¿ ã‚·ã‚°ãƒ ã‚¢ã‚¤ ã‚¤ã‚³ãƒ¼ãƒ« 1 ã‹ã‚‰ ã‚¨ãƒŒ ãƒ­ã‚° ãƒ”ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ã‚¢ã‚¤ ã‚»ãƒŸã‚³ãƒ­ãƒ³ ã‚·ãƒ¼ã‚¿ã€
- $\hat{\theta}_{\text{MLE}}$: æœ€å°¤æ¨å®šé‡ï¼ˆãƒãƒƒãƒˆã¯ã€Œæ¨å®šé‡ã€ã®å°ï¼‰
- $\arg\max_\theta$: $\theta$ ã‚’å‹•ã‹ã—ã¦æœ€å¤§ã«ã™ã‚‹å€¤
- $\sum_{i=1}^{N}$: $N$ å€‹ã®ãƒ‡ãƒ¼ã‚¿ã«ã¤ã„ã¦ã®å’Œ
- $\log p(x_i; \theta)$: $i$ ç•ªç›®ã®ãƒ‡ãƒ¼ã‚¿ã®å¯¾æ•°å°¤åº¦
- ã‚»ãƒŸã‚³ãƒ­ãƒ³ ;: $x_i$ ã¯ãƒ‡ãƒ¼ã‚¿ï¼ˆå›ºå®šï¼‰ã€$\theta$ ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå¤‰æ•°ï¼‰ã‚’åŒºåˆ¥
:::

:::details Q4: $I(\theta) = -\mathbb{E}\left[\nabla_\theta^2 \log p(\mathbf{x}; \theta)\right]$
**èª­ã¿**: ã€Œã‚¢ã‚¤ ã‚·ãƒ¼ã‚¿ ã‚¤ã‚³ãƒ¼ãƒ« ãƒã‚¤ãƒŠã‚¹ ã‚¤ãƒ¼ ãƒŠãƒ–ãƒ©ã‚·ãƒ¼ã‚¿äºŒä¹— ãƒ­ã‚° ãƒ”ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ ã‚»ãƒŸã‚³ãƒ­ãƒ³ ã‚·ãƒ¼ã‚¿ã€
- $I(\theta)$: Fisheræƒ…å ±é‡ï¼ˆè¡Œåˆ—ï¼‰[^8]
- $-\mathbb{E}[\cdot]$: æœŸå¾…å€¤ã®ãƒã‚¤ãƒŠã‚¹
- $\nabla_\theta^2$: $\theta$ ã«é–¢ã™ã‚‹ãƒ˜ã‚·ã‚¢ãƒ³ï¼ˆ2æ¬¡å¾®åˆ†ï¼‰
- ç›´æ„Ÿ: å¯¾æ•°å°¤åº¦ã®æ›²ç‡ã®æœŸå¾…å€¤ã€‚æ›²ç‡ãŒå¤§ãã„ = ãƒ‡ãƒ¼ã‚¿ãŒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã¤ã„ã¦æƒ…å ±ã‚’å¤šãæŒã¤
:::

:::details Q5: $\sqrt{N}(\bar{X}_N - \mu) \xrightarrow{d} \mathcal{N}(0, \sigma^2)$
**èª­ã¿**: ã€Œãƒ«ãƒ¼ãƒˆã‚¨ãƒŒ ã‹ã‘ã‚‹ ã‚¨ãƒƒã‚¯ã‚¹ãƒãƒ¼ã‚¨ãƒŒ ãƒã‚¤ãƒŠã‚¹ ãƒŸãƒ¥ãƒ¼ ã¯ åˆ†å¸ƒåæŸã§ ãƒãƒ¼ãƒãƒ« ã‚¼ãƒ­ ã‚·ã‚°ãƒäºŒä¹— ã«åæŸã™ã‚‹ã€
- $\bar{X}_N = \frac{1}{N}\sum_{i=1}^{N}X_i$: æ¨™æœ¬å¹³å‡
- $\xrightarrow{d}$: åˆ†å¸ƒåæŸï¼ˆdistribution convergenceï¼‰
- ã“ã‚Œã¯ä¸­å¿ƒæ¥µé™å®šç†ï¼ˆCLTï¼‰ã®è¡¨ç¾
- $\sqrt{N}$ ã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€åˆ†æ•£ãŒä¸€å®šã®ã¾ã¾åˆ†å¸ƒå½¢çŠ¶ãŒã‚¬ã‚¦ã‚¹ã«è¿‘ã¥ã
:::

:::details Q6: $p(\mathbf{x}) = \prod_{t=1}^{T} p(x_t \mid x_{<t})$
**èª­ã¿**: ã€Œãƒ”ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ ã‚¤ã‚³ãƒ¼ãƒ« ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ ãƒ†ã‚£ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« 1 ã‹ã‚‰ ãƒ†ã‚£ãƒ¼ ãƒ”ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ãƒ†ã‚£ãƒ¼ ã‚®ãƒ–ãƒ³ ã‚¨ãƒƒã‚¯ã‚¹ ãƒ¬ã‚¹ã‚¶ãƒ³ ãƒ†ã‚£ãƒ¼ã€
- $p(\mathbf{x})$: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å…¨ä½“ã®åŒæ™‚ç¢ºç‡
- $\prod_{t=1}^{T}$: æ™‚åˆ»1ã‹ã‚‰Tã¾ã§ã®ç©
- $p(x_t \mid x_{<t})$: éå»ã®ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‚’æ¡ä»¶ã¨ã—ãŸ $x_t$ ã®æ¡ä»¶ä»˜ãç¢ºç‡
- ã“ã‚Œã¯ç¢ºç‡ã®é€£é–è¦å‰‡ã€‚LLMã®è‡ªå·±å›å¸°ç”Ÿæˆã®æ•°å­¦çš„åŸºç›¤ [^5]
:::

:::details Q7: $\text{Var}(\hat{\theta}) \geq \frac{1}{N \cdot I(\theta)}$
**èª­ã¿**: ã€Œãƒãƒªã‚¢ãƒ³ã‚¹ ã‚·ãƒ¼ã‚¿ãƒãƒƒãƒˆ ã¯ 1 ã‚ã‚‹ ã‚¨ãƒŒ ã‚¢ã‚¤ ã‚·ãƒ¼ã‚¿ ä»¥ä¸Šã€
- ã“ã‚Œã¯CramÃ©r-Raoä¸‹ç•Œ [^8]
- ã©ã‚“ãªä¸åæ¨å®šé‡ã§ã‚‚ã€åˆ†æ•£ã¯Fisheræƒ…å ±é‡ã®é€†æ•°ä»¥ä¸‹ã«ã¯ãªã‚‰ãªã„
- $N$ ãŒå¢—ãˆã‚‹ã¨ä¸‹ç•Œã¯å°ã•ããªã‚‹ = ã‚ˆã‚Šç²¾å¯†ãªæ¨å®šãŒå¯èƒ½
:::

:::details Q8: $p(\mathbf{x} \mid \boldsymbol{\eta}) = h(\mathbf{x}) \exp(\boldsymbol{\eta}^\top \mathbf{T}(\mathbf{x}) - A(\boldsymbol{\eta}))$
**èª­ã¿**: ã€Œãƒ”ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ ã‚®ãƒ–ãƒ³ ã‚¤ãƒ¼ã‚¿ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¨ã‚¤ãƒ ã‚¨ãƒƒã‚¯ã‚¹ ã‹ã‘ã‚‹ ã‚¨ã‚¯ã‚¹ãƒãƒãƒ³ã‚·ãƒ£ãƒ« ã‚¤ãƒ¼ã‚¿ ãƒˆãƒ©ãƒ³ã‚¹ãƒãƒ¼ã‚º ãƒ†ã‚£ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ ãƒã‚¤ãƒŠã‚¹ ã‚¨ãƒ¼ ã‚¤ãƒ¼ã‚¿ã€
- ã“ã‚Œã¯æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®æ¨™æº–å½¢
- $\boldsymbol{\eta}$: è‡ªç„¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã‚¤ãƒ¼ã‚¿ï¼‰
- $\mathbf{T}(\mathbf{x})$: ååˆ†çµ±è¨ˆé‡
- $A(\boldsymbol{\eta})$: å¯¾æ•°æ­£è¦åŒ–å®šæ•°ï¼ˆå¯¾æ•°åˆ†é…é–¢æ•°ï¼‰
- $h(\mathbf{x})$: åŸºåº•æ¸¬åº¦
:::

:::details Q9: $q_\phi(\mathbf{z} \mid \mathbf{x}) = \mathcal{N}(\boldsymbol{\mu}_\phi(\mathbf{x}), \text{diag}(\boldsymbol{\sigma}^2_\phi(\mathbf{x})))$
**èª­ã¿**: ã€Œã‚­ãƒ¥ãƒ¼ ãƒ•ã‚¡ã‚¤ ã‚¼ãƒƒãƒˆ ã‚®ãƒ–ãƒ³ ã‚¨ãƒƒã‚¯ã‚¹ ã‚¤ã‚³ãƒ¼ãƒ« ãƒãƒ¼ãƒãƒ« ãƒŸãƒ¥ãƒ¼ãƒ•ã‚¡ã‚¤ ã‚¨ãƒƒã‚¯ã‚¹ ãƒ€ã‚¤ã‚¢ã‚° ã‚·ã‚°ãƒãƒ•ã‚¡ã‚¤äºŒä¹— ã‚¨ãƒƒã‚¯ã‚¹ã€
- $q_\phi$: å¤‰åˆ†è¿‘ä¼¼åˆ†å¸ƒï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\phi$ ã®NNï¼‰[^2]
- $\mathbf{z}$: æ½œåœ¨å¤‰æ•°
- $\boldsymbol{\mu}_\phi(\mathbf{x})$: NNãŒå‡ºåŠ›ã™ã‚‹å¹³å‡ãƒ™ã‚¯ãƒˆãƒ«
- $\text{diag}(\boldsymbol{\sigma}^2_\phi(\mathbf{x}))$: NNãŒå‡ºåŠ›ã™ã‚‹å¯¾è§’å…±åˆ†æ•£è¡Œåˆ—
- ã“ã‚Œã¯VAEã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å‡ºåŠ›ã€‚ç¬¬10å›ã§å®Œå…¨å°å‡ºã€‚
:::

:::details Q10: $\mathcal{L}(\theta) = -\frac{1}{T}\sum_{t=1}^{T} \log p_\theta(x_t \mid x_{<t})$
**èª­ã¿**: ã€Œã‚¨ãƒ« ã‚·ãƒ¼ã‚¿ ã‚¤ã‚³ãƒ¼ãƒ« ãƒã‚¤ãƒŠã‚¹ ãƒ†ã‚£ãƒ¼ã¶ã‚“ã®ã‚¤ãƒ ã‚·ã‚°ãƒ ãƒ†ã‚£ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« 1 ã‹ã‚‰ ãƒ†ã‚£ãƒ¼ ãƒ­ã‚° ãƒ”ãƒ¼ã‚·ãƒ¼ã‚¿ ã‚¨ãƒƒã‚¯ã‚¹ãƒ†ã‚£ãƒ¼ ã‚®ãƒ–ãƒ³ ã‚¨ãƒƒã‚¯ã‚¹ãƒ¬ã‚¹ã‚¶ãƒ³ãƒ†ã‚£ãƒ¼ã€
- ã“ã‚Œã¯LLMã®Cross-Entropy Lossï¼ˆç¬¬1å›ã§å°å…¥ï¼‰
- $-\log p_\theta(x_t \mid x_{<t})$: æ­£è§£ãƒˆãƒ¼ã‚¯ãƒ³ã®è² ã®å¯¾æ•°ç¢ºç‡
- $\frac{1}{T}\sum_{t=1}^{T}$: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å…¨ä½“ã®å¹³å‡
- Perplexity $= \exp(\mathcal{L})$ ã¯ã€Œå®ŸåŠ¹çš„ãªé¸æŠè‚¢æ•°ã€
:::

### 5.2 LaTeXè¨˜è¿°ãƒ†ã‚¹ãƒˆ

ä»¥ä¸‹ã®æ•°å¼ã‚’LaTeXã§è¨˜è¿°ã›ã‚ˆã€‚

:::details Q1: ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®PDF
```latex
f(x \mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
```
:::

:::details Q2: ãƒ™ã‚¤ã‚ºã®å®šç†ï¼ˆé€£ç¶šç‰ˆï¼‰
```latex
p(\theta \mid \mathcal{D}) = \frac{p(\mathcal{D} \mid \theta) p(\theta)}{p(\mathcal{D})}
= \frac{p(\mathcal{D} \mid \theta) p(\theta)}{\int p(\mathcal{D} \mid \theta') p(\theta') d\theta'}
```
:::

:::details Q3: ä¸­å¿ƒæ¥µé™å®šç†
```latex
\frac{\bar{X}_N - \mu}{\sigma / \sqrt{N}} \xrightarrow{d} \mathcal{N}(0, 1)
\quad \text{as } N \to \infty
```
:::

:::details Q4: æŒ‡æ•°å‹åˆ†å¸ƒæ—
```latex
p(\mathbf{x} \mid \boldsymbol{\eta}) = h(\mathbf{x}) \exp\left(\boldsymbol{\eta}^\top \mathbf{T}(\mathbf{x}) - A(\boldsymbol{\eta})\right)
```
:::

:::details Q5: Fisheræƒ…å ±é‡ã¨CramÃ©r-Raoä¸‹ç•Œ
```latex
I(\theta) = \mathbb{E}\left[\left(\frac{\partial}{\partial \theta} \log p(X; \theta)\right)^2\right],
\quad \text{Var}(\hat{\theta}) \geq \frac{1}{n I(\theta)}
```
:::

### 5.3 ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ†ã‚¹ãƒˆ

:::details Q1: æ¡ä»¶ä»˜ãç¢ºç‡ã®è¨ˆç®—
æ•°å¼: $P(Y=1 \mid X=0) = \frac{P(X=0, Y=1)}{P(X=0)} = \frac{P(X=0, Y=1)}{\sum_y P(X=0, Y=y)}$

```python
joint = np.array([[0.3, 0.1], [0.2, 0.4]])  # P(X,Y)
p_y1_given_x0 = joint[0, 1] / joint[0, :].sum()
print(f"P(Y=1|X=0) = {p_y1_given_x0:.4f}")
# Expected: 0.1 / (0.3 + 0.1) = 0.25
```
:::

:::details Q2: MLE for Poisson distribution
æ•°å¼: $\hat{\lambda}_{\text{MLE}} = \bar{x} = \frac{1}{N}\sum_{i=1}^{N} x_i$

```python
data = np.random.poisson(lam=4.5, size=1000)
lambda_mle = data.mean()
print(f"Î»_MLE = {lambda_mle:.4f} (true: 4.5)")
```
:::

:::details Q3: 2D Gaussian sampling and Mahalanobis distance
æ•°å¼: $d_M(\mathbf{x}) = \sqrt{(\mathbf{x}-\boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{x}-\boldsymbol{\mu})}$

```python
mu = np.array([1.0, 2.0])
Sigma = np.array([[2.0, 0.8], [0.8, 1.0]])
Sigma_inv = np.linalg.inv(Sigma)

x = np.array([3.0, 4.0])
diff = x - mu
d_mahal = np.sqrt(diff @ Sigma_inv @ diff)
d_euclid = np.linalg.norm(diff)
print(f"Mahalanobis: {d_mahal:.4f}, Euclidean: {d_euclid:.4f}")
```
:::

:::details Q4: CLT verification â€” exponential distribution
æ•°å¼: $X_i \sim \text{Exp}(\lambda)$, $\mathbb{E}[X_i] = 1/\lambda$, $\text{Var}(X_i) = 1/\lambda^2$

```python
lam = 2.0
N = 100
n_experiments = 50000

means = np.array([np.random.exponential(1/lam, N).mean() for _ in range(n_experiments)])
standardized = (means - 1/lam) / (1/(lam * np.sqrt(N)))
print(f"Standardized mean: {standardized.mean():.4f} (should â‰ˆ 0)")
print(f"Standardized std:  {standardized.std():.4f} (should â‰ˆ 1)")
```
:::

:::details Q5: Beta-Bernoulli conjugate update
æ•°å¼: Prior $\text{Beta}(\alpha, \beta)$ + Data $(h, t)$ â†’ Posterior $\text{Beta}(\alpha+h, \beta+t)$

```python
alpha, beta = 2.0, 5.0  # prior: we think Î¸ is low
data = np.array([1, 1, 1, 0, 1, 1, 0, 1, 1, 1])  # 8 heads, 2 tails
h, t = data.sum(), len(data) - data.sum()
post_a, post_b = alpha + h, beta + t
print(f"Prior mean: {alpha/(alpha+beta):.3f}")
print(f"Posterior mean: {post_a/(post_a+post_b):.3f}")
print(f"MLE: {h/len(data):.3f}")
# Posterior is pulled between prior and MLE
```
:::

### 5.4 è«–æ–‡èª­è§£ãƒ†ã‚¹ãƒˆ

ä»¥ä¸‹ã®VAEè«–æ–‡ [^2] ã®Abstractã‹ã‚‰ç¢ºç‡è«–ã®è¦ç´ ã‚’æŠ½å‡ºã›ã‚ˆã€‚

:::details VAEåŸè«–æ–‡ Pass 1
**Kingma & Welling (2013). "Auto-Encoding Variational Bayes"**

ç¢ºç‡è«–çš„è¦ç´ ã®æŠ½å‡º:
1. **æ½œåœ¨å¤‰æ•°**: $\mathbf{z}$ â€” è¦³æ¸¬ã•ã‚Œãªã„ç¢ºç‡å¤‰æ•°
2. **ç”Ÿæˆãƒ¢ãƒ‡ãƒ«**: $p_\theta(\mathbf{x} \mid \mathbf{z})$ â€” æ¡ä»¶ä»˜ãåˆ†å¸ƒï¼ˆãƒ‡ã‚³ãƒ¼ãƒ€ï¼‰
3. **äº‹å‰åˆ†å¸ƒ**: $p(\mathbf{z}) = \mathcal{N}(\mathbf{0}, \mathbf{I})$ â€” æ¨™æº–æ­£è¦åˆ†å¸ƒ
4. **äº‹å¾Œåˆ†å¸ƒ**: $p_\theta(\mathbf{z} \mid \mathbf{x})$ â€” ãƒ™ã‚¤ã‚ºã®å®šç†ã§å¾—ã‚‰ã‚Œã‚‹ãŒè¨ˆç®—å›°é›£
5. **å¤‰åˆ†è¿‘ä¼¼**: $q_\phi(\mathbf{z} \mid \mathbf{x})$ â€” äº‹å¾Œåˆ†å¸ƒã®è¿‘ä¼¼ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ï¼‰
6. **ELBO**: $\mathcal{L}(\theta, \phi; \mathbf{x}) = \mathbb{E}_{q_\phi}[\log p_\theta(\mathbf{x} \mid \mathbf{z})] - D_{KL}[q_\phi(\mathbf{z} \mid \mathbf{x}) \| p(\mathbf{z})]$
7. **MLE**: ELBOã®æœ€å¤§åŒ–ã¯å‘¨è¾ºå°¤åº¦ $\log p_\theta(\mathbf{x})$ ã®ä¸‹ç•Œã‚’æœ€å¤§åŒ–
8. **Reparameterization trick**: $\mathbf{z} = \boldsymbol{\mu} + \boldsymbol{\sigma} \odot \boldsymbol{\epsilon}$, $\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$

æœ¬è¬›ç¾©ã§å­¦ã‚“ã å…¨ã¦ã®é“å…·ãŒä½¿ã‚ã‚Œã¦ã„ã‚‹ã€‚
:::

### 5.5 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸

**Challenge 1**: ãƒ™ã‚¤ã‚ºæ¨è«–ã®å¯è¦–åŒ–

```python
import numpy as np

def bayesian_sequential_visualization(true_p: float, n_obs: int, prior_a: float = 1, prior_b: float = 1):
    """Visualize sequential Bayesian updating via summary statistics."""
    np.random.seed(42)
    a, b = prior_a, prior_b

    print(f"True Î¸ = {true_p}")
    print(f"{'Obs':>4} {'Data':>5} {'Post Mean':>10} {'Post Std':>10} {'95% CI':>20} {'MLE':>8}")
    print("-" * 65)

    heads_total, tails_total = 0, 0
    for i in range(1, n_obs + 1):
        x = np.random.binomial(1, true_p)
        heads_total += x
        tails_total += (1 - x)
        a_new = prior_a + heads_total
        b_new = prior_b + tails_total
        mean = a_new / (a_new + b_new)
        std = np.sqrt(a_new * b_new / ((a_new + b_new)**2 * (a_new + b_new + 1)))
        # 95% credible interval (approximate via normal)
        ci_low = max(0, mean - 1.96 * std)
        ci_high = min(1, mean + 1.96 * std)
        mle = heads_total / i
        if i <= 10 or i % 10 == 0 or i == n_obs:
            print(f"{i:>4} {'H' if x else 'T':>5} {mean:>10.4f} {std:>10.4f} "
                  f"[{ci_low:.3f}, {ci_high:.3f}]{' ':>4} {mle:>8.4f}")

bayesian_sequential_visualization(0.65, 100)
```

**Challenge 2**: Fisheræƒ…å ±é‡ã®æ•°å€¤è¨ˆç®—

```python
import numpy as np

def numerical_fisher_information(log_pdf_fn, theta: float, n_samples: int = 100000, dt: float = 1e-5):
    """Numerically compute Fisher information.

    I(Î¸) = E[(d/dÎ¸ log p(x;Î¸))Â²]

    Uses score function variance and Hessian methods, compares both.
    """
    # Method 1: Score function variance
    # Sample from p(x; Î¸) â€” need a sampler
    # For Bernoulli: x ~ Bernoulli(Î¸)
    samples = np.random.binomial(1, theta, n_samples).astype(float)

    # Score: d/dÎ¸ log p(x; Î¸) = x/Î¸ - (1-x)/(1-Î¸)
    scores = samples / theta - (1 - samples) / (1 - theta)
    I_score = np.mean(scores**2)

    # Method 2: Negative expected Hessian
    # dÂ²/dÎ¸Â² log p(x; Î¸) = -x/Î¸Â² - (1-x)/(1-Î¸)Â²
    hessians = -samples / theta**2 - (1 - samples) / (1 - theta)**2
    I_hessian = -np.mean(hessians)

    # Theory: I(Î¸) = 1/(Î¸(1-Î¸))
    I_theory = 1 / (theta * (1 - theta))

    print(f"Î¸ = {theta}")
    print(f"  Score variance:  I = {I_score:.4f}")
    print(f"  Negative Hessian: I = {I_hessian:.4f}")
    print(f"  Theory:          I = {I_theory:.4f}")
    return I_score, I_hessian, I_theory

print("=== Numerical Fisher Information for Bernoulli ===\n")
for theta in [0.1, 0.3, 0.5, 0.7, 0.9]:
    numerical_fisher_information(None, theta)
    print()
```

### 5.6 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸3: MLEæ¯”è¼ƒ â€” åˆ†å¸ƒã®å½“ã¦ã¯ã‚

```python
import numpy as np
from scipy.stats import norm, expon, gamma as gamma_dist

def fit_and_compare(data: np.ndarray):
    """Fit multiple distributions to data and compare log-likelihoods."""
    results = []

    # Gaussian MLE
    mu, sigma = data.mean(), data.std()
    ll_gauss = np.sum(norm.logpdf(data, mu, sigma))
    results.append(("Gaussian", ll_gauss, f"Î¼={mu:.3f}, Ïƒ={sigma:.3f}"))

    # Exponential MLE (for positive data only)
    if data.min() > 0:
        lam = 1 / data.mean()
        ll_exp = np.sum(expon.logpdf(data, scale=1/lam))
        results.append(("Exponential", ll_exp, f"Î»={lam:.3f}"))

    # Gamma MLE (method of moments)
    if data.min() > 0:
        mean_d = data.mean()
        var_d = data.var()
        alpha_hat = mean_d**2 / var_d
        beta_hat = mean_d / var_d
        ll_gamma = np.sum(gamma_dist.logpdf(data, alpha_hat, scale=1/beta_hat))
        results.append(("Gamma", ll_gamma, f"Î±={alpha_hat:.3f}, Î²={beta_hat:.3f}"))

    print(f"{'Distribution':<15} {'Log-Lik':>12} {'Parameters':<30}")
    print("-" * 60)
    for name, ll, params in sorted(results, key=lambda x: -x[1]):
        print(f"{name:<15} {ll:>12.2f} {params:<30}")
    print(f"\nBest fit: {sorted(results, key=lambda x: -x[1])[0][0]}")

# Test 1: Data from Gamma(3, 2)
np.random.seed(42)
data_gamma = np.random.gamma(3, 0.5, 500)  # shape=3, scale=0.5
print("=== Fitting data from Gamma(3, 0.5) ===\n")
fit_and_compare(data_gamma)

# Test 2: Data from mixture of Gaussians
data_mix = np.concatenate([
    np.random.normal(-2, 0.5, 300),
    np.random.normal(2, 0.8, 200)
])
print("\n=== Fitting data from Gaussian Mixture ===\n")
fit_and_compare(data_mix)
print("â†’ Single Gaussian is a bad fit for bimodal data!")
```

### 5.7 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸4: ç¢ºç‡å¤‰æ•°ã®å¤‰æ›ã‚’å¯è¦–åŒ–

```python
import numpy as np

def visualize_transform_stats(n_samples: int = 100000):
    """Demonstrate change of variables with various transformations."""
    np.random.seed(42)

    # Standard normal samples
    X = np.random.normal(0, 1, n_samples)

    transforms = [
        ("XÂ²", lambda x: x**2, "Chi-squared(1)"),
        ("exp(X)", lambda x: np.exp(x), "Log-Normal(0,1)"),
        ("|X|", lambda x: np.abs(x), "Half-Normal"),
        ("Î¦(X)", lambda x: norm.cdf(x), "Uniform(0,1)"),
        ("XÂ³", lambda x: x**3, "Heavy-tailed"),
    ]

    from scipy.stats import norm as norm_dist

    print(f"{'Transform':<12} {'Mean':>8} {'Std':>8} {'Skew':>8} {'Kurt':>8} {'Min':>8} {'Max':>8}")
    print("-" * 60)

    for name, fn, desc in transforms:
        Y = fn(X)
        # Remove inf/nan for safety
        Y = Y[np.isfinite(Y)]
        mean = Y.mean()
        std = Y.std()
        skew = np.mean(((Y - mean) / std)**3)
        kurt = np.mean(((Y - mean) / std)**4) - 3  # excess kurtosis
        print(f"{name:<12} {mean:>8.4f} {std:>8.4f} {skew:>8.4f} {kurt:>8.4f} {Y.min():>8.4f} {Y.max():>8.2f}")

    print(f"\nKey insight: Î¦(X) ~ Uniform(0,1) â€” the probability integral transform")
    print("This is the foundation of inverse transform sampling.")

visualize_transform_stats()
```

:::message
**ç¢ºç‡ç©åˆ†å¤‰æ›**: $X \sim F$ ã®ã¨ã $F(X) \sim \text{Uniform}(0,1)$ã€‚é€†ã« $U \sim \text{Uniform}(0,1)$ ã‹ã‚‰ $F^{-1}(U)$ ã§ä»»æ„ã®åˆ†å¸ƒã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§ãã‚‹ã€‚ã“ã‚ŒãŒé€†å¤‰æ›ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®åŸç†ã§ã‚ã‚Šã€Normalizing Flowï¼ˆç¬¬25å›ï¼‰ã®ç†è«–çš„å‡ºç™ºç‚¹ã§ã‚‚ã‚ã‚‹ã€‚
:::

### 5.8 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸5: å…±åˆ†æ•£è¡Œåˆ—ã®å›ºæœ‰å€¤åˆ†è§£ã¨ç¢ºç‡æ¥•å††

```python
import numpy as np

def probability_ellipse(mu: np.ndarray, Sigma: np.ndarray, n_samples: int = 5000):
    """Compute probability ellipse properties from covariance matrix."""
    # Eigendecomposition of Î£
    eigenvalues, eigenvectors = np.linalg.eigh(Sigma)

    print(f"Î¼ = {mu}")
    print(f"Î£ = \n{Sigma}")
    print(f"\nEigenvalues: {eigenvalues}")
    print(f"Eigenvectors:\n{eigenvectors}")

    # Ellipse axes: sqrt(eigenvalue) * chi2_quantile
    # For 95% confidence: chi2(2, 0.95) â‰ˆ 5.991
    chi2_95 = 5.991
    axis_lengths = np.sqrt(eigenvalues * chi2_95)
    angle = np.arctan2(eigenvectors[1, 1], eigenvectors[0, 1]) * 180 / np.pi

    print(f"\n95% probability ellipse:")
    print(f"  Semi-axis 1: {axis_lengths[0]:.4f}")
    print(f"  Semi-axis 2: {axis_lengths[1]:.4f}")
    print(f"  Rotation angle: {angle:.1f}Â°")
    print(f"  Area: {np.pi * axis_lengths[0] * axis_lengths[1]:.4f}")

    # Verify with samples
    samples = np.random.multivariate_normal(mu, Sigma, n_samples)
    # Mahalanobis distance for each sample
    diff = samples - mu
    Sigma_inv = np.linalg.inv(Sigma)
    mahal_sq = np.sum(diff @ Sigma_inv * diff, axis=1)
    # Points inside 95% ellipse have mahal_sq < chi2_95
    inside_95 = (mahal_sq < chi2_95).mean()
    print(f"\n  Empirical coverage (95% ellipse): {inside_95:.1%}")

    # Correlation coefficient
    rho = Sigma[0, 1] / np.sqrt(Sigma[0, 0] * Sigma[1, 1])
    print(f"  Correlation: Ï = {rho:.4f}")

# Example: highly correlated 2D Gaussian
mu = np.array([1.0, 2.0])
Sigma = np.array([[2.0, 1.5],
                   [1.5, 3.0]])
probability_ellipse(mu, Sigma)
```

### 5.9 ã‚»ãƒ«ãƒ•ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] ç¢ºç‡ç©ºé–“ $(\Omega, \mathcal{F}, P)$ ã®3è¦ç´ ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Ïƒ-åŠ æ³•æ—ãŒãªãœå¿…è¦ã‹ã€ç›´æ„Ÿçš„ã«èª¬æ˜ã§ãã‚‹
- [ ] æœŸå¾…å€¤ã®ç·šå½¢æ€§ã‚’è¨¼æ˜ãªã—ã§ä½¿ãˆã‚‹
- [ ] ãƒ™ã‚¤ã‚ºã®å®šç†ã‚’å°å‡ºã—ã€äº‹å¾Œâˆå°¤åº¦Ã—äº‹å‰ã¨è¨€ãˆã‚‹
- [ ] å…±å½¹äº‹å‰åˆ†å¸ƒã®æ„å‘³ã¨ä¸»è¦ãªçµ„ã¿åˆã‚ã›ã‚’3ã¤ä»¥ä¸ŠæŒ™ã’ã‚‰ã‚Œã‚‹
- [ ] ã‚¬ã‚¦ã‚¹åˆ†å¸ƒãƒ»ãƒ™ãƒ«ãƒŒãƒ¼ã‚¤åˆ†å¸ƒãƒ»ãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒã®PDFã‚’æ›¸ã‘ã‚‹
- [ ] å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã®æ¡ä»¶ä»˜ãåˆ†å¸ƒã‚’å°å‡ºã§ãã‚‹
- [ ] æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®æ¨™æº–å½¢ã‚’æ›¸ãã€ã‚¬ã‚¦ã‚¹ã‚’å½“ã¦ã¯ã‚ã‚‰ã‚Œã‚‹
- [ ] MLEã®å°å‡ºæ‰‹é †ï¼ˆå¯¾æ•°å°¤åº¦â†’å¾®åˆ†â†’0ã¨ãŠãï¼‰ã‚’å®Ÿè¡Œã§ãã‚‹
- [ ] Fisheræƒ…å ±é‡ã®å®šç¾©ã¨2ã¤ã®è¡¨ç¾ã‚’æ›¸ã‘ã‚‹
- [ ] CramÃ©r-Raoä¸‹ç•Œã‚’è¿°ã¹ã€MLEã®æ¼¸è¿‘æœ‰åŠ¹æ€§ã¨æ¥ç¶šã§ãã‚‹
- [ ] CLTã‚’è¿°ã¹ã€ãªãœã‚¬ã‚¦ã‚¹ãŒé »å‡ºã™ã‚‹ã‹èª¬æ˜ã§ãã‚‹
- [ ] LLMã®æå¤±é–¢æ•°ã‚’æ¡ä»¶ä»˜ãç¢ºç‡ã®é€£é–è¦å‰‡ã§åˆ†è§£ã§ãã‚‹

:::message
**é€²æ—: 85% å®Œäº†** è¨˜å·èª­è§£10å•ã€LaTeX5å•ã€ã‚³ãƒ¼ãƒ‰ç¿»è¨³5å•ã€è«–æ–‡èª­è§£1å•ã€å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸2å•ã‚’ã‚¯ãƒªã‚¢ã€‚Zone 5 å®Œäº†ã€‚
:::

---

## ğŸš€ 6. æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨æ¬¡å›äºˆå‘Š

### 6.1 ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹

| ãƒªã‚½ãƒ¼ã‚¹ | URL | ç‰¹å¾´ |
|:---------|:----|:-----|
| 3Blue1Brown: Bayes | YouTube | è¦–è¦šçš„ç›´æ„Ÿ |
| StatQuest | YouTube | çµ±è¨ˆã®åŸºç¤ã‚’ä¸å¯§ã« |
| MIT 18.650 | OCW | æ•°ç†çµ±è¨ˆã®è¬›ç¾© |
| Stanford CS229 Notes | Web | MLè¦–ç‚¹ã®ç¢ºç‡è«– |

:::details ç”¨èªé›† â€” æœ¬è¬›ç¾©ã®å…¨ç”¨èª

| ç”¨èª | è‹±èª | å®šç¾© |
|:-----|:-----|:-----|
| ç¢ºç‡ç©ºé–“ | Probability space | $(\Omega, \mathcal{F}, P)$ ã®ä¸‰ã¤çµ„ |
| Ïƒ-åŠ æ³•æ— | Ïƒ-algebra | è£œé›†åˆãƒ»å¯ç®—åˆä½µã§é–‰ã˜ãŸäº‹è±¡ã®æ— |
| ç¢ºç‡æ¸¬åº¦ | Probability measure | $P: \mathcal{F} \to [0,1]$, $P(\Omega)=1$ |
| ç¢ºç‡å¤‰æ•° | Random variable | å¯æ¸¬é–¢æ•° $X: \Omega \to \mathbb{R}$ |
| æœŸå¾…å€¤ | Expectation | $\mathbb{E}[X] = \int x \, dP$ |
| åˆ†æ•£ | Variance | $\text{Var}(X) = \mathbb{E}[(X-\mathbb{E}[X])^2]$ |
| å…±åˆ†æ•£ | Covariance | $\text{Cov}(X,Y) = \mathbb{E}[(X-\mu_X)(Y-\mu_Y)]$ |
| æ¡ä»¶ä»˜ãç¢ºç‡ | Conditional probability | $P(A \mid B) = P(A \cap B)/P(B)$ |
| ãƒ™ã‚¤ã‚ºã®å®šç† | Bayes' theorem | äº‹å¾Œâˆå°¤åº¦Ã—äº‹å‰ |
| å…±å½¹äº‹å‰åˆ†å¸ƒ | Conjugate prior | äº‹å¾Œã¨åŒã˜åˆ†å¸ƒæ—ã®äº‹å‰åˆ†å¸ƒ |
| æŒ‡æ•°å‹åˆ†å¸ƒæ— | Exponential family | $p(x \mid \eta) = h(x)\exp(\eta^\top T(x) - A(\eta))$ |
| ååˆ†çµ±è¨ˆé‡ | Sufficient statistic | ãƒ‡ãƒ¼ã‚¿ã®å…¨æƒ…å ±ã‚’ä¿æŒã™ã‚‹çµ±è¨ˆé‡ |
| æœ€å°¤æ¨å®š | MLE | $\hat{\theta} = \arg\max \sum \log p(x_i; \theta)$ |
| MAPæ¨å®š | MAP | MLE + äº‹å‰åˆ†å¸ƒ |
| Fisheræƒ…å ±é‡ | Fisher information | $I(\theta) = \mathbb{E}[s(x;\theta)s(x;\theta)^\top]$ |
| CramÃ©r-Raoä¸‹ç•Œ | CramÃ©r-Rao bound | $\text{Var}(\hat{\theta}) \geq 1/(nI(\theta))$ |
| å¤§æ•°ã®æ³•å‰‡ | Law of large numbers | $\bar{X}_N \to \mu$ |
| ä¸­å¿ƒæ¥µé™å®šç† | Central limit theorem | $\sqrt{N}(\bar{X}_N - \mu) \to \mathcal{N}(0,\sigma^2)$ |
| ç‹¬ç«‹åŒåˆ†å¸ƒ | i.i.d. | å„ãƒ‡ãƒ¼ã‚¿ç‚¹ãŒåŒã˜åˆ†å¸ƒã‹ã‚‰ç‹¬ç«‹ã«ã‚µãƒ³ãƒ—ãƒ« |
:::

```mermaid
mindmap
  root((ç¬¬4å›: ç¢ºç‡è«–))
    ç¢ºç‡ç©ºé–“
      Kolmogorovå…¬ç†
      Ïƒ-åŠ æ³•æ—
      ç¢ºç‡æ¸¬åº¦
    ç¢ºç‡å¤‰æ•°
      æœŸå¾…å€¤ãƒ»åˆ†æ•£
      ç‹¬ç«‹æ€§
      i.i.d.
    ãƒ™ã‚¤ã‚ºæ¨è«–
      ãƒ™ã‚¤ã‚ºã®å®šç†
      å…±å½¹äº‹å‰åˆ†å¸ƒ
      MAPæ¨å®š
    ç¢ºç‡åˆ†å¸ƒ
      é›¢æ•£: Bernoulli, Categorical
      é€£ç¶š: Gaussian, Gamma, Beta
      å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒ
      æŒ‡æ•°å‹åˆ†å¸ƒæ—
    æ¨å®šç†è«–
      MLE
      Fisheræƒ…å ±é‡
      CramÃ©r-Raoä¸‹ç•Œ
    æ¥µé™å®šç†
      å¤§æ•°ã®æ³•å‰‡
      ä¸­å¿ƒæ¥µé™å®šç†
      Berry-Esseené™ç•Œ
    LLMæ¥ç¶š
      æ¡ä»¶ä»˜ãç¢ºç‡
      è‡ªå·±å›å¸°ç”Ÿæˆ
      Categoricalåˆ†å¸ƒ
```

:::message
**é€²æ—: 90% å®Œäº†** ç¢ºç‡è«–ã®ç ”ç©¶ç³»è­œã€æ¨è–¦æ›¸ç±ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹ã€ç”¨èªé›†ã€çŸ¥è­˜ãƒã‚¤ãƒ³ãƒ‰ãƒãƒƒãƒ—ã‚’ç¶²ç¾…ã€‚Zone 6 ã‚¯ãƒªã‚¢ã€‚
:::

---

### 6.2 æœ¬è¬›ç¾©ã®æ ¸å¿ƒ â€” 3ã¤ã®æŒã¡å¸°ã‚Š

1. **ç¢ºç‡ã¯ã€Œã‚ã‹ã‚‰ãªã•ã€ã®è¨€èªã§ã‚ã‚‹ã€‚** ç¢ºç‡ç©ºé–“ $(\Omega, \mathcal{F}, P)$ ã¨ã„ã†å³å¯†ãªæ çµ„ã¿ã®ä¸Šã«ã€ç¢ºç‡å¤‰æ•°ãƒ»æœŸå¾…å€¤ãƒ»æ¡ä»¶ä»˜ãç¢ºç‡ãŒå®šç¾©ã•ã‚Œã‚‹ã€‚ã“ã®è¨€èªãªã—ã«ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯è¨˜è¿°ã§ããªã„ã€‚

2. **ãƒ™ã‚¤ã‚ºã®å®šç†ã¯ã€Œå­¦ç¿’ã€ã®æ•°å¼ã ã€‚** äº‹å‰åˆ†å¸ƒï¼ˆä¿¡å¿µï¼‰+ å°¤åº¦ï¼ˆãƒ‡ãƒ¼ã‚¿ï¼‰â†’ äº‹å¾Œåˆ†å¸ƒï¼ˆæ›´æ–°ã•ã‚ŒãŸä¿¡å¿µï¼‰ã€‚VAEã®ELBOã‚‚ã€LLMã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚‚ã€ã“ã®æ§‹é€ ã®å¤‰ç¨®ã ã€‚

3. **MLEã¯æ¡ä»¶ä»˜ãCategoricalåˆ†å¸ƒã®æœ€é©åŒ–ã«å¸°ç€ã™ã‚‹ã€‚** LLMã®å­¦ç¿’ã¯ã€å„æ™‚åˆ» $t$ ã§ $p(x_t \mid x_{<t})$ ã‚’Categoricalåˆ†å¸ƒã¨ã—ã¦MLEæ¨å®šã™ã‚‹ã“ã¨ã€‚æœ¬è¬›ç¾©ã§å­¦ã‚“ã å…¨ã¦ã®é“å…·ãŒã“ã“ã«é›†ç´„ã•ã‚Œã‚‹ã€‚

### 6.3 FAQ

:::details Q: ãƒ™ã‚¤ã‚ºã¨é »åº¦ä¸»ç¾©ã€çµå±€ã©ã¡ã‚‰ãŒæ­£ã—ã„ã®ã‹ï¼Ÿ
ã€Œæ­£ã—ã•ã€ã®åŸºæº–ãŒç•°ãªã‚‹ã€‚é »åº¦ä¸»ç¾©ã¯ã€Œæ¨å®šé‡ã®é•·æœŸçš„æŒ¯ã‚‹èˆã„ã€ï¼ˆç¹°ã‚Šè¿”ã—å®Ÿé¨“ï¼‰ã§è©•ä¾¡ã—ã€ãƒ™ã‚¤ã‚ºã¯ã€Œç¾åœ¨ã®çŸ¥è­˜ã®ä¸‹ã§ã®ç¢ºä¿¡åº¦ã€ã§è©•ä¾¡ã™ã‚‹ã€‚MLã®æ–‡è„ˆã§ã¯:

- **MLE**ï¼ˆé »åº¦ä¸»ç¾©å¯„ã‚Šï¼‰: è¨ˆç®—ãŒç°¡å˜ã€æ¼¸è¿‘çš„ã«æœ€é©ã€å¤§ãƒ‡ãƒ¼ã‚¿å‘ã
- **ãƒ™ã‚¤ã‚ºæ¨è«–**: ä¸ç¢ºå®Ÿæ€§ã®å®šé‡åŒ–ãŒè‡ªç„¶ã€å°ãƒ‡ãƒ¼ã‚¿å‘ãã€äº‹å‰çŸ¥è­˜ã‚’æ´»ç”¨å¯èƒ½

å®Ÿç”¨ä¸Šã¯ã€Œã©ã¡ã‚‰ã‹ä¸€æ–¹ã€ã§ã¯ãªãã€å•é¡Œã«å¿œã˜ã¦ä½¿ã„åˆ†ã‘ã‚‹ã€‚VAEã¯å¤‰åˆ†ãƒ™ã‚¤ã‚ºã€LLMã®æå¤±é–¢æ•°ã¯MLEã ã€‚
:::

:::details Q: ãªãœæ­£è¦åˆ†å¸ƒãŒã“ã‚“ãªã«é »å‡ºã™ã‚‹ã®ã‹ï¼Ÿ
3ã¤ã®ç†ç”±ãŒã‚ã‚‹:

1. **ä¸­å¿ƒæ¥µé™å®šç†**: å¤šæ•°ã®ç‹¬ç«‹ãªå¾®å°åŠ¹æœã®å’Œã¯æ­£è¦åˆ†å¸ƒã«è¿‘ã¥ã
2. **æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼**: å¹³å‡ã¨åˆ†æ•£ã‚’å›ºå®šã—ãŸã¨ãã€ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æœ€å¤§ã®åˆ†å¸ƒãŒæ­£è¦åˆ†å¸ƒ
3. **è¨ˆç®—ã®éƒ½åˆ**: æ­£è¦åˆ†å¸ƒã®ç©ãƒ»å’Œãƒ»æ¡ä»¶ä»˜ããŒå…¨ã¦é–‰ã˜ãŸå½¢ã«ãªã‚‹

3ã¤ç›®ãŒå®Ÿç”¨ä¸Šæœ€ã‚‚é‡è¦ã ã€‚GANã®æ½œåœ¨ç©ºé–“ $\mathbf{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ ã‚„VAEã®äº‹å‰åˆ†å¸ƒã‚‚ã€è¨ˆç®—ã®å®¹æ˜“ã•ãŒé¸æŠã®ä¸»å› ã ã€‚
:::

:::details Q: æŒ‡æ•°å‹åˆ†å¸ƒæ—ã¯å®Ÿéš›ã«ã©ã“ã§ä½¿ã†ã®ã‹ï¼Ÿ
è‡³ã‚‹æ‰€ã§ã€‚

- **VAE**: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ›ã¯ã‚¬ã‚¦ã‚¹åˆ†å¸ƒï¼ˆæŒ‡æ•°å‹åˆ†å¸ƒæ—ï¼‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- **EBM**: $p(\mathbf{x}) = \frac{1}{Z}\exp(-E(\mathbf{x}))$ ã¯æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®ä¸€èˆ¬åŒ–
- **GLM**: ä¸€èˆ¬åŒ–ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”åˆ†å¸ƒã¯æŒ‡æ•°å‹åˆ†å¸ƒæ—
- **Softmax**: Categoricalåˆ†å¸ƒã¯æŒ‡æ•°å‹åˆ†å¸ƒæ—ã€‚LLMã®å‡ºåŠ›åˆ†å¸ƒãã®ã‚‚ã®

ç¬¬27å›ï¼ˆEBMï¼‰ã¨ç¬¬9å›ï¼ˆå¤‰åˆ†æ¨è«–ï¼‰ã§æœ¬æ ¼çš„ã«æ´»ç”¨ã™ã‚‹ã€‚
:::

:::details Q: CramÃ©r-Raoä¸‹ç•Œã‚’çŸ¥ã£ã¦ä½•ã®å½¹ã«ç«‹ã¤ã®ã‹ï¼Ÿ
ã€Œã“ã®æ¨å®šå•é¡Œã§ã“ã‚Œä»¥ä¸Šã®ç²¾åº¦ã¯åŸç†çš„ã«ä¸å¯èƒ½ã€ã¨ã„ã†é™ç•Œã‚’çŸ¥ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚

- ãƒ¢ãƒ‡ãƒ«è¨­è¨ˆ: æ¨å®šé‡ã®åˆ†æ•£ãŒCRä¸‹ç•Œã«è¿‘ã‘ã‚Œã°ã€ã“ã‚Œä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿ã¯ä¸è¦
- å®Ÿé¨“è¨ˆç”»: Fisheræƒ…å ±é‡ãŒå¤§ãã„å®Ÿé¨“æ¡ä»¶ã‚’é¸ã¶ã“ã¨ã§ã€å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§ç²¾å¯†ãªæ¨å®šãŒå¯èƒ½
- ç†è«–è§£æ: NNã®è¡¨ç¾åŠ›ã¨Fisheræƒ…å ±é‡ã®é–¢ä¿‚ã¯æ´»ç™ºãªç ”ç©¶åˆ†é‡
:::

:::details Q: ã€Œç¢ºç‡å¯†åº¦é–¢æ•°ã®å€¤ãŒ1ã‚’è¶…ãˆã‚‹ã€ã®ã¯é–“é•ã„ã§ã¯ï¼Ÿ
ã„ã„ãˆã€æ­£ã—ã„ã€‚PDFã¯ç¢ºç‡ã§ã¯ãªã„ã€‚ç¢ºç‡ã¯å¯†åº¦ã®**ç©åˆ†**ã§å¾—ã‚‰ã‚Œã‚‹:

$$
P(a \leq X \leq b) = \int_a^b f(x) dx
$$

$f(x)$ è‡ªä½“ã¯éè² ã§ã‚ã‚Œã°ã„ãã‚‰ã§ã‚‚å¤§ããã¦ã‚ˆã„ã€‚ä¾‹ãˆã° $\mathcal{N}(0, 0.01)$ ã®ãƒ”ãƒ¼ã‚¯ã¯ $f(0) = \frac{1}{\sqrt{2\pi \cdot 0.01}} \approx 3.99$ ã§ã€1ã‚’å¤§ããè¶…ãˆã‚‹ã€‚ç©åˆ†ã™ã‚‹ã¨å¿…ãš1ã«ãªã‚‹ãŒã€å¯†åº¦å€¤ãŒ1ã‚’è¶…ãˆã‚‹ã“ã¨è‡ªä½“ã¯ä½•ã®å•é¡Œã‚‚ãªã„ã€‚

```python
import numpy as np
sigma = 0.1
peak = 1 / np.sqrt(2 * np.pi * sigma**2)
print(f"N(0, {sigma**2}) peak density: {peak:.4f} >> 1.0")
print("But âˆ«f(x)dx = 1.0 always!")
```
:::

:::details Q: Multinomialåˆ†å¸ƒã¨Categoricalåˆ†å¸ƒã®é•ã„ã¯ï¼Ÿ
Categoricalåˆ†å¸ƒã¯ã€Œã‚µã‚¤ã‚³ãƒ­ã‚’1å›æŒ¯ã‚‹ã€: $x \in \{1, \ldots, K\}$, $P(x=k) = \pi_k$ã€‚

Multinomialåˆ†å¸ƒã¯ã€Œã‚µã‚¤ã‚³ãƒ­ã‚’ $n$ å›æŒ¯ã£ã¦ã€å„é¢ã®å‡ºãŸå›æ•°ã‚’è¨˜éŒ²ã™ã‚‹ã€: $(c_1, \ldots, c_K) \sim \text{Multi}(n, \boldsymbol{\pi})$, $\sum_k c_k = n$ã€‚

LLMã®æ–‡è„ˆã§ã¯:
- 1ãƒˆãƒ¼ã‚¯ãƒ³ã®äºˆæ¸¬ = Categoricalåˆ†å¸ƒ
- ãƒãƒƒãƒå†…ã®å…¨ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã®çµ±è¨ˆ = Multinomialåˆ†å¸ƒ

Categorical = Multinomial($n=1$, $\boldsymbol{\pi}$) ã ã€‚
:::

:::details Q: ã“ã®ç¢ºç‡è«–ã®çŸ¥è­˜ã¯ç¬¬5å›ï¼ˆæ¸¬åº¦è«–ï¼‰ã§ã©ã†æ‹¡å¼µã•ã‚Œã‚‹ã®ã‹ï¼Ÿ
æœ¬è¬›ç¾©ã§ã¯ã€Œç¢ºç‡å¯†åº¦é–¢æ•° $f(x)$ ãŒå­˜åœ¨ã™ã‚‹ã€ã¨æš—é»™ã«ä»®å®šã—ãŸã€‚ã ãŒ:

- é›¢æ•£ã¨é€£ç¶šãŒæ··ã˜ã£ãŸåˆ†å¸ƒã¯ï¼Ÿ
- $\mathbb{R}^d$ ä¸Šã®å…¨ã¦ã®éƒ¨åˆ†é›†åˆã«ç¢ºç‡ã‚’å®šç¾©ã§ãã‚‹ã‹ï¼Ÿ
- ã€Œã»ã¨ã‚“ã©ç¢ºå®Ÿã«ã€ã¨ã¯ä½•ã‹ï¼Ÿ

ç¬¬5å›ã§ã¯æ¸¬åº¦è«–ã®è¨€è‘‰ã§ $f(x) = \frac{dP}{d\lambda}$ ï¼ˆRadon-Nikodymå°é–¢æ•°ï¼‰ã¨ã—ã¦å¯†åº¦é–¢æ•°ã‚’å³å¯†ã«å®šç¾©ã™ã‚‹ã€‚ã•ã‚‰ã«ç¢ºç‡éç¨‹ï¼ˆMarkové€£é–ã€Browné‹å‹•ï¼‰ã‚’å°å…¥ã—ã€æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®SDEå®šå¼åŒ–ã¸ã®æ©‹æ¸¡ã—ã‚’è¡Œã†ã€‚
:::

### 6.4 ç¢ºç‡è«–ã§ã‚ˆãã‚ã‚‹ã€Œç½ ã€

:::details ç½ 1: P(A|B) â‰  P(B|A) â€” æ¡ä»¶ã®é€†è»¢
ã€Œé›¨ã®ã¨ãå‚˜ã‚’æŒã¤ç¢ºç‡90%ã€ã¨ã€Œå‚˜ã‚’æŒã£ã¦ã„ã‚‹ã¨ãé›¨ã®ç¢ºç‡ã€ã¯å…¨ãé•ã†ã€‚ãƒ™ã‚¤ã‚ºã®å®šç†ãªã—ã«ã“ã®2ã¤ã‚’æ··åŒã™ã‚‹ã®ãŒã€Œæ¤œå¯Ÿå®˜ã®èª¤è¬¬ã€ã ã€‚DNAé‘‘å®šã§ã€Œä¸€è‡´ã—ãŸ = çŠ¯äººã€ã¨çµè«–ã™ã‚‹ã®ã¯ $P(\text{ä¸€è‡´} \mid \text{çŠ¯äºº})$ ã¨ $P(\text{çŠ¯äºº} \mid \text{ä¸€è‡´})$ ã®æ··åŒã€‚
:::

:::details ç½ 2: ç‹¬ç«‹ã¨ç„¡ç›¸é–¢ã¯é•ã†
ç„¡ç›¸é–¢: $\text{Cov}(X, Y) = 0$ï¼ˆç·šå½¢é–¢ä¿‚ãŒãªã„ï¼‰
ç‹¬ç«‹: $P(X, Y) = P(X)P(Y)$ï¼ˆã‚ã‚‰ã‚†ã‚‹é–¢ä¿‚ãŒãªã„ï¼‰

ç‹¬ç«‹ â†’ ç„¡ç›¸é–¢ã ãŒã€é€†ã¯æˆã‚Šç«‹ãŸãªã„ã€‚$X \sim \mathcal{N}(0,1)$, $Y = X^2$ ã¯ç„¡ç›¸é–¢ã ãŒç‹¬ç«‹ã§ã¯ãªã„ã€‚
```python
import numpy as np
np.random.seed(42)
X = np.random.normal(0, 1, 100000)
Y = X**2
print(f"Cov(X, XÂ²) = {np.cov(X, Y)[0,1]:.4f} â‰ˆ 0 (uncorrelated)")
print(f"But E[Y|X=2] = 4, E[Y|X=-2] = 4 â†’ clearly not independent!")
```
:::

:::details ç½ 3: åˆ†æ•£0ã§ã‚‚åˆ†å¸ƒã¯æ±ºã¾ã‚‰ãªã„
CramÃ©r-Raoä¸‹ç•Œ $\text{Var} \geq 1/(nI)$ ã¯ä¸åæ¨å®šé‡ã«ã—ã‹é©ç”¨ã•ã‚Œãªã„ã€‚ãƒã‚¤ã‚¢ã‚¹ã®ã‚ã‚‹æ¨å®šé‡ã¯CRä¸‹ç•Œã‚’ä¸‹å›ã‚‹ã“ã¨ãŒã‚ã‚‹ï¼ˆJames-Steinã®ç¸®å°æ¨å®šé‡ï¼‰ã€‚ã€Œãƒã‚¤ã‚¢ã‚¹ã‚’è¨±å®¹ã™ã‚‹ä»£ã‚ã‚Šã«MSEã‚’ä¸‹ã’ã‚‹ã€ã®ã¯ã€MLã§ã¯æ­£å‰‡åŒ–ã¨ã—ã¦æ—¥å¸¸çš„ã«è¡Œã‚ã‚Œã‚‹ã€‚
:::

:::details ç½ 4: MLEã¯å¸¸ã«æœ€è‰¯ã§ã¯ãªã„
å°ã‚µãƒ³ãƒ—ãƒ«ã§ã¯MLEã®ãƒã‚¤ã‚¢ã‚¹ãŒå•é¡Œã«ãªã‚‹ã€‚åˆ†æ•£æ¨å®šé‡ $\hat{\sigma}^2_{\text{MLE}} = \frac{1}{N}\sum(x_i - \bar{x})^2$ ã¯ $\sigma^2$ ã‚’éå°è©•ä¾¡ã™ã‚‹ã€‚James-Steinã®å®šç†ãŒç¤ºã™ã®ã¯ã€3æ¬¡å…ƒä»¥ä¸Šã§ã¯MLEãŒã€Œè¨±å®¹å¯èƒ½ã§ãªã„ã€ï¼ˆadmissible ã§ãªã„ï¼‰ã¨ã„ã†è¡æ’ƒçš„äº‹å®Ÿã ã€‚
:::

:::details ç½ 5: äº‹å‰åˆ†å¸ƒãŒã€Œä¸»è¦³çš„ã€ã¯æ¬ ç‚¹ã‹ï¼Ÿ
é »åº¦ä¸»ç¾©è€…ã¯ãƒ™ã‚¤ã‚ºã®ã€Œä¸»è¦³æ€§ã€ã‚’æ‰¹åˆ¤ã™ã‚‹ã€‚ã ãŒ:
- ã€Œäº‹å‰åˆ†å¸ƒãªã—ã€ã¯ã€Œä¸€æ§˜äº‹å‰åˆ†å¸ƒã€ã¨ç­‰ä¾¡ â€” ã“ã‚Œã‚‚ä¸»è¦³çš„
- å¼±æƒ…å ±äº‹å‰åˆ†å¸ƒã¯ã€ç‰©ç†çš„åˆ¶ç´„ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç¯„å›²ç­‰ï¼‰ã‚’è‡ªç„¶ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
- ãƒ‡ãƒ¼ã‚¿ãŒååˆ†ã‚ã‚Œã°äº‹å‰åˆ†å¸ƒã®å½±éŸ¿ã¯æ¶ˆãˆã‚‹ï¼ˆäº‹å¾Œä¸€è‡´æ€§ï¼‰

å®Ÿç”¨çš„ã«ã¯ã€äº‹å‰åˆ†å¸ƒã¯ã€Œæ­£å‰‡åŒ–ã®ä¸€å½¢æ…‹ã€ã¨å‰²ã‚Šåˆ‡ã£ã¦ã‚ˆã„ã€‚
:::

### 6.5 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

| æ—¥ | å†…å®¹ | æ‰€è¦æ™‚é–“ |
|:---|:-----|:---------|
| Day 1 | Zone 0-2ï¼ˆä½“é¨“ãƒ»ç›´æ„Ÿï¼‰+ Zone 3 å‰åŠï¼ˆ3.1-3.4ï¼‰ | 2æ™‚é–“ |
| Day 2 | Zone 3 å¾ŒåŠï¼ˆ3.5-3.10 Boss Battleï¼‰ | 2æ™‚é–“ |
| Day 3 | Zone 4ï¼ˆå®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ + GMM + ãƒ™ã‚¤ã‚ºæ¨è«–ï¼‰ | 2æ™‚é–“ |
| Day 4 | Zone 5ï¼ˆãƒ†ã‚¹ãƒˆ + ãƒãƒ£ãƒ¬ãƒ³ã‚¸å®Ÿè£…ï¼‰ | 1.5æ™‚é–“ |
| Day 5 | Zone 6-7ï¼ˆç™ºå±• + æŒ¯ã‚Šè¿”ã‚Šï¼‰ | 1æ™‚é–“ |
| Day 6 | å¾©ç¿’: ä¸»è¦å®šç†ã‚’ç´™ã«å†å°å‡º | 1æ™‚é–“ |
| Day 7 | ç¬¬5å›ã®äºˆç¿’: æ¸¬åº¦è«–ã®ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ | 1æ™‚é–“ |

### 6.6 é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼

```python
import numpy as np

lecture4_progress = {
    "Zone 0: Quick Start": True,
    "Zone 1: Experience": True,
    "Zone 2: Intuition": True,
    "Zone 3.1: Probability Space": False,
    "Zone 3.2: Random Variables": False,
    "Zone 3.3: Bayes' Theorem": False,
    "Zone 3.4: Distributions": False,
    "Zone 3.5: Multivariate Normal": False,
    "Zone 3.6: Exponential Family": False,
    "Zone 3.7: MLE": False,
    "Zone 3.8: Fisher Information": False,
    "Zone 3.9: LLN & CLT": False,
    "Zone 3.10: Boss Battle": False,
    "Zone 4: Implementation": False,
    "Zone 5: Experiments": False,
    "Zone 6: Advanced": False,
    "Zone 7: Review": False,
}

completed = sum(v for v in lecture4_progress.values())
total = len(lecture4_progress)
print(f"=== ç¬¬4å› é€²æ—: {completed}/{total} ({100*completed/total:.0f}%) ===\n")
for zone, done in lecture4_progress.items():
    status = "[x]" if done else "[ ]"
    print(f"  {status} {zone}")
```

### 6.7 æ¬¡å›äºˆå‘Š â€” ç¬¬5å›: æ¸¬åº¦è«–çš„ç¢ºç‡è«–ãƒ»ç¢ºç‡éç¨‹å…¥é–€

ç¬¬4å›ã§ç¢ºç‡åˆ†å¸ƒã‚’ã€Œä½¿ãˆã‚‹ã€ã‚ˆã†ã«ãªã£ãŸã€‚ã ãŒã€ä»¥ä¸‹ã®å•ã„ã«ç­”ãˆã‚‰ã‚Œã‚‹ã ã‚ã†ã‹:

- ã€Œç¢ºç‡å¯†åº¦é–¢æ•°ã€ã¨ã¯å³å¯†ã«ä½•ã‹ï¼Ÿ ãªãœç‚¹ $x$ ã§ã® $f(x)$ ã¯ç¢ºç‡ã§ã¯ãªã„ã®ã‹ï¼Ÿ
- é›¢æ•£ã¨é€£ç¶šãŒæ··ã˜ã£ãŸåˆ†å¸ƒã‚’ã©ã†æ‰±ã†ã‹ï¼Ÿ
- ã€Œã»ã¨ã‚“ã©ç¢ºå®Ÿã«åæŸã™ã‚‹ã€ã®ã€Œã»ã¨ã‚“ã©ã€ã¨ã¯ï¼Ÿ
- Browné‹å‹•ã¯ãªãœå¾®åˆ†ä¸å¯èƒ½ãªã®ã‹ï¼Ÿ
- æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®forward processã‚’è¨˜è¿°ã™ã‚‹SDEã¨ã¯ä½•ã‹ï¼Ÿ

ç¬¬5å›ã§ã¯**æ¸¬åº¦è«–**ã®è¨€è‘‰ã§ç¢ºç‡è«–ã‚’å†æ§‹ç¯‰ã™ã‚‹ã€‚Lebesgueç©åˆ†ã€Radon-Nikodymå°é–¢æ•°ã€ç¢ºç‡éç¨‹ã€Markové€£é–ã€Browné‹å‹• â€” æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®æ•°å­¦çš„åŸºç›¤ãŒã“ã“ã«åŸ‹ã¾ã£ã¦ã„ã‚‹ã€‚

ãã—ã¦ `%timeit` ãŒåˆç™»å ´ã™ã‚‹ã€‚Monte Carloç©åˆ†ã®è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’æ¸¬ã‚Šå§‹ã‚ã‚‹ã¨ã€Pythonã®ã€Œé…ã•ã€ãŒå°‘ã—ãšã¤è¦‹ãˆã¦ãã‚‹......ã€‚

:::message
**é€²æ—: 100% å®Œäº†** ç¬¬4å›: ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦ â€” å…¨ã‚¾ãƒ¼ãƒ³ã‚¯ãƒªã‚¢ã€‚ãŠç–²ã‚Œã•ã¾ã§ã—ãŸã€‚ç¢ºç‡ã®è¨€èªã‚’æ‰‹ã«å…¥ã‚ŒãŸä»Šã€ç¬¬5å›ã§æ¸¬åº¦è«–ã¨ã„ã†ã€Œç¢ºç‡ã®æ–‡æ³•ã€ã‚’å³å¯†ã«å®šç¾©ã™ã‚‹æ—…ã«å‡ºã‚ˆã†ã€‚
:::

---


### 6.8 ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **ç¾å®Ÿã®ãƒ‡ãƒ¼ã‚¿ã¯æ­£è¦åˆ†å¸ƒã«å¾“ã‚ãªã„ã€‚ãã‚Œã§ã‚‚ä»®å®šã™ã‚‹"æœ¬å½“ã®ç†ç”±"ã¯ä½•ã‹ï¼Ÿ**

CLTãŒã€Œå¤šæ•°ã®ç‹¬ç«‹å¾®å°åŠ¹æœã®å’Œâ†’æ­£è¦åˆ†å¸ƒã€ã‚’ä¿è¨¼ã™ã‚‹ã‹ã‚‰ï¼Ÿ ãã‚Œã¯ç†ç”±ã®ä¸€ã¤ã ã€‚ã ãŒæœ¬è³ªã¯ã‚‚ã£ã¨æ·±ã„ã€‚

- ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã¯**æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åˆ†å¸ƒ**ã ã€‚å¹³å‡ã¨åˆ†æ•£ã ã‘ã‚’çŸ¥ã£ã¦ã„ã‚‹ã¨ãã€ãã‚Œä»¥ä¸Šã®ä»®å®šã‚’ç½®ã‹ãªã„ã€Œæœ€ã‚‚æƒ…å ±é‡ã®å°‘ãªã„ã€åˆ†å¸ƒãŒã‚¬ã‚¦ã‚¹ã 
- ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®æ¼”ç®—ã¯**é–‰ã˜ã¦ã„ã‚‹**ã€‚å’Œãƒ»æ¡ä»¶ä»˜ããƒ»å‘¨è¾ºãŒå…¨ã¦ã‚¬ã‚¦ã‚¹ã®ã¾ã¾ã€‚ã“ã‚Œã¯è¨ˆç®—ä¸Šã®å¥‡è·¡ã¨è¨€ã£ã¦ã‚ˆã„
- ãã—ã¦ã€æ­£è¦åˆ†å¸ƒãŒã€Œé–“é•ã£ã¦ã„ã‚‹ã€ã“ã¨ã¯**ã‚ã‹ã£ã¦ã„ã‚‹**ä¸Šã§ä½¿ã†ã€‚é‡è¦ãªã®ã¯ã€Œã©ã®ç¨‹åº¦é–“é•ã£ã¦ã„ã‚‹ã‹ã€ã‚’å®šé‡åŒ–ã™ã‚‹ã“ã¨ â€” KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ï¼ˆç¬¬6å›ï¼‰ãŒãã®é“å…·ã 

:::details ãƒ™ã‚¤ã‚ºè„³ä»®èª¬ â€” è„³ã¯ç¢ºç‡è¨ˆç®—æ©Ÿã‹ï¼Ÿ
èªçŸ¥ç§‘å­¦ã«ã¯ã€Œè„³ã¯ãƒ™ã‚¤ã‚ºæ¨è«–ã‚’è¡Œã£ã¦ã„ã‚‹ã€ã¨ã„ã†ä»®èª¬ãŒã‚ã‚‹ã€‚æ„Ÿè¦šå…¥åŠ›ï¼ˆå°¤åº¦ï¼‰ã¨çµŒé¨“ï¼ˆäº‹å‰åˆ†å¸ƒï¼‰ã‚’çµ„ã¿åˆã‚ã›ã¦ä¸–ç•Œã®çŠ¶æ…‹ï¼ˆäº‹å¾Œåˆ†å¸ƒï¼‰ã‚’æ¨å®šã™ã‚‹ã€‚

éŒ¯è¦–ç¾è±¡ã¯ã€å¼·ã„äº‹å‰åˆ†å¸ƒãŒå¼±ã„å°¤åº¦ã‚’ä¸Šæ›¸ãã™ã‚‹ä¾‹ã¨ã—ã¦è§£é‡ˆã•ã‚Œã‚‹ã€‚VAEã®ãƒ‡ã‚³ãƒ¼ãƒ€ãŒã€Œã¼ã‚„ã‘ãŸã€ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã®ã¯ã€äº‹å‰åˆ†å¸ƒ $p(\mathbf{z}) = \mathcal{N}(\mathbf{0}, \mathbf{I})$ ãŒéåº¦ã«æ»‘ã‚‰ã‹ãªæ½œåœ¨ç©ºé–“ã‚’å¼·åˆ¶ã™ã‚‹ãŸã‚ â€” ã‚ã‚‹æ„å‘³ã€è„³ã®éŒ¯è¦–ã¨åŒã˜æ§‹é€ ã ã€‚

ã€Œæ­£è¦åˆ†å¸ƒã‚’ä»®å®šã™ã‚‹ã€ã®ã¯ã€è„³ãŒã€Œä¸–ç•Œã¯æ»‘ã‚‰ã‹ã ã€ã¨ä»®å®šã™ã‚‹ã®ã¨åŒã˜ã‹ã‚‚ã—ã‚Œãªã„ã€‚
:::

ã•ã‚‰ã«è€ƒãˆã¦ã¿ã‚ˆã†:

- **LLMã®å‡ºåŠ›åˆ†å¸ƒã¯Categoricalã€‚** æ­£è¦åˆ†å¸ƒã§ã¯ãªã„ã€‚ã ãŒCategoricalåˆ†å¸ƒã®è‡ªç„¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆlogitï¼‰ã¯é€£ç¶šå€¤ã§ã€ãã®ç©ºé–“ã§ã¯æ­£è¦åˆ†å¸ƒçš„ãªä»®å®šãŒä½¿ã‚ã‚Œã‚‹
- **æ¬¡å…ƒã®å‘ªã„**: 100æ¬¡å…ƒã®ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®ã‚µãƒ³ãƒ—ãƒ«ã¯ã€ã»ã¼ç¢ºå®Ÿã«åŸç‚¹ã‹ã‚‰ $\sqrt{100} = 10$ ã®è·é›¢ã«ã‚ã‚‹ã€‚ã€Œé«˜æ¬¡å…ƒã®ã‚¬ã‚¦ã‚¹ã¯çƒæ®»ã«é›†ä¸­ã™ã‚‹ã€â€” ã“ã‚ŒãŒæ­£è¦åˆ†å¸ƒã®ç›´æ„ŸãŒå´©å£Šã™ã‚‹ç¬é–“ã 
- **æ­£è¦åˆ†å¸ƒã¯"æœ€ã‚‚ç„¡çŸ¥ãª"åˆ†å¸ƒ**: æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åŸç†ã«ã‚ˆã‚Šã€å¹³å‡ã¨åˆ†æ•£ã—ã‹çŸ¥ã‚‰ãªã„ã¨ãã€ä½™è¨ˆãªä»®å®šã‚’æœ€ã‚‚å°‘ãªãã™ã‚‹åˆ†å¸ƒãŒã‚¬ã‚¦ã‚¹ã€‚ã€ŒçŸ¥ã‚‰ãªã„ã“ã¨ã‚’æ­£ç›´ã«èªã‚ã‚‹åˆ†å¸ƒã€ã¨ã‚‚è¨€ãˆã‚‹

```python
import numpy as np

# High-dimensional Gaussian: samples concentrate on a thin shell
dims = [2, 10, 100, 1000]
n_samples = 10000

print("=== High-dimensional Gaussian Concentration ===")
print(f"{'d':>6} {'E[||x||]':>10} {'âˆšd':>8} {'Std':>8} {'Std/Mean':>10}")
print("-" * 45)
for d in dims:
    samples = np.random.normal(0, 1, (n_samples, d))
    norms = np.linalg.norm(samples, axis=1)
    print(f"{d:>6} {norms.mean():>10.4f} {np.sqrt(d):>8.4f} {norms.std():>8.4f} {norms.std()/norms.mean():>10.4f}")

print("\nâ†’ In high dimensions, ALL samples are near distance âˆšd from origin")
print("â†’ The 'center' of a Gaussian is EMPTY in high dimensions!")
print("â†’ This is why VAE latent spaces need careful design (ç¬¬10å›)")
```

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Bayes, T., Price, R. (1763). "An Essay towards solving a Problem in the Doctrine of Chances." *Philosophical Transactions of the Royal Society of London*, 53, 370-418.
@[card](https://doi.org/10.1098/rstl.1763.0053)

[^2]: Kingma, D.P., Welling, M. (2013). "Auto-Encoding Variational Bayes." *arXiv preprint*.
@[card](https://arxiv.org/abs/1312.6114)

[^3]: Hinton, G., Vinyals, O., Dean, J. (2015). "Distilling the Knowledge in a Neural Network." *arXiv preprint*.
@[card](https://arxiv.org/abs/1503.02531)

[^4]: Ho, J., Jain, A., Abbeel, P. (2020). "Denoising Diffusion Probabilistic Models." *NeurIPS*.
@[card](https://arxiv.org/abs/2006.11239)

[^5]: Malach, E. (2023). "Auto-Regressive Next-Token Predictors are Universal Learners." *arXiv preprint*.
@[card](https://arxiv.org/abs/2309.06979)

[^6]: Kolmogorov, A.N. (1933). *Grundbegriffe der Wahrscheinlichkeitsrechnung*. Springer. English translation: *Foundations of the Theory of Probability* (1956).
@[card](https://www.york.ac.uk/depts/maths/histstat/kolmogorov_foundations.pdf)
â€»å¤–éƒ¨å¤§å­¦PDFã®ãŸã‚ãƒªãƒ³ã‚¯åˆ‡ã‚Œã®å¯èƒ½æ€§ã‚ã‚Šï¼ˆãƒŸãƒ©ãƒ¼: [Internet Archive](https://archive.org/details/kolmogorov_202112) ã‚‚å‚ç…§ï¼‰

[^7]: LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., Huang, F.J. (2006). "A Tutorial on Energy-Based Learning." *Predicting Structured Data*, MIT Press.

[^8]: CramÃ©r, H. (1946). *Mathematical Methods of Statistics*. Princeton University Press. Rao, C.R. (1945). "Information and the Accuracy Attainable in the Estimation of Statistical Parameters." *Bulletin of the Calcutta Mathematical Society*, 37, 81-91.

[^9]: HyvÃ¤rinen, A. (2005). "Estimation of Non-Normalized Statistical Models by Score Matching." *Journal of Machine Learning Research*, 6, 695-709.

[^10]: Song, Y., Sohl-Dickstein, J., Kingma, D.P., Kumar, A., Ermon, S., Poole, B. (2020). "Score-Based Generative Modeling through Stochastic Differential Equations." *ICLR 2021 (Oral)*.
@[card](https://arxiv.org/abs/2011.13456)

[^11]: Rezende, D.J., Mohamed, S. (2015). "Variational Inference with Normalizing Flows." *ICML 2015*.
@[card](https://arxiv.org/abs/1505.05770)

[^12]: Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W. (2021). "LoRA: Low-Rank Adaptation of Large Language Models." *ICLR 2022*.
@[card](https://arxiv.org/abs/2106.09685)

### æ•™ç§‘æ›¸

- Bishop, C.M. (2006). *Pattern Recognition and Machine Learning*. Springer. [PDF available from Microsoft Research]
- Murphy, K.P. (2022). *Probabilistic Machine Learning: An Introduction*. MIT Press. [Free online]
- Wasserman, L. (2004). *All of Statistics*. Springer.
- Casella, G., Berger, R.L. (2002). *Statistical Inference*. 2nd ed. Duxbury/Thomson.

---

## è¨˜æ³•è¦ç´„

| è¨˜å· | æ„å‘³ | åˆå‡º |
|:-----|:-----|:-----|
| $\Omega$ | æ¨™æœ¬ç©ºé–“ | 3.1 |
| $\mathcal{F}$ | Ïƒ-åŠ æ³•æ— | 3.1 |
| $P$ | ç¢ºç‡æ¸¬åº¦ | 3.1 |
| $X, Y, Z$ | ç¢ºç‡å¤‰æ•° | 3.2 |
| $\mathbb{E}[\cdot]$ | æœŸå¾…å€¤ | 3.2 |
| $\text{Var}(\cdot)$ | åˆ†æ•£ | 3.2 |
| $\text{Cov}(\cdot, \cdot)$ | å…±åˆ†æ•£ | 3.2 |
| $\boldsymbol{\mu}$ | å¹³å‡ãƒ™ã‚¯ãƒˆãƒ« | 3.5 |
| $\boldsymbol{\Sigma}$ | å…±åˆ†æ•£è¡Œåˆ— | 3.5 |
| $\boldsymbol{\Lambda}$ | ç²¾åº¦è¡Œåˆ— $= \boldsymbol{\Sigma}^{-1}$ | 3.5 |
| $\boldsymbol{\eta}$ | è‡ªç„¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | 3.6 |
| $\mathbf{T}(\mathbf{x})$ | ååˆ†çµ±è¨ˆé‡ | 3.6 |
| $A(\boldsymbol{\eta})$ | å¯¾æ•°æ­£è¦åŒ–å®šæ•° | 3.6 |
| $\theta$ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | 3.7 |
| $\hat{\theta}$ | æ¨å®šé‡ | 3.7 |
| $I(\theta)$ | Fisheræƒ…å ±é‡ | 3.8 |
| $\mathcal{D}$ | ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ | 3.3 |
| $\sim$ | ã€Œã«ã—ãŸãŒã†ã€ | å…¨èˆ¬ |
| $\propto$ | æ¯”ä¾‹ã™ã‚‹ | 3.3 |
| $\xrightarrow{P}$ | ç¢ºç‡åæŸ | 3.11 |
| $\xrightarrow{d}$ | åˆ†å¸ƒåæŸ | 3.11 |
| $\xrightarrow{\text{a.s.}}$ | æ¦‚åæŸ | 3.11 |
| $\overset{\text{i.i.d.}}{\sim}$ | ç‹¬ç«‹åŒåˆ†å¸ƒ | 3.2 |
| $\bar{X}_N$ | æ¨™æœ¬å¹³å‡ $\frac{1}{N}\sum X_i$ | 3.11 |
| $\ell(\theta)$ | å¯¾æ•°å°¤åº¦ | 3.7 |
| $s(\mathbf{x}; \theta)$ | ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_\theta \log p$ | 3.8 |
| $h(\mathbf{x})$ | åŸºåº•æ¸¬åº¦ | 3.6 |
| $\boldsymbol{\pi}$ | Categorical/Dirichletã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | 1.1 |
| $\Gamma(\cdot)$ | ã‚¬ãƒ³ãƒé–¢æ•° | 3.4 |
| $\binom{n}{k}$ | äºŒé …ä¿‚æ•° | 3.4 |
| $|\boldsymbol{\Sigma}|$ | è¡Œåˆ—å¼ | 3.5 |
| $\delta(\cdot)$ | ãƒ‡ã‚£ãƒ©ãƒƒã‚¯ã®ãƒ‡ãƒ«ã‚¿é–¢æ•° | 4.5 |

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
