---
title: "ç¬¬3å›: ç·šå½¢ä»£æ•° II: SVDãƒ»è¡Œåˆ—å¾®åˆ†ãƒ»ãƒ†ãƒ³ã‚½ãƒ« â€” ä¸‡èƒ½ãƒŠã‚¤ãƒ•SVDã¨é€†ä¼æ’­ã®æ•°å­¦ ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
emoji: "ğŸ”¬"
type: "tech"
topics: ["machinelearning", "deeplearning", "linearalgebra", "python"]
published: true
---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” SVDã¨è‡ªå‹•å¾®åˆ†ã‚’ã‚³ãƒ¼ãƒ‰ã§æ“ã‚‹

### 4.1 SVDç”»åƒåœ§ç¸®ã®å®Œå…¨å®Ÿè£…

```python
import numpy as np

def svd_compress(A, k):
    """Compress matrix A to rank-k using SVD"""
    U, s, Vt = np.linalg.svd(A, full_matrices=False)
    return U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :], s

def compression_stats(m, n, k):
    """Calculate compression statistics"""
    original = m * n
    compressed = k * (m + n + 1)
    return compressed / original

# Demo with synthetic image
np.random.seed(42)
m, n = 256, 192
# Create structured image: smooth gradients + edges
x = np.linspace(0, 8*np.pi, m)
y = np.linspace(0, 6*np.pi, n)
X, Y = np.meshgrid(y, x)
image = (np.sin(X) * np.cos(Y) + 0.3 * np.sin(3*X + 2*Y) +
         0.5 * np.sign(np.sin(X/2)))
image += 0.05 * np.random.randn(m, n)  # small noise

_, s = np.linalg.svd(image, full_matrices=False)

print("SVD Image Compression Results:")
print(f"Image size: {m}x{n} = {m*n:,} values")
print(f"{'rank':>6} {'ratio':>10} {'error':>10} {'PSNR(dB)':>10}")
print("-" * 42)

for k in [1, 5, 10, 20, 50, 100]:
    A_k, _ = svd_compress(image, k)
    ratio = compression_stats(m, n, k)
    mse = np.mean((image - A_k)**2)
    max_val = np.max(np.abs(image))
    psnr = 10 * np.log10(max_val**2 / mse) if mse > 0 else float('inf')
    rel_error = np.linalg.norm(image - A_k, 'fro') / np.linalg.norm(image, 'fro')
    print(f"{k:6d} {ratio:9.1%} {rel_error:10.6f} {psnr:10.2f}")
```

### 4.2 Randomized SVD â€” å¤§è¦æ¨¡è¡Œåˆ—ã®åŠ¹ç‡çš„ãªSVD

é€šå¸¸ã®SVDã¯ $O(\min(mn^2, m^2n))$ã€‚æ•°ä¸‡Ã—æ•°ä¸‡ã®è¡Œåˆ—ã«ã¯é…ã™ãã‚‹ã€‚**Randomized SVD** ã¯ãƒ©ãƒ³ãƒ€ãƒ å°„å½±ã§æ¬¡å…ƒã‚’è½ã¨ã—ã¦ã‹ã‚‰SVDã‚’è¨ˆç®—ã™ã‚‹ã€‚

```python
import numpy as np

def randomized_svd(A, k, n_oversamples=10, n_iter=2):
    """
    Randomized SVD (Halko, Martinsson, Tropp 2011)

    Parameters:
        A: (m, n) matrix
        k: target rank
        n_oversamples: oversampling parameter (default 10)
        n_iter: power iterations for accuracy (default 2)

    Returns:
        U, s, Vt: truncated SVD components
    """
    m, n = A.shape
    p = k + n_oversamples

    # Step 1: Random projection
    Omega = np.random.randn(n, p)
    Y = A @ Omega  # (m, p)

    # Step 2: Power iteration (improves accuracy for slow singular value decay)
    for _ in range(n_iter):
        Y = A @ (A.T @ Y)

    # Step 3: QR factorization of Y
    Q, _ = np.linalg.qr(Y)  # (m, p) orthonormal

    # Step 4: Form small matrix and compute its SVD
    B = Q.T @ A  # (p, n) â€” much smaller!
    U_hat, s, Vt = np.linalg.svd(B, full_matrices=False)

    # Step 5: Recover left singular vectors
    U = Q @ U_hat

    return U[:, :k], s[:k], Vt[:k, :]

# Benchmark
np.random.seed(42)
m, n, true_rank = 5000, 3000, 20
U_true = np.linalg.qr(np.random.randn(m, true_rank))[0]
V_true = np.linalg.qr(np.random.randn(n, true_rank))[0]
s_true = np.logspace(1, -1, true_rank)
A = U_true @ np.diag(s_true) @ V_true.T + 0.01 * np.random.randn(m, n)

import time

# Full SVD
t0 = time.time()
U_f, s_f, Vt_f = np.linalg.svd(A, full_matrices=False)
t_full = time.time() - t0

# Randomized SVD
k = 20
t0 = time.time()
U_r, s_r, Vt_r = randomized_svd(A, k)
t_rand = time.time() - t0

A_full = U_f[:, :k] @ np.diag(s_f[:k]) @ Vt_f[:k, :]
A_rand = U_r @ np.diag(s_r) @ Vt_r

print(f"Matrix size: {m}x{n}")
print(f"Full SVD:       {t_full:.3f}s, error = {np.linalg.norm(A - A_full, 'fro'):.6f}")
print(f"Randomized SVD: {t_rand:.3f}s, error = {np.linalg.norm(A - A_rand, 'fro'):.6f}")
print(f"Speedup: {t_full/t_rand:.1f}x")
```

### 4.3 Reverse Mode è‡ªå‹•å¾®åˆ†ã®å®Œå…¨å®Ÿè£…

Zone 1.5 ã§ç°¡æ˜“ç‰ˆã‚’ç¤ºã—ãŸã€‚ã“ã“ã§ã¯ã‚ˆã‚Šæœ¬æ ¼çš„ãªå®Ÿè£…ã‚’ç¤ºã™ã€‚

```python
import numpy as np

class Value:
    """Scalar autograd engine (Reverse Mode AD)"""
    def __init__(self, data, _children=(), _op='', label=''):
        self.data = float(data)
        self.grad = 0.0
        self._backward = lambda: None
        self._prev = set(_children)
        self._op = _op
        self.label = label

    def __repr__(self):
        return f"Value({self.data:.4f}, grad={self.grad:.4f})"

    def __add__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        out = Value(self.data + other.data, (self, other), '+')
        def _backward():
            self.grad += out.grad
            other.grad += out.grad
        out._backward = _backward
        return out

    def __mul__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        out = Value(self.data * other.data, (self, other), '*')
        def _backward():
            self.grad += other.data * out.grad
            other.grad += self.data * out.grad
        out._backward = _backward
        return out

    def __pow__(self, other):
        assert isinstance(other, (int, float))
        out = Value(self.data ** other, (self,), f'**{other}')
        def _backward():
            self.grad += other * (self.data ** (other - 1)) * out.grad
        out._backward = _backward
        return out

    def __neg__(self):
        return self * -1

    def __sub__(self, other):
        return self + (-other)

    def __truediv__(self, other):
        return self * other**-1

    def __radd__(self, other):
        return self + other

    def __rmul__(self, other):
        return self * other

    def exp(self):
        out = Value(np.exp(self.data), (self,), 'exp')
        def _backward():
            self.grad += out.data * out.grad
        out._backward = _backward
        return out

    def log(self):
        out = Value(np.log(self.data), (self,), 'log')
        def _backward():
            self.grad += (1.0 / self.data) * out.grad
        out._backward = _backward
        return out

    def tanh(self):
        t = np.tanh(self.data)
        out = Value(t, (self,), 'tanh')
        def _backward():
            self.grad += (1 - t**2) * out.grad
        out._backward = _backward
        return out

    def backward(self):
        topo = []
        visited = set()
        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for child in v._prev:
                    build_topo(child)
                topo.append(v)
        build_topo(self)
        self.grad = 1.0
        for node in reversed(topo):
            node._backward()

# Demo: simple neural network loss
x1 = Value(2.0, label='x1')
x2 = Value(0.0, label='x2')
w1 = Value(-3.0, label='w1')
w2 = Value(1.0, label='w2')
b = Value(6.8813735870195432, label='b')

# Forward: neuron
n = x1*w1 + x2*w2 + b
o = n.tanh()

# Backward
o.backward()

print("Forward:  o =", o)
print("Gradients:")
print(f"  do/dx1 = {x1.grad:.4f}")
print(f"  do/dx2 = {x2.grad:.4f}")
print(f"  do/dw1 = {w1.grad:.4f}")
print(f"  do/dw2 = {w2.grad:.4f}")
print(f"  do/db  = {b.grad:.4f}")
```

### 4.4 æ¡ä»¶æ•°ã¨æ•°å€¤å®‰å®šæ€§

#### IEEE 754 æµ®å‹•å°æ•°ç‚¹

| å‹ | ãƒ“ãƒƒãƒˆæ•° | ä»®æ•°éƒ¨ | æŒ‡æ•°éƒ¨ | æœ‰åŠ¹æ¡ | ç¯„å›² |
|:---|:--------|:------|:------|:------|:-----|
| float16 (half) | 16 | 10 | 5 | ~3.3æ¡ | $\pm 6.5 \times 10^4$ |
| bfloat16 | 16 | 7 | 8 | ~2.4æ¡ | $\pm 3.4 \times 10^{38}$ |
| float32 (single) | 32 | 23 | 8 | ~7.2æ¡ | $\pm 3.4 \times 10^{38}$ |
| float64 (double) | 64 | 52 | 11 | ~15.9æ¡ | $\pm 1.8 \times 10^{308}$ |

**æ©Ÿæ¢°å­¦ç¿’ã§ã®ä½¿ã„åˆ†ã‘**: æ¨è«–=float16/bfloat16ã€å­¦ç¿’=float32(ãƒã‚¹ã‚¿ãƒ¼ã‚¦ã‚§ã‚¤ãƒˆ)+bfloat16(forward/backward)ã€ç²¾å¯†è¨ˆç®—=float64ã€‚

#### Log-Sum-Exp trick

Softmaxã®è¨ˆç®—ã§å¿…é ˆã®æ•°å€¤å®‰å®šåŒ–æŠ€æ³•:

$$
\log \sum_i e^{x_i} = c + \log \sum_i e^{x_i - c}, \quad c = \max_i x_i
$$

```python
import numpy as np

# Naive vs numerically stable log-sum-exp
x = np.array([1000.0, 1001.0, 1002.0])

# Naive: overflow!
try:
    naive = np.log(np.sum(np.exp(x)))
    print(f"Naive: {naive}")
except:
    print("Naive: OVERFLOW")

# Stable: subtract max
c = np.max(x)
stable = c + np.log(np.sum(np.exp(x - c)))
print(f"Stable: {stable:.4f}")
print(f"Expected: {1002 + np.log(np.exp(-2) + np.exp(-1) + 1):.4f}")
```

### 4.5 ã‚¹ãƒ‘ãƒ¼ã‚¹è¡Œåˆ—ã¨GPUä¸Šã®è¡Œåˆ—æ¼”ç®—

#### ã‚¹ãƒ‘ãƒ¼ã‚¹è¡Œåˆ—

å¤§è¦æ¨¡ãªè¡Œåˆ—ã®å¤šãã¯ã‚¹ãƒ‘ãƒ¼ã‚¹ï¼ˆã»ã¨ã‚“ã©ã®è¦ç´ ãŒ0ï¼‰ã€‚SciPyã®ç–è¡Œåˆ—è¡¨ç¾ã‚’ä½¿ã†ã¨ãƒ¡ãƒ¢ãƒªã¨è¨ˆç®—é‡ã‚’å¤§å¹…ã«å‰Šæ¸›ã§ãã‚‹ã€‚

```python
import numpy as np
from scipy import sparse
from scipy.sparse.linalg import svds

# Dense vs Sparse comparison
n = 10000
density = 0.001  # 0.1% non-zero

# Create sparse matrix
A_sparse = sparse.random(n, n, density=density, format='csr')
print(f"Matrix size: {n}x{n} = {n**2:,} elements")
print(f"Non-zero: {A_sparse.nnz:,} ({density:.1%})")
print(f"Dense memory: {n**2 * 8 / 1e6:.1f} MB")
print(f"Sparse memory: {(A_sparse.data.nbytes + A_sparse.indices.nbytes + A_sparse.indptr.nbytes) / 1e6:.1f} MB")

# Sparse SVD (top-k singular values only)
import time
k = 10
t0 = time.time()
U, s, Vt = svds(A_sparse, k=k)
t_sparse = time.time() - t0
print(f"\nSparse SVD (top-{k}): {t_sparse:.3f}s")
print(f"Top singular values: {np.round(s[::-1][:5], 4)}")
```

| å½¢å¼ | èª¬æ˜ | é•·æ‰€ | ç”¨é€” |
|:-----|:-----|:-----|:-----|
| CSR (Compressed Sparse Row) | è¡Œã”ã¨ã«éã‚¼ãƒ­è¦ç´ ã‚’æ ¼ç´ | è¡Œã‚¹ãƒ©ã‚¤ã‚¹ãŒé«˜é€Ÿ | è¡Œåˆ—Ã—ãƒ™ã‚¯ãƒˆãƒ« |
| CSC (Compressed Sparse Column) | åˆ—ã”ã¨ã«éã‚¼ãƒ­è¦ç´ ã‚’æ ¼ç´ | åˆ—ã‚¹ãƒ©ã‚¤ã‚¹ãŒé«˜é€Ÿ | è»¢ç½®æ“ä½œ |
| COO (Coordinate) | (è¡Œ, åˆ—, å€¤) ã®ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆ | æ§‹ç¯‰ãŒç°¡å˜ | åˆæœŸæ§‹ç¯‰ |

#### GPUä¸Šã®è¡Œåˆ—æ¼”ç®—

| ãƒ©ã‚¤ãƒ–ãƒ©ãƒª | ç”¨é€” | ç‰¹å¾´ |
|:----------|:-----|:-----|
| cuBLAS | å¯†è¡Œåˆ—æ¼”ç®— | NVIDIA GPUä¸Šã®BLAS |
| cuSPARSE | ç–è¡Œåˆ—æ¼”ç®— | GPUä¸Šã®ã‚¹ãƒ‘ãƒ¼ã‚¹è¡Œåˆ—ç© |
| cuSOLVER | å›ºæœ‰å€¤åˆ†è§£ãƒ»SVD | GPUä¸Šã®LAPACK |
| Tensor Core | æ··åˆç²¾åº¦è¡Œåˆ—ç© | FP16/BF16ã§é«˜é€ŸåŒ– |

```python
# PyTorch GPU example (conceptual)
# import torch
#
# A = torch.randn(4096, 4096, device='cuda', dtype=torch.float16)
# B = torch.randn(4096, 4096, device='cuda', dtype=torch.float16)
#
# # Tensor Core accelerated matrix multiply
# C = torch.mm(A, B)  # uses Tensor Core if available
#
# # GPU SVD
# U, s, Vt = torch.linalg.svd(A.float())
```

### 4.6 ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—ã®é«˜åº¦ãªãƒ‘ã‚¿ãƒ¼ãƒ³

Einsteinã®ç¸®ç´„è¨˜æ³•ã¨NumPyã®`einsum`ã¯ã€è¤‡é›‘ãªãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—ã‚’ç°¡æ½”ã«è¡¨ç¾ã™ã‚‹å¼·åŠ›ãªãƒ„ãƒ¼ãƒ«ã ã€‚ã“ã“ã§ã¯Transformerå®Ÿè£…ã§é »å‡ºã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¶²ç¾…ã™ã‚‹ã€‚

```python
import numpy as np

# Setup: Batch=2, Heads=4, SeqLen=8, HeadDim=16
B, H, T, d = 2, 4, 8, 16

Q = np.random.randn(B, H, T, d)  # Query
K = np.random.randn(B, H, T, d)  # Key
V = np.random.randn(B, H, T, d)  # Value

# Pattern 1: Attention scores (Q @ K^T)
# Naive: for b, h, i, j: scores[b,h,i,j] = Î£â‚– Q[b,h,i,k] * K[b,h,j,k]
scores_loop = np.zeros((B, H, T, T))
for b in range(B):
    for h in range(H):
        scores_loop[b, h] = Q[b, h] @ K[b, h].T

# einsum: 'bhik,bhjk->bhij'
scores_einsum = np.einsum('bhik,bhjk->bhij', Q, K)

# Verify
assert np.allclose(scores_loop, scores_einsum)
print("âœ“ Pattern 1: Q @ K^T")

# Pattern 2: Scaled dot-product attention
scores = scores_einsum / np.sqrt(d)
attn = np.exp(scores - scores.max(axis=-1, keepdims=True))  # numerical stability
attn = attn / attn.sum(axis=-1, keepdims=True)  # softmax

# Apply attention to values: O[b,h,i,k] = Î£â±¼ attn[b,h,i,j] * V[b,h,j,k]
output_loop = np.zeros((B, H, T, d))
for b in range(B):
    for h in range(H):
        output_loop[b, h] = attn[b, h] @ V[b, h]

output_einsum = np.einsum('bhij,bhjk->bhik', attn, V)
assert np.allclose(output_loop, output_einsum)
print("âœ“ Pattern 2: Attn @ V")

# Pattern 3: Multi-head concatenation and projection
# Flatten heads: (B, H, T, d) -> (B, T, H*d)
output_concat = output_einsum.transpose(0, 2, 1, 3).reshape(B, T, H * d)

# Alternative using einsum reshape
# Not directly supported, but can use reshape + einsum for projection

W_out = np.random.randn(H * d, H * d)  # output projection
final_output = output_concat @ W_out
print(f"âœ“ Pattern 3: Multi-head concat -> shape {final_output.shape}")

# Pattern 4: Layer normalization gradient
# Given: x (B, T, d), Î³ (d,), Î² (d,)
# LN(x) = Î³ * (x - Î¼) / Ïƒ + Î²
x = np.random.randn(B, T, d)
gamma = np.random.randn(d)
beta = np.random.randn(d)

mu = x.mean(axis=-1, keepdims=True)
sigma = x.std(axis=-1, keepdims=True)
x_norm = (x - mu) / (sigma + 1e-5)
y = gamma * x_norm + beta

# Gradient w.r.t. input (simplified, assuming dy/dx chain rule)
# This is complex; showing structure only
dy = np.random.randn(B, T, d)  # upstream gradient
dx_norm = dy * gamma  # element-wise

# Full gradient includes sigma and mu terms
# d(sigma)/dx = (x - mu) / (d * sigma)
# d(mu)/dx = 1/d
# Chain rule: complex but mechanical
print("âœ“ Pattern 4: LayerNorm gradient structure")

# Pattern 5: Batch matrix multiplication with different shapes
A = np.random.randn(B, 10, 20)
B_mat = np.random.randn(B, 20, 30)
C = np.einsum('bij,bjk->bik', A, B_mat)
assert C.shape == (B, 10, 30)
print("âœ“ Pattern 5: Batched matmul")

# Pattern 6: Outer product in batch
v1 = np.random.randn(B, T, d)
v2 = np.random.randn(B, T, d)
outer = np.einsum('bti,btj->btij', v1, v2)
assert outer.shape == (B, T, d, d)
print("âœ“ Pattern 6: Batched outer product")

# Pattern 7: Trace over specific dimensions
# Compute trace of outer[b, t, :, :] for all b, t
traces = np.einsum('btii->bt', outer)
assert traces.shape == (B, T)
print("âœ“ Pattern 7: Batched trace")

print("\n=== einsum Performance Tips ===")
print("1. Use optimize='optimal' for complex contractions")
print("2. Explicit is better than implicit: write all indices")
print("3. Profile: sometimes @ is faster for simple matmul")
print("4. Memory: einsum can create large intermediate tensors")
```

:::message
**einsum ã®ã‚³ã‚¹ãƒˆ**: `einsum` ã¯å¯èª­æ€§ãŒé«˜ã„ãŒã€å¸¸ã«æœ€é€Ÿã¨ã¯é™ã‚‰ãªã„ã€‚2ã¤ã®è¡Œåˆ—ã®ç©ã§ã¯ `@` æ¼”ç®—å­ã®æ–¹ãŒ BLAS ãƒ«ãƒ¼ãƒãƒ³ã«ç›´æ¥ãƒãƒƒãƒ—ã•ã‚Œé«˜é€Ÿãªå ´åˆãŒã‚ã‚‹ã€‚`optimize='optimal'` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ç¸®ç´„é †åºã‚’æœ€é©åŒ–ã™ã‚‹ãŒã€æœ€æ‚ªã‚±ãƒ¼ã‚¹ã§ã¯æŒ‡æ•°æ™‚é–“ã‹ã‹ã‚‹ï¼ˆå°è¦æ¨¡ãƒ†ãƒ³ã‚½ãƒ«ãªã‚‰å•é¡Œãªã„ï¼‰ã€‚
:::

### 4.7 è¡Œåˆ—åˆ†è§£ã®å¿œç”¨: QRåˆ†è§£ã¨Choleskyåˆ†è§£

SVDä»¥å¤–ã®è¡Œåˆ—åˆ†è§£ã‚‚ã€æ•°å€¤è¨ˆç®—ã§é‡è¦ãªå½¹å‰²ã‚’æœãŸã™ã€‚

#### QRåˆ†è§£: ç›´äº¤åŒ–ã¨æœ€å°äºŒä¹—æ³•

$$
A = QR, \quad Q^\top Q = I, \quad R \text{ ã¯ä¸Šä¸‰è§’}
$$

QRåˆ†è§£ã¯ã€åˆ—ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç›´äº¤åŒ–ã™ã‚‹**Gram-Schmidtæ³•**ã®å®‰å®šåŒ–ç‰ˆã ã€‚æœ€å°äºŒä¹—å•é¡Œ $\min_\mathbf{x} \|A\mathbf{x} - \mathbf{b}\|^2$ ã‚’è§£ãã¨ãã€$A = QR$ ã¨ã™ã‚‹ã¨:

$$
A^\top A \mathbf{x} = A^\top \mathbf{b} \quad \Rightarrow \quad R^\top Q^\top Q R \mathbf{x} = R^\top Q^\top \mathbf{b} \quad \Rightarrow \quad R \mathbf{x} = Q^\top \mathbf{b}
$$

$R$ ã¯ä¸Šä¸‰è§’ãªã®ã§ã€å¾Œé€€ä»£å…¥ã§ $O(n^2)$ ã§è§£ã‘ã‚‹ã€‚

```python
import numpy as np

# QR decomposition for least squares
A = np.random.randn(100, 50)  # overdetermined system
b = np.random.randn(100)

# Method 1: Normal equations (numerically unstable for ill-conditioned A)
x_normal = np.linalg.solve(A.T @ A, A.T @ b)

# Method 2: QR decomposition (stable)
Q, R = np.linalg.qr(A)
x_qr = np.linalg.solve(R, Q.T @ b)

# Method 3: SVD (most stable, but slowest)
U, s, Vt = np.linalg.svd(A, full_matrices=False)
x_svd = Vt.T @ (np.diag(1/s) @ (U.T @ b))

print(f"Normal equations: {np.linalg.norm(A @ x_normal - b):.6f}")
print(f"QR decomposition: {np.linalg.norm(A @ x_qr - b):.6f}")
print(f"SVD (pseudoinverse): {np.linalg.norm(A @ x_svd - b):.6f}")
print(f"\nSolution agreement (QR vs SVD): {np.allclose(x_qr, x_svd)}")

# Condition number matters
print(f"Condition number: {np.linalg.cond(A):.2e}")
```

#### Choleskyåˆ†è§£: æ­£å®šå€¤è¡Œåˆ—ã®é«˜é€Ÿåˆ†è§£

$$
A = LL^\top, \quad L \text{ ã¯ä¸‹ä¸‰è§’}
$$

æ­£å®šå€¤å¯¾ç§°è¡Œåˆ— $A$ ï¼ˆä¾‹: å…±åˆ†æ•£è¡Œåˆ—ï¼‰ã«å¯¾ã—ã¦ã€Choleskyåˆ†è§£ã¯ $O(n^3/3)$ ã§è¨ˆç®—ã§ãã€LUåˆ†è§£ã®2å€é«˜é€Ÿã ã€‚

**å¿œç”¨: å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**

$$
\mathbf{z} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma}) \quad \Leftrightarrow \quad \mathbf{z} = \boldsymbol{\mu} + L \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})
$$

$\boldsymbol{\Sigma} = LL^\top$ ã¨Choleskyåˆ†è§£ã™ã‚Œã°ã€æ¨™æº–æ­£è¦åˆ†å¸ƒã®ã‚µãƒ³ãƒ—ãƒ« $\boldsymbol{\epsilon}$ ã‚’ç·šå½¢å¤‰æ›ã™ã‚‹ã ã‘ã§ã€ä»»æ„ã®å…±åˆ†æ•£ã‚’æŒã¤ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§ãã‚‹ã€‚

```python
import numpy as np

# Cholesky decomposition for sampling
mu = np.array([1.0, 2.0, 3.0])
Sigma = np.array([[2.0, 0.5, 0.3],
                   [0.5, 1.5, 0.2],
                   [0.3, 0.2, 1.0]])

# Verify positive definite
eigenvalues = np.linalg.eigvalsh(Sigma)
print(f"Eigenvalues: {eigenvalues}")
assert np.all(eigenvalues > 0), "Matrix must be positive definite"

# Cholesky decomposition
L = np.linalg.cholesky(Sigma)
print(f"L @ L.T matches Sigma: {np.allclose(L @ L.T, Sigma)}")

# Sampling
n_samples = 10000
epsilon = np.random.randn(n_samples, 3)
samples = mu + epsilon @ L.T  # broadcasting

# Verify sample statistics
sample_mean = samples.mean(axis=0)
sample_cov = np.cov(samples.T)

print(f"\nTrue mean: {mu}")
print(f"Sample mean: {sample_mean}")
print(f"\nTrue covariance:\n{Sigma}")
print(f"Sample covariance:\n{sample_cov}")
print(f"Covariance error: {np.linalg.norm(sample_cov - Sigma):.4f}")
```

### 4.8 è¡Œåˆ—å¾®åˆ†ã®æ•°å€¤æ¤œè¨¼ãƒ‘ã‚¿ãƒ¼ãƒ³

è¡Œåˆ—å¾®åˆ†ã‚’æ‰‹ã§å°å‡ºã—ãŸã‚‰ã€å¿…ãšæ•°å€¤å¾®åˆ†ã§æ¤œè¨¼ã™ã‚‹ã€‚ã“ã‚Œã¯ç ”ç©¶ã§ã‚‚å®Ÿå‹™ã§ã‚‚ä¸å¯æ¬ ãªãƒ‡ãƒãƒƒã‚°æ‰‹æ³•ã€‚

```python
import numpy as np

def numerical_gradient_matrix(f, X, eps=1e-7):
    """Compute numerical gradient df/dX for scalar-valued f"""
    grad = np.zeros_like(X)
    m, n = X.shape
    for i in range(m):
        for j in range(n):
            X_plus = X.copy(); X_plus[i, j] += eps
            X_minus = X.copy(); X_minus[i, j] -= eps
            grad[i, j] = (f(X_plus) - f(X_minus)) / (2 * eps)
    return grad

# Example: verify d/dX tr(AXB) = A^T B^T
A = np.random.randn(3, 4)
B = np.random.randn(5, 3)
X = np.random.randn(4, 5)

def f_trace(X_):
    return np.trace(A @ X_ @ B)

grad_analytical = A.T @ B.T
grad_numerical = numerical_gradient_matrix(f_trace, X)

print(f"d/dX tr(AXB) = A^T B^T")
print(f"Match: {np.allclose(grad_analytical, grad_numerical)}")
print(f"Max error: {np.max(np.abs(grad_analytical - grad_numerical)):.2e}")

# Example: verify d/dX ||X||_F^2 = 2X
def f_frob(X_):
    return np.sum(X_**2)

grad_analytical_2 = 2 * X
grad_numerical_2 = numerical_gradient_matrix(f_frob, X)
print(f"\nd/dX ||X||_F^2 = 2X")
print(f"Match: {np.allclose(grad_analytical_2, grad_numerical_2)}")

# Example: verify d/dX ln det(X) = X^{-T}
X_sq = np.random.randn(4, 4)
X_sq = X_sq @ X_sq.T + 2 * np.eye(4)  # positive definite

def f_logdet(X_):
    return np.log(np.linalg.det(X_))

grad_analytical_3 = np.linalg.inv(X_sq).T
grad_numerical_3 = numerical_gradient_matrix(f_logdet, X_sq)
print(f"\nd/dX ln det(X) = X^{{-T}}")
print(f"Match: {np.allclose(grad_analytical_3, grad_numerical_3)}")
```

:::message
**å®Ÿè·µã®ãƒ«ãƒ¼ãƒ«**: è¡Œåˆ—å¾®åˆ†ã‚’å°å‡ºã—ãŸã‚‰ã€**å¿…ãš** `numerical_gradient_matrix` ã§æ¤œè¨¼ã™ã‚‹ã€‚ä¸€è‡´ã—ãªã‘ã‚Œã°å°å‡ºã«é–“é•ã„ãŒã‚ã‚‹ã€‚ã“ã®ç¿’æ…£ãŒã€Backpropã®å®Ÿè£…ãƒã‚°ã‚’é˜²ãã€‚
:::

### 4.9 è¡Œåˆ—å¾®åˆ†ã®ãƒ‡ãƒãƒƒã‚°æŠ€æ³•

å®Ÿè£…ã—ãŸå‹¾é…ãŒæ­£ã—ã„ã‹ç¢ºèªã™ã‚‹ãŸã‚ã®ä½“ç³»çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã€‚

```python
import numpy as np

def gradient_check(f, grad_f, X, eps=1e-7, rtol=1e-5, atol=1e-7, sample_size=10):
    """
    Comprehensive gradient checker for matrix-valued functions.

    Parameters:
        f: scalar-valued function f(X)
        grad_f: analytical gradient function, returns dL/dX
        X: input matrix
        eps: finite difference step size
        rtol: relative tolerance
        atol: absolute tolerance
        sample_size: number of random entries to check (for large matrices)

    Returns:
        dict with check results
    """
    grad_analytical = grad_f(X)
    grad_numerical = np.zeros_like(X)

    m, n = X.shape
    total_entries = m * n

    # For large matrices, sample random entries
    if total_entries > sample_size**2:
        indices = np.random.choice(total_entries, size=min(sample_size, total_entries), replace=False)
        check_indices = [(i // n, i % n) for i in indices]
    else:
        check_indices = [(i, j) for i in range(m) for j in range(n)]

    errors = []
    for i, j in check_indices:
        X_plus = X.copy(); X_plus[i, j] += eps
        X_minus = X.copy(); X_minus[i, j] -= eps
        grad_numerical[i, j] = (f(X_plus) - f(X_minus)) / (2 * eps)

        anal = grad_analytical[i, j]
        numer = grad_numerical[i, j]
        err = abs(anal - numer)
        rel_err = err / (abs(anal) + abs(numer) + 1e-10)
        errors.append((i, j, anal, numer, err, rel_err))

    # Summary statistics
    errors_arr = np.array([e[4] for e in errors])
    rel_errors_arr = np.array([e[5] for e in errors])

    max_error = errors_arr.max()
    max_rel_error = rel_errors_arr.max()
    mean_error = errors_arr.mean()

    # Check pass/fail
    passed = np.allclose(
        [e[2] for e in errors],  # analytical
        [e[3] for e in errors],  # numerical
        rtol=rtol, atol=atol
    )

    result = {
        'passed': passed,
        'max_absolute_error': max_error,
        'max_relative_error': max_rel_error,
        'mean_absolute_error': mean_error,
        'num_checked': len(check_indices),
        'worst_entries': sorted(errors, key=lambda x: x[5], reverse=True)[:5]
    }

    return result

# Example 1: Simple quadratic form dL/dW = 2X^T(XW - y)
def test_linear_regression_gradient():
    X = np.random.randn(50, 10)
    W = np.random.randn(10, 3)
    y = np.random.randn(50, 3)

    def loss(W_):
        return np.linalg.norm(X @ W_ - y)**2

    def grad_loss(W_):
        return 2 * X.T @ (X @ W_ - y)

    result = gradient_check(loss, grad_loss, W)

    print("=== Linear Regression Gradient Check ===")
    print(f"Passed: {result['passed']}")
    print(f"Max absolute error: {result['max_absolute_error']:.2e}")
    print(f"Max relative error: {result['max_relative_error']:.2e}")
    print(f"Entries checked: {result['num_checked']}")

    if not result['passed']:
        print("\nWorst 3 entries:")
        for i, j, anal, numer, err, rel_err in result['worst_entries'][:3]:
            print(f"  [{i},{j}]: analytical={anal:.6f}, numerical={numer:.6f}, rel_err={rel_err:.2e}")

# Example 2: Softmax gradient
def test_softmax_gradient():
    z = np.random.randn(5, 10)  # logits
    y = np.random.randint(0, 10, size=5)  # true labels

    def softmax(z_):
        e = np.exp(z_ - z_.max(axis=1, keepdims=True))
        return e / e.sum(axis=1, keepdims=True)

    def cross_entropy_loss(z_):
        s = softmax(z_)
        # Clip for numerical stability
        return -np.sum(np.log(s[range(len(y)), y] + 1e-10))

    def cross_entropy_grad(z_):
        s = softmax(z_)
        grad = s.copy()
        grad[range(len(y)), y] -= 1
        return grad

    result = gradient_check(cross_entropy_loss, cross_entropy_grad, z)

    print("\n=== Softmax + Cross-Entropy Gradient Check ===")
    print(f"Passed: {result['passed']}")
    print(f"Max absolute error: {result['max_absolute_error']:.2e}")
    print(f"Max relative error: {result['max_relative_error']:.2e}")

# Example 3: Matrix trace dL/dW = A^T
def test_trace_gradient():
    A = np.random.randn(8, 10)
    W = np.random.randn(10, 8)

    def loss(W_):
        return np.trace(A @ W_)

    def grad_loss(W_):
        return A.T

    result = gradient_check(loss, grad_loss, W)

    print("\n=== Trace Gradient Check ===")
    print(f"Passed: {result['passed']}")
    print(f"Max absolute error: {result['max_absolute_error']:.2e}")

# Run all tests
test_linear_regression_gradient()
test_softmax_gradient()
test_trace_gradient()

print("\n=== Debugging Tips ===")
print("1. Start with small matrices (3x3) where you can inspect all entries")
print("2. Use atol=1e-6 for typical float32, atol=1e-10 for float64")
print("3. If check fails: print worst entries and manually verify the formula")
print("4. Common bugs: forgot transpose, wrong sign, off-by-one in broadcasting")
print("5. Numerical gradient is O(Îµ) error; analytical should be O(ÎµÂ²) better")
```

### 4.10 å®Ÿè·µçš„ãªè¡Œåˆ—æ¼”ç®—ã®è½ã¨ã—ç©´

å®Ÿè£…ã§é »å‡ºã™ã‚‹ãƒã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨å›é¿æ³•ã€‚

```python
import numpy as np

print("=== Common Matrix Operation Pitfalls ===\n")

# Pitfall 1: In-place operations breaking autograd
A = np.array([[1.0, 2.0], [3.0, 4.0]])
B = A  # B is a view, not a copy!
B[0, 0] = 999
print(f"Pitfall 1: In-place modification")
print(f"  A[0,0] = {A[0,0]} (expected 1.0, got {A[0,0]})")
print(f"  Solution: Use A.copy() when you need independence\n")

# Pitfall 2: Broadcasting ambiguity
v = np.array([1, 2, 3])  # shape (3,) â€” is this row or column?
M = np.random.randn(3, 5)

result1 = M + v[:, None]  # explicit column: (3,1) broadcasts to (3,5)
result2 = M + v  # implicit: (3,) broadcasts to (3,5) â€” same as row broadcast!
# result3 = v + M  # same as result2

print(f"Pitfall 2: 1D array broadcasting ambiguity")
print(f"  v.shape = {v.shape} â€” neither row nor column!")
print(f"  M + v[:, None] shape: {result1.shape} (column broadcast)")
print(f"  M + v shape: {result2.shape} (row broadcast)")
print(f"  Solution: Always use explicit reshape for clarity\n")

# Pitfall 3: @ vs * operator
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

mat_product = A @ B  # matrix multiplication
elem_product = A * B  # element-wise (Hadamard)

print(f"Pitfall 3: @ vs * operator")
print(f"  A @ B (matrix):\n{mat_product}")
print(f"  A * B (element-wise):\n{elem_product}")
print(f"  These are VERY different!\n")

# Pitfall 4: inv() vs solve()
A = np.random.randn(100, 100)
b = np.random.randn(100)

import time

# Bad: compute inverse explicitly
t0 = time.time()
x_inv = np.linalg.inv(A) @ b
t_inv = time.time() - t0

# Good: use solve()
t0 = time.time()
x_solve = np.linalg.solve(A, b)
t_solve = time.time() - t0

print(f"Pitfall 4: inv() vs solve()")
print(f"  inv(A) @ b: {t_inv*1000:.2f} ms")
print(f"  solve(A, b): {t_solve*1000:.2f} ms")
print(f"  Speedup: {t_inv/t_solve:.1f}x")
print(f"  Error: {np.linalg.norm(x_inv - x_solve):.2e}")
print(f"  Solution: NEVER compute inverse for solving Ax=b\n")

# Pitfall 5: Numerical instability in softmax
logits = np.array([1000.0, 1001.0, 1002.0])

# Naive softmax: overflow!
try:
    naive_softmax = np.exp(logits) / np.sum(np.exp(logits))
    print(f"Naive softmax: {naive_softmax}")
except:
    print(f"Pitfall 5: Naive softmax overflows!")

# Stable softmax
stable_softmax = np.exp(logits - logits.max()) / np.sum(np.exp(logits - logits.max()))
print(f"  Stable softmax: {stable_softmax}")
print(f"  Solution: Subtract max before exp\n")

# Pitfall 6: Precision loss in variance calculation
data = np.array([1e8, 1e8 + 1, 1e8 + 2, 1e8 + 3])

# Naive: Var(X) = E[XÂ²] - E[X]Â²
mean = data.mean()
var_naive = (data**2).mean() - mean**2
var_correct = ((data - mean)**2).mean()

print(f"Pitfall 6: Numerical precision in variance")
print(f"  Naive (E[XÂ²] - E[X]Â²): {var_naive:.10f}")
print(f"  Correct ((X-Î¼)Â²): {var_correct:.10f}")
print(f"  Relative error: {abs(var_naive - var_correct)/var_correct:.2e}")
print(f"  Solution: Use two-pass algorithm or Welford's method\n")
```

:::message
**é€²æ—: 90% å®Œäº†** SVDç”»åƒåœ§ç¸®ã€Randomized SVDã€Reverse Mode AD ã®å®Œå…¨å®Ÿè£…ã€æ¡ä»¶æ•°ã¨æ•°å€¤å®‰å®šæ€§ã€ã‚¹ãƒ‘ãƒ¼ã‚¹è¡Œåˆ—ã€QR/Choleskyåˆ†è§£ã€è¡Œåˆ—å¾®åˆ†ã®æ•°å€¤æ¤œè¨¼ã€å®Ÿè·µçš„ãƒ‡ãƒãƒƒã‚°æŠ€æ³•ã‚’ç¿’å¾—ã—ãŸã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

### 5.1 è¨˜å·èª­è§£ãƒ†ã‚¹ãƒˆ

ä»¥ä¸‹ã®æ•°å¼ã‚’å£°ã«å‡ºã—ã¦èª­ã¿ã€æ„å‘³ã‚’èª¬æ˜ã›ã‚ˆã€‚

:::details Q1: $A = U \Sigma V^\top$
**èª­ã¿**: ã€Œ$A$ ã‚¤ã‚³ãƒ¼ãƒ« $U$ ã‚·ã‚°ãƒ $V$ ãƒˆãƒ©ãƒ³ã‚¹ãƒãƒ¼ã‚ºã€

**æ„å‘³**: è¡Œåˆ— $A$ ã®ç‰¹ç•°å€¤åˆ†è§£ã€‚$U$ ã¨ $V$ ã¯ç›´äº¤è¡Œåˆ—ã€$\Sigma$ ã¯ç‰¹ç•°å€¤ã‚’å¯¾è§’ã«æŒã¤è¡Œåˆ—ã€‚ä»»æ„ã®é•·æ–¹å½¢è¡Œåˆ—ã«é©ç”¨å¯èƒ½ã€‚
:::

:::details Q2: $A_k = \sum_{i=1}^{k} \sigma_i \mathbf{u}_i \mathbf{v}_i^\top$
**èª­ã¿**: ã€Œ$A$ ã‚µãƒ– $k$ ã¯ã€$i$ ã‚¤ã‚³ãƒ¼ãƒ« 1 ã‹ã‚‰ $k$ ã¾ã§ã€ã‚·ã‚°ãƒ $i$ ãƒ¦ãƒ¼ $i$ ãƒ–ã‚¤ $i$ ãƒˆãƒ©ãƒ³ã‚¹ãƒãƒ¼ã‚ºã®å’Œã€

**æ„å‘³**: rank-$k$ ã®æˆªæ–­SVDã€‚ä¸Šä½ $k$ å€‹ã®ç‰¹ç•°å€¤æˆåˆ†ã®å’Œã€‚Eckart-Youngå®šç†ã«ã‚ˆã‚Šã€ã“ã‚Œã¯Frobeniusãƒãƒ«ãƒ ã§æœ€é©ãª rank-$k$ è¿‘ä¼¼ã€‚
:::

:::details Q3: $J = \frac{\partial \mathbf{f}}{\partial \mathbf{x}} \in \mathbb{R}^{m \times n}$
**èª­ã¿**: ã€Œ$J$ ã‚¤ã‚³ãƒ¼ãƒ« ãƒ‘ãƒ¼ã‚·ãƒ£ãƒ« $\mathbf{f}$ ãƒ‘ãƒ¼ã‚·ãƒ£ãƒ« $\mathbf{x}$ã€$m$ ã‹ã‘ã‚‹ $n$ ã®å®Ÿæ•°è¡Œåˆ—ã€

**æ„å‘³**: ãƒ™ã‚¯ãƒˆãƒ«é–¢æ•° $\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m$ ã®ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã€‚$J_{ij} = \partial f_i / \partial x_j$ã€‚å…¥åŠ›ã®å¾®å°å¤‰åŒ–ãŒå‡ºåŠ›ã«ã©ã†å½±éŸ¿ã™ã‚‹ã‹ã‚’ç·šå½¢è¿‘ä¼¼ã™ã‚‹è¡Œåˆ—ã€‚
:::

:::details Q4: $\boldsymbol{\delta}_l = (W_{l+1}^\top \boldsymbol{\delta}_{l+1}) \odot \sigma'(\mathbf{z}_l)$
**èª­ã¿**: ã€Œãƒ‡ãƒ«ã‚¿ $l$ ã‚¤ã‚³ãƒ¼ãƒ«ã€$W$ $l$ãƒ—ãƒ©ã‚¹1 ãƒˆãƒ©ãƒ³ã‚¹ãƒãƒ¼ã‚º ãƒ‡ãƒ«ã‚¿ $l$ãƒ—ãƒ©ã‚¹1ã€ãƒãƒ€ãƒãƒ¼ãƒ‰ç© ã‚·ã‚°ãƒãƒ—ãƒ©ã‚¤ãƒ  $\mathbf{z}_l$ã€

**æ„å‘³**: Backpropagationã®å†å¸°å¼ã€‚ç¬¬ $l$ å±¤ã®èª¤å·®ä¿¡å·ã¯ã€ç¬¬ $l+1$ å±¤ã®èª¤å·®ã‚’é‡ã¿è¡Œåˆ—ã§é€†ä¼æ’­ã—ã€æ´»æ€§åŒ–é–¢æ•°ã®å¾®åˆ†ã¨è¦ç´ ã”ã¨ã«æ›ã‘ã‚‹ã€‚
:::

:::details Q5: $A^+ = V \Sigma^+ U^\top$
**èª­ã¿**: ã€Œ$A$ ãƒ—ãƒ©ã‚¹ ã‚¤ã‚³ãƒ¼ãƒ« $V$ ã‚·ã‚°ãƒãƒ—ãƒ©ã‚¹ $U$ ãƒˆãƒ©ãƒ³ã‚¹ãƒãƒ¼ã‚ºã€

**æ„å‘³**: Moore-Penroseæ“¬ä¼¼é€†è¡Œåˆ—ã®SVDã«ã‚ˆã‚‹æ§‹æˆã€‚$\Sigma^+$ ã¯éã‚¼ãƒ­ç‰¹ç•°å€¤ã®é€†æ•°ã‚’å¯¾è§’ã«æŒã¤ã€‚æ­£å‰‡ã§ãªã„è¡Œåˆ—ã‚„é•·æ–¹å½¢è¡Œåˆ—ã«å¯¾ã™ã‚‹ã€Œé€†è¡Œåˆ—ã€ã®ä¸€èˆ¬åŒ–ã€‚
:::

### 5.2 ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ†ã‚¹ãƒˆ

ä»¥ä¸‹ã®æ•°å¼ã‚’NumPyã‚³ãƒ¼ãƒ‰ã«ç¿»è¨³ã›ã‚ˆã€‚

:::details Q1: $\hat{\mathbf{x}} = V \Sigma^+ U^\top \mathbf{b}$ï¼ˆæ“¬ä¼¼é€†è¡Œåˆ—ã«ã‚ˆã‚‹æœ€å°äºŒä¹—è§£ï¼‰
```python
U, s, Vt = np.linalg.svd(A, full_matrices=False)
S_pinv = np.diag(1.0 / s)  # assumes all singular values > 0
x_hat = Vt.T @ S_pinv @ U.T @ b
# or simply: x_hat = np.linalg.pinv(A) @ b
```
:::

:::details Q2: $\text{tr}(A^\top B) = \sum_{ij} A_{ij} B_{ij}$ï¼ˆFrobeniuså†…ç©ï¼‰
```python
# Method 1: trace
result1 = np.trace(A.T @ B)
# Method 2: element-wise (faster, no matrix multiply)
result2 = np.sum(A * B)
# Method 3: einsum
result3 = np.einsum('ij,ij->', A, B)
```
:::

:::details Q3: $S_{bhij} = \frac{1}{\sqrt{d_k}} \sum_k Q_{bhik} K_{bhjk}$ï¼ˆMulti-Head Attention ã‚¹ã‚³ã‚¢ï¼‰
```python
scores = np.einsum('bhik,bhjk->bhij', Q, K) / np.sqrt(dk)
# or: scores = Q @ K.transpose(0, 1, 3, 2) / np.sqrt(dk)
```
:::

### 5.3 ãƒŸãƒ‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: SVDã§ç”»åƒã®ãƒã‚¤ã‚ºé™¤å»

```python
import numpy as np

# Create image with noise
np.random.seed(42)
m, n = 128, 128
x = np.linspace(0, 4*np.pi, m)
y = np.linspace(0, 4*np.pi, n)
X, Y = np.meshgrid(y, x)
clean = np.sin(X) * np.cos(Y) + 0.5 * np.sin(2*X + Y)

# Add noise
noise_level = 0.5
noisy = clean + noise_level * np.random.randn(m, n)

# SVD denoising: try different ranks
U, s, Vt = np.linalg.svd(noisy, full_matrices=False)

print(f"Original signal energy: {np.linalg.norm(clean, 'fro'):.4f}")
print(f"Noise energy: {noise_level * np.sqrt(m * n):.4f}")
print(f"{'rank':>6} {'error_vs_clean':>15} {'error_vs_noisy':>15} {'SNR(dB)':>10}")

best_k, best_error = 0, float('inf')
for k in [1, 3, 5, 10, 20, 50, 100]:
    denoised = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]
    err_clean = np.linalg.norm(denoised - clean, 'fro') / np.linalg.norm(clean, 'fro')
    err_noisy = np.linalg.norm(denoised - noisy, 'fro') / np.linalg.norm(noisy, 'fro')
    signal_power = np.mean(denoised**2)
    noise_power = np.mean((denoised - clean)**2)
    snr = 10 * np.log10(signal_power / noise_power) if noise_power > 0 else float('inf')
    print(f"{k:6d} {err_clean:15.6f} {err_noisy:15.6f} {snr:10.2f}")
    if err_clean < best_error:
        best_error = err_clean
        best_k = k

print(f"\nBest rank for denoising: {best_k} (error = {best_error:.6f})")
```

### 5.4 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸: Forward Mode AD with Dual Numbers

```python
import numpy as np

class Dual:
    """Dual number for Forward Mode AD"""
    def __init__(self, val, deriv=0.0):
        self.val = float(val)
        self.deriv = float(deriv)

    def __repr__(self):
        return f"Dual({self.val:.6f}, d={self.deriv:.6f})"

    def __add__(self, other):
        o = other if isinstance(other, Dual) else Dual(other)
        return Dual(self.val + o.val, self.deriv + o.deriv)

    def __radd__(self, other):
        return self + other

    def __sub__(self, other):
        o = other if isinstance(other, Dual) else Dual(other)
        return Dual(self.val - o.val, self.deriv - o.deriv)

    def __rsub__(self, other):
        return Dual(other) - self

    def __mul__(self, other):
        o = other if isinstance(other, Dual) else Dual(other)
        return Dual(self.val * o.val, self.val * o.deriv + self.deriv * o.val)

    def __rmul__(self, other):
        return self * other

    def __truediv__(self, other):
        o = other if isinstance(other, Dual) else Dual(other)
        return Dual(self.val / o.val,
                    (self.deriv * o.val - self.val * o.deriv) / o.val**2)

    def __pow__(self, n):
        return Dual(self.val**n, n * self.val**(n-1) * self.deriv)

def sin_d(x):
    return Dual(np.sin(x.val), np.cos(x.val) * x.deriv)

def cos_d(x):
    return Dual(np.cos(x.val), -np.sin(x.val) * x.deriv)

def exp_d(x):
    e = np.exp(x.val)
    return Dual(e, e * x.deriv)

def log_d(x):
    return Dual(np.log(x.val), x.deriv / x.val)

# Test: f(x) = sin(x^2) * exp(-x) + log(x)
def f(x):
    return sin_d(x**2) * exp_d(-1 * x) + log_d(x)

# Compute derivative at x = 1.5
x = Dual(1.5, 1.0)  # seed: dx/dx = 1
result = f(x)
print(f"f(1.5)  = {result.val:.8f}")
print(f"f'(1.5) = {result.deriv:.8f}")

# Numerical verification
h = 1e-8
def f_float(x):
    return np.sin(x**2) * np.exp(-x) + np.log(x)
numerical = (f_float(1.5 + h) - f_float(1.5 - h)) / (2 * h)
print(f"Numerical: {numerical:.8f}")
print(f"Match: {abs(result.deriv - numerical) < 1e-6}")
```

### 5.5 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸: 2å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®å­¦ç¿’

Backpropagation ã‚’ä½¿ã£ã¦ã€2å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ã‚¼ãƒ­ã‹ã‚‰å­¦ç¿’ã•ã›ã‚‹ã€‚

```python
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))

def sigmoid_deriv(x):
    s = sigmoid(x)
    return s * (1 - s)

# XOR problem
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=float)
y = np.array([[0], [1], [1], [0]], dtype=float)

# Initialize weights
np.random.seed(42)
W1 = np.random.randn(2, 8) * 0.5  # 2 inputs -> 8 hidden
b1 = np.zeros((1, 8))
W2 = np.random.randn(8, 1) * 0.5  # 8 hidden -> 1 output
b2 = np.zeros((1, 1))

lr = 1.0
losses = []

for epoch in range(5000):
    # Forward pass
    z1 = X @ W1 + b1
    h1 = sigmoid(z1)
    z2 = h1 @ W2 + b2
    h2 = sigmoid(z2)

    # Loss (MSE)
    loss = np.mean((h2 - y)**2)
    losses.append(loss)

    # Backward pass
    dL_dh2 = 2 * (h2 - y) / len(X)
    delta2 = dL_dh2 * sigmoid_deriv(z2)

    dL_dW2 = h1.T @ delta2
    dL_db2 = np.sum(delta2, axis=0, keepdims=True)

    delta1 = (delta2 @ W2.T) * sigmoid_deriv(z1)
    dL_dW1 = X.T @ delta1
    dL_db1 = np.sum(delta1, axis=0, keepdims=True)

    # Update
    W2 -= lr * dL_dW2
    b2 -= lr * dL_db2
    W1 -= lr * dL_dW1
    b1 -= lr * dL_db1

    if epoch % 1000 == 0:
        print(f"Epoch {epoch:5d}: loss = {loss:.6f}")

# Final predictions
z1 = X @ W1 + b1
h1 = sigmoid(z1)
z2 = h1 @ W2 + b2
predictions = sigmoid(z2)

print(f"\nFinal predictions:")
for i in range(len(X)):
    print(f"  {X[i]} -> {predictions[i, 0]:.4f} (target: {y[i, 0]:.0f})")
print(f"\nLoss: {losses[0]:.6f} -> {losses[-1]:.6f}")
```

### 5.6 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸: è¡Œåˆ—å¾®åˆ†ã®å°å‡ºã¨æ¤œè¨¼

ä»¥ä¸‹ã®è¡Œåˆ—å¾®åˆ†ã‚’æ‰‹ã§å°å‡ºã—ã€æ•°å€¤æ¤œè¨¼ã›ã‚ˆã€‚

:::details Challenge 1: $\frac{\partial}{\partial W} \|XW - Y\|_F^2$
**å°å‡º**:

$$
L = \text{tr}((XW-Y)^\top(XW-Y)) = \text{tr}(W^\top X^\top XW - 2Y^\top XW + Y^\top Y)
$$

$$
\frac{\partial L}{\partial W} = 2X^\top XW - 2X^\top Y = 2X^\top(XW - Y)
$$

**æ¤œè¨¼**:
```python
import numpy as np

X = np.random.randn(10, 5)
W = np.random.randn(5, 3)
Y = np.random.randn(10, 3)

# Analytical
grad_analytical = 2 * X.T @ (X @ W - Y)

# Numerical
eps = 1e-7
grad_numerical = np.zeros_like(W)
for i in range(W.shape[0]):
    for j in range(W.shape[1]):
        W_plus = W.copy(); W_plus[i, j] += eps
        W_minus = W.copy(); W_minus[i, j] -= eps
        f_plus = np.sum((X @ W_plus - Y)**2)
        f_minus = np.sum((X @ W_minus - Y)**2)
        grad_numerical[i, j] = (f_plus - f_minus) / (2 * eps)

print(f"Match: {np.allclose(grad_analytical, grad_numerical)}")
```
:::

:::details Challenge 2: $\frac{\partial}{\partial \mathbf{x}} \text{softmax}(\mathbf{x})^\top \mathbf{a}$
**å°å‡º**:

$L = \mathbf{s}^\top \mathbf{a} = \sum_i s_i a_i$ ã¨ã™ã‚‹ã€‚

$$
\frac{\partial L}{\partial x_j} = \sum_i a_i \frac{\partial s_i}{\partial x_j} = \sum_i a_i s_i(\delta_{ij} - s_j) = a_j s_j - s_j \sum_i a_i s_i
$$

$$
\frac{\partial L}{\partial \mathbf{x}} = \mathbf{s} \odot \mathbf{a} - (\mathbf{s}^\top \mathbf{a}) \mathbf{s} = \mathbf{s} \odot (\mathbf{a} - (\mathbf{s}^\top \mathbf{a}) \mathbf{1})
$$

**æ¤œè¨¼**:
```python
import numpy as np

def softmax(z):
    e = np.exp(z - np.max(z))
    return e / np.sum(e)

x = np.array([1.0, 2.0, 0.5])
a = np.array([3.0, 1.0, 2.0])
s = softmax(x)

# Analytical
grad_analytical = s * (a - np.dot(s, a))

# Numerical
eps = 1e-7
grad_numerical = np.zeros(len(x))
for j in range(len(x)):
    x_plus = x.copy(); x_plus[j] += eps
    x_minus = x.copy(); x_minus[j] -= eps
    grad_numerical[j] = (softmax(x_plus) @ a - softmax(x_minus) @ a) / (2 * eps)

print(f"Analytical: {np.round(grad_analytical, 6)}")
print(f"Numerical:  {np.round(grad_numerical, 6)}")
print(f"Match: {np.allclose(grad_analytical, grad_numerical)}")
```
:::

### 5.7 LaTeX è¨˜è¿°ãƒ†ã‚¹ãƒˆ

ä»¥ä¸‹ã®æ•°å¼ã‚’LaTeX ã§è¨˜è¿°ã›ã‚ˆã€‚

:::details Q1: SVDã®å®šç¾©
```latex
A = U \Sigma V^\top = \sum_{i=1}^{r} \sigma_i \mathbf{u}_i \mathbf{v}_i^\top
```
:::

:::details Q2: Backpropagation ã®å†å¸°å¼
```latex
\boldsymbol{\delta}_l = (W_{l+1}^\top \boldsymbol{\delta}_{l+1}) \odot \sigma'(\mathbf{z}_l)
```
:::

:::details Q3: Eckart-Young å®šç†
```latex
\min_{\text{rank}(B) \leq k} \|A - B\|_F = \sqrt{\sum_{i=k+1}^{r} \sigma_i^2}
```
:::

:::details Q4: Normalizing Flow ã®å¤‰æ•°å¤‰æ›å…¬å¼
```latex
p_Y(\mathbf{y}) = p_X(\mathbf{f}^{-1}(\mathbf{y})) \cdot |\det(J_{\mathbf{f}^{-1}}(\mathbf{y}))|
```
:::

:::details Q5: Cross-Entropy + Softmax ã®å‹¾é…
```latex
\frac{\partial}{\partial \mathbf{z}} \left[ -\sum_i y_i \log \text{softmax}(\mathbf{z})_i \right] = \text{softmax}(\mathbf{z}) - \mathbf{y}
```
:::

### 5.8 è‡ªå·±ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

| # | ãƒã‚§ãƒƒã‚¯é …ç›® | é”æˆ |
|:--|:-----------|:-----|
| 1 | SVDã®å®šç¾©ï¼ˆ$A = U\Sigma V^\top$ï¼‰ã‚’ç™½ç´™ã«æ›¸ã‘ã‚‹ | [ ] |
| 2 | å›ºæœ‰å€¤åˆ†è§£ã¨ã®é–¢ä¿‚ã‚’èª¬æ˜ã§ãã‚‹ | [ ] |
| 3 | Eckart-Youngå®šç†ã®æ„å‘³ã‚’èª¬æ˜ã§ãã‚‹ | [ ] |
| 4 | æ“¬ä¼¼é€†è¡Œåˆ—ã®SVDã«ã‚ˆã‚‹æ§‹æˆã‚’æ›¸ã‘ã‚‹ | [ ] |
| 5 | PCA ã‚’SVDã§å°å‡ºã§ãã‚‹ | [ ] |
| 6 | Kroneckerç©ã®æ€§è³ªã‚’3ã¤æŒ™ã’ã‚‰ã‚Œã‚‹ | [ ] |
| 7 | einsum ã§è¡Œåˆ—ç©ãƒ»ãƒãƒƒãƒè¡Œåˆ—ç©ã‚’æ›¸ã‘ã‚‹ | [ ] |
| 8 | ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®å®šç¾©ã¨å¹¾ä½•å­¦çš„æ„å‘³ã‚’èª¬æ˜ã§ãã‚‹ | [ ] |
| 9 | ãƒ˜ã‚·ã‚¢ãƒ³ã®æ­£å®šå€¤æ€§ã¨æœ€é©åŒ–ã®é–¢ä¿‚ã‚’èª¬æ˜ã§ãã‚‹ | [ ] |
| 10 | Matrix Cookbook ã®ä¸»è¦å…¬å¼ã‚’5ã¤æ›¸ã‘ã‚‹ | [ ] |
| 11 | é€£é–å¾‹ã®ãƒ™ã‚¯ãƒˆãƒ«ç‰ˆã‚’æ›¸ã‘ã‚‹ | [ ] |
| 12 | Backpropã®å†å¸°å¼ $\boldsymbol{\delta}_l$ ã‚’å°å‡ºã§ãã‚‹ | [ ] |
| 13 | Forward vs Reverse Mode AD ã®è¨ˆç®—é‡ã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹ | [ ] |
| 14 | Log-Sum-Exp trick ã®å¿…è¦æ€§ã‚’èª¬æ˜ã§ãã‚‹ | [ ] |

:::message
**é€²æ—: 90% å®Œäº†** è¨˜å·èª­è§£ãƒ†ã‚¹ãƒˆã€ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ†ã‚¹ãƒˆã€SVDãƒã‚¤ã‚ºé™¤å»ã€Forward Mode ADå®Ÿè£…ã€è‡ªå·±ãƒã‚§ãƒƒã‚¯ã‚’å®Œäº†ã—ãŸã€‚
:::

---

## ğŸš€ 6. æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨æ¬¡å›äºˆå‘Š

### 6.1 NumPy / SciPy ã® SVDãƒ»å¾®åˆ†é–¢é€£ãƒãƒ¼ãƒˆã‚·ãƒ¼ãƒˆ

| ç›®çš„ | é–¢æ•° | æ³¨æ„ç‚¹ |
|:-----|:-----|:------|
| SVD (full) | `np.linalg.svd(A, full_matrices=True)` | $U: m \times m$ |
| SVD (economy) | `np.linalg.svd(A, full_matrices=False)` | $U: m \times \min(m,n)$ |
| æ“¬ä¼¼é€†è¡Œåˆ— | `np.linalg.pinv(A)` | å†…éƒ¨ã§SVDä½¿ç”¨ |
| Truncated SVD | `scipy.sparse.linalg.svds(A, k)` | ç–è¡Œåˆ—å‘ã‘ã€‚ä¸Šä½$k$å€‹ |
| æ•°å€¤å‹¾é… | `scipy.optimize.approx_fprime(x, f, eps)` | ãƒ‡ãƒãƒƒã‚°ç”¨ã€‚æœ¬ç•ªã§ã¯ä½¿ã‚ãªã„ |
| Kroneckerç© | `np.kron(A, B)` | çµæœã¯å¤§è¡Œåˆ— |
| einsum | `np.einsum(subscripts, *operands)` | `optimize=True` æ¨å¥¨ |

:::details ç”¨èªé›†
| è‹±èª | æ—¥æœ¬èª | è¨˜å· |
|:-----|:------|:-----|
| Singular Value Decomposition | ç‰¹ç•°å€¤åˆ†è§£ | $A = U\Sigma V^\top$ |
| Singular value | ç‰¹ç•°å€¤ | $\sigma_i$ |
| Left/Right singular vector | å·¦/å³ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ« | $\mathbf{u}_i, \mathbf{v}_i$ |
| Low-rank approximation | ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ | $A_k$ |
| Pseudoinverse | æ“¬ä¼¼é€†è¡Œåˆ— | $A^+$ |
| Jacobian | ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ | $J$ |
| Hessian | ãƒ˜ã‚·ã‚¢ãƒ³ | $H$ |
| Gradient | å‹¾é… | $\nabla f$ |
| Chain rule | é€£é–å¾‹ | |
| Automatic differentiation | è‡ªå‹•å¾®åˆ† | AD |
| Forward mode | å‰é€²ãƒ¢ãƒ¼ãƒ‰ | tangent propagation |
| Reverse mode | é€†ä¼æ’­ãƒ¢ãƒ¼ãƒ‰ | adjoint propagation |
| Backpropagation | èª¤å·®é€†ä¼æ’­æ³• | BP |
| Dual number | åŒå¯¾æ•° | $a + b\epsilon$ |
| Wengert list | Wengert ãƒªã‚¹ãƒˆ | è¨ˆç®—ãƒˆãƒ¬ãƒ¼ã‚¹ |
| Kronecker product | ã‚¯ãƒ­ãƒãƒƒã‚«ãƒ¼ç© | $A \otimes B$ |
| Einstein notation | ã‚¢ã‚¤ãƒ³ã‚·ãƒ¥ã‚¿ã‚¤ãƒ³è¨˜æ³• | æ·»å­—ã®æš—é»™çš„ç¸®ç´„ |
| Tikhonov regularization | ãƒãƒ›ãƒãƒ•æ­£å‰‡åŒ– | Ridgeå›å¸° |
| Condition number | æ¡ä»¶æ•° | $\kappa(A)$ |
:::

```mermaid
mindmap
  root((ç·šå½¢ä»£æ•° II))
    SVD
      å­˜åœ¨å®šç†
      Eckart-Young
      ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼
      æ“¬ä¼¼é€†è¡Œåˆ—
      PCA via SVD
    ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—
      Kroneckerç©
      Einsteinè¨˜æ³•
      einsum
    è¡Œåˆ—å¾®åˆ†
      å‹¾é…
      ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³
      ãƒ˜ã‚·ã‚¢ãƒ³
      Matrix Cookbook
    é€£é–å¾‹
      ã‚¹ã‚«ãƒ©ãƒ¼ç‰ˆ
      ãƒ™ã‚¯ãƒˆãƒ«ç‰ˆ
      Backpropagation
    è‡ªå‹•å¾®åˆ†
      Forward Mode
      Reverse Mode
      Dual Numbers
      Wengert List
```

:::message
**é€²æ—: 95% å®Œäº†** SVDãƒ»è¡Œåˆ—å¾®åˆ†ãƒ»è‡ªå‹•å¾®åˆ†ã®ç ”ç©¶æœ€å‰ç·šã€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®é€²åŒ–ã€æ¨è–¦ãƒªã‚½ãƒ¼ã‚¹ã‚’ç¢ºèªã—ãŸã€‚
:::

### 6.2 æœ¬è¬›ç¾©ã®3ã¤ã®ãƒã‚¤ãƒ³ãƒˆ

**1. SVDã¯è¡Œåˆ—ã®ä¸‡èƒ½ãƒŠã‚¤ãƒ•**

$$
A = U \Sigma V^\top = \sum_{i=1}^{r} \sigma_i \mathbf{u}_i \mathbf{v}_i^\top
$$

ä»»æ„ã®è¡Œåˆ—ã‚’åˆ†è§£ã§ãã€ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ã®æœ€é©æ€§ï¼ˆEckart-Youngå®šç†[^3]ï¼‰ãŒä¿è¨¼ã•ã‚Œã‚‹ã€‚PCAã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã€LoRA[^10]ã€ç”»åƒåœ§ç¸® â€” å…¨ã¦SVDã®å¿œç”¨ã€‚

**2. è¡Œåˆ—å¾®åˆ† + é€£é–å¾‹ = Backpropagation**

$$
\frac{\partial L}{\partial W_l} = \boldsymbol{\delta}_l \mathbf{h}_{l-1}^\top, \quad \boldsymbol{\delta}_l = (W_{l+1}^\top \boldsymbol{\delta}_{l+1}) \odot \sigma'(\mathbf{z}_l)
$$

ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®ç©ã¨ã—ã¦é€£é–å¾‹ã‚’æ›¸ãã€é€†é †ã«ä¼æ’­ã™ã‚‹ã“ã¨ã§å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹¾é…ã‚’1å›ã®backward passã§è¨ˆç®—[^2]ã€‚

**3. è‡ªå‹•å¾®åˆ†ã¯Reverse ModeãŒæ©Ÿæ¢°å­¦ç¿’ã®æ¨™æº–**

$$
\text{Forward: } O(n) \text{ passes for } n \text{ inputs} \quad \text{vs} \quad \text{Reverse: } O(1) \text{ pass for scalar output}
$$

æå¤±é–¢æ•°ã¯ã‚¹ã‚«ãƒ©ãƒ¼ï¼ˆ$m = 1$ï¼‰ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯è†¨å¤§ï¼ˆ$n \sim 10^9$ï¼‰ã€‚Reverse mode[^7][^8]ãŒå”¯ä¸€ã®ç¾å®Ÿçš„ãªé¸æŠè‚¢ã€‚

### 6.3 FAQ

:::details Q: SVDã¨å›ºæœ‰å€¤åˆ†è§£ã¯ã©ã†ä½¿ã„åˆ†ã‘ã‚‹ï¼Ÿ
- **æ­£æ–¹å¯¾ç§°è¡Œåˆ—** â†’ å›ºæœ‰å€¤åˆ†è§£ï¼ˆ`eigh`ï¼‰ãŒåŠ¹ç‡çš„
- **é•·æ–¹å½¢è¡Œåˆ—** â†’ SVDä¸€æŠï¼ˆå›ºæœ‰å€¤åˆ†è§£ã¯ä½¿ãˆãªã„ï¼‰
- **æ­£æ–¹éå¯¾ç§°è¡Œåˆ—** â†’ SVDæ¨å¥¨ï¼ˆå›ºæœ‰å€¤ãŒè¤‡ç´ æ•°ã«ãªã‚Šå¾—ã‚‹ï¼‰
- **PCA** â†’ ã©ã¡ã‚‰ã§ã‚‚ã‚ˆã„ãŒã€SVDã®æ–¹ãŒæ•°å€¤å®‰å®š

å®Ÿå‹™ã§ã¯SVDã‚’ä½¿ã£ã¦ãŠã‘ã°é–“é•ã„ãªã„ã€‚å›ºæœ‰å€¤åˆ†è§£ã¯SVDã®ç‰¹æ®Šã‚±ãƒ¼ã‚¹ã€‚
:::

:::details Q: Backpropagation ã®å‹¾é…ã¯å¸¸ã«æ­£ç¢ºã‹ï¼Ÿ
è‡ªå‹•å¾®åˆ†ã®å‹¾é…ã¯**æ©Ÿæ¢°ç²¾åº¦**ï¼ˆ$\sim 10^{-16}$ for float64ï¼‰ã¾ã§æ­£ç¢ºã€‚æ•°å€¤å¾®åˆ†ï¼ˆ$\sim 10^{-8}$ï¼‰ã‚ˆã‚Šã¯ã‚‹ã‹ã«ç²¾åº¦ãŒé«˜ã„ã€‚

ãŸã ã—ã€ä»¥ä¸‹ã®å ´åˆã«æ•°å€¤çš„å•é¡ŒãŒç”Ÿã˜ã‚‹:
1. **å‹¾é…æ¶ˆå¤±**: sigmoid/tanhã®é£½å’Œé ˜åŸŸã§å‹¾é…ãŒã»ã¼0 â†’ ReLUç³»æ´»æ€§åŒ–é–¢æ•°ã§è§£æ±º
2. **å‹¾é…çˆ†ç™º**: æ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®ç©ãŒã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ â†’ å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã§è§£æ±º
3. **éå¾®åˆ†ç‚¹**: ReLUã® $x = 0$ â†’ sub-gradient ã§ä»£ç”¨ï¼ˆå®Ÿç”¨ä¸Šã¯å•é¡Œã«ãªã‚‰ãªã„ï¼‰
:::

:::details Q: einsum ã¨ @ ã¯ã©ã¡ã‚‰ã‚’ä½¿ã†ã¹ãï¼Ÿ
- **2Dã®å˜ç´”ãªè¡Œåˆ—ç©** â†’ `@`ï¼ˆèª­ã¿ã‚„ã™ã„ï¼‰
- **ãƒãƒƒãƒå‡¦ç†ã€è¤‡é›‘ãªç¸®ç´„** â†’ `einsum`ï¼ˆè¡¨ç¾åŠ›ãŒé«˜ã„ï¼‰
- **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**: ã»ã¼åŒç­‰ã€‚`einsum` ã¯ `optimize=True` ã§æœ€é©ãªç¸®ç´„é †åºã‚’é¸æŠ

Transformerå®Ÿè£…ã§ã¯ `einsum` ãŒã‚ˆãä½¿ã‚ã‚Œã‚‹ï¼ˆç‰¹ã«Multi-Head Attentionã® $B \times H \times T \times d$ ãƒ†ãƒ³ã‚½ãƒ«æ“ä½œï¼‰ã€‚
:::

:::details Q: LoRAã¯ãªãœä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ã§å‹•ãã®ã‹ï¼Ÿ
çµŒé¨“çš„ã«ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚ã®é‡ã¿æ›´æ–° $\Delta W$ ãŒä½ãƒ©ãƒ³ã‚¯æ§‹é€ ã‚’æŒã¤ã“ã¨ãŒçŸ¥ã‚‰ã‚Œã¦ã„ã‚‹ã€‚ã¤ã¾ã‚Šã€$\Delta W$ ã®SVDã‚’å–ã‚‹ã¨ã€ä¸Šä½æ•°å€‹ã®ç‰¹ç•°å€¤ãŒæ”¯é…çš„ã§ã€æ®‹ã‚Šã¯ã»ã¼0ã€‚

ç†è«–çš„ãªèª¬æ˜ã¯å®Œå…¨ã§ã¯ãªã„ãŒã€ä¸€ã¤ã®ä»®èª¬ã¯: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã€Œäº‹å‰å­¦ç¿’ã§ç²å¾—ã—ãŸè¡¨ç¾ç©ºé–“ã®ä¸­ã®ã€ã‚¿ã‚¹ã‚¯å›ºæœ‰ã®ä½æ¬¡å…ƒéƒ¨åˆ†ç©ºé–“ã‚’èª¿æ•´ã™ã‚‹æ“ä½œã€ã§ã‚ã‚Šã€ãã®éƒ¨åˆ†ç©ºé–“ã®æ¬¡å…ƒãŒ $r \ll d$ ã§ã‚ã‚‹ã¨ã„ã†ã“ã¨ã€‚
:::

:::details Q: JAXã®gradã¨PyTorchã®backwardã®é•ã„ã¯ï¼Ÿ
**PyTorch**: ãƒ†ãƒ¼ãƒ—ãƒ™ãƒ¼ã‚¹ã€‚è¨ˆç®—ã‚’å®Ÿè¡Œã™ã‚‹ãŸã³ã«ã‚°ãƒ©ãƒ•ã‚’è¨˜éŒ²ã—ã€`.backward()` ã§é€†ä¼æ’­ã€‚

**JAX**: ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ™ãƒ¼ã‚¹ã€‚é–¢æ•°ã‚’ä¸€åº¦ãƒˆãƒ¬ãƒ¼ã‚¹ã—ã¦è¨ˆç®—ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰ã—ã€`jax.grad(f)` ã§å‹¾é…é–¢æ•°ã‚’è¿”ã™ã€‚

ä¸»ãªé•ã„:
1. JAXã® `grad` ã¯**é–¢æ•°**ã‚’è¿”ã™ï¼ˆé«˜éšé–¢æ•°ï¼‰ã€‚PyTorchã® `.backward()` ã¯**å‰¯ä½œç”¨**ã§å‹¾é…ã‚’è“„ç©
2. JAXã¯ `jit` ã§ XLA ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å¯èƒ½ã€‚PyTorchã¯ `torch.compile` ã§åŒç­‰ã®ã“ã¨ãŒå¯èƒ½
3. JAXã® `vmap` ã§ãƒãƒƒãƒå‡¦ç†ã‚’è‡ªå‹•åŒ–ã€‚PyTorchã¯ `torch.vmap` ã§åŒç­‰
:::

:::details Q: Truncated SVD ã® k ã¯ã©ã†é¸ã¶ã¹ãã‹ï¼Ÿ
ç›®çš„ã«ã‚ˆã£ã¦ç•°ãªã‚‹:

1. **ãƒ‡ãƒ¼ã‚¿åœ§ç¸®ãƒ»ãƒã‚¤ã‚ºé™¤å»**: ç´¯ç©ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼ˆ$\sum_{i=1}^k \sigma_i^2 / \sum_{i=1}^r \sigma_i^2$ï¼‰ãŒ 90-99% ã«ãªã‚‹ $k$
2. **PCA / å¯è¦–åŒ–**: $k = 2$ or $3$ï¼ˆäººé–“ãŒè¦‹ã‚‰ã‚Œã‚‹æ¬¡å…ƒï¼‰
3. **LoRA**: çµŒé¨“çš„ã« $r = 4, 8, 16, 64$ ç¨‹åº¦ã€‚ã‚¿ã‚¹ã‚¯ã¨å…ƒã®ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«ä¾å­˜
4. **æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ **: Cross-validation ã§æœ€é©ãª $k$ ã‚’é¸æŠ

ç‰¹ç•°å€¤ã®scree plotï¼ˆç‰¹ç•°å€¤ã‚’é™é †ã«ãƒ—ãƒ­ãƒƒãƒˆã—ãŸã‚°ãƒ©ãƒ•ï¼‰ã§ã€Œè‚˜ã€ï¼ˆæ€¥æ¿€ã«æ¸›è¡°ãŒç·©ã‚„ã‹ã«ãªã‚‹ç‚¹ï¼‰ã‚’è¦‹ã¤ã‘ã‚‹ã®ãŒä¸€èˆ¬çš„ãªçµŒé¨“å‰‡ã€‚
:::

:::details Q: ãƒ˜ã‚·ã‚¢ãƒ³ã‚’è¨ˆç®—ã™ã‚‹ã®ã¯å®Ÿç”¨çš„ã‹ï¼Ÿ
$n$ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ˜ã‚·ã‚¢ãƒ³ã¯ $n \times n$ è¡Œåˆ—ã€‚LLMã§ã¯ $n \sim 10^9$ ãªã®ã§ã€$10^{18}$ è¦ç´ ã®ãƒ˜ã‚·ã‚¢ãƒ³ã¯æ ¼ç´ä¸å¯èƒ½ã€‚

å®Ÿç”¨çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:
1. **ãƒ˜ã‚·ã‚¢ãƒ³-ãƒ™ã‚¯ãƒˆãƒ«ç©**: $H\mathbf{v}$ ã ã‘ãªã‚‰ $O(n)$ ã§è¨ˆç®—å¯èƒ½ï¼ˆForward-over-Reverse ADï¼‰
2. **å¯¾è§’è¿‘ä¼¼**: $H$ ã®å¯¾è§’è¦ç´ ã ã‘è¨ˆç®—ã€‚AdaGrad, Adam ãŒä½¿ã†
3. **ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼**: L-BFGS ãŒ $H^{-1}$ ã‚’ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼
4. **Fisheræƒ…å ±è¡Œåˆ—**: æœŸå¾…å€¤ãƒ˜ã‚·ã‚¢ãƒ³ã®è¿‘ä¼¼ã€‚Natural Gradient ã§ä½¿ç”¨

ãƒ˜ã‚·ã‚¢ãƒ³è‡ªä½“ã‚’é™½ã«è¨ˆç®—ã™ã‚‹ã®ã¯ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒæ•°åƒä»¥ä¸‹ã®å ´åˆã®ã¿ã€‚
:::

### 6.4 ã‚ˆãã‚ã‚‹é–“é•ã„

| é–“é•ã„ | æ­£ã—ã„ç†è§£ |
|:------|:---------|
| SVDã¯æ­£æ–¹è¡Œåˆ—ã«ã—ã‹ä½¿ãˆãªã„ | **ä»»æ„ã®** $m \times n$ è¡Œåˆ—ã«ä½¿ãˆã‚‹ |
| ç‰¹ç•°å€¤ã¯å›ºæœ‰å€¤ã¨åŒã˜ | ç‰¹ç•°å€¤ã¯ $A^\top A$ ã®å›ºæœ‰å€¤ã®**å¹³æ–¹æ ¹** |
| Backpropã¯è¿‘ä¼¼çš„ãªå‹¾é…è¨ˆç®— | Reverse Mode AD ã§ã‚ã‚Šã€**æ©Ÿæ¢°ç²¾åº¦ã§å³å¯†** |
| Forward Mode ãŒå¸¸ã«é…ã„ | $n \ll m$ ãªã‚‰ Forward Mode ã®æ–¹ãŒåŠ¹ç‡çš„ |
| `pinv` ã¯ `inv` ã®ä»£ã‚ã‚Šã«ä½¿ãˆã‚‹ | `pinv` ã¯æœ€å°äºŒä¹—è§£ã‚’è¿”ã™ã€‚æ­£å‰‡è¡Œåˆ—ãªã‚‰ `solve` ã‚’ä½¿ã† |
| å‹¾é…ã¯å¸¸ã«å‹¾é…é™ä¸‹ã®æ–¹å‘ã‚’ç¤ºã™ | å‹¾é…ã¯æœ€æ€¥**ä¸Šæ˜‡**æ–¹å‘ã€‚$-\nabla f$ ãŒé™ä¸‹æ–¹å‘ |
| Softmaxã®å‹¾é…ã¯è¤‡é›‘ | Cross-Entropyã¨çµ„ã¿åˆã‚ã›ã‚‹ã¨ $\mathbf{s} - \mathbf{y}$ ã¨ã‚·ãƒ³ãƒ—ãƒ«ã« |

### 6.5 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ1é€±é–“ãƒ—ãƒ©ãƒ³ï¼‰

| æ—¥ | å†…å®¹ | æ‰€è¦æ™‚é–“ |
|:---|:-----|:--------|
| Day 1 | Zone 0-2 é€šèª­ | 30åˆ† |
| Day 2 | Zone 3 å‰åŠï¼ˆ3.1-3.5: SVDï¼‰ | 45åˆ† |
| Day 3 | Zone 3 ä¸­ç›¤ï¼ˆ3.6-3.8: ãƒ†ãƒ³ã‚½ãƒ«ãƒ»è¡Œåˆ—å¾®åˆ†ï¼‰ | 45åˆ† |
| Day 4 | Zone 3 å¾ŒåŠï¼ˆ3.9-3.12: é€£é–å¾‹ãƒ»è‡ªå‹•å¾®åˆ†ãƒ»Boss Battleï¼‰ | 60åˆ† |
| Day 5 | Zone 4ï¼ˆå®Ÿè£…ï¼‰ | 45åˆ† |
| Day 6 | Zone 5ï¼ˆãƒ†ã‚¹ãƒˆï¼‰ | 30åˆ† |
| Day 7 | å¾©ç¿’: 2Ã—2è¡Œåˆ—ã®SVDã‚’æ‰‹è¨ˆç®— + è‡ªå·±ãƒã‚§ãƒƒã‚¯ | 30åˆ† |

### 6.6 é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼

```python
"""ç¬¬3å› ç·šå½¢ä»£æ•° II ã®å­¦ç¿’é€²æ—ãƒã‚§ãƒƒã‚«ãƒ¼"""

topics = {
    "SVDã®å®šç¾©ã¨å­˜åœ¨å®šç†": False,
    "Eckart-Youngå®šç†": False,
    "ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ã®å¿œç”¨": False,
    "æ“¬ä¼¼é€†è¡Œåˆ— (Moore-Penrose)": False,
    "PCA ã® SVD ã«ã‚ˆã‚‹å°å‡º": False,
    "Kroneckerç©": False,
    "Einsteinè¨˜æ³• / einsum": False,
    "å‹¾é…ãƒ»ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ãƒ»ãƒ˜ã‚·ã‚¢ãƒ³": False,
    "Matrix Cookbook ä¸»è¦å…¬å¼": False,
    "é€£é–å¾‹ (ãƒ™ã‚¯ãƒˆãƒ«ç‰ˆ)": False,
    "Backpropagation å®Œå…¨å°å‡º": False,
    "Forward Mode AD (Dual Numbers)": False,
    "Reverse Mode AD": False,
    "Transformer 1å±¤ã®å®Œå…¨å¾®åˆ†": False,
    "æ•°å€¤å®‰å®šæ€§ / Log-Sum-Exp": False,
}

completed = sum(topics.values())
total = len(topics)
print(f"=== ç¬¬3å› é€²æ—: {completed}/{total} ({100*completed/total:.0f}%) ===")
for topic, done in topics.items():
    mark = "âœ“" if done else " "
    print(f"  [{mark}] {topic}")

if completed == total:
    print("\nç¬¬3å› å®Œå…¨ã‚¯ãƒªã‚¢ï¼ ç¬¬4å›ï¼ˆç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦ï¼‰ã¸é€²ã‚‚ã†ã€‚")
elif completed >= total * 0.7:
    print("\nã‚ˆãã§ããŸã€‚æ®‹ã‚Šã¯ç¬¬4å›ã‚’èª­ã‚“ã å¾Œã«æˆ»ã£ã¦ç¢ºèªã—ã‚ˆã†ã€‚")
else:
    print("\nZone 3 ã‚’ä¸­å¿ƒã«ã‚‚ã†ä¸€åº¦å¾©ç¿’ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã™ã‚‹ã€‚")
```

### 6.7 æ¬¡å›äºˆå‘Š: ç¬¬4å›ã€Œç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦ã€

ç¬¬4å›ã§ã¯ã€ç¢ºç‡åˆ†å¸ƒã®è¨˜è¿°ãƒ»æ“ä½œãƒ»æ¨å®šã‚’å®Œå…¨ã«ç¿’å¾—ã™ã‚‹:

1. **ç¢ºç‡ç©ºé–“ã®å®šç¾©** â€” Kolmogorovã®å…¬ç†ã‹ã‚‰ç¢ºç‡åˆ†å¸ƒã¾ã§
2. **ãƒ™ã‚¤ã‚ºã®å®šç†** â€” äº‹å‰åˆ†å¸ƒ Ã— å°¤åº¦ â†’ äº‹å¾Œåˆ†å¸ƒ
3. **æŒ‡æ•°å‹åˆ†å¸ƒæ—** â€” æ­£è¦åˆ†å¸ƒã‚‚ãƒã‚¢ã‚½ãƒ³ã‚‚Bernoulliã‚‚çµ±ä¸€çš„ã«ç†è§£
4. **æœ€å°¤æ¨å®šï¼ˆMLEï¼‰** â€” æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ = ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«åˆ†å¸ƒã®MLE

**ã‚­ãƒ¼ã¨ãªã‚‹LLM/Transformeræ¥ç‚¹**:
- æ¡ä»¶ä»˜ãç¢ºç‡ â†’ è‡ªå·±å›å¸° $p(x_t \mid x_{<t})$
- Softmax â†’ ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«åˆ†å¸ƒ
- Cross-Entropy â†’ å¯¾æ•°å°¤åº¦ã®è² 

> **ç¬¬3å›ã®é™ç•Œ**: è¡Œåˆ—ã‚’ã€Œåˆ†è§£ã€ã—ã€Œå¾®åˆ†ã€ã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸã€‚ã ãŒã€ãƒ‡ãƒ¼ã‚¿ã®ã€Œä¸ç¢ºå®Ÿæ€§ã€ã‚’æ‰±ã†ã«ã¯ç¢ºç‡è«–ãŒå¿…è¦ã€‚ã€Œã“ã®äºˆæ¸¬ã¯ã©ã®ãã‚‰ã„ç¢ºã‹ã‚‰ã—ã„ã®ã‹ã€ã‚’æ•°å­¦çš„ã«è¨˜è¿°ã™ã‚‹é“å…·ãŒã€ç¬¬4å›ã§æ‰‹ã«å…¥ã‚‹ã€‚

:::message
**é€²æ—: 100% å®Œäº†!** ç¬¬3å›ã€Œç·šå½¢ä»£æ•° II: SVDãƒ»è¡Œåˆ—å¾®åˆ†ãƒ»ãƒ†ãƒ³ã‚½ãƒ«ã€ã‚’å®Œèµ°ã—ãŸã€‚SVDã®å­˜åœ¨å®šç†ãƒ»Eckart-Youngå®šç†ã‹ã‚‰å§‹ã¾ã‚Šã€ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—ãƒ»Einsteinè¨˜æ³•ã€è¡Œåˆ—å¾®åˆ†ã€é€£é–å¾‹ã€Backpropagationã€è‡ªå‹•å¾®åˆ†ã‚’çµŒã¦ã€Transformer 1å±¤ã®å®Œå…¨å¾®åˆ†ã‚’å°å‡ºã—ãŸã€‚ãŠç–²ã‚Œã•ã¾ã§ã—ãŸã€‚
:::

### 6.8 ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **SVDã¯ä¸‡èƒ½ãƒŠã‚¤ãƒ•ã€‚ç”»åƒåœ§ç¸®ã‚‚PCAã‚‚æ¨è–¦ã‚‚LoRAã‚‚ã€å…¨ã¦ã€ŒåŒã˜è¨ˆç®—ã€ã«å¸°ç€ã™ã‚‹ã®ã§ã¯ï¼Ÿ**

ã“ã®å•ã„ã®æ„å‘³ã‚’è€ƒãˆã¦ã¿ã¦ã»ã—ã„ã€‚

ä¸€è¦‹ã™ã‚‹ã¨å…¨ãç•°ãªã‚‹å•é¡Œ â€” ç”»åƒã‚’åœ§ç¸®ã™ã‚‹ã€ãƒ‡ãƒ¼ã‚¿ã®æ¬¡å…ƒã‚’å‰Šæ¸›ã™ã‚‹ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¥½ã¿ã‚’äºˆæ¸¬ã™ã‚‹ã€LLMã‚’åŠ¹ç‡çš„ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ â€” ãŒã€å…¨ã¦ã€Œè¡Œåˆ—ã‚’ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ã™ã‚‹ã€ã¨ã„ã†åŒä¸€ã®æ•°å­¦çš„æ“ä½œã«å¸°ç€ã™ã‚‹ã€‚

ã“ã‚Œã¯å¶ç„¶ã§ã¯ãªã„ã€‚**å®Ÿä¸–ç•Œã®ãƒ‡ãƒ¼ã‚¿ã®å¤šãã¯æœ¬è³ªçš„ã«ä½æ¬¡å…ƒ**ã ã‹ã‚‰ã ã€‚100ä¸‡ç”»ç´ ã®ç”»åƒã‚‚ã€æ•°ç™¾ã®ç‰¹ç•°å€¤ã§ååˆ†ã«è¡¨ç¾ã§ãã‚‹ã€‚10å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®LLMã®é‡ã¿æ›´æ–°ã‚‚ã€rank-16ç¨‹åº¦ã§ååˆ†ãªå ´åˆãŒå¤šã„ã€‚

Eckart-Youngå®šç†ã¯ã€ã“ã®ä½æ¬¡å…ƒæ§‹é€ ã‚’ã€Œæœ€é©ã«ã€æŠ½å‡ºã™ã‚‹æ–¹æ³•ã‚’ä¿è¨¼ã™ã‚‹ã€‚SVDã¯ã€ãƒ‡ãƒ¼ã‚¿ã®æœ¬è³ªçš„ãªæ¬¡å…ƒã‚’è¦‹æŠœã**æ™®éçš„ãªé“å…·**ã ã€‚

:::details è­°è«–ãƒã‚¤ãƒ³ãƒˆ
1. **SVDã®é™ç•Œã¯ã©ã“ã«ã‚ã‚‹ã‹ï¼Ÿ** â€” SVDã¯ç·šå½¢éƒ¨åˆ†ç©ºé–“ã‚’è¦‹ã¤ã‘ã‚‹ã€‚ãƒ‡ãƒ¼ã‚¿ãŒéç·šå½¢å¤šæ§˜ä½“ä¸Šã«ã‚ã‚‹å ´åˆï¼ˆä¾‹: æ‰‹æ›¸ãæ•°å­—ã®ç”»åƒï¼‰ã€ã‚«ãƒ¼ãƒãƒ«PCAã‚„ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãªã©éç·šå½¢æ‰‹æ³•ãŒå¿…è¦ã€‚
2. **è‡ªå‹•å¾®åˆ†ã¯"çŸ¥èƒ½"ã®ä¸€éƒ¨ã‹ï¼Ÿ** â€” å­¦ç¿’ã¨ã¯å‹¾é…ã«å¾“ã£ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã™ã‚‹ã“ã¨ã€‚è‡ªå‹•å¾®åˆ†ãŒãªã‘ã‚Œã°æ·±å±¤å­¦ç¿’ã¯å­˜åœ¨ã—ãªã„ã€‚è¨ˆç®—ã®ã€Œå·»ãæˆ»ã—ã€ãŒå­¦ç¿’ã‚’å¯èƒ½ã«ã™ã‚‹ã¨ã„ã†äº‹å®Ÿã¯ã€çŸ¥èƒ½ã®æœ¬è³ªã«ä½•ã‚’ç¤ºå”†ã™ã‚‹ã‹ã€‚
3. **è¡Œåˆ—å¾®åˆ†ã®è¨ˆç®—é‡ã¯AIã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®é™ç•Œã‚’æ±ºã‚ã‚‹ã‹ï¼Ÿ** â€” Transformerã®å­¦ç¿’ã¯Attentionã®å¾®åˆ†ãŒæ”¯é…çš„ã€‚FlashAttention[^12]ã¯ã“ã®è¨ˆç®—ã‚’å†æ§‹æˆã—ã¦é«˜é€ŸåŒ–ã—ãŸã€‚è¡Œåˆ—å¾®åˆ†ã®åŠ¹ç‡åŒ–ãŒã€æ¬¡ä¸–ä»£AIã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’æ±ºå®šã™ã‚‹ã€‚
:::

---

### 6.9 æœ€æ–°ç ”ç©¶ (2020-2026)

#### 6.9.1 SVDã¨ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯

SVDã¯æ·±å±¤å­¦ç¿’ã«ãŠã„ã¦ã€é‡ã¿è¡Œåˆ—ã®åˆæœŸåŒ–ã€åœ§ç¸®ã€è§£æã«åºƒãä½¿ã‚ã‚Œã¦ã„ã‚‹ã€‚æœ€è¿‘ã®ç ”ç©¶ã¯ã€SVDã‚’ã€Œå¾®åˆ†å¯èƒ½ãªå±¤ã€ã¨ã—ã¦ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«çµ„ã¿è¾¼ã‚€ã“ã¨ã§ã€æ–°ãŸãªè¡¨ç¾åŠ›ã‚’ç²å¾—ã—ã¦ã„ã‚‹[^14]ã€‚

**å¾®åˆ†å¯èƒ½SVDå±¤**: ç‰¹ç•°å€¤åˆ†è§£ã‚’å‹¾é…é™ä¸‹æ³•ã§å­¦ç¿’å¯èƒ½ã«ã™ã‚‹ãŸã‚ã€é‡è¤‡ç‰¹ç•°å€¤ã®å ´åˆã§ã‚‚å®‰å®šã—ãŸå‹¾é…ä¼æ’­ã‚’ä¿è¨¼ã™ã‚‹æŠ€è¡“ãŒé–‹ç™ºã•ã‚ŒãŸã€‚ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®é‡ã¿ã‚’ $W = U\Sigma V^\top$ ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã—ã€$U, V$ ã¯ç›´äº¤/ãƒ¦ãƒ‹ã‚¿ãƒªã€$\Sigma$ ã¯å­¦ç¿’ã•ã‚Œã€ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒã‚¹ã‚„ä½ãƒ©ãƒ³ã‚¯æ€§ã‚’æ­£å‰‡åŒ–é …ã§ä¿ƒé€²ã™ã‚‹[^15]ã€‚

**é‡ã¿è¡Œåˆ—ã®ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼**: æ·±å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®åœ§ç¸®ã«ãŠã„ã¦ã€SVDã«ã‚ˆã‚‹ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ãŒè¨ˆç®—é‡å‰Šæ¸›ã«è²¢çŒ®ã™ã‚‹ã€‚ç‰¹ã«ã€ç•³ã¿è¾¼ã¿å±¤ã‚„Transformerã®Attentioné‡ã¿è¡Œåˆ—ã«é©ç”¨ã™ã‚‹ã“ã¨ã§ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’å¤§å¹…ã«å‰Šæ¸›ã—ã¤ã¤ç²¾åº¦ã‚’ç¶­æŒã§ãã‚‹[^16]ã€‚

**SVDã¨LoRAã®ç†è«–çš„æ¥ç¶š**: LoRA (Low-Rank Adaptation)[^10]ã¯ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚ã®é‡ã¿æ›´æ–° $\Delta W$ ãŒæœ¬è³ªçš„ã«ä½ãƒ©ãƒ³ã‚¯æ§‹é€ ã‚’æŒã¤ã¨ã„ã†çµŒé¨“å‰‡ã«åŸºã¥ãã€‚ã“ã‚Œã¯ã€$\Delta W$ ã®SVDã‚’å–ã‚‹ã¨ä¸Šä½æ•°å€‹ã®ç‰¹ç•°å€¤ãŒæ”¯é…çš„ã§ã‚ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã€Eckart-Youngå®šç†[^3]ãŒãã®ç†è«–çš„è£ä»˜ã‘ã‚’ä¸ãˆã‚‹ã€‚

#### 6.9.2 Randomized SVDã®é€²åŒ–

å¤§è¦æ¨¡è¡Œåˆ—ã«å¯¾ã™ã‚‹SVDã®è¨ˆç®—ã‚³ã‚¹ãƒˆã¯ $O(\min(mn^2, m^2n))$ ã§ã‚ã‚Šã€æ•°ç™¾ä¸‡Ã—æ•°ç™¾ä¸‡ã®è¡Œåˆ—ã«ã¯å®Ÿç”¨çš„ã§ãªã„ã€‚Randomized SVD[^17]ã¯ã€ãƒ©ãƒ³ãƒ€ãƒ å°„å½±ã§æ¬¡å…ƒã‚’è½ã¨ã—ã¦ã‹ã‚‰SVDã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ã§ã€è¨ˆç®—é‡ã‚’åŠ‡çš„ã«å‰Šæ¸›ã™ã‚‹ã€‚

**GPUå®Ÿè£…ã®æœ€é©åŒ–**: 2021å¹´ã®arXivè«–æ–‡[^18]ã¯ã€Randomized SVDã‚’GPUä¸Šã§åŠ¹ç‡çš„ã«å®Ÿè£…ã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã—ãŸã€‚BLAS-3æ¼”ç®—ï¼ˆè¡Œåˆ—ç©ï¼‰ã‚’ãƒ“ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ãƒ–ãƒ­ãƒƒã‚¯ã¨ã—ã¦å†æ§‹æˆã™ã‚‹ã“ã¨ã§ã€CPUã®æ•°åå€ã®é«˜é€ŸåŒ–ã‚’é”æˆã—ãŸã€‚

$$
\text{Complexity: } O(k(m+n)\log k + k^2 \min(m,n)) \quad (\text{Full SVD: } O(\min(mn^2, m^2n)))
$$

**é©å¿œçš„ãƒ©ãƒ³ã‚¯é¸æŠ**: å¾“æ¥ã®Randomized SVDã¯ãƒ©ãƒ³ã‚¯kã‚’äº‹å‰ã«å›ºå®šã™ã‚‹å¿…è¦ãŒã‚ã£ãŸãŒã€æœ€è¿‘ã®æ‰‹æ³•ã¯ç‰¹ç•°å€¤ã®æ¸›è¡°ç‡ã«åŸºã¥ã„ã¦å‹•çš„ã«kã‚’èª¿æ•´ã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ç²¾åº¦ã¨è¨ˆç®—ã‚³ã‚¹ãƒˆã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’è‡ªå‹•çš„ã«æœ€é©åŒ–ã§ãã‚‹ã€‚

**è¡Œåˆ—è£œå®Œã¸ã®å¿œç”¨**: Randomized SVDã¯ã€ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã‘ã‚‹æ¬ æå€¤è£œå®Œå•é¡Œï¼ˆNetflix Prizeï¼‰ã§é‡è¦ãªå½¹å‰²ã‚’æœãŸã™ã€‚$10^6 \times 10^6$ ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼-ã‚¢ã‚¤ãƒ†ãƒ è¡Œåˆ—ã®99%ãŒæ¬ æã—ã¦ã„ã‚‹ã‚±ãƒ¼ã‚¹ã§ã‚‚ã€ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ã«ã‚ˆã‚ŠåŠ¹ç‡çš„ã«è£œå®Œã§ãã‚‹[^19]ã€‚

#### 6.9.3 è‡ªå‹•å¾®åˆ†ã®æœ€æ–°å‹•å‘

Automatic Differentiationã¯æ·±å±¤å­¦ç¿’ã®åŸºç›¤æŠ€è¡“ã§ã‚ã‚Šã€å¸¸ã«é€²åŒ–ã—ç¶šã‘ã¦ã„ã‚‹ã€‚

**ãƒ†ãƒ³ã‚½ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¸ã®å¿œç”¨**: ç‰©ç†å­¦ã§ç™ºå±•ã—ãŸãƒ†ãƒ³ã‚½ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç†è«–ã¨è‡ªå‹•å¾®åˆ†ã®èåˆãŒé€²ã‚“ã§ã„ã‚‹[^20]ã€‚ãƒ†ãƒ³ã‚½ãƒ«ç¹°ã‚Šè¾¼ã¿ç¾¤ï¼ˆTRGï¼‰ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å¾®åˆ†å¯èƒ½ã«ã™ã‚‹ã“ã¨ã§ã€é‡å­å¤šä½“ç³»ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨æ©Ÿæ¢°å­¦ç¿’ã‚’çµ±åˆã§ãã‚‹ã€‚

**Forward-mode ADã®å¾©æ¨©**: Reverse-mode ADãŒæ”¯é…çš„ã ãŒã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚ˆã‚Šã‚‚å‡ºåŠ›æ•°ãŒå¤šã„å ´åˆï¼ˆä¾‹: ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®æ„Ÿåº¦è§£æï¼‰ã§ã¯ã€Forward-mode ADã®æ–¹ãŒåŠ¹ç‡çš„ã [^21]ã€‚ç‰¹ã«ãƒ†ãƒ³ã‚½ãƒ«ç¹°ã‚Šè¾¼ã¿ã§ã¯ã€å±€æ‰€ãƒ†ãƒ³ã‚½ãƒ«ã®å‹¾é…ã‚’é †æ–¹å‘ã«ä¼æ’­ã•ã›ã‚‹ã“ã¨ã§ã€æ·±ã„è¨ˆç®—ã‚°ãƒ©ãƒ•ã‚’å›é¿ã§ãã‚‹ã€‚

**å¤‰åˆ†å¾®åˆ†ã®è‡ªå‹•åŒ–**: é–¢æ•°ç©ºé–“ä¸Šã®å¾®åˆ†ï¼ˆå¤‰åˆ†å¾®åˆ†ï¼‰ã‚’è‡ªå‹•åŒ–ã™ã‚‹æŠ€è¡“ãŒç™ºå±•ã—ã¦ã„ã‚‹[^22]ã€‚ã“ã‚Œã¯ç‰©ç†å­¦ã‚„å¿œç”¨æ•°å­¦ã§åºƒãä½¿ã‚ã‚Œã‚‹ãŒã€æ©Ÿæ¢°å­¦ç¿’ã§ã‚‚Neural ODEã‚„PDEåˆ¶ç´„æœ€é©åŒ–ã§é‡è¦æ€§ãŒå¢—ã—ã¦ã„ã‚‹ã€‚

$$
\frac{\delta F[\phi]}{\delta \phi(x)} \quad \text{(functional derivative)}
$$

**é«˜éšå¾®åˆ†ã®åŠ¹ç‡åŒ–**: ãƒ˜ã‚·ã‚¢ãƒ³è¨ˆç®—ã¯ $O(n^2)$ ã®ã‚³ã‚¹ãƒˆãŒã‹ã‹ã‚‹ãŒã€ãƒ˜ã‚·ã‚¢ãƒ³-ãƒ™ã‚¯ãƒˆãƒ«ç© $H\mathbf{v}$ ã¯ Forward-over-Reverse AD ã§ $O(n)$ ã§è¨ˆç®—å¯èƒ½ã€‚ã“ã‚Œã¯2æ¬¡æœ€é©åŒ–æ³•ï¼ˆNewtonæ³•ã€Natural Gradientï¼‰ã§é‡è¦ã ã€‚

#### 6.9.4 è¡Œåˆ—å¾®åˆ†ã®è¨ˆç®—è¤‡é›‘æ€§ç†è«–

è¡Œåˆ—å¾®åˆ†ã®è¨ˆç®—ã«ã¯ã€æ¼”ç®—å›æ•°ã ã‘ã§ãªããƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚é‡è¦ã ã€‚

**FlashAttentionã®é©æ–°**: Transformer ã®Attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã¯ã€Softmaxã®è¡Œåˆ—å¾®åˆ†ãŒæ”¯é…çš„ã‚³ã‚¹ãƒˆã ã£ãŸã€‚FlashAttention[^12]ã¯ã€ãƒ¡ãƒ¢ãƒªéšå±¤ï¼ˆHBM â†” SRAMï¼‰ã‚’æ„è­˜ã—ãŸè¨ˆç®—é †åºã®å†æ§‹æˆã«ã‚ˆã‚Šã€IOå›æ•°ã‚’ $O(N^2)$ ã‹ã‚‰ $O(N^2/M)$ ã«å‰Šæ¸›ã—ãŸï¼ˆ$M$: SRAMã‚µã‚¤ã‚ºï¼‰ã€‚ã“ã‚Œã¯ã€ŒåŒã˜FLOPSã§ã‚‚é€Ÿã„ã€ã¨ã„ã†ã€è¨ˆç®—é‡ç†è«–ã®é™ç•Œã‚’è¶…ãˆãŸæœ€é©åŒ–ã ã€‚

**å› æœå¾‹ã¨è¨ˆç®—ã‚°ãƒ©ãƒ•**: Backpropagationã¯è¨ˆç®—ã‚°ãƒ©ãƒ•ã®å› æœæ§‹é€ ã«ä¾å­˜ã™ã‚‹ã€‚ã‚°ãƒ©ãƒ•ã®ã€Œå¹…ã€ï¼ˆåŒæ™‚ã«ç”Ÿãã¦ã„ã‚‹ãƒãƒ¼ãƒ‰æ•°ï¼‰ãŒãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ±ºå®šã—ã€ã€Œæ·±ã•ã€ãŒé€†ä¼æ’­ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’æ±ºå®šã™ã‚‹ã€‚CheckpointingæŠ€è¡“ã¯ã€æ·±ã•ã¨å¹…ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’åˆ¶å¾¡ã™ã‚‹ã€‚

**æ•°å€¤å®‰å®šæ€§ã¨ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°**: è¡Œåˆ—å¾®åˆ†ã®æ•°å€¤èª¤å·®ã¯ã€æ¡ä»¶æ•° $\kappa(A) = \sigma_{\max}/\sigma_{\min}$ ã«ä¾å­˜ã™ã‚‹ã€‚LayerNormã‚„BatchNormã¯ã€æ´»æ€§åŒ–ã®æ¡ä»¶æ•°ã‚’æŠ‘åˆ¶ã™ã‚‹ã“ã¨ã§ã€æ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã®å‹¾é…ä¼æ’­ã‚’å®‰å®šåŒ–ã™ã‚‹ã€‚

```python
# Condition number and gradient stability
import numpy as np

A = np.random.randn(100, 100)
U, s, Vt = np.linalg.svd(A, full_matrices=False)

# Create ill-conditioned matrix
s_ill = np.logspace(0, -10, 100)  # condition number = 1e10
A_ill = U @ np.diag(s_ill) @ Vt

# Create well-conditioned matrix
s_well = np.ones(100)  # condition number = 1
A_well = U @ np.diag(s_well) @ Vt

print(f"Ill-conditioned:  Îº = {np.linalg.cond(A_ill):.2e}")
print(f"Well-conditioned: Îº = {np.linalg.cond(A_well):.2e}")

# Gradient computation stability
def loss_fn(W, x, y):
    return np.linalg.norm(W @ x - y)**2

x = np.random.randn(100)
y = np.random.randn(100)

# Numerical gradient (finite difference)
eps = 1e-7
grad_num_ill = np.zeros_like(A_ill)
for i in range(min(5, 100)):  # sample for speed
    for j in range(min(5, 100)):
        A_plus = A_ill.copy(); A_plus[i,j] += eps
        A_minus = A_ill.copy(); A_minus[i,j] -= eps
        grad_num_ill[i,j] = (loss_fn(A_plus, x, y) - loss_fn(A_minus, x, y)) / (2*eps)

# Analytical gradient
pred_ill = A_ill @ x
grad_analytical_ill = 2 * np.outer(pred_ill - y, x)

# Check first 5x5 block
error = np.max(np.abs(grad_num_ill[:5,:5] - grad_analytical_ill[:5,:5]))
print(f"Gradient error (ill-conditioned): {error:.2e}")

# â†’ Ill-conditioned matrices amplify numerical errors in gradient computation
```

#### 6.9.5 é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã¨SVD

é‡å­ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ãŠã‘ã‚‹SVDã®å½¹å‰²ãŒæ³¨ç›®ã•ã‚Œã¦ã„ã‚‹ã€‚

**é‡å­SVDã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **: é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ä¸Šã§ã®SVDè¨ˆç®—ã¯ã€å¤å…¸ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã® $O(mn^2)$ ã‚’æŒ‡æ•°çš„ã«æ”¹å–„ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚ãŸã ã—ã€é‡å­çŠ¶æ…‹ã®èª­ã¿å‡ºã—ã‚³ã‚¹ãƒˆã‚’å«ã‚ãŸå…¨ä½“ã®è¤‡é›‘æ€§ã¯ä¾ç„¶ã¨ã—ã¦ç ”ç©¶ä¸­ã ã€‚

**ãƒ†ãƒ³ã‚½ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨è¡Œåˆ—ç©çŠ¶æ…‹**: é‡å­å¤šä½“ç³»ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ã€æ³¢å‹•é–¢æ•°ã‚’è¡Œåˆ—ç©çŠ¶æ…‹ï¼ˆMPSï¼‰ã§è¡¨ç¾ã™ã‚‹ã€‚MPSã®æœ€é©åŒ–ã¯SVDã®åå¾©é©ç”¨ã§ã‚ã‚Šã€é‡å­ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆã®ãƒ©ãƒ³ã‚¯ãŒè¨ˆç®—è¤‡é›‘æ€§ã‚’æ±ºå®šã™ã‚‹ã€‚

```python
# Singular value decay and entanglement entropy
import numpy as np

# Random matrix representing quantum state
psi = np.random.randn(2**10, 2**10) + 1j * np.random.randn(2**10, 2**10)
psi = psi / np.linalg.norm(psi)

# SVD
U, s, Vt = np.linalg.svd(psi, full_matrices=False)

# Entanglement entropy: S = -Î£ Î»áµ¢ log Î»áµ¢, where Î»áµ¢ = sáµ¢Â²
lambda_sq = s**2
lambda_sq = lambda_sq / lambda_sq.sum()  # normalize
entropy = -np.sum(lambda_sq * np.log(lambda_sq + 1e-16))

print(f"Entanglement entropy: {entropy:.4f}")
print(f"Max entropy (uniform): {np.log(len(s)):.4f}")
print(f"Entropy/Max: {entropy/np.log(len(s)):.2%}")

# Truncation rank for 99% fidelity
cumsum = np.cumsum(lambda_sq)
rank_99 = np.searchsorted(cumsum, 0.99) + 1
print(f"Rank for 99% fidelity: {rank_99} (out of {len(s)})")
```

### 6.10 SVDã¨ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®æœªæ¥

SVDã¯ã€ãƒ‡ãƒ¼ã‚¿åœ§ç¸®ãƒ»ãƒã‚¤ã‚ºé™¤å»ãƒ»æ½œåœ¨æ§‹é€ ç™ºè¦‹ã®æ™®éçš„ãƒ„ãƒ¼ãƒ«ã§ã‚ã‚Šç¶šã‘ã‚‹ã€‚ã—ã‹ã—ã€ãã®é©ç”¨ç¯„å›²ã¯é€²åŒ–ã—ã¦ã„ã‚‹ã€‚

**éè² å€¤è¡Œåˆ—åˆ†è§£ï¼ˆNMFï¼‰ã¨ã®æ¯”è¼ƒ**: SVDã¯è² ã®å€¤ã‚’è¨±ã™ãŒã€NMFï¼ˆNon-negative Matrix Factorizationï¼‰ã¯ $A \approx WH$, $W, H \geq 0$ ã¨åˆ†è§£ã™ã‚‹ã€‚NMFã¯ç”»åƒã®ã€Œéƒ¨å“ã€ã‚„æ–‡æ›¸ã®ãƒˆãƒ”ãƒƒã‚¯æ§‹é€ ã‚’è§£é‡ˆã—ã‚„ã™ã„å½¢ã§æŠ½å‡ºã§ãã‚‹ã€‚SVDã¨NMFã¯è£œå®Œçš„ãªé–¢ä¿‚ã«ã‚ã‚‹ã€‚

**ãƒ†ãƒ³ã‚½ãƒ«åˆ†è§£**: 3æ¬¡å…ƒä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿ã¯ãƒ†ãƒ³ã‚½ãƒ«ã¨ã—ã¦æ‰±ã†ã€‚Tuckeråˆ†è§£ã‚„CPåˆ†è§£ã¯ã€SVDã®ãƒ†ãƒ³ã‚½ãƒ«ç‰ˆã ã€‚å‹•ç”»ãƒ‡ãƒ¼ã‚¿ï¼ˆæ™‚é–“Ã—é«˜ã•Ã—å¹…ï¼‰ã‚„fMRIè„³ç”»åƒï¼ˆæ™‚é–“Ã—xÃ—yÃ—zï¼‰ã®è§£æã§å¨åŠ›ã‚’ç™ºæ®ã™ã‚‹ã€‚

**æ·±å±¤å­¦ç¿’ã¨ã®èåˆ**: SVDã¯ã€Œç·šå½¢ã€ã®ä¸–ç•Œã®é“å…·ã ãŒã€æ·±å±¤å­¦ç¿’ã¯ã€Œéç·šå½¢ã€ã ã€‚ã—ã‹ã—ã€å„å±¤ã®ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã‚’SVDã§è§£æã™ã‚‹ã“ã¨ã§ã€å‹¾é…ä¼æ’­ã®å®‰å®šæ€§ã‚„è¡¨ç¾åŠ›ã‚’å®šé‡åŒ–ã§ãã‚‹ã€‚ã“ã‚Œã¯ã€Œãªãœã“ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯å­¦ç¿’ã§ãã‚‹ã®ã‹ã€ã¨ã„ã†ç†è«–è§£æã®éµã¨ãªã‚‹ã€‚

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Å., & Polosukhin, I. (2017). Attention Is All You Need. *NeurIPS 2017*.
@[card](https://arxiv.org/abs/1706.03762)

[^2]: Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by back-propagating errors. *Nature*, 323, 533-536.
@[card](https://doi.org/10.1038/323533a0)

[^3]: Eckart, C. & Young, G. (1936). The Approximation of One Matrix by Another of Lower Rank. *Psychometrika*, 1, 211-218.
@[card](https://doi.org/10.1007/BF02288367)

[^5]: Pearson, K. (1901). On Lines and Planes of Closest Fit to Systems of Points in Space. *Philosophical Magazine*, 2(11), 559-572.
@[card](https://doi.org/10.1080/14786440109462720)

[^6]: Hotelling, H. (1933). Analysis of a complex of statistical variables into principal components. *Journal of Educational Psychology*, 24(6), 417-441.
@[card](https://doi.org/10.1037/h0071325)

[^7]: Baydin, A. G., Pearlmutter, B. A., Radul, A. A., & Siskind, J. M. (2018). Automatic Differentiation in Machine Learning: a Survey. *JMLR*, 18(153), 1-43.
@[card](https://arxiv.org/abs/1502.05767)

[^10]: Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W. (2022). LoRA: Low-Rank Adaptation of Large Language Models. *ICLR 2022*.
@[card](https://arxiv.org/abs/2106.09685)

[^12]: Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & RÃ©, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. *NeurIPS 2022*.
@[card](https://arxiv.org/abs/2205.14135)

[^13]: Rezende, D. J. & Mohamed, S. (2015). Variational Inference with Normalizing Flows. *ICML 2015*.
@[card](https://arxiv.org/abs/1505.05770)

[^14]: Mathiasen, A. & HvilshÃ¸j, F. (2020). What if Neural Networks had SVDs? *NeurIPS 2020*.
@[card](https://proceedings.neurips.cc/paper/2020/file/d61e4bbd6393c9111e6526ea173a7c8b-Paper.pdf)

[^15]: Differentiable SVD Layer. (2024). Emergent Mind Topics.
@[card](https://www.emergentmind.com/topics/differentiable-singular-value-decomposition-svd-layer)

[^16]: Low-Rank Matrix Approximation for Neural Network Compression. (2025). *arXiv preprint*.
@[card](https://arxiv.org/pdf/2504.20078)

[^17]: Halko, N., Martinsson, P. G., & Tropp, J. A. (2011). Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. *SIAM Review*, 53(2), 217-288.

[^18]: Feng, X., Xie, Y., Song, M., Yu, W., & Tang, J. (2021). Efficient GPU Implementation of Randomized SVD and Its Applications. *arXiv preprint*.
@[card](https://arxiv.org/abs/2110.03423)

[^19]: Feng, X., et al. (2018). Faster Matrix Completion Using Randomized SVD. *arXiv preprint*.
@[card](https://arxiv.org/abs/1810.06860)

[^20]: Liu, J. G., et al. (2019). Differentiable Programming Tensor Networks. *arXiv preprint*.
@[card](https://arxiv.org/abs/1903.09650)

[^21]: Forward-mode automatic differentiation for the tensor renormalization group. (2026). *arXiv preprint*.
@[card](https://arxiv.org/html/2602.08987)

[^22]: Automating Variational Differentiation. (2024). *arXiv preprint*.
@[card](https://arxiv.org/html/2406.16154)

### æ•™ç§‘æ›¸

[^8]: Griewank, A. & Walther, A. (2008). *Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation* (2nd ed.). SIAM.

[^9]: Petersen, K. B. & Pedersen, M. S. (2012). *The Matrix Cookbook*. Technical Report, DTU.

---

## è¨˜æ³•è¦ç´„

| è¨˜å· | æ„å‘³ | åˆå‡º |
|:-----|:-----|:-----|
| $A, B, W$ | è¡Œåˆ—ï¼ˆå¤§æ–‡å­—ï¼‰ | 3.1 |
| $\mathbf{x}, \mathbf{v}$ | ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆå¤ªå­—å°æ–‡å­—ï¼‰ | 3.7 |
| $\sigma_i$ | ç‰¹ç•°å€¤ | 3.1 |
| $\mathbf{u}_i, \mathbf{v}_i$ | å·¦/å³ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ« | 3.1 |
| $U, \Sigma, V$ | SVDã®æ§‹æˆè¡Œåˆ— | 3.1 |
| $A^+$ | Moore-Penroseæ“¬ä¼¼é€†è¡Œåˆ— | 3.4 |
| $A_k$ | rank-$k$ æˆªæ–­SVD | 3.2 |
| $\nabla f$ | å‹¾é… | 3.7 |
| $J$ | ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ | 3.7 |
| $H$ | ãƒ˜ã‚·ã‚¢ãƒ³ | 3.7 |
| $\boldsymbol{\delta}_l$ | ç¬¬$l$å±¤ã®èª¤å·®ä¿¡å· | 3.9 |
| $\otimes$ | Kroneckerç© | 3.6 |
| $\odot$ | Hadamardç©ï¼ˆè¦ç´ ã”ã¨ã®ç©ï¼‰ | 3.9 |
| $\text{vec}(A)$ | è¡Œåˆ—ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ– | 3.6 |
| $\kappa(A)$ | æ¡ä»¶æ•° | 4.4 |

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
