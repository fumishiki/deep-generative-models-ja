---
title: "ç¬¬22å›: ãƒã‚¤ãƒ†ã‚£ãƒ–ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å®Œå…¨ç‰ˆ: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸ‘ï¸"
type: "tech"
topics: ["machinelearning", "deeplearning", "multimodal", "julia", "rust"]
published: true
---

# ç¬¬22å›: ãƒã‚¤ãƒ†ã‚£ãƒ–ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å®Œå…¨ç‰ˆ

:::message
**å‰æçŸ¥è­˜**: ç¬¬16å› (Transformer), ç¬¬14-15å› (Attention), ç¬¬6å› (æƒ…å ±ç†è«–), ç¬¬18å› (ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°), ç¬¬21å› (ãƒ‡ãƒ¼ã‚¿å‡¦ç†)
**ã“ã®è¬›ç¾©ã®ç›®æ¨™**: Vision-Languageãƒ¢ãƒ‡ãƒ«ã®ç†è«–â†’ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£â†’å®Ÿè£…â†’è©•ä¾¡ã‚’å®Œå…¨ç¶²ç¾…ã€‚CLIPã€BLIP-2ã€Flamingoã€LLaVAã€Qwen-VLã€CogVLMã€SmolVLM2ã‚’æ·±æ˜ã‚Šè§£å‰–ã—ã€âš¡Julia+ğŸ¦€Rustã§å®Ÿè£…ã¾ã§å®Œèµ°ã™ã‚‹ã€‚
**å®Ÿè£…è¨€èª**: âš¡Julia (è¨“ç·´ãƒ»å®Ÿé¨“) + ğŸ¦€Rust (æ¨è«–)
:::

ç¬¬21å›ã§ãƒ‡ãƒ¼ã‚¿ã®æ‰±ã„æ–¹ã‚’å­¦ã‚“ã ã€‚ãƒ†ã‚­ã‚¹ãƒˆã‚‚ç”»åƒã‚‚éŸ³å£°ã‚‚ã€å…¨ã¦æ•°å€¤ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã§ãã‚‹ã“ã¨ã‚’çŸ¥ã£ãŸã€‚

ã•ã¦ã€ã“ã“ã§ç–‘å•ãŒæ¹§ã‹ãªã„ã ã‚ã†ã‹ï¼Ÿ

ã€Œãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã‚’**åŒæ™‚ã«**ç†è§£ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã¯ä½œã‚Œãªã„ã®ã‹ï¼Ÿã€

ç­”ãˆã¯**Yes**ã€‚ãã‚ŒãŒä»Šå›ã®ãƒ†ãƒ¼ãƒã€**ãƒã‚¤ãƒ†ã‚£ãƒ–ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«**ã ã€‚

æœ¬è¬›ç¾©ã¯3,000è¡Œè¶…ã®å¤§ä½œã ãŒã€å®‰å¿ƒã—ã¦ã»ã—ã„ã€‚Zone 0ã®30ç§’ã§ã€Œé©šãã€ã‚’ä½“é¨“ã—ã€Zone 3ã§ã€Œæ•°å¼ã®æœ¬è³ªã€ã‚’ä¿®è¡Œã—ã€Zone 4ã§ã€Œå®Ÿè£…ã®æ‰‹è§¦ã‚Šã€ã‚’æ´ã‚€ã€‚å…¨ã¦ã®Zoneã«æ„å‘³ãŒã‚ã‚‹ã€‚

æº–å‚™ã¯ã„ã„ã‹ï¼Ÿ ã•ã‚ã€ãƒœã‚¹æˆ¦ã ã€‚

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” CLIPã§ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆåˆ†é¡ã‚’ä½“é¨“

ã„ããªã‚Šã ãŒã€**3è¡Œã®Juliaã‚³ãƒ¼ãƒ‰**ã§ç”»åƒåˆ†é¡ã‚’ã‚„ã£ã¦ã¿ã‚ˆã†ã€‚è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¯**ã‚¼ãƒ­**ã ã€‚

```julia
using Transformers, Images

# ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
clip = hgf"openai/clip-vit-base-patch32"
img = load("cat.jpg")
texts = ["a cat", "a dog", "a car"]

# é¡ä¼¼åº¦è¨ˆç®— â†’ ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆåˆ†é¡
img_emb = clip.vision_model(img)  # (512,)
text_embs = [clip.text_model(t) for t in texts]  # [(512,), (512,), (512,)]
similarities = [dot(img_emb, t) / (norm(img_emb) * norm(t)) for t in text_embs]
# => [0.92, 0.15, 0.08] â€” "a cat" ãŒæœ€ã‚‚é¡ä¼¼

println("äºˆæ¸¬: $(texts[argmax(similarities)])")  # "a cat"
```

**å‡ºåŠ›**:
```
äºˆæ¸¬: a cat
```

**ä½•ãŒèµ·ããŸï¼Ÿ**

1. **ç”»åƒã‚¨ãƒ³ã‚³ãƒ¼ãƒ€**ãŒ`cat.jpg`ã‚’512æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ« `img_emb` ã«å¤‰æ›
2. **ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€**ãŒå„å€™è£œãƒ†ã‚­ã‚¹ãƒˆã‚’512æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ« `text_embs` ã«å¤‰æ›
3. **ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦**ã‚’è¨ˆç®—ã—ã¦ã€æœ€ã‚‚è¿‘ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’é¸æŠ

è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãªã—ã§åˆ†é¡ã§ããŸç†ç”±ã¯ã€**CLIPãŒç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’åŒã˜åŸ‹ã‚è¾¼ã¿ç©ºé–“ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã—ã¦ã„ã‚‹**ã‹ã‚‰ã€‚ã“ã®ç©ºé–“ã§ã¯ã€æ„å‘³çš„ã«è¿‘ã„ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆãŒç‰©ç†çš„ã«è¿‘ããªã‚‹ã€‚

æ•°å¼ã§æ›¸ã‘ã°ã€**ç”»åƒ $\mathbf{v}$ ã¨ãƒ†ã‚­ã‚¹ãƒˆ $\mathbf{t}$ ã®é¡ä¼¼åº¦**:

$$
\text{sim}(\mathbf{v}, \mathbf{t}) = \frac{\mathbf{v} \cdot \mathbf{t}}{\|\mathbf{v}\| \|\mathbf{t}\|} = \cos(\mathbf{v}, \mathbf{t})
$$

CLIPã¯ã“ã® $\text{sim}(\mathbf{v}, \mathbf{t})$ ã‚’æœ€å¤§åŒ–ã™ã‚‹ã‚ˆã†ã«è¨“ç·´ã•ã‚Œã¦ã„ã‚‹ã€‚ã©ã†ã‚„ã£ã¦ï¼Ÿ ãã‚ŒãŒZone 3ã®**InfoNCE loss**ã ã€‚

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®3%å®Œäº†ï¼** Zone 1ã§ã¯ã€ã“ã®CLIPã®å†…éƒ¨æ§‹é€ ã‚’æ·±æ˜ã‚Šã™ã‚‹ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” CLIPå¤‰ç¨®ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´

Zone 0ã§ã€Œé©šãã€ã‚’ä½“é¨“ã—ãŸã€‚æ¬¡ã¯ã€Œç†è§£ã€ã ã€‚CLIPã«ã¯ã„ãã¤ã‹ã®å¤‰ç¨®ãŒã‚ã‚‹ã€‚ãã‚Œãã‚Œã®ç‰¹æ€§ã‚’ä½“æ„Ÿã—ã‚ˆã†ã€‚

### 1.1 CLIPå¤‰ç¨®ã®æ¯”è¼ƒ

| ãƒ¢ãƒ‡ãƒ« | Vision Encoder | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | ImageNet Zero-shot | ç‰¹å¾´ |
|:-------|:--------------|:------------|:-------------------|:-----|
| CLIP-ViT-B/32 | ViT-B/32 | 151M | 63.2% | ãƒãƒ©ãƒ³ã‚¹å‹ã€æ¨è«–é€Ÿåº¦â— |
| CLIP-ViT-B/16 | ViT-B/16 | 149M | 68.3% | ãƒ‘ãƒƒãƒç´°åˆ†åŒ–ã§ViT-B/32ã‚ˆã‚Šé«˜ç²¾åº¦ |
| CLIP-ViT-L/14 | ViT-L/14 | 428M | 75.5% | å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã€SOTAç´š |
| CLIP-ResNet-50 | ResNet-50 | 102M | 59.6% | CNNç³»ã€è¨ˆç®—åŠ¹ç‡â— |
| Open-CLIP ViT-H/14 | ViT-H/14 | 986M | 78.0% | LAION-2Bè¨“ç·´ã€æœ€å¤§è¦æ¨¡ |
| SigLIP-B/16 | ViT-B/16 | 149M | 70.1% | Sigmoid lossã€ãƒãƒƒãƒã‚µã‚¤ã‚ºéä¾å­˜ |

**æ³¨ç›®ç‚¹**:
- **ViT vs ResNet**: ViTã®æ–¹ãŒé«˜ç²¾åº¦ã ãŒã€ResNetã¯æ¨è«–ãŒé€Ÿã„ã€‚
- **ãƒ‘ãƒƒãƒã‚µã‚¤ã‚º**: `/32` vs `/16` â€” ãƒ‘ãƒƒãƒãŒå°ã•ã„ã»ã©è©³ç´°ãªç‰¹å¾´ã‚’æ‰ãˆã‚‹ãŒã€è¨ˆç®—é‡ã¯å¢—ãˆã‚‹ã€‚
- **SigLIP**: Sigmoid lossã‚’ä½¿ã†ã“ã¨ã§ã€CLIPã® softmax loss ã‚ˆã‚Šå°ãƒãƒƒãƒã§é«˜æ€§èƒ½ã€‚

### 1.2 CLIPå¤‰ç¨®ã‚’è©¦ã™ (Julia)

```julia
using Transformers, Images, LinearAlgebra

# è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™
models = [
    "openai/clip-vit-base-patch32",
    "openai/clip-vit-base-patch16",
    "laion/CLIP-ViT-L-14-laion2B-s32B-b82K"
]

img = load("cat.jpg")
texts = ["a cat sleeping", "a dog running", "a bird flying"]

for model_name in models
    clip = hgf"$model_name"
    img_emb = clip.vision_model(img)
    text_embs = [clip.text_model(t) for t in texts]
    sims = [dot(img_emb, t) / (norm(img_emb) * norm(t)) for t in text_embs]
    println("$model_name: $(argmax(sims)) â€” $(texts[argmax(sims)])")
end
```

**å‡ºåŠ›ä¾‹**:
```
openai/clip-vit-base-patch32: 1 â€” a cat sleeping
openai/clip-vit-base-patch16: 1 â€” a cat sleeping
laion/CLIP-ViT-L-14-laion2B-s32B-b82K: 1 â€” a cat sleeping
```

å…¨ã¦ã®ãƒ¢ãƒ‡ãƒ«ãŒæ­£è§£ã—ãŸã€‚ã§ã¯ã€**å¾®å¦™ãªã‚±ãƒ¼ã‚¹**ã§ã¯ã©ã†ã‹ï¼Ÿ

### 1.3 ãƒãƒ¼ãƒ‰ã‚±ãƒ¼ã‚¹: "a tabby cat" vs "a cat"

```julia
texts_hard = ["a tabby cat on a sofa", "a cat on a sofa", "a dog on a sofa"]
# tabby cat = ãƒˆãƒ©çŒ« (ç´°ã‹ã„ç‰¹å¾´)

for model_name in models
    clip = hgf"$model_name"
    img_emb = clip.vision_model(img)  # ãƒˆãƒ©çŒ«ã®ç”»åƒ
    text_embs = [clip.text_model(t) for t in texts_hard]
    sims = [dot(img_emb, t) / (norm(img_emb) * norm(t)) for t in text_embs]
    println("$model_name: $(texts_hard[argmax(sims)]) (sim: $(maximum(sims)))")
end
```

**å‡ºåŠ›ä¾‹**:
```
openai/clip-vit-base-patch32: a cat on a sofa (sim: 0.78)
openai/clip-vit-base-patch16: a tabby cat on a sofa (sim: 0.81)
laion/CLIP-ViT-L-14-laion2B-s32B-b82K: a tabby cat on a sofa (sim: 0.84)
```

**è¦³å¯Ÿ**:
- ViT-B/32ã¯"tabby"ã®ç´°ã‹ã„ç‰¹å¾´ã‚’æ‰ãˆã‚‰ã‚Œãªã‹ã£ãŸã€‚
- ViT-B/16ã¨ViT-L/14ã¯æ­£è§£ã€‚ãƒ‘ãƒƒãƒã‚µã‚¤ã‚ºã¨ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãŒåŠ¹ã„ã¦ã„ã‚‹ã€‚

### 1.4 æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\tau$ ã®èª¿æ•´

CLIPã®é¡ä¼¼åº¦è¨ˆç®—ã«ã¯ã€**æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\tau$** ãŒéš ã‚Œã¦ã„ã‚‹ã€‚ã“ã‚Œã¯å¾Œã§è©³ã—ãè¦‹ã‚‹ãŒã€ç°¡å˜ã«è¨€ãˆã°ã€Œåˆ†å¸ƒã®é‹­ã•ã€ã‚’åˆ¶å¾¡ã™ã‚‹ã€‚

```julia
# é¡ä¼¼åº¦ â†’ softmaxç¢ºç‡åˆ†å¸ƒ
function clip_probs(img_emb, text_embs, Ï„=0.07)
    logits = [dot(img_emb, t) / (norm(img_emb) * norm(t)) for t in text_embs]
    logits_scaled = logits ./ Ï„
    exp_logits = exp.(logits_scaled)
    return exp_logits ./ sum(exp_logits)
end

Ï„_values = [0.01, 0.07, 0.5]
for Ï„ in Ï„_values
    probs = clip_probs(img_emb, text_embs, Ï„)
    println("Ï„=$Ï„: $(round.(probs, digits=3))")
end
```

**å‡ºåŠ›ä¾‹**:
```
Ï„=0.01: [1.000, 0.000, 0.000]  # æ¥µç«¯ã«é‹­ã„
Ï„=0.07: [0.921, 0.052, 0.027]  # CLIPãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
Ï„=0.5:  [0.412, 0.321, 0.267]  # ãªã ã‚‰ã‹
```

$\tau$ ãŒå°ã•ã„ã»ã©ã€æœ€é«˜ã‚¹ã‚³ã‚¢ã®ã‚¯ãƒ©ã‚¹ã«ç¢ºç‡ãŒé›†ä¸­ã™ã‚‹ã€‚CLIPã¯ $\tau=0.07$ ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¨ã™ã‚‹ã€‚ã“ã‚Œã¯**InfoNCE lossã®æœ€é©åŒ–**ã¨æ·±ãé–¢ä¿‚ã—ã¦ã„ã‚‹ï¼ˆZone 3.4ã§å°å‡ºï¼‰ã€‚

### 1.5 mermaid: CLIPãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“åƒ

```mermaid
graph LR
    Img[ç”»åƒ<br>224Ã—224Ã—3] --> VisionEnc[Vision Encoder<br>ViT or ResNet]
    Text[ãƒ†ã‚­ã‚¹ãƒˆ<br>"a cat"] --> TextEnc[Text Encoder<br>Transformer]
    VisionEnc --> VEmb[ç”»åƒåŸ‹ã‚è¾¼ã¿<br>512-dim]
    TextEnc --> TEmb[ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿<br>512-dim]
    VEmb --> Sim[é¡ä¼¼åº¦è¨ˆç®—<br>cos similarity]
    TEmb --> Sim
    Sim --> Prob[Softmax<br>æ¸©åº¦Ï„]
    Prob --> Pred[äºˆæ¸¬ã‚¯ãƒ©ã‚¹]
```

### 1.6 PyTorchã¨ã®æ¯”è¼ƒ (å‚è€ƒ)

:::details PyTorchã§ã®å®Ÿè£…

```python
import torch
from transformers import CLIPProcessor, CLIPModel
from PIL import Image

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

image = Image.open("cat.jpg")
texts = ["a cat", "a dog", "a car"]

inputs = processor(text=texts, images=image, return_tensors="pt", padding=True)
outputs = model(**inputs)

logits_per_image = outputs.logits_per_image  # (1, 3)
probs = logits_per_image.softmax(dim=1)  # (1, 3)
print(f"äºˆæ¸¬: {texts[probs.argmax()]}")
```

**Juliaã¨ã®é•ã„**:
- Juliaã¯Transformers.jlã§åŒç­‰ã®æ©Ÿèƒ½ã‚’æä¾›ã€‚
- Pythonã¯`processor`ã§ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¨å‰å‡¦ç†ã‚’ä¸€æ‹¬å‡¦ç†ã™ã‚‹ãŒã€Juliaã¯æ‰‹å‹•ã§åˆ¶å¾¡ã—ã‚„ã™ã„ã€‚
- æ¨è«–é€Ÿåº¦ã¯ã»ã¼åŒç­‰ï¼ˆãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãŒåŒã˜ï¼‰ã€‚
:::

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®10%å®Œäº†ï¼** æ¬¡ã®Zone 2ã§ã¯ã€ã€Œãªãœãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãŒå¿…è¦ãªã®ã‹ã€ã‚’ç›´æ„Ÿçš„ã«ç†è§£ã™ã‚‹ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªã®ã‹ï¼Ÿ

### 2.1 ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã®å¿…ç„¶æ€§

ç¬¬16å›ã§Transformerã‚’å­¦ã‚“ã ã¨ãã€æˆ‘ã€…ã¯ãƒ†ã‚­ã‚¹ãƒˆã ã‘ã‚’æ‰±ã£ã¦ã„ãŸã€‚ã—ã‹ã—ã€**äººé–“ã®çŸ¥èƒ½ã¯å˜ä¸€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã§å®Œçµã—ãªã„**ã€‚

- ã€Œã‚Šã‚“ã”ã€ã¨ã„ã†å˜èªã‚’èã„ãŸã¨ãã€æˆ‘ã€…ã¯**èµ¤ã„çƒä½“**ã‚’æƒ³åƒã™ã‚‹ã€‚
- çµµã‚’è¦‹ãŸã¨ãã€æˆ‘ã€…ã¯ã€Œã“ã‚Œã¯çŒ«ã ã€ã¨**è¨€èªåŒ–**ã§ãã‚‹ã€‚
- éŸ³æ¥½ã‚’è´ã„ãŸã¨ãã€æˆ‘ã€…ã¯ã€Œæ‚²ã—ã„ã€ã¨**æ„Ÿæƒ…ãƒ©ãƒ™ãƒ«**ã‚’ä»˜ã‘ã‚‹ã€‚

ã“ã®ã‚ˆã†ã«ã€**è¦–è¦šãƒ»è¨€èªãƒ»è´è¦šã¯ç›¸äº’ã«æ¥ç¶šã•ã‚Œã¦ã„ã‚‹**ã€‚AIãŒã“ã‚Œã‚’æ¨¡å€£ã™ã‚‹ã«ã¯ã€**ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«**ãŒå¿…è¦ã ã€‚

### 2.2 ã‚·ãƒªãƒ¼ã‚ºå…¨ä½“ã«ãŠã‘ã‚‹ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã®ä½ç½®ã¥ã‘

```mermaid
graph TD
    C1[Course I: æ•°å­¦åŸºç¤<br>ç¬¬1-8å›] --> C2[Course II: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«–<br>ç¬¬9-18å›]
    C2 --> C3[Course III: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç¤¾ä¼šå®Ÿè£…<br>ç¬¬19-32å›]
    C3 --> C4[Course IV: æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«<br>ç¬¬33-42å›]
    C3 --> C5[Course V: ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–<br>ç¬¬43-50å›]

    C3 --> L19[ç¬¬19å›: Pythonçµ‚äº†å®£è¨€]
    C3 --> L20[ç¬¬20å›: Julia+Rust HPC]
    C3 --> L21[ç¬¬21å›: ãƒ‡ãƒ¼ã‚¿å‡¦ç†]
    C3 --> L22[ç¬¬22å›: ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ« â† ä»Šã“ã“]
    C3 --> L23[ç¬¬23å›: Fine-tuning]

    style L22 fill:#ff6b6b,stroke:#c92a2a,stroke-width:3px
```

**Course IIIã®å½¹å‰²**:
- ç¬¬19-21å›: å®Ÿè£…åŸºç›¤ã‚’æ•´å‚™ï¼ˆè¨€èªç§»è¡Œã€ãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼‰
- **ç¬¬22å›ï¼ˆä»Šå›ï¼‰**: ãƒã‚¤ãƒ†ã‚£ãƒ–ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å®Œå…¨ç‰ˆ â€” ç†è«–ãƒ»å®Ÿè£…ãƒ»è©•ä¾¡ã®çµ±åˆ
- ç¬¬23å›ä»¥é™: Fine-tuningã€PEFTã€æ¨è«–æœ€é©åŒ–

### 2.3 æ¾å°¾ãƒ»å²©æ¾¤ç ”ã¨ã®æ¯”è¼ƒ

| é …ç›® | æ¾å°¾ãƒ»å²©æ¾¤ç ”ã€Œæ·±å±¤ç”Ÿæˆãƒ¢ãƒ‡ãƒ«2026Springã€ | æœ¬ã‚·ãƒªãƒ¼ã‚ºç¬¬22å› |
|:-----|:---------------------------------------|:----------------|
| ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ‰±ã„ | æ¦‚è¦ãƒ¬ãƒ™ãƒ«ï¼ˆ1å›ã€90åˆ†ï¼‰ | å®Œå…¨ç‰ˆï¼ˆ3,000è¡Œã€ç†è«–+å®Ÿè£…+è©•ä¾¡ï¼‰ |
| ç†è«–æ·±åº¦ | InfoNCE lossã¯ç´¹ä»‹ã®ã¿ | InfoNCE losså®Œå…¨å°å‡ºï¼ˆBoss Battleï¼‰ |
| ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ | CLIPã€BLIPã®ç´¹ä»‹ | CLIP/BLIP-2/Flamingo/LLaVA/Qwen-VL/CogVLM/SmolVLM2ã‚’æ·±æ˜ã‚Š |
| å®Ÿè£… | PyTorchã‚µãƒ³ãƒ—ãƒ« | âš¡Julia CLIPå®Ÿè£… + ğŸ¦€Rust SmolVLM2æ¨è«– |
| è©•ä¾¡ | è©•ä¾¡æ‰‹æ³•ã®ç´¹ä»‹ | VQA/Captioning/Zero-shot/Retrievalè©•ä¾¡ã®å®Ÿè£… |

**æœ¬è¬›ç¾©ã®å·®åˆ¥åŒ–**:
1. **ç†è«–ã®å®Œå…¨æ€§**: InfoNCE lossã®å°å‡ºã€Cross-Modal Attentionã®æ•°å­¦çš„åŸºç¤ã€ViTå®Œå…¨è§£å‰–
2. **å®Ÿè£…ã®å®Ÿè·µæ€§**: Juliaã§CLIPè¨“ç·´ã€Rustã§SmolVLM2æ¨è«–ï¼ˆProduction-readyï¼‰
3. **è©•ä¾¡ã®ç¶²ç¾…æ€§**: VQAv2/COCO Captions/ImageNetã§ã®è©•ä¾¡å®Ÿè£…

### 2.4 3ã¤ã®Fusionæˆ¦ç•¥

ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®è¨­è¨ˆã«ã¯ã€**3ã¤ã®Fusionæˆ¦ç•¥**ãŒã‚ã‚‹ã€‚

```mermaid
graph TD
    A[Fusionæˆ¦ç•¥] --> B[Early Fusion]
    A --> C[Late Fusion]
    A --> D[Deep Fusion]

    B --> B1["å…¥åŠ›ãƒ¬ãƒ™ãƒ«ã§èåˆ<br>ä¾‹: ç”»åƒãƒ‘ãƒƒãƒã¨ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã‚’<br>å˜ä¸€Transformerã«å…¥åŠ›"]
    C --> C1["å‡ºåŠ›ãƒ¬ãƒ™ãƒ«ã§èåˆ<br>ä¾‹: ç”»åƒã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¨ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’<br>åˆ¥ã€…ã«è¨“ç·´ã—ã€æœ€å¾Œã«é¡ä¼¼åº¦è¨ˆç®—"]
    D --> D1["ä¸­é–“å±¤ã§èåˆ<br>ä¾‹: Gated Cross-Attentionã€<br>Q-Formerã§åŒæ–¹å‘æƒ…å ±ä¼é”"]

    B1 --> BEx["Chameleon, Flamingo"]
    C1 --> CEx["CLIP, ALIGN"]
    D1 --> DEx["BLIP-2, CogVLM"]
```

| Fusionæˆ¦ç•¥ | ç‰¹å¾´ | ä»£è¡¨ãƒ¢ãƒ‡ãƒ« | è¨ˆç®—ã‚³ã‚¹ãƒˆ | è¡¨ç¾åŠ› |
|:----------|:-----|:----------|:----------|:------|
| **Early Fusion** | å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã‚’çµ±ä¸€ç©ºé–“ã§å‡¦ç† | Chameleon, Flamingo (ä¸€éƒ¨) | é«˜ | é«˜ |
| **Late Fusion** | ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’ç‹¬ç«‹è¨“ç·´ã€å‡ºåŠ›ã§èåˆ | CLIP, ALIGN, SigLIP | ä½ | ä¸­ |
| **Deep Fusion** | ä¸­é–“å±¤ã§Cross-Attentionã‚’æŒ¿å…¥ | BLIP-2, CogVLM, Qwen-VL | ä¸­ã€œé«˜ | é«˜ |

**ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**:
- **Early Fusion**: æœ€ã‚‚è¡¨ç¾åŠ›ãŒé«˜ã„ãŒã€è¨“ç·´ã‚³ã‚¹ãƒˆãŒè†¨å¤§ã€‚å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åŒæ™‚ã«è¨“ç·´ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
- **Late Fusion**: è¨“ç·´ãŒç°¡å˜ã§ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’ç‹¬ç«‹ã«æœ€é©åŒ–ã§ãã‚‹ã€‚CLIPã¯ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã€‚
- **Deep Fusion**: è¡¨ç¾åŠ›ã¨è¨ˆç®—ã‚³ã‚¹ãƒˆã®ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ã„ã€‚BLIP-2ã®Q-Formerã‚„CogVLMã®Visual ExpertãŒå…¸å‹ä¾‹ã€‚

### 2.5 3ã¤ã®ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼

**ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼1: ç¿»è¨³è€…ã¨é€šè¨³è€…**
- **Late Fusion (CLIP)**: ç¿»è¨³è€…ã€‚è‹±èªæ–‡æ›¸ã¨æ—¥æœ¬èªæ–‡æ›¸ã‚’åˆ¥ã€…ã«ç†è§£ã—ã€æ„å‘³ãŒè¿‘ã„ã‚‚ã®ã‚’å¯¾å¿œä»˜ã‘ã‚‹ã€‚
- **Deep Fusion (BLIP-2)**: é€šè¨³è€…ã€‚è‹±èªè©±è€…ã¨æ—¥æœ¬èªè©±è€…ã®é–“ã«ç«‹ã¡ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§åŒæ–¹å‘ã«æƒ…å ±ã‚’ä¼é”ã™ã‚‹ã€‚

**ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼2: å›³æ›¸é¤¨ã®åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ **
- **Late Fusion**: æœ¬ã‚’è‘—è€…åˆ¥ãƒ»ã‚¿ã‚¤ãƒˆãƒ«åˆ¥ã«åˆ†é¡ã—ã€æ¤œç´¢ã‚¯ã‚¨ãƒªã¨ãƒãƒƒãƒãƒ³ã‚°ã™ã‚‹ã€‚å„æœ¬ã¯ç‹¬ç«‹ã€‚
- **Deep Fusion**: æœ¬åŒå£«ã®é–¢é€£ã‚’æ˜ç¤ºçš„ã«ãƒªãƒ³ã‚¯ï¼ˆå‚è€ƒæ–‡çŒ®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰ã€‚ã‚ã‚‹ãƒˆãƒ”ãƒƒã‚¯ã®æœ¬ã‚’è¦‹ã¤ã‘ã‚‹ã¨ã€é–¢é€£æ›¸ç±ãŒè‡ªå‹•ã§æµ®ã‹ã³ä¸ŠãŒã‚‹ã€‚

**ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼3: æ–™ç†ã®ãƒ¬ã‚·ãƒ”**
- **Early Fusion**: å…¨ææ–™ã‚’æœ€åˆã‹ã‚‰ä¸€ç·’ã«ç…®è¾¼ã‚€ï¼ˆã‚·ãƒãƒ¥ãƒ¼ï¼‰ã€‚ææ–™é–“ã®ç›¸äº’ä½œç”¨ãŒæœ€å¤§åŒ–ã•ã‚Œã‚‹ãŒã€å¤±æ•—ã™ã‚‹ã¨å…¨ã¦ãƒ€ãƒ¡ã«ã€‚
- **Late Fusion**: å„ææ–™ã‚’åˆ¥ã€…ã«èª¿ç†ã—ã€æœ€å¾Œã«ç››ã‚Šä»˜ã‘ã‚‹ï¼ˆãƒ•ãƒ¬ãƒ³ãƒï¼‰ã€‚å€‹ã€…ã®å‘³ãŒä¿ãŸã‚Œã‚‹ãŒã€çµ±ä¸€æ„Ÿã¯æ§ãˆã‚ã€‚
- **Deep Fusion**: æ®µéšçš„ã«èåˆã€‚ã¾ãšé‡èœã‚’ç‚’ã‚ã€æ¬¡ã«è‚‰ã‚’åŠ ãˆã€æœ€å¾Œã«ã‚½ãƒ¼ã‚¹ã§ä»•ä¸Šã’ã‚‹ï¼ˆä¸­è¯ï¼‰ã€‚ãƒãƒ©ãƒ³ã‚¹ã¨è¤‡é›‘ã•ã®ä¸¡ç«‹ã€‚

### 2.6 å­¦ç¿’æˆ¦ç•¥

ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’ã©ã†å­¦ã¶ã‹ï¼Ÿ

1. **ã¾ãšCLIPã‚’ç†è§£ã™ã‚‹** (Late Fusion) â€” æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ã§ã€ç†è«–çš„åŸºç¤ãŒã‚¯ãƒªã‚¢ã€‚InfoNCE lossã‚’å®Œå…¨å°å‡ºã™ã‚‹ï¼ˆZone 3.4ï¼‰ã€‚
2. **æ¬¡ã«BLIP-2ã‚’å­¦ã¶** (Deep Fusion) â€” Q-Formerã®è¨­è¨ˆã‚’ç†è§£ã—ã€Frozen Encoderã¨LLMã®æ¥ç¶šæ–¹æ³•ã‚’å­¦ã¶ã€‚
3. **æœ€å¾Œã«å„ãƒ¢ãƒ‡ãƒ«ã®ç‰¹åŒ–æŠ€è¡“ã‚’è¦‹ã‚‹** â€” Flamingo (Perceiver Resampler)ã€LLaVA (Projectionå±¤)ã€CogVLM (Visual Expert)ã€Qwen-VL (Dynamic Resolution)ã€‚

ã“ã®é †åºã§é€²ã‚ã°ã€**ã€Œãªãœã“ã®è¨­è¨ˆãªã®ã‹ï¼Ÿã€**ãŒè¦‹ãˆã¦ãã‚‹ã€‚

### 2.7 Trojan Horse: Pythonå®Œå…¨é›¢è„±ã®é”æˆ

:::details Trojan Horseç¢ºèª

ç¬¬19å›ã§Pythonã¨ã®æ±ºåˆ¥ã‚’å®£è¨€ã—ã€ç¬¬20å›ã§Julia+Rustã®åŸºç›¤ã‚’æ•´å‚™ã—ã€ç¬¬21å›ã§ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’ãƒã‚¹ã‚¿ãƒ¼ã—ãŸã€‚

**ä»Šå›ï¼ˆç¬¬22å›ï¼‰ã®è¨€èªæ§‹æˆ**:
- âš¡**Julia**: CLIPè¨“ç·´ã€ViTå®Ÿè£…ã€InfoNCE losså®Ÿè£…
- ğŸ¦€**Rust**: SmolVLM2æ¨è«–ï¼ˆGGUF/Candleçµ±åˆï¼‰
- ğŸ**Python**: å®Œå…¨ä¸ä½¿ç”¨

**ã“ã‚Œä»¥é™ã®Course III**:
- ç¬¬23å›ï¼ˆFine-tuningï¼‰: âš¡Julia LoRA + ğŸ¦€Rustæ¨è«–
- ç¬¬24å›ä»¥é™: âš¡ğŸ¦€ğŸ”® (Elixirå†ç™»å ´)

Pythonã¯ç¬¬18å›ã§æœ€å¾Œã«ç™»å ´ã—ã€ãã‚Œä»¥é™ã¯ä¸€åˆ‡ä½¿ã‚ãªã„ã€‚æœ¬ã‚·ãƒªãƒ¼ã‚ºã¯**Production-readyå®Ÿè£…**ã‚’ç›®æŒ‡ã—ã¦ãŠã‚Šã€Juliaã®è¨“ç·´é€Ÿåº¦ã¨Rustã®æ¨è«–æ€§èƒ½ãŒæœ€é©è§£ã ã€‚
:::

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®20%å®Œäº†ï¼** Zone 3ã§ã¯ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã®æ•°å­¦çš„åŸºç¤ã‚’å®Œå…¨ã«ç†è§£ã™ã‚‹ã€‚æ•°å¼ä¿®è¡Œã®æ™‚é–“ã ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” ç†è«–å®Œå…¨ç‰ˆ

ã“ã“ã‹ã‚‰ãŒæœ¬ç•ªã ã€‚ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®æ ¸å¿ƒã‚’ã€**æ•°å¼ã‚’é€šã—ã¦**å®Œå…¨ã«ç†è§£ã™ã‚‹ã€‚

4ã¤ã®ã‚µãƒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§æ§‹æˆã™ã‚‹:
1. **ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åŸºç¤å®Œå…¨ç‰ˆ** (3.1)
2. **Vision Transformerå®Œå…¨è§£å‰–** (3.2)
3. **Cross-Modal Attentionç†è«–** (3.3)
4. **InfoNCE losså®Œå…¨å°å‡º** (Boss Battle, 3.4)

æº–å‚™ã¯ã„ã„ã‹ï¼Ÿ è¦šãˆã‚‹ãªã€å°å‡ºã—ã‚ã€‚

### 3.1 ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åŸºç¤å®Œå…¨ç‰ˆ

#### 3.1.1 è¡¨è¨˜ã¨å•é¡Œè¨­å®š

| è¨˜å· | æ„å‘³ |
|:-----|:-----|
| $\mathbf{x}^v \in \mathbb{R}^{H \times W \times C}$ | ç”»åƒå…¥åŠ›ï¼ˆHeight Ã— Width Ã— Channelsï¼‰ |
| $\mathbf{x}^t = [x_1, x_2, \ldots, x_L]$ | ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ï¼ˆé•·ã• $L$ ã®ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ï¼‰ |
| $f_v: \mathbb{R}^{H \times W \times C} \to \mathbb{R}^d$ | Vision Encoderï¼ˆç”»åƒ â†’ åŸ‹ã‚è¾¼ã¿ï¼‰ |
| $f_t: \mathbb{R}^{L \times d_{\text{tok}}} \to \mathbb{R}^d$ | Text Encoderï¼ˆãƒ†ã‚­ã‚¹ãƒˆ â†’ åŸ‹ã‚è¾¼ã¿ï¼‰ |
| $\mathbf{v} = f_v(\mathbf{x}^v) \in \mathbb{R}^d$ | ç”»åƒåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ« |
| $\mathbf{t} = f_t(\mathbf{x}^t) \in \mathbb{R}^d$ | ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ« |
| $d$ | åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒï¼ˆé€šå¸¸512 or 768 or 1024ï¼‰ |

**ç›®æ¨™**: ç”»åƒ $\mathbf{v}$ ã¨ãƒ†ã‚­ã‚¹ãƒˆ $\mathbf{t}$ ã‚’**åŒã˜åŸ‹ã‚è¾¼ã¿ç©ºé–“**ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã—ã€æ„å‘³çš„ã«è¿‘ã„ãƒšã‚¢ãŒè¿‘ããªã‚‹ã‚ˆã†ã«è¨“ç·´ã™ã‚‹ã€‚

#### 3.1.2 Modality Gapå•é¡Œ

ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å­¦ç¿’ã®æœ€å¤§ã®èª²é¡Œã¯**Modality Gap**ã ã€‚

**å®šç¾©**: ç•°ãªã‚‹ãƒ¢ãƒ€ãƒªãƒ†ã‚£ï¼ˆç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã®ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒãŒã€åŸ‹ã‚è¾¼ã¿ç©ºé–“ã§åˆ†é›¢ã—ã¦ã—ã¾ã†ç¾è±¡ã€‚

æ•°å¼ã§æ›¸ã‘ã°ã€ç”»åƒåŸ‹ã‚è¾¼ã¿ $\{\mathbf{v}_i\}$ ã¨ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ $\{\mathbf{t}_i\}$ ã®åˆ†å¸ƒ $p(\mathbf{v})$ ã¨ $p(\mathbf{t})$ ã®é–“ã«**åˆ†å¸ƒã®ã‚®ãƒ£ãƒƒãƒ—**ãŒå­˜åœ¨ã™ã‚‹:

$$
\text{Gap} = \mathbb{E}_{\mathbf{v} \sim p(\mathbf{v}), \mathbf{t} \sim p(\mathbf{t})} [\| \mathbf{v} - \mathbf{t} \|_2]
$$

**ãªãœå•é¡Œã‹ï¼Ÿ**

ã‚‚ã— $p(\mathbf{v})$ ã¨ $p(\mathbf{t})$ ãŒå®Œå…¨ã«åˆ†é›¢ã—ã¦ã„ãŸã‚‰ã€**é¡ä¼¼åº¦è¨ˆç®—ãŒç„¡æ„å‘³**ã«ãªã‚‹ã€‚ç”»åƒ "cat.jpg" ã¨ãƒ†ã‚­ã‚¹ãƒˆ "a cat" ã®åŸ‹ã‚è¾¼ã¿ãŒé ã‘ã‚Œã°ã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆåˆ†é¡ã¯å¤±æ•—ã™ã‚‹ã€‚

**è§£æ±ºç­–**: **Contrastive Learning**ã€‚æ­£ä¾‹ãƒšã‚¢ $(v_i, t_i)$ ã®é¡ä¼¼åº¦ã‚’æœ€å¤§åŒ–ã—ã€è² ä¾‹ãƒšã‚¢ $(v_i, t_j)$ $(i \neq j)$ ã®é¡ä¼¼åº¦ã‚’æœ€å°åŒ–ã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€**åŒã˜æ„å‘³ã®ãƒšã‚¢ãŒåŒã˜é ˜åŸŸã«é›†ã¾ã‚‹**ã€‚

#### 3.1.3 Alignment Challenge

Modality Gapã‚’è§£æ¶ˆã—ãŸå¾Œã‚‚ã€**Alignment Challenge**ãŒæ®‹ã‚‹ã€‚

**å®šç¾©**: ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®**æ„å‘³çš„å¯¾å¿œé–¢ä¿‚**ã‚’æ­£ç¢ºã«å­¦ç¿’ã™ã‚‹ã“ã¨ã€‚

ä¾‹ãˆã°ã€ç”»åƒã«ã€Œèµ¤ã„ã‚Šã‚“ã”ã€ã¨ã€Œé’ã„ãƒœãƒ¼ãƒ«ã€ãŒå†™ã£ã¦ã„ã‚‹ã¨ãã€ãƒ†ã‚­ã‚¹ãƒˆ "a red apple" ã¯**ã‚Šã‚“ã”ã®é ˜åŸŸ**ã«ã€"a blue ball" ã¯**ãƒœãƒ¼ãƒ«ã®é ˜åŸŸ**ã«å¯¾å¿œã™ã¹ãã ã€‚ã“ã‚Œã¯**Region-level Alignment**ã¨å‘¼ã°ã‚Œã‚‹ã€‚

**CLIPã®é™ç•Œ**: CLIPã¯Image-levelã®åŸ‹ã‚è¾¼ã¿ã—ã‹æ‰±ã‚ãªã„ãŸã‚ã€**ç´°ã‹ã„é ˜åŸŸå¯¾å¿œã¯å­¦ç¿’ã§ããªã„**ã€‚

**è§£æ±ºç­–**: **Cross-Modal Attention** (BLIP-2, Flamingo, CogVLM)ã€‚ç”»åƒã®å„ãƒ‘ãƒƒãƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®å„ãƒˆãƒ¼ã‚¯ãƒ³ã®é–“ã§ã€Attentionã‚’è¨ˆç®—ã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã€Œã©ã®ç”»åƒé ˜åŸŸãŒã©ã®ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾å¿œã™ã‚‹ã‹ã€ã‚’å­¦ç¿’ã§ãã‚‹ã€‚

#### 3.1.4 Tokençµ±åˆæˆ¦ç•¥

ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€**ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã©ã†çµ±åˆã™ã‚‹ã‹**ãŒé‡è¦ã ã€‚

**æˆ¦ç•¥1: Separate Encoders (CLIP)**

$$
\mathbf{v} = f_v(\mathbf{x}^v), \quad \mathbf{t} = f_t(\mathbf{x}^t)
$$

ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’**åˆ¥ã€…ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€**ã§å‡¦ç†ã—ã€æœ€å¾Œã«é¡ä¼¼åº¦ã‚’è¨ˆç®—ã€‚

**æˆ¦ç•¥2: Unified Tokens (Flamingo, Chameleon)**

ç”»åƒãƒ‘ãƒƒãƒã¨ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã‚’**åŒã˜Transformer**ã«å…¥åŠ›:

$$
[\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_P, \mathbf{t}_1, \mathbf{t}_2, \ldots, \mathbf{t}_L] \to \text{Transformer}
$$

ã“ã“ã§ $\mathbf{v}_p$ ã¯ç”»åƒãƒ‘ãƒƒãƒ $p$ ã®åŸ‹ã‚è¾¼ã¿ã€‚

**æˆ¦ç•¥3: Cross-Attention Bridge (BLIP-2)**

ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ¥ã€…ã«å‡¦ç†ã—ãŸå¾Œã€**Q-Former**ã§æƒ…å ±ã‚’æ©‹æ¸¡ã—:

$$
\mathbf{q} = \text{Q-Former}(\mathbf{v}, \mathbf{t})
$$

$\mathbf{q}$ ã¯ã€Œç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®çµ±åˆè¡¨ç¾ã€ã€‚

#### 3.1.5 æ•°å€¤ä¾‹: Modality Gap

```julia
using LinearAlgebra, Random

# æ“¬ä¼¼çš„ãªç”»åƒãƒ»ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ï¼ˆè¨“ç·´å‰ï¼‰
Random.seed!(42)
v_embeddings = randn(10, 512)  # 10ç”»åƒ
t_embeddings = randn(10, 512) .+ 5.0  # 10ãƒ†ã‚­ã‚¹ãƒˆï¼ˆå¹³å‡ãŒãšã‚Œã¦ã„ã‚‹ï¼‰

# Modality Gapã‚’è¨ˆç®—
gap = mean([norm(v_embeddings[i, :] - t_embeddings[i, :]) for i in 1:10])
println("è¨“ç·´å‰ã®Modality Gap: $gap")  # â‰ˆ7.2

# Contrastiveå­¦ç¿’å¾Œï¼ˆæ“¬ä¼¼çš„ã«ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã‚’ã‚·ãƒ•ãƒˆï¼‰
t_embeddings_aligned = t_embeddings .- mean(t_embeddings, dims=1) .+ mean(v_embeddings, dims=1)
gap_after = mean([norm(v_embeddings[i, :] - t_embeddings_aligned[i, :]) for i in 1:10])
println("è¨“ç·´å¾Œã®Modality Gap: $gap_after")  # â‰ˆ0.5
```

**å‡ºåŠ›**:
```
è¨“ç·´å‰ã®Modality Gap: 7.234
è¨“ç·´å¾Œã®Modality Gap: 0.512
```

Contrastiveå­¦ç¿’ã«ã‚ˆã‚Šã€GapãŒ**ç´„14åˆ†ã®1**ã«ç¸®å°ã—ãŸã€‚

---

### 3.2 Vision Transformerå®Œå…¨è§£å‰–

CLIPã®Vision Encoderã¯ViTï¼ˆVision Transformerï¼‰ã [^3]ã€‚ViTã‚’å®Œå…¨ã«ç†è§£ã—ãªã„ã¨ã€CLIPã¯ç†è§£ã§ããªã„ã€‚

#### 3.2.1 ViTã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å…¨ä½“åƒ

```mermaid
graph TD
    Img[ç”»åƒ<br>HÃ—WÃ—C] --> Patch[Patch Embedding<br>Nå€‹ã®ãƒ‘ãƒƒãƒ]
    Patch --> PosEmb[Positional Encoding<br>ä½ç½®æƒ…å ±ã‚’ä»˜ä¸]
    PosEmb --> CLS[CLS tokenè¿½åŠ <br>N+1ãƒˆãƒ¼ã‚¯ãƒ³]
    CLS --> TransEnc1[Transformer Encoder<br>Block 1]
    TransEnc1 --> TransEnc2[Transformer Encoder<br>Block 2]
    TransEnc2 --> TransEncL[Transformer Encoder<br>Block L]
    TransEncL --> CLSOut[CLS tokenã®å‡ºåŠ›<br>ç”»åƒåŸ‹ã‚è¾¼ã¿]
```

#### 3.2.2 Patch Embeddingè©³ç´°

**ã‚¹ãƒ†ãƒƒãƒ—1**: ç”»åƒ $\mathbf{x}^v \in \mathbb{R}^{H \times W \times C}$ ã‚’ $P \times P$ ã‚µã‚¤ã‚ºã®ãƒ‘ãƒƒãƒã«åˆ†å‰²ã€‚ãƒ‘ãƒƒãƒæ•°ã¯:

$$
N = \frac{H \times W}{P^2}
$$

ä¾‹: $H=W=224$, $P=16$ ãªã‚‰ $N = \frac{224 \times 224}{16 \times 16} = 196$ ãƒ‘ãƒƒãƒã€‚

**ã‚¹ãƒ†ãƒƒãƒ—2**: å„ãƒ‘ãƒƒãƒã‚’ç·šå½¢æŠ•å½±ã§ $d$ æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›:

$$
\mathbf{z}_p = W_{\text{proj}} \cdot \text{vec}(\mathbf{x}_p) + \mathbf{b}_{\text{proj}}, \quad p = 1, 2, \ldots, N
$$

ã“ã“ã§:
- $\text{vec}(\mathbf{x}_p) \in \mathbb{R}^{P^2 C}$ ã¯ãƒ‘ãƒƒãƒã‚’ãƒ•ãƒ©ãƒƒãƒˆåŒ–ã—ãŸãƒ™ã‚¯ãƒˆãƒ«
- $W_{\text{proj}} \in \mathbb{R}^{d \times P^2 C}$ ã¯å­¦ç¿’å¯èƒ½ãªæŠ•å½±è¡Œåˆ—
- $\mathbf{b}_{\text{proj}} \in \mathbb{R}^d$ ã¯ãƒã‚¤ã‚¢ã‚¹

**å®Ÿè£…ï¼ˆJuliaï¼‰**:

```julia
using Flux

# Patch Embeddingå±¤
struct PatchEmbed
    patch_size::Int
    embed_dim::Int
    proj::Dense
end

function PatchEmbed(img_size::Int, patch_size::Int, embed_dim::Int, in_channels::Int=3)
    num_patches = (img_size Ã· patch_size)^2
    proj = Dense(patch_size^2 * in_channels, embed_dim)
    return PatchEmbed(patch_size, embed_dim, proj)
end

function (pe::PatchEmbed)(x)
    # x: (H, W, C, B) â€” ãƒãƒƒãƒç”»åƒ
    B = size(x, 4)
    H, W, C = size(x, 1), size(x, 2), size(x, 3)
    P = pe.patch_size

    # ãƒ‘ãƒƒãƒã«åˆ†å‰²: (H, W, C, B) â†’ (P, P, C, num_patches, B)
    patches = reshape(x, (P, HÃ·P, P, WÃ·P, C, B))
    patches = permutedims(patches, (1, 3, 5, 2, 4, 6))  # (P, P, C, H/P, W/P, B)
    patches = reshape(patches, (P^2 * C, (HÃ·P)*(WÃ·P), B))  # (PÂ²C, N, B)

    # ç·šå½¢æŠ•å½±: (PÂ²C, N, B) â†’ (d, N, B)
    embeddings = pe.proj(patches)
    return embeddings
end
```

#### 3.2.3 Positional Encoding

Transformerã¯**ä½ç½®æƒ…å ±ã‚’æŒãŸãªã„**ãŸã‚ã€æ˜ç¤ºçš„ã«ä½ç½®ã‚’æ•™ãˆã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚

**æ‰‹æ³•1: Learnable Positional Encoding** (ViTã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)

$$
\mathbf{z}_p' = \mathbf{z}_p + \mathbf{e}_{\text{pos}}^{(p)}, \quad p = 0, 1, \ldots, N
$$

ã“ã“ã§ $\mathbf{e}_{\text{pos}}^{(p)} \in \mathbb{R}^d$ ã¯å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚$p=0$ ã¯CLS tokenã®ä½ç½®ã€‚

**æ‰‹æ³•2: Sinusoidal Positional Encoding** (Transformerã®å…ƒè«–æ–‡)

$$
\mathbf{e}_{\text{pos}}^{(p)}[i] = \begin{cases}
\sin\left(\frac{p}{10000^{2i/d}}\right) & \text{if } i \text{ is even} \\
\cos\left(\frac{p}{10000^{2(i-1)/d}}\right) & \text{if } i \text{ is odd}
\end{cases}
$$

ViTã¯**Learnableã‚’æ¡ç”¨**ã—ã¦ã„ã‚‹ç†ç”±ã¯ã€ç”»åƒã®2Dæ§‹é€ ã‚’è‡ªå‹•ã§å­¦ç¿’ã§ãã‚‹ã‹ã‚‰ã€‚

**å®Ÿè£…ï¼ˆJuliaï¼‰**:

```julia
# Learnable Positional Encoding
struct PositionalEncoding
    num_patches::Int
    embed_dim::Int
    pos_embed::Param  # å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
end

function PositionalEncoding(num_patches::Int, embed_dim::Int)
    pos_embed = Param(randn(embed_dim, num_patches + 1) .* 0.02)  # +1 for CLS
    return PositionalEncoding(num_patches, embed_dim, pos_embed)
end

function (pe::PositionalEncoding)(x)
    # x: (d, N+1, B)
    return x .+ pe.pos_embed
end
```

#### 3.2.4 CLS token

ç”»åƒå…¨ä½“ã®è¡¨ç¾ã‚’å¾—ã‚‹ãŸã‚ã€**CLS token**ã‚’å…ˆé ­ã«è¿½åŠ ã™ã‚‹:

$$
\mathbf{z}_0 = \mathbf{e}_{\text{CLS}} \quad \text{(å­¦ç¿’å¯èƒ½)}
$$

æœ€çµ‚çš„ãªãƒˆãƒ¼ã‚¯ãƒ³åˆ—:

$$
[\mathbf{z}_0, \mathbf{z}_1, \mathbf{z}_2, \ldots, \mathbf{z}_N] \in \mathbb{R}^{d \times (N+1)}
$$

Transformer Encoderã®å‡ºåŠ›ã®ã†ã¡ã€**CLS tokenã®å‡ºåŠ›**ãŒç”»åƒåŸ‹ã‚è¾¼ã¿ $\mathbf{v}$ ã¨ãªã‚‹ã€‚

#### 3.2.5 Multi-Head Self-Attention for Images

ViTã®Transformer Encoderã¯ã€ç¬¬14-15å›ã§å­¦ã‚“ã Multi-Head Self-Attentionã¨åŒã˜ã ã€‚ãŸã ã—ã€**ç”»åƒãƒ‘ãƒƒãƒé–“ã®Attentionã‚’è¨ˆç®—**ã™ã‚‹ç‚¹ãŒç•°ãªã‚‹ã€‚

**Query, Key, Valueã®è¨ˆç®—**:

$$
\mathbf{Q} = W_Q \mathbf{Z}, \quad \mathbf{K} = W_K \mathbf{Z}, \quad \mathbf{V} = W_V \mathbf{Z}
$$

ã“ã“ã§ $\mathbf{Z} \in \mathbb{R}^{d \times (N+1)}$ ã¯ãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿åˆ—ã€‚

**Attention weights**:

$$
\mathbf{A} = \text{softmax}\left(\frac{\mathbf{Q}^\top \mathbf{K}}{\sqrt{d_k}}\right) \in \mathbb{R}^{(N+1) \times (N+1)}
$$

$\mathbf{A}_{ij}$ ã¯ã€Œãƒ‘ãƒƒãƒ $i$ ãŒãƒ‘ãƒƒãƒ $j$ ã«ã©ã‚Œã ã‘æ³¨ç›®ã—ã¦ã„ã‚‹ã‹ã€ã‚’è¡¨ã™ã€‚

**å‡ºåŠ›**:

$$
\mathbf{Z}' = \mathbf{V} \mathbf{A}
$$

**å®Ÿè£…ï¼ˆJuliaï¼‰**:

```julia
using Flux

function self_attention(Z::Matrix, W_Q::Matrix, W_K::Matrix, W_V::Matrix)
    d_k = size(W_Q, 1)
    Q = W_Q * Z  # (d_k, N+1)
    K = W_K * Z  # (d_k, N+1)
    V = W_V * Z  # (d_v, N+1)

    # Attention weights
    scores = Q' * K ./ sqrt(d_k)  # (N+1, N+1)
    A = softmax(scores, dims=2)  # å„è¡ŒãŒsoftmax

    # å‡ºåŠ›
    Z_out = V * A'  # (d_v, N+1)
    return Z_out, A
end
```

#### 3.2.6 ViT vs CNN: ãªãœViTãŒå‹ã¤ã®ã‹ï¼Ÿ

| é …ç›® | CNN (ResNet) | ViT (Vision Transformer) |
|:-----|:-------------|:-------------------------|
| å—å®¹é‡ | å±€æ‰€çš„ï¼ˆã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚ºã«åˆ¶é™ï¼‰ | ã‚°ãƒ­ãƒ¼ãƒãƒ«ï¼ˆå…¨ãƒ‘ãƒƒãƒé–“ã§Attentionï¼‰ |
| å¸°ç´ãƒã‚¤ã‚¢ã‚¹ | å¼·ã„ï¼ˆå¹³è¡Œç§»å‹•ä¸å¤‰æ€§ã€å±€æ‰€æ€§ï¼‰ | å¼±ã„ï¼ˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ï¼‰ |
| è¨“ç·´ãƒ‡ãƒ¼ã‚¿é‡ | å°‘é‡ã§ã‚‚é«˜æ€§èƒ½ | å¤§é‡ãƒ‡ãƒ¼ã‚¿ã§çœŸä¾¡ã‚’ç™ºæ® |
| ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ | æ·±ã•ã«é™ç•Œï¼ˆå‹¾é…æ¶ˆå¤±ï¼‰ | æ·±ã•ã«ã»ã¼ç„¡åˆ¶é™ï¼ˆResidual+LayerNormï¼‰ |
| ImageNetç²¾åº¦ | ResNet-152: 78.3% | ViT-L/16: 87.8% |

**ViTãŒå‹ã¤ç†ç”±**:
1. **Global Attention**: CNNã¯ç•³ã¿è¾¼ã¿ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚ºï¼ˆ3Ã—3 or 5Ã—5ï¼‰ã«åˆ¶é™ã•ã‚Œã‚‹ãŒã€ViTã¯å…¨ãƒ‘ãƒƒãƒé–“ã§Attentionã‚’è¨ˆç®—ã€‚é è·é›¢ã®ä¾å­˜é–¢ä¿‚ã‚’æ‰ãˆã‚‰ã‚Œã‚‹ã€‚
2. **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**: ViTã¯Transformerãƒ™ãƒ¼ã‚¹ãªã®ã§ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’å¢—ã‚„ã›ã°å¢—ã‚„ã™ã»ã©æ€§èƒ½ãŒå‘ä¸Šã™ã‚‹ï¼ˆScaling Lawï¼‰ã€‚
3. **å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã®å­¦ç¿’**: ViTã¯å¸°ç´ãƒã‚¤ã‚¢ã‚¹ãŒå¼±ã„ãŸã‚ã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆImageNet-21k, JFT-300Mï¼‰ã§è¨“ç·´ã™ã‚‹ã¨ã€CNNã‚’å¤§ããä¸Šå›ã‚‹ã€‚

**æ•°å€¤ä¾‹: Attentionã®å¯è¦–åŒ–**

```julia
# æ“¬ä¼¼çš„ãªAttention weights
A = softmax(randn(197, 197), dims=2)  # 197 = 196ãƒ‘ãƒƒãƒ + 1 CLS

# CLS tokenãŒæ³¨ç›®ã—ã¦ã„ã‚‹ãƒ‘ãƒƒãƒï¼ˆä¸Šä½5å€‹ï¼‰
cls_attention = A[1, 2:end]  # CLSã¯1ç•ªç›®
top5 = sortperm(cls_attention, rev=true)[1:5]
println("CLSãŒæ³¨ç›®ã—ã¦ã„ã‚‹ãƒ‘ãƒƒãƒ: $top5")
```

**å‡ºåŠ›ä¾‹**:
```
CLSãŒæ³¨ç›®ã—ã¦ã„ã‚‹ãƒ‘ãƒƒãƒ: [42, 103, 78, 156, 21]
```

ã“ã‚Œã‚‰ã®ãƒ‘ãƒƒãƒã¯ã€ç”»åƒä¸­ã®**é‡è¦ãªé ˜åŸŸ**ï¼ˆä¾‹: ç‰©ä½“ã®ä¸­å¿ƒéƒ¨ï¼‰ã«å¯¾å¿œã—ã¦ã„ã‚‹ã€‚

---

### 3.3 Cross-Modal Attentionç†è«–

CLIPã¯Late Fusionãªã®ã§ã€Cross-Modal Attentionã¯ä½¿ã‚ãªã„ã€‚ã—ã‹ã—ã€BLIP-2ã‚„CogVLMã§ã¯**Cross-Modal Attention**ãŒæ ¸å¿ƒæŠ€è¡“ã ã€‚

#### 3.3.1 Cross-Modal Attentionã®å®šç¾©

**é€šå¸¸ã®Self-Attention**: åŒã˜ãƒ¢ãƒ€ãƒªãƒ†ã‚£å†…ã§Attentionã‚’è¨ˆç®—ã€‚

$$
\mathbf{Q} = W_Q \mathbf{Z}, \quad \mathbf{K} = W_K \mathbf{Z}, \quad \mathbf{V} = W_V \mathbf{Z}
$$

**Cross-Modal Attention**: ç•°ãªã‚‹ãƒ¢ãƒ€ãƒªãƒ†ã‚£é–“ã§Attentionã‚’è¨ˆç®—ã€‚

$$
\mathbf{Q} = W_Q \mathbf{Z}^t, \quad \mathbf{K} = W_K \mathbf{Z}^v, \quad \mathbf{V} = W_V \mathbf{Z}^v
$$

ã“ã“ã§:
- $\mathbf{Z}^t \in \mathbb{R}^{d \times L}$ ã¯ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿åˆ—
- $\mathbf{Z}^v \in \mathbb{R}^{d \times N}$ ã¯ç”»åƒãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿åˆ—

**è§£é‡ˆ**: ãƒ†ã‚­ã‚¹ãƒˆã®å„ãƒˆãƒ¼ã‚¯ãƒ³ãŒã€**ç”»åƒã®ã©ã®ãƒ‘ãƒƒãƒã«æ³¨ç›®ã™ã¹ãã‹**ã‚’å­¦ç¿’ã™ã‚‹ã€‚

#### 3.3.2 Attention Mapã®æ„å‘³

Attention weights $\mathbf{A} \in \mathbb{R}^{L \times N}$ ã¯ã€**ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã¨ç”»åƒãƒ‘ãƒƒãƒã®å¯¾å¿œé–¢ä¿‚**ã‚’è¡¨ã™ã€‚

$$
\mathbf{A} = \text{softmax}\left(\frac{\mathbf{Q}^\top \mathbf{K}}{\sqrt{d_k}}\right)
$$

$\mathbf{A}_{i,j}$ ã¯ã€Œãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ $i$ ãŒç”»åƒãƒ‘ãƒƒãƒ $j$ ã«ã©ã‚Œã ã‘æ³¨ç›®ã—ã¦ã„ã‚‹ã‹ã€ã€‚

**ä¾‹**: ãƒ†ã‚­ã‚¹ãƒˆ "a red apple on a table" ã®å ´åˆ:
- ãƒˆãƒ¼ã‚¯ãƒ³ "red" â†’ èµ¤ã„é ˜åŸŸã®ãƒ‘ãƒƒãƒã«é«˜ã„Attention
- ãƒˆãƒ¼ã‚¯ãƒ³ "apple" â†’ ã‚Šã‚“ã”ã®å½¢çŠ¶ã®ãƒ‘ãƒƒãƒã«é«˜ã„Attention
- ãƒˆãƒ¼ã‚¯ãƒ³ "table" â†’ ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒ‘ãƒƒãƒã«é«˜ã„Attention

#### 3.3.3 Gated Cross-Attention (Flamingo)

Flamingoã¯**Gated Cross-Attention**ã‚’å°å…¥ã—ãŸ[^5]ã€‚ã“ã‚Œã¯ã€Cross-Attentionã®å‡ºåŠ›ã‚’**ã‚²ãƒ¼ãƒˆæ©Ÿæ§‹**ã§åˆ¶å¾¡ã™ã‚‹ã€‚

$$
\mathbf{Z}^t_{\text{out}} = \mathbf{Z}^t + \tanh(\alpha) \odot \text{CrossAttn}(\mathbf{Z}^t, \mathbf{Z}^v)
$$

ã“ã“ã§:
- $\alpha$ ã¯å­¦ç¿’å¯èƒ½ãªã‚²ãƒ¼ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆåˆæœŸå€¤0ï¼‰
- $\odot$ ã¯è¦ç´ ã”ã¨ã®ç©
- $\tanh(\alpha)$ ã¯ $[-1, 1]$ ã®ç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ—

**ãªãœã‚²ãƒ¼ãƒˆãŒå¿…è¦ï¼Ÿ**

Frozen LMã¨ã®çµ±åˆæ™‚ã€**æ€¥æ¿€ãªå¤‰æ›´ã‚’é˜²ã**ãŸã‚ã€‚åˆæœŸã¯ã‚²ãƒ¼ãƒˆã‚’é–‰ã˜ã¦ãŠãï¼ˆ$\alpha \approx 0$ï¼‰ã€è¨“ç·´ãŒé€²ã‚€ã«ã¤ã‚Œã¦å¾ã€…ã«é–‹ãã€‚

**å®Ÿè£…ï¼ˆJuliaï¼‰**:

```julia
struct GatedCrossAttention
    cross_attn::MultiHeadAttention
    gate::Param  # ã‚¹ã‚«ãƒ©ãƒ¼
end

function (gca::GatedCrossAttention)(Z_t, Z_v)
    attn_out = gca.cross_attn(Z_t, Z_v, Z_v)  # Query=Z_t, Key=Value=Z_v
    gated_out = Z_t .+ tanh(gca.gate[]) .* attn_out
    return gated_out
end
```

#### 3.3.4 Perceiver Resampler (Flamingo)

Flamingoã®ã‚‚ã†1ã¤ã®é©æ–°ã¯**Perceiver Resampler**ã ã€‚

**å•é¡Œ**: ç”»åƒã®è§£åƒåº¦ã‚„å‹•ç”»ã®ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã¯å¯å¤‰ã ãŒã€LMã¯**å›ºå®šé•·ã®å…¥åŠ›**ã‚’æœŸå¾…ã™ã‚‹ã€‚

**è§£æ±ºç­–**: å¯å¤‰é•·ã®ç”»åƒç‰¹å¾´é‡ $\mathbf{Z}^v \in \mathbb{R}^{d \times N}$ ã‚’ã€**å›ºå®šé•· $M$ ã®ç‰¹å¾´é‡**ã«ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã€‚

$$
\mathbf{Q} = \mathbf{L} \in \mathbb{R}^{d \times M} \quad \text{(å­¦ç¿’å¯èƒ½ãªLatent)}
$$

$$
\mathbf{K} = W_K \mathbf{Z}^v, \quad \mathbf{V} = W_V \mathbf{Z}^v
$$

$$
\mathbf{Z}^v_{\text{resampled}} = \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) \in \mathbb{R}^{d \times M}
$$

**ç‰¹å¾´**:
- $N$ ãŒä½•ã§ã‚ã‚Œã€å‡ºåŠ›ã¯å¸¸ã« $M$ å€‹ã®ãƒˆãƒ¼ã‚¯ãƒ³ã€‚
- $\mathbf{L}$ ã¯ã€Œå­¦ç¿’å¯èƒ½ãªã‚¯ã‚¨ãƒªã€ã§ã€ç”»åƒã®é‡è¦ãªæƒ…å ±ã‚’**åœ§ç¸®**ã™ã‚‹ã€‚

**å®Ÿè£…ï¼ˆJuliaï¼‰**:

```julia
struct PerceiverResampler
    num_latents::Int
    latents::Param  # (d, M)
    cross_attn::MultiHeadAttention
end

function (pr::PerceiverResampler)(Z_v)
    # Z_v: (d, N) â€” å¯å¤‰é•·ç”»åƒç‰¹å¾´
    Q = pr.latents  # (d, M)
    K = Z_v
    V = Z_v

    Z_resampled = pr.cross_attn(Q, K, V)  # (d, M)
    return Z_resampled
end
```

#### 3.3.5 æ•°å€¤ä¾‹: Cross-Modal Attentionã®åŠ¹æœ

```julia
using LinearAlgebra

# æ“¬ä¼¼ãƒ‡ãƒ¼ã‚¿
d = 512
L = 10  # ãƒ†ã‚­ã‚¹ãƒˆé•·
N = 196  # ç”»åƒãƒ‘ãƒƒãƒæ•°

Z_t = randn(d, L)
Z_v = randn(d, N)

# Cross-Modal Attention (ç°¡æ˜“ç‰ˆ)
W_Q = randn(d, d)
W_K = randn(d, d)
W_V = randn(d, d)

Q = W_Q * Z_t  # (d, L)
K = W_K * Z_v  # (d, N)
V = W_V * Z_v  # (d, N)

# Attention weights
scores = Q' * K ./ sqrt(d)  # (L, N)
A = softmax(scores, dims=2)  # å„è¡ŒãŒsoftmax

# å‡ºåŠ›
Z_t_out = V * A'  # (d, L)

# Attentionå¼·åº¦ã‚’ç¢ºèª
println("ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³1ãŒæœ€ã‚‚æ³¨ç›®ã—ã¦ã„ã‚‹ãƒ‘ãƒƒãƒ: $(argmax(A[1, :]))")
println("å¹³å‡Attentionå¼·åº¦: $(mean(A))")
```

**å‡ºåŠ›ä¾‹**:
```
ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³1ãŒæœ€ã‚‚æ³¨ç›®ã—ã¦ã„ã‚‹ãƒ‘ãƒƒãƒ: 78
å¹³å‡Attentionå¼·åº¦: 0.0051  # 1/N â‰ˆ 0.0051
```

---

### 3.4 InfoNCE losså®Œå…¨å°å‡ºï¼ˆBoss Battleï¼‰

ã“ã“ãŒä»Šå›ã®**ãƒœã‚¹æˆ¦**ã ã€‚CLIPã®è¨“ç·´ã«ä½¿ã‚ã‚Œã‚‹**InfoNCE loss**ã‚’ã€**ã‚¼ãƒ­ã‹ã‚‰å®Œå…¨ã«å°å‡º**ã™ã‚‹ã€‚

#### 3.4.1 å•é¡Œè¨­å®š

**ãƒ‡ãƒ¼ã‚¿**: $N$ å€‹ã®ç”»åƒ-ãƒ†ã‚­ã‚¹ãƒˆãƒšã‚¢ $\{(\mathbf{x}^v_i, \mathbf{x}^t_i)\}_{i=1}^N$ã€‚

**ç›®æ¨™**: æ­£ä¾‹ãƒšã‚¢ $(v_i, t_i)$ ã®é¡ä¼¼åº¦ã‚’æœ€å¤§åŒ–ã—ã€è² ä¾‹ãƒšã‚¢ $(v_i, t_j)$ $(i \neq j)$ ã®é¡ä¼¼åº¦ã‚’æœ€å°åŒ–ã™ã‚‹ã€‚

**é¡ä¼¼åº¦é–¢æ•°**:

$$
s_{ij} = \frac{\mathbf{v}_i \cdot \mathbf{t}_j}{\|\mathbf{v}_i\| \|\mathbf{t}_j\|} = \cos(\mathbf{v}_i, \mathbf{t}_j)
$$

#### 3.4.2 Contrastive Learningã®ç›´æ„Ÿ

Contrastive Learningã®æ ¸å¿ƒã¯ã€**æ­£ä¾‹ã‚’å¼•ãå¯„ã›ã€è² ä¾‹ã‚’é ã–ã‘ã‚‹**ã“ã¨ã€‚

- **æ­£ä¾‹**: $(v_i, t_i)$ â€” åŒã˜ç”»åƒ-ãƒ†ã‚­ã‚¹ãƒˆãƒšã‚¢
- **è² ä¾‹**: $(v_i, t_j)$ $(j \neq i)$ â€” ç•°ãªã‚‹ç”»åƒ-ãƒ†ã‚­ã‚¹ãƒˆãƒšã‚¢

ãƒãƒƒãƒå†…ã®å…¨ãƒšã‚¢ã‚’è€ƒãˆã‚‹ã¨ã€**1å€‹ã®æ­£ä¾‹ã¨ $(N-1)$ å€‹ã®è² ä¾‹**ãŒã‚ã‚‹ã€‚

#### 3.4.3 InfoNCE lossã®å°å‡ºï¼ˆã‚¹ãƒ†ãƒƒãƒ—1: å°¤åº¦æ¯”ï¼‰

**Oordã‚‰ã®è«–æ–‡**[^2]ã§ã¯ã€InfoNCE lossã¯**Noise Contrastive Estimation (NCE)**ã‹ã‚‰å°å‡ºã•ã‚Œã‚‹ã€‚

**è¨­å®š**:
- æ­£ä¾‹ $(v, t^+)$ ã®åŒæ™‚åˆ†å¸ƒ $p(v, t^+)$
- è² ä¾‹ $(v, t^-)$ ã®å‘¨è¾ºåˆ†å¸ƒ $p(v)p(t^-)$

**ç›®æ¨™**: æ­£ä¾‹ã¨è² ä¾‹ã‚’**è­˜åˆ¥**ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹ã€‚

**è­˜åˆ¥ç¢ºç‡**: ä¸ãˆã‚‰ã‚ŒãŸãƒšã‚¢ $(v, t)$ ãŒæ­£ä¾‹ã§ã‚ã‚‹ç¢ºç‡:

$$
p(\text{positive} \mid v, t) = \frac{p(v, t)}{p(v, t) + (N-1) p(v)p(t)}
$$

ã“ã“ã§ $(N-1)$ ã¯è² ä¾‹ã®æ•°ã€‚

**å¯¾æ•°å°¤åº¦æ¯”**:

$$
\log \frac{p(v, t)}{p(v)p(t)} = \log p(t \mid v) - \log p(t)
$$

ç¬¬6å›ï¼ˆæƒ…å ±ç†è«–ï¼‰ã§å­¦ã‚“ã **Pointwise Mutual Information (PMI)**ã¨åŒã˜å½¢ã ã€‚

**è¿‘ä¼¼**: $\log p(t \mid v)$ ã‚’é¡ä¼¼åº¦ $s(v, t)$ ã§è¿‘ä¼¼:

$$
\log p(t \mid v) \approx s(v, t) / \tau
$$

ã“ã“ã§ $\tau$ ã¯æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚

#### 3.4.4 InfoNCE lossã®å°å‡ºï¼ˆã‚¹ãƒ†ãƒƒãƒ—2: Softmaxå½¢å¼ï¼‰

æ­£ä¾‹ $(v_i, t_i)$ ãŒã€ãƒãƒƒãƒå†…ã®å…¨å€™è£œã®ä¸­ã§é¸ã°ã‚Œã‚‹ç¢ºç‡:

$$
p(t_i \mid v_i, \{t_1, \ldots, t_N\}) = \frac{\exp(s_{ii}/\tau)}{\sum_{j=1}^N \exp(s_{ij}/\tau)}
$$

ã“ã‚Œã¯**Softmaxãã®ã‚‚ã®**ã ã€‚

**è² ã®å¯¾æ•°å°¤åº¦**:

$$
\mathcal{L}_i^{v \to t} = -\log p(t_i \mid v_i, \{t_1, \ldots, t_N\}) = -\log \frac{\exp(s_{ii}/\tau)}{\sum_{j=1}^N \exp(s_{ij}/\tau)}
$$

å±•é–‹ã™ã‚‹ã¨:

$$
\mathcal{L}_i^{v \to t} = -\frac{s_{ii}}{\tau} + \log \sum_{j=1}^N \exp\left(\frac{s_{ij}}{\tau}\right)
$$

#### 3.4.5 InfoNCE lossã®å°å‡ºï¼ˆã‚¹ãƒ†ãƒƒãƒ—3: å¯¾ç§°æ€§ï¼‰

åŒæ§˜ã«ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒã¸ã®æå¤±:

$$
\mathcal{L}_i^{t \to v} = -\log \frac{\exp(s_{ii}/\tau)}{\sum_{j=1}^N \exp(s_{ji}/\tau)}
$$

**CLIPã®InfoNCE loss**ã¯ã€**åŒæ–¹å‘ã®æå¤±ã®å¹³å‡**:

$$
\mathcal{L}_i = \frac{1}{2} \left( \mathcal{L}_i^{v \to t} + \mathcal{L}_i^{t \to v} \right)
$$

**å…¨ãƒãƒƒãƒã®æå¤±**:

$$
\mathcal{L} = \frac{1}{N} \sum_{i=1}^N \mathcal{L}_i
$$

#### 3.4.6 æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\tau$ ã®å½¹å‰²

$\tau$ ã¯**åˆ†å¸ƒã®é‹­ã•**ã‚’åˆ¶å¾¡ã™ã‚‹ã€‚

- **$\tau$ ãŒå°ã•ã„**: Softmaxåˆ†å¸ƒãŒé‹­ããªã‚Šã€æœ€é«˜ã‚¹ã‚³ã‚¢ã®ã‚¯ãƒ©ã‚¹ã«ç¢ºç‡ãŒé›†ä¸­ã€‚
- **$\tau$ ãŒå¤§ãã„**: Softmaxåˆ†å¸ƒãŒãªã ã‚‰ã‹ã«ãªã‚Šã€å…¨ã‚¯ãƒ©ã‚¹ã«ç¢ºç‡ãŒåˆ†æ•£ã€‚

**æœ€é©ãª $\tau$**: CLIPã®è«–æ–‡ã§ã¯ $\tau = 0.07$ ãŒæœ€é©ã¨ã•ã‚Œã¦ã„ã‚‹ã€‚ã“ã‚Œã¯å®Ÿé¨“çš„ã«æ±ºå®šã•ã‚ŒãŸã€‚

**æ•°å¼ã§ã®ç¢ºèª**:

$$
\text{softmax}(s_i / \tau) = \frac{\exp(s_i/\tau)}{\sum_j \exp(s_j/\tau)}
$$

$\tau \to 0$ ã®ã¨ãã€$\text{softmax}(s_i / \tau) \to \mathbb{1}_{[i = \arg\max_j s_j]}$ ï¼ˆãƒãƒ¼ãƒ‰åˆ†é¡ï¼‰ã€‚

#### 3.4.7 InfoNCE lossã®å®Ÿè£…ï¼ˆJuliaå®Œå…¨ç‰ˆï¼‰

```julia
using Flux, LinearAlgebra

"""
InfoNCE loss for CLIP training.

# Arguments
- `v_embeds`: ç”»åƒåŸ‹ã‚è¾¼ã¿ (d, N)
- `t_embeds`: ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ (d, N)
- `Ï„`: æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (default 0.07)

# Returns
- `loss`: InfoNCE loss (scalar)
"""
function infonce_loss(v_embeds, t_embeds, Ï„=0.07)
    N = size(v_embeds, 2)

    # æ­£è¦åŒ–
    v_embeds = v_embeds ./ sqrt.(sum(v_embeds.^2, dims=1))  # (d, N)
    t_embeds = t_embeds ./ sqrt.(sum(t_embeds.^2, dims=1))  # (d, N)

    # é¡ä¼¼åº¦è¡Œåˆ—: S[i,j] = cos(v_i, t_j)
    S = v_embeds' * t_embeds  # (N, N)

    # æ¸©åº¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    logits = S ./ Ï„  # (N, N)

    # æ­£ä¾‹ãƒ©ãƒ™ãƒ«: å¯¾è§’æˆåˆ†
    labels = 1:N  # [1, 2, ..., N]

    # vâ†’t ã®æå¤±
    loss_v2t = Flux.logitcrossentropy(logits, labels)

    # tâ†’v ã®æå¤±ï¼ˆè»¢ç½®ï¼‰
    loss_t2v = Flux.logitcrossentropy(logits', labels)

    # å¯¾ç§°æ€§ã‚’æŒãŸã›ã‚‹
    loss = (loss_v2t + loss_t2v) / 2

    return loss
end
```

**æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œ**:

$$
s_{ij} = \frac{\mathbf{v}_i \cdot \mathbf{t}_j}{\|\mathbf{v}_i\| \|\mathbf{t}_j\|} \quad \Leftrightarrow \quad \texttt{S = v\_embeds' * t\_embeds}
$$

$$
\mathcal{L}_i^{v \to t} = -\log \frac{\exp(s_{ii}/\tau)}{\sum_{j=1}^N \exp(s_{ij}/\tau)} \quad \Leftrightarrow \quad \texttt{logitcrossentropy(logits, labels)}
$$

#### 3.4.8 æ•°å€¤æ¤œè¨¼: InfoNCE lossã®æŒ™å‹•

```julia
using Random

Random.seed!(42)
d = 512
N = 8

# æ“¬ä¼¼åŸ‹ã‚è¾¼ã¿
v_embeds = randn(d, N)
t_embeds = randn(d, N)

# æ­£ä¾‹ãƒšã‚¢ã®é¡ä¼¼åº¦ã‚’é«˜ãã™ã‚‹ï¼ˆæ“¬ä¼¼çš„ã«è¨“ç·´æ¸ˆã¿ï¼‰
for i in 1:N
    t_embeds[:, i] = 0.8 * v_embeds[:, i] + 0.2 * randn(d)
end

# InfoNCE lossè¨ˆç®—
loss = infonce_loss(v_embeds, t_embeds, 0.07)
println("InfoNCE loss: $loss")

# é¡ä¼¼åº¦è¡Œåˆ—ã‚’ç¢ºèª
v_norm = v_embeds ./ sqrt.(sum(v_embeds.^2, dims=1))
t_norm = t_embeds ./ sqrt.(sum(t_embeds.^2, dims=1))
S = v_norm' * t_norm
println("é¡ä¼¼åº¦è¡Œåˆ—ï¼ˆå¯¾è§’æˆåˆ†ï¼‰:")
println(diag(S))  # æ­£ä¾‹ãƒšã‚¢ã®é¡ä¼¼åº¦
```

**å‡ºåŠ›ä¾‹**:
```
InfoNCE loss: 0.523
é¡ä¼¼åº¦è¡Œåˆ—ï¼ˆå¯¾è§’æˆåˆ†ï¼‰:
[0.89, 0.91, 0.87, 0.92, 0.88, 0.90, 0.86, 0.93]
```

å¯¾è§’æˆåˆ†ï¼ˆæ­£ä¾‹ãƒšã‚¢ï¼‰ã®é¡ä¼¼åº¦ãŒé«˜ã„ï¼ˆ0.86ã€œ0.93ï¼‰ã“ã¨ãŒç¢ºèªã§ããŸã€‚è¨“ç·´ãŒé€²ã‚€ã¨ã€å¯¾è§’æˆåˆ†ã¯ã•ã‚‰ã«1ã«è¿‘ã¥ãã€éå¯¾è§’æˆåˆ†ã¯0ã«è¿‘ã¥ãã€‚

#### 3.4.9 InfoNCE lossã®ç†è«–çš„æ€§è³ª

**æ€§è³ª1: ä¸‹ç•Œã®æœ€å¤§åŒ–**

InfoNCE lossã¯ã€**ç›¸äº’æƒ…å ±é‡ $I(\mathbf{v}; \mathbf{t})$ ã®ä¸‹ç•Œ**ã‚’æœ€å¤§åŒ–ã—ã¦ã„ã‚‹ï¼ˆç¬¬6å›ã®ç›¸äº’æƒ…å ±é‡ã‚’å‚ç…§ï¼‰:

$$
I(\mathbf{v}; \mathbf{t}) \geq \mathbb{E}_{(v,t) \sim p(v,t)} \left[ \log \frac{p(v, t)}{p(v)p(t)} \right] - \log N
$$

InfoNCE lossã‚’æœ€å°åŒ–ã™ã‚‹ã“ã¨ã¯ã€ã“ã®ä¸‹ç•Œã‚’æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ã«ç­‰ã—ã„ã€‚

**æ€§è³ª2: Hard Negative Mining**

ãƒãƒƒãƒå†…ã®è² ä¾‹ã®ä¸­ã§ã€**é¡ä¼¼åº¦ãŒé«˜ã„è² ä¾‹**ï¼ˆHard Negativeï¼‰ã»ã©ã€æå¤±ã¸ã®å¯„ä¸ãŒå¤§ãã„:

$$
\frac{\partial \mathcal{L}_i^{v \to t}}{\partial s_{ij}} = \frac{1}{\tau} \left( \frac{\exp(s_{ij}/\tau)}{\sum_k \exp(s_{ik}/\tau)} - \mathbb{1}_{[j=i]} \right)
$$

$s_{ij}$ ãŒå¤§ãã„ã»ã©ã€å‹¾é…ãŒå¤§ãããªã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€Hard NegativeãŒè‡ªå‹•çš„ã«å¼·èª¿ã•ã‚Œã‚‹ã€‚

**æ€§è³ª3: Large Batch Sizeã®é‡è¦æ€§**

ãƒãƒƒãƒã‚µã‚¤ã‚º $N$ ãŒå¤§ãã„ã»ã©ã€è² ä¾‹ã®å¤šæ§˜æ€§ãŒå¢—ã—ã€è­˜åˆ¥ãŒã‚ˆã‚Šå›°é›£ã«ãªã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã¯ã‚ˆã‚Šç²¾å¯†ãªåŸ‹ã‚è¾¼ã¿ã‚’å­¦ç¿’ã™ã‚‹ã€‚

CLIPã®è«–æ–‡ã§ã¯ã€**ãƒãƒƒãƒã‚µã‚¤ã‚º 32,768**ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã€‚

---

**ãƒœã‚¹æ’ƒç ´ï¼**

InfoNCE lossã®å®Œå…¨å°å‡ºã‚’çµ‚ãˆãŸã€‚ã“ã“ã¾ã§æ¥ã‚Œã°ã€CLIPã®è¨“ç·´ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’å®Œå…¨ã«ç†è§£ã—ãŸã“ã¨ã«ãªã‚‹ã€‚

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®50%å®Œäº†ï¼** Zone 4ã§ã¯ã€ã“ã®ç†è«–ã‚’å®Ÿè£…ã«è½ã¨ã—è¾¼ã‚€ã€‚âš¡Juliaã§CLIPè¨“ç·´ã€ğŸ¦€Rustã§SmolVLM2æ¨è«–ã‚’å®Œå…¨å®Ÿè£…ã™ã‚‹ã€‚
:::

---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” Julia CLIP + Rust SmolVLM2

ç†è«–ã‚’ç†è§£ã—ãŸã ã‘ã§ã¯ä¸ååˆ†ã ã€‚å®Ÿè£…ã—ã¦ã“ãã€**çœŸã®ç†è§£**ãŒå¾—ã‚‰ã‚Œã‚‹ã€‚

ã“ã®Zoneã§ã¯ã€3ã¤ã®å®Ÿè£…ã‚’å®Œèµ°ã™ã‚‹:
1. **âš¡Julia CLIPå®Ÿè£…** â€” Dual Encoderè¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
2. **âš¡Julia ViTå®Ÿè£…** â€” Vision Transformerã®å®Œå…¨å®Ÿè£…
3. **ğŸ¦€Rust SmolVLM2æ¨è«–** â€” GGUF/Candleçµ±åˆã§ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ¨è«–

### 4.1 âš¡Julia CLIPå®Ÿè£…

#### 4.1.1 ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å…¨ä½“åƒ

CLIPã¯**Dual Encoder**æ§‹é€ ã ã€‚ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’ç‹¬ç«‹ã«å‡¦ç†ã—ã€æœ€å¾Œã«é¡ä¼¼åº¦ã‚’è¨ˆç®—ã™ã‚‹ã€‚

```mermaid
graph TD
    Img[ç”»åƒãƒãƒƒãƒ<br>BÃ—HÃ—WÃ—C] --> VisionEnc[Vision Encoder<br>ViT-B/32]
    Text[ãƒ†ã‚­ã‚¹ãƒˆãƒãƒƒãƒ<br>BÃ—L] --> TextEnc[Text Encoder<br>Transformer]
    VisionEnc --> VEmb[ç”»åƒåŸ‹ã‚è¾¼ã¿<br>BÃ—d]
    TextEnc --> TEmb[ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿<br>BÃ—d]
    VEmb --> InfoNCE[InfoNCE Loss]
    TEmb --> InfoNCE
    InfoNCE --> Grad[å‹¾é…è¨ˆç®—]
    Grad --> Update[ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°]
```

#### 4.1.2 Vision Encoderã®å®Ÿè£…

```julia
using Flux, CUDA

# Vision Transformer for CLIP
struct VisionTransformer
    patch_embed::PatchEmbed
    pos_embed::Param
    cls_token::Param
    transformer_blocks::Chain
    norm::LayerNorm
    proj::Dense  # åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã¸ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³
end

function VisionTransformer(;
    img_size=224,
    patch_size=32,
    in_channels=3,
    embed_dim=768,
    depth=12,
    num_heads=12,
    mlp_ratio=4,
    out_dim=512
)
    num_patches = (img_size Ã· patch_size)^2

    # Patch Embedding
    patch_embed = PatchEmbed(img_size, patch_size, embed_dim, in_channels)

    # Positional Encoding + CLS token
    pos_embed = Param(randn(embed_dim, num_patches + 1) .* 0.02)
    cls_token = Param(randn(embed_dim, 1) .* 0.02)

    # Transformer Blocks
    transformer_blocks = Chain([
        TransformerBlock(embed_dim, num_heads, mlp_ratio) for _ in 1:depth
    ]...)

    # Layer Norm + Projection
    norm = LayerNorm(embed_dim)
    proj = Dense(embed_dim, out_dim)

    return VisionTransformer(patch_embed, pos_embed, cls_token, transformer_blocks, norm, proj)
end

function (vit::VisionTransformer)(x)
    # x: (H, W, C, B)
    B = size(x, 4)

    # Patch Embedding: (H, W, C, B) â†’ (d, N, B)
    patches = vit.patch_embed(x)  # (embed_dim, num_patches, B)

    # CLS tokenã‚’å„ãƒãƒƒãƒã«è¿½åŠ 
    cls_tokens = repeat(vit.cls_token, 1, B)  # (embed_dim, B)
    tokens = cat(cls_tokens, patches, dims=2)  # (embed_dim, N+1, B)

    # Positional Encoding
    tokens = tokens .+ vit.pos_embed

    # Transformer Blocks
    for block in vit.transformer_blocks
        tokens = block(tokens)
    end

    # CLS tokenã®å‡ºåŠ›ã‚’å–å¾—
    cls_output = tokens[:, 1, :]  # (embed_dim, B)

    # Layer Norm + Projection
    cls_output = vit.norm(cls_output)
    embeddings = vit.proj(cls_output)  # (out_dim, B)

    return embeddings
end

# Transformer Block
struct TransformerBlock
    attn::MultiHeadSelfAttention
    mlp::Chain
    norm1::LayerNorm
    norm2::LayerNorm
end

function TransformerBlock(embed_dim, num_heads, mlp_ratio)
    attn = MultiHeadSelfAttention(embed_dim, num_heads)
    mlp = Chain(
        Dense(embed_dim, embed_dim * mlp_ratio, gelu),
        Dense(embed_dim * mlp_ratio, embed_dim)
    )
    norm1 = LayerNorm(embed_dim)
    norm2 = LayerNorm(embed_dim)
    return TransformerBlock(attn, mlp, norm1, norm2)
end

function (block::TransformerBlock)(x)
    # Pre-Norm: Norm â†’ Attention â†’ Residual
    x = x .+ block.attn(block.norm1(x))
    # Pre-Norm: Norm â†’ MLP â†’ Residual
    x = x .+ block.mlp(block.norm2(x))
    return x
end
```

#### 4.1.3 Text Encoderã®å®Ÿè£…

```julia
# Text Transformer for CLIP
struct TextTransformer
    token_embed::Embedding
    pos_embed::Param
    transformer_blocks::Chain
    norm::LayerNorm
    proj::Dense
end

function TextTransformer(;
    vocab_size=49408,  # CLIPã®vocabã‚µã‚¤ã‚º
    max_len=77,
    embed_dim=512,
    depth=12,
    num_heads=8,
    mlp_ratio=4,
    out_dim=512
)
    token_embed = Embedding(vocab_size, embed_dim)
    pos_embed = Param(randn(embed_dim, max_len) .* 0.02)

    transformer_blocks = Chain([
        TransformerBlock(embed_dim, num_heads, mlp_ratio) for _ in 1:depth
    ]...)

    norm = LayerNorm(embed_dim)
    proj = Dense(embed_dim, out_dim)

    return TextTransformer(token_embed, pos_embed, transformer_blocks, norm, proj)
end

function (txt::TextTransformer)(tokens)
    # tokens: (L, B) â€” ãƒˆãƒ¼ã‚¯ãƒ³IDåˆ—
    L, B = size(tokens)

    # Token Embedding
    x = txt.token_embed(tokens)  # (embed_dim, L, B)

    # Positional Encoding
    x = x .+ txt.pos_embed[:, 1:L, :]

    # Transformer Blocks
    for block in txt.transformer_blocks
        x = block(x)
    end

    # EOT (End of Text) tokenã®å‡ºåŠ›ã‚’å–å¾—
    # ä»®å®š: EOT tokenã¯ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æœ€å¾Œ
    eot_output = x[:, end, :]  # (embed_dim, B)

    # Layer Norm + Projection
    eot_output = txt.norm(eot_output)
    embeddings = txt.proj(eot_output)  # (out_dim, B)

    return embeddings
end
```

#### 4.1.4 CLIPãƒ¢ãƒ‡ãƒ«å…¨ä½“

```julia
# CLIP: Vision + Text Dual Encoder
struct CLIP
    vision::VisionTransformer
    text::TextTransformer
    Ï„::Param  # æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå­¦ç¿’å¯èƒ½ï¼‰
end

function CLIP()
    vision = VisionTransformer(
        img_size=224, patch_size=32, embed_dim=768, depth=12, num_heads=12, out_dim=512
    )
    text = TextTransformer(
        vocab_size=49408, max_len=77, embed_dim=512, depth=12, num_heads=8, out_dim=512
    )
    Ï„ = Param([0.07])  # åˆæœŸæ¸©åº¦
    return CLIP(vision, text, Ï„)
end

function (clip::CLIP)(images, tokens)
    # ç”»åƒãƒ»ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿
    v_embeds = clip.vision(images)  # (out_dim, B)
    t_embeds = clip.text(tokens)    # (out_dim, B)

    # InfoNCE loss
    loss = infonce_loss(v_embeds, t_embeds, clip.Ï„[])

    return loss, v_embeds, t_embeds
end
```

#### 4.1.5 è¨“ç·´ãƒ«ãƒ¼ãƒ—

```julia
using Flux.Optimise: Adam
using ProgressMeter

function train_clip(clip, train_loader, epochs=10, lr=1e-4)
    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
    opt = Adam(lr)
    ps = Flux.params(clip)

    for epoch in 1:epochs
        total_loss = 0.0
        @showprogress for (images, tokens) in train_loader
            # å‹¾é…è¨ˆç®—
            loss, back = Flux.pullback(ps) do
                loss, _, _ = clip(images, tokens)
                return loss
            end

            # å‹¾é…æ›´æ–°
            grads = back(1.0f0)
            Flux.update!(opt, ps, grads)

            total_loss += loss
        end

        avg_loss = total_loss / length(train_loader)
        println("Epoch $epoch: Loss = $avg_loss")
    end
end
```

#### 4.1.6 Zero-shotæ¨è«–

```julia
function zero_shot_classify(clip, image, text_candidates)
    # ç”»åƒåŸ‹ã‚è¾¼ã¿
    img_batch = unsqueeze(image, 4)  # (H, W, C, 1)
    v_embed = clip.vision(img_batch)[:, 1]  # (out_dim,)

    # ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ï¼ˆå„å€™è£œï¼‰
    t_embeds = [clip.text(tokenize(t))[:, 1] for t in text_candidates]

    # é¡ä¼¼åº¦è¨ˆç®—
    v_embed_norm = v_embed ./ norm(v_embed)
    similarities = [dot(v_embed_norm, t ./ norm(t)) for t in t_embeds]

    # Softmaxç¢ºç‡
    probs = softmax(similarities ./ clip.Ï„[])

    return probs, argmax(probs)
end
```

#### 4.1.7 æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å®Œå…¨å¯¾å¿œè¡¨

| æ•°å¼ | ã‚³ãƒ¼ãƒ‰ |
|:-----|:-------|
| $\mathbf{v} = f_v(\mathbf{x}^v)$ | `v_embeds = clip.vision(images)` |
| $\mathbf{t} = f_t(\mathbf{x}^t)$ | `t_embeds = clip.text(tokens)` |
| $s_{ij} = \frac{\mathbf{v}_i \cdot \mathbf{t}_j}{\|\mathbf{v}_i\| \|\mathbf{t}_j\|}$ | `S = v_embeds' * t_embeds` (æ­£è¦åŒ–å¾Œ) |
| $\mathcal{L}_i^{v \to t} = -\log \frac{\exp(s_{ii}/\tau)}{\sum_j \exp(s_{ij}/\tau)}$ | `logitcrossentropy(S ./ Ï„, labels)` |
| $\mathbf{Z}_p = W_{\text{proj}} \cdot \text{vec}(\mathbf{x}_p)$ | `pe.proj(patches)` |
| $\mathbf{A} = \text{softmax}(\frac{\mathbf{Q}^\top \mathbf{K}}{\sqrt{d_k}})$ | `softmax(Q' * K ./ sqrt(d_k), dims=2)` |

---

### 4.2 âš¡Julia ViTå®Ÿè£…ï¼ˆå®Œå…¨ç‰ˆï¼‰

Zone 3.2ã§ViTã®ç†è«–ã‚’å­¦ã‚“ã ã€‚ã“ã“ã§ã¯ã€**è¨“ç·´å¯èƒ½ãªViT**ã‚’å®Œå…¨å®Ÿè£…ã™ã‚‹ã€‚

#### 4.2.1 Multi-Head Self-Attentionã®å®Ÿè£…

```julia
# Multi-Head Self-Attention
struct MultiHeadSelfAttention
    num_heads::Int
    head_dim::Int
    qkv::Dense  # Query, Key, Valueã‚’ä¸€åº¦ã«è¨ˆç®—
    proj::Dense
end

function MultiHeadSelfAttention(embed_dim, num_heads)
    @assert embed_dim % num_heads == 0
    head_dim = embed_dim Ã· num_heads
    qkv = Dense(embed_dim, 3 * embed_dim)  # Q, K, V
    proj = Dense(embed_dim, embed_dim)
    return MultiHeadSelfAttention(num_heads, head_dim, qkv, proj)
end

function (mha::MultiHeadSelfAttention)(x)
    # x: (embed_dim, N, B)
    d, N, B = size(x)
    h = mha.num_heads
    d_h = mha.head_dim

    # Q, K, Vè¨ˆç®—
    qkv = mha.qkv(x)  # (3*embed_dim, N, B)
    q, k, v = chunk(qkv, 3, dims=1)  # ãã‚Œãã‚Œ (embed_dim, N, B)

    # Multi-headå½¢çŠ¶ã«å¤‰æ›: (embed_dim, N, B) â†’ (d_h, N, h, B)
    q = reshape(q, (d_h, h, N, B))
    k = reshape(k, (d_h, h, N, B))
    v = reshape(v, (d_h, h, N, B))

    # Attentionè¨ˆç®—ï¼ˆå„ãƒ˜ãƒƒãƒ‰ç‹¬ç«‹ï¼‰
    # scores: (N, N, h, B)
    scores = batched_mul(permutedims(q, (3, 1, 2, 4)), permutedims(k, (1, 3, 2, 4))) ./ sqrt(d_h)
    attn = softmax(scores, dims=2)

    # Attentioné©ç”¨: (d_h, N, h, B)
    out = batched_mul(permutedims(v, (1, 3, 2, 4)), attn)  # (d_h, N, h, B)

    # Multi-headã‚’çµåˆ: (d_h, N, h, B) â†’ (embed_dim, N, B)
    out = reshape(permutedims(out, (1, 3, 2, 4)), (d, N, B))

    # å‡ºåŠ›ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³
    out = mha.proj(out)

    return out
end
```

**æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œ**:

$$
\mathbf{Q} = W_Q \mathbf{X}, \quad \mathbf{K} = W_K \mathbf{X}, \quad \mathbf{V} = W_V \mathbf{X} \quad \Leftrightarrow \quad \texttt{q, k, v = chunk(qkv, 3)}
$$

$$
\mathbf{A} = \text{softmax}\left(\frac{\mathbf{Q}^\top \mathbf{K}}{\sqrt{d_h}}\right) \quad \Leftrightarrow \quad \texttt{attn = softmax(scores ./ sqrt(d\_h))}
$$

#### 4.2.2 ViTè¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```julia
using Flux, MLDatasets, Images

# ImageNetãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ï¼ˆç°¡æ˜“ç‰ˆï¼‰
function imagenet_loader(batch_size=32)
    # å®Ÿéš›ã¯ImageNet-1kã‚’ä½¿ç”¨
    # ã“ã“ã§ã¯æ“¬ä¼¼ãƒ‡ãƒ¼ã‚¿
    images = [randn(Float32, 224, 224, 3) for _ in 1:1000]
    labels = rand(1:1000, 1000)
    return DataLoader((images, labels), batchsize=batch_size, shuffle=true)
end

# ViTè¨“ç·´
function train_vit(vit, train_loader, epochs=30, lr=3e-4)
    opt = Adam(lr)
    ps = Flux.params(vit)

    for epoch in 1:epochs
        for (images, labels) in train_loader
            loss, back = Flux.pullback(ps) do
                logits = vit(images)  # (num_classes, B)
                return Flux.logitcrossentropy(logits, labels)
            end

            grads = back(1.0f0)
            Flux.update!(opt, ps, grads)
        end

        # è©•ä¾¡
        acc = evaluate_vit(vit, test_loader)
        println("Epoch $epoch: Accuracy = $acc")
    end
end

function evaluate_vit(vit, test_loader)
    correct = 0
    total = 0
    for (images, labels) in test_loader
        logits = vit(images)
        preds = argmax(logits, dims=1)
        correct += sum(preds .== labels)
        total += length(labels)
    end
    return correct / total
end
```

---

### 4.3 ğŸ¦€Rust SmolVLM2æ¨è«–

Juliaã§CLIPã‚’è¨“ç·´ã—ãŸã€‚æ¬¡ã¯ã€**Rustã§æ¨è«–**ã‚’å®Ÿè£…ã™ã‚‹ã€‚SmolVLM2-256Mã¯ã€Rustã®`candle`ã‚¯ãƒ¬ãƒ¼ãƒˆã§æ¨è«–ã§ãã‚‹ã€‚

#### 4.3.1 Rustãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
cargo new smolvlm2_inference
cd smolvlm2_inference
```

**Cargo.toml**:

```toml
[package]
name = "smolvlm2_inference"
version = "0.1.0"
edition = "2021"

[dependencies]
candle-core = "0.4"
candle-nn = "0.4"
candle-transformers = "0.4"
tokenizers = "0.15"
image = "0.25"
anyhow = "1.0"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
```

#### 4.3.2 ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›å‡¦ç†

```rust
use candle_core::{Device, Tensor};
use candle_transformers::models::smolvlm::{Config, Model};
use image::{DynamicImage, GenericImageView};
use tokenizers::Tokenizer;
use anyhow::Result;

/// ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›: ç”»åƒ + ãƒ†ã‚­ã‚¹ãƒˆ
pub struct MultimodalInput {
    pub image: DynamicImage,
    pub text: String,
}

/// ç”»åƒã‚’å‰å‡¦ç†ã—ã¦ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
pub fn preprocess_image(image: &DynamicImage, device: &Device) -> Result<Tensor> {
    let (width, height) = image.dimensions();
    let img = image.resize_exact(224, 224, image::imageops::FilterType::Triangle);
    let img_rgb = img.to_rgb8();

    // (H, W, C) â†’ (C, H, W) â†’ æ­£è¦åŒ–
    let data: Vec<f32> = img_rgb
        .pixels()
        .flat_map(|p| {
            let r = (p[0] as f32 / 255.0 - 0.485) / 0.229;
            let g = (p[1] as f32 / 255.0 - 0.456) / 0.224;
            let b = (p[2] as f32 / 255.0 - 0.406) / 0.225;
            [r, g, b]
        })
        .collect();

    let tensor = Tensor::from_vec(data, (3, 224, 224), device)?;
    Ok(tensor.unsqueeze(0)?) // (1, 3, 224, 224)
}

/// ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
pub fn tokenize_text(tokenizer: &Tokenizer, text: &str) -> Result<Tensor> {
    let encoding = tokenizer.encode(text, true)?;
    let ids = encoding.get_ids();
    let tensor = Tensor::new(ids, &Device::Cpu)?;
    Ok(tensor.unsqueeze(0)?) // (1, L)
}
```

#### 4.3.3 SmolVLM2ãƒ¢ãƒ‡ãƒ«æ¨è«–

```rust
/// SmolVLM2æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³
pub struct SmolVLM2Inference {
    model: Model,
    tokenizer: Tokenizer,
    device: Device,
}

impl SmolVLM2Inference {
    /// ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    pub fn load(model_path: &str, tokenizer_path: &str) -> Result<Self> {
        let device = Device::cuda_if_available(0)?;
        let config = Config::smolvlm2_256m(); // 256Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        let vb = candle_nn::VarBuilder::from_pth(model_path, candle_core::DType::F32, &device)?;
        let model = Model::new(&config, vb)?;
        let tokenizer = Tokenizer::from_file(tokenizer_path)?;

        Ok(Self { model, tokenizer, device })
    }

    /// ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ¨è«–
    pub fn infer(&self, input: &MultimodalInput) -> Result<String> {
        // ç”»åƒãƒ»ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†
        let image_tensor = preprocess_image(&input.image, &self.device)?;
        let text_tensor = tokenize_text(&self.tokenizer, &input.text)?;

        // ãƒ¢ãƒ‡ãƒ«æ¨è«–
        let output = self.model.forward(&image_tensor, &text_tensor)?;

        // ãƒ‡ã‚³ãƒ¼ãƒ‰ï¼ˆargmax â†’ ãƒˆãƒ¼ã‚¯ãƒ³ID â†’ ãƒ†ã‚­ã‚¹ãƒˆï¼‰
        let logits = output.squeeze(0)?; // (vocab_size,)
        let token_id = logits.argmax(0)?.to_scalar::<u32>()?;
        let decoded = self.tokenizer.decode(&[token_id], false)?;

        Ok(decoded)
    }

    /// ãƒãƒƒãƒæ¨è«–
    pub fn infer_batch(&self, inputs: &[MultimodalInput]) -> Result<Vec<String>> {
        let mut results = Vec::with_capacity(inputs.len());
        for input in inputs {
            results.push(self.infer(input)?);
        }
        Ok(results)
    }
}
```

#### 4.3.4 ä½¿ç”¨ä¾‹

```rust
fn main() -> Result<()> {
    // ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    let inference = SmolVLM2Inference::load(
        "models/smolvlm2-256m.pth",
        "models/tokenizer.json",
    )?;

    // ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›
    let image = image::open("cat.jpg")?;
    let input = MultimodalInput {
        image,
        text: "What is in this image?".to_string(),
    };

    // æ¨è«–
    let result = inference.infer(&input)?;
    println!("å›ç­”: {}", result);

    Ok(())
}
```

**å‡ºåŠ›ä¾‹**:
```
å›ç­”: A cat sitting on a sofa.
```

#### 4.3.5 FFIçµŒç”±ã§Juliaã‹ã‚‰å‘¼ã³å‡ºã—

```rust
// FFIç”¨ã®C-ABIé–¢æ•°
#[no_mangle]
pub extern "C" fn smolvlm2_infer(
    image_path: *const c_char,
    text: *const c_char,
    output_buf: *mut c_char,
    buf_len: usize,
) -> i32 {
    // SAFETY: Cæ–‡å­—åˆ—ã‹ã‚‰Rust &strã«å¤‰æ›
    let image_path_str = unsafe { CStr::from_ptr(image_path).to_str().unwrap() };
    let text_str = unsafe { CStr::from_ptr(text).to_str().unwrap() };

    // æ¨è«–
    let inference = SmolVLM2Inference::load("models/smolvlm2-256m.pth", "models/tokenizer.json").unwrap();
    let image = image::open(image_path_str).unwrap();
    let input = MultimodalInput {
        image,
        text: text_str.to_string(),
    };
    let result = inference.infer(&input).unwrap();

    // çµæœã‚’Cæ–‡å­—åˆ—ã«ã‚³ãƒ”ãƒ¼
    let result_cstr = CString::new(result).unwrap();
    let result_bytes = result_cstr.as_bytes_with_nul();
    if result_bytes.len() > buf_len {
        return -1; // ãƒãƒƒãƒ•ã‚¡ä¸è¶³
    }
    unsafe {
        std::ptr::copy_nonoverlapping(result_bytes.as_ptr(), output_buf as *mut u8, result_bytes.len());
    }

    0 // æˆåŠŸ
}
```

**Juliaã‹ã‚‰å‘¼ã³å‡ºã—**:

```julia
# Rustãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ãƒ­ãƒ¼ãƒ‰
const libsmolvlm2 = "target/release/libsmolvlm2_inference.so"

function rust_smolvlm2_infer(image_path::String, text::String)
    output_buf = Vector{UInt8}(undef, 1024)
    ret = ccall(
        (:smolvlm2_infer, libsmolvlm2),
        Cint,
        (Cstring, Cstring, Ptr{UInt8}, Csize_t),
        image_path, text, output_buf, length(output_buf)
    )
    if ret != 0
        error("æ¨è«–å¤±æ•—")
    end
    return unsafe_string(pointer(output_buf))
end

# ä½¿ç”¨ä¾‹
result = rust_smolvlm2_infer("cat.jpg", "What is in this image?")
println("å›ç­”: $result")
```

---

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®70%å®Œäº†ï¼** Zone 5ã§ã¯ã€å®Ÿè£…ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã™ã‚‹ã€‚VQAã€Captioningã€Zero-shotåˆ†é¡ã€Retrievalã®4ã¤ã®ã‚¿ã‚¹ã‚¯ã§æ€§èƒ½ã‚’æ¸¬å®šã™ã‚‹ã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” è©•ä¾¡å®Ÿè£…

å®Ÿè£…ã—ãŸCLIPã¨SmolVLM2ã®æ€§èƒ½ã‚’ã€**4ã¤ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯**ã§è©•ä¾¡ã™ã‚‹ã€‚

### 5.1 VQA (Visual Question Answering) è©•ä¾¡

#### 5.1.1 VQAv2ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

VQAv2[^14]ã¯ã€Visual Question Answeringã®æ¨™æº–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€‚

**æ§‹æˆ**:
- è¨“ç·´: 214Kè³ªå•
- æ¤œè¨¼: 104Kè³ªå•
- å„è³ªå•ã«10å€‹ã®äººé–“ã«ã‚ˆã‚‹ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å›ç­”

**è©•ä¾¡æŒ‡æ¨™**: Accuracy

$$
\text{Accuracy} = \frac{1}{N} \sum_{i=1}^N \min\left(1, \frac{\text{num\_annotators\_agree}(a_i)}{3}\right)
$$

ã“ã“ã§ $a_i$ ã¯ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬å›ç­”ã€‚3äººä»¥ä¸Šã®ã‚¢ãƒãƒ†ãƒ¼ã‚¿ãŒåŒæ„ã™ã‚Œã°ã€ã‚¹ã‚³ã‚¢ã¯1ã€‚

#### 5.1.2 VQAè©•ä¾¡å®Ÿè£…ï¼ˆJuliaï¼‰

```julia
using JSON3, Images

# VQAv2ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
struct VQADataset
    images::Vector{String}  # ç”»åƒãƒ‘ã‚¹
    questions::Vector{String}
    answers::Vector{Vector{String}}  # å„è³ªå•ã«10å€‹ã®å›ç­”
end

function load_vqav2(json_path::String)
    data = JSON3.read(read(json_path, String))
    images = [q["image_id"] for q in data["questions"]]
    questions = [q["question"] for q in data["questions"]]
    answers = [a["answers"] for a in data["annotations"]]
    return VQADataset(images, questions, answers)
end

# VQA Accuracyè¨ˆç®—
function vqa_accuracy(predictions, ground_truths)
    total = 0.0
    for (pred, gts) in zip(predictions, ground_truths)
        # å„ground truthã¨ã®ä¸€è‡´æ•°
        matches = sum([lowercase(pred) == lowercase(gt) for gt in gts])
        score = min(1.0, matches / 3)
        total += score
    end
    return total / length(predictions)
end

# SmolVLM2ã§VQAè©•ä¾¡
function evaluate_vqa(smolvlm2, dataset::VQADataset)
    predictions = String[]
    for (img_path, question) in zip(dataset.images, dataset.questions)
        input = MultimodalInput(load(img_path), question)
        answer = smolvlm2.infer(input)
        push!(predictions, answer)
    end

    acc = vqa_accuracy(predictions, dataset.answers)
    println("VQAv2 Accuracy: $(acc * 100)%")
    return acc
end
```

#### 5.1.3 VQAè©•ä¾¡çµæœï¼ˆä¾‹ï¼‰

```julia
# æ“¬ä¼¼è©•ä¾¡çµæœ
vqa_dataset = load_vqav2("vqav2_val.json")
smolvlm2 = load_smolvlm2("models/smolvlm2-256m.pth")
acc = evaluate_vqa(smolvlm2, vqa_dataset)
```

**å‡ºåŠ›ä¾‹**:
```
VQAv2 Accuracy: 68.3%
```

SmolVLM2-256Mã¯ã€ã‚ãšã‹256Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§68.3%ã®ç²¾åº¦ã‚’é”æˆã€‚ã“ã‚Œã¯ã€Idefics-80Bï¼ˆ17ãƒ¶æœˆå‰ã®ãƒ¢ãƒ‡ãƒ«ï¼‰ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã ã€‚

#### 5.1.4 VQAå¤±æ•—ä¾‹ã®åˆ†æ

VQAãƒ¢ãƒ‡ãƒ«ã®**å¼±ç‚¹**ã‚’ç†è§£ã™ã‚‹ãŸã‚ã€å¤±æ•—ä¾‹ã‚’è¦‹ã¦ã¿ã‚ˆã†ã€‚

**ä¾‹1: æ•°å€¤ã‚«ã‚¦ãƒ³ãƒ†ã‚£ãƒ³ã‚°**

```julia
# è³ªå•: "How many cats are in the image?"
# æ­£è§£: "3"
# SmolVLM2äºˆæ¸¬: "several"
```

**åŸå› **: å°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã¯**æ­£ç¢ºãªã‚«ã‚¦ãƒ³ãƒ†ã‚£ãƒ³ã‚°**ãŒè‹¦æ‰‹ã€‚ã€Œseveralã€ã€Œmanyã€ã®ã‚ˆã†ãª**æ›–æ˜§ãªè¡¨ç¾**ã«é€ƒã’ã‚‹ã€‚

**è§£æ±ºç­–**: ã‚«ã‚¦ãƒ³ãƒ†ã‚£ãƒ³ã‚°å°‚ç”¨ã®ãƒ˜ãƒƒãƒ‰ã‚’è¿½åŠ ã™ã‚‹ã‹ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚«ã‚¦ãƒ³ãƒ†ã‚£ãƒ³ã‚°ã‚µãƒ³ãƒ—ãƒ«ã§å¼·åŒ–ã™ã‚‹ã€‚

**ä¾‹2: ç´°ã‹ã„ãƒ†ã‚­ã‚¹ãƒˆèª­ã¿å–ã‚Š**

```julia
# è³ªå•: "What does the sign say?"
# æ­£è§£: "Stop"
# SmolVLM2äºˆæ¸¬: "traffic sign"
```

**åŸå› **: ç”»åƒè§£åƒåº¦ï¼ˆ224Ã—224ï¼‰ãŒä½ã™ãã¦ã€ç´°ã‹ã„ãƒ†ã‚­ã‚¹ãƒˆãŒèª­ã‚ãªã„ã€‚

**è§£æ±ºç­–**: Qwen-VLã®ã‚ˆã†ã«**Dynamic Resolution**ã‚’å°å…¥ã—ã€é«˜è§£åƒåº¦å…¥åŠ›ã‚’è¨±å¯ã™ã‚‹ã€‚

**ä¾‹3: æ¨è«–ãŒå¿…è¦ãªè³ªå•**

```julia
# è³ªå•: "Is it likely to rain soon?"
# ç”»åƒ: æ›‡ã‚Šç©º
# æ­£è§£: "yes"
# SmolVLM2äºˆæ¸¬: "cloudy"
```

**åŸå› **: è³ªå•ã¯ã€Œé›¨ãŒé™ã‚‹ã‹ã€ã‚’èã„ã¦ã„ã‚‹ãŒã€ãƒ¢ãƒ‡ãƒ«ã¯ã€Œæ›‡ã£ã¦ã„ã‚‹ã€ã¨ã„ã†**è¦³å¯Ÿäº‹å®Ÿ**ã ã‘ã‚’ç­”ãˆã‚‹ã€‚**æ¨è«–èƒ½åŠ›**ãŒä¸è¶³ã€‚

**è§£æ±ºç­–**: Chain-of-Thought (CoT) ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ã‚’å°å…¥ã—ã€ã€Œæ›‡ã£ã¦ã„ã‚‹ â†’ é›¨ãŒé™ã‚Šãã†ã€ã¨ã„ã†æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—ã‚’æ˜ç¤ºçš„ã«å­¦ç¿’ã•ã›ã‚‹ã€‚

---

### 5.2 Image Captioningè©•ä¾¡

#### 5.2.1 COCO Captionsãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

COCO Captions[^15]ã¯ã€Image Captioningã®æ¨™æº–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€‚

**æ§‹æˆ**:
- è¨“ç·´: 82Kç”»åƒã€å„ç”»åƒã«5ã¤ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³
- æ¤œè¨¼: 40Kç”»åƒ

**è©•ä¾¡æŒ‡æ¨™**: BLEUã€METEORã€CIDErã€SPICE

**è©•ä¾¡æŒ‡æ¨™ã®ç‰¹å¾´**:

| æŒ‡æ¨™ | æ¸¬å®šå†…å®¹ | ç‰¹å¾´ | ç¯„å›² |
|:-----|:---------|:-----|:-----|
| **BLEU-4** | n-gramä¸€è‡´ï¼ˆn=1,2,3,4ï¼‰ | æ©Ÿæ¢°ç¿»è¨³ã‹ã‚‰å€Ÿç”¨ã€‚ç°¡æ½”ãªã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’å¥½ã‚€ | 0-1 |
| **METEOR** | Unigramä¸€è‡´ + åŒç¾©èª + stemming | å˜èªã®æŸ”è»Ÿæ€§ã‚’è€ƒæ…® | 0-1 |
| **CIDEr** | TF-IDFé‡ã¿ä»˜ãn-gramé¡ä¼¼åº¦ | äººé–“ã®åˆ¤æ–­ã¨æœ€ã‚‚ç›¸é–¢ãŒé«˜ã„ | 0-10 |
| **SPICE** | Scene Graphä¸€è‡´ | æ„å‘³çš„æ­£ç¢ºæ€§ã‚’æ¸¬å®šï¼ˆç‰©ä½“ãƒ»å±æ€§ãƒ»é–¢ä¿‚ï¼‰ | 0-1 |
| **ROUGE-L** | æœ€é•·å…±é€šéƒ¨åˆ†åˆ— | æ–‡æ§‹é€ ã®é¡ä¼¼æ€§ | 0-1 |

#### 5.2.2 CIDErå®Ÿè£…ï¼ˆJuliaï¼‰

```julia
using StatsBase

# CIDEr: Consensus-based Image Description Evaluation
function cider_score(candidate::String, references::Vector{String})
    # n-gramã®TF-IDFé‡ã¿ã‚’è¨ˆç®—
    candidate_ngrams = extract_ngrams(candidate, n=4)
    ref_ngrams = [extract_ngrams(ref, n=4) for ref in references]

    # TF-IDFè¨ˆç®—
    candidate_tfidf = compute_tfidf(candidate_ngrams)
    ref_tfidfs = [compute_tfidf(ng) for ng in ref_ngrams]

    # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®å¹³å‡
    similarities = [cosine_similarity(candidate_tfidf, ref_tf) for ref_tf in ref_tfidfs]
    return mean(similarities)
end

function extract_ngrams(text::String, n::Int=4)
    tokens = split(lowercase(text))
    ngrams = Dict{String, Int}()
    for i in 1:(length(tokens) - n + 1)
        ng = join(tokens[i:i+n-1], " ")
        ngrams[ng] = get(ngrams, ng, 0) + 1
    end
    return ngrams
end

function compute_tfidf(ngrams::Dict{String, Int})
    # ç°¡æ˜“TF-IDFï¼ˆå®Ÿéš›ã¯ã‚³ãƒ¼ãƒ‘ã‚¹å…¨ä½“ã®IDFã‚’ä½¿ç”¨ï¼‰
    tf = ngrams
    idf = Dict(k => log(1.0 + 1.0 / v) for (k, v) in tf)
    return Dict(k => tf[k] * idf[k] for k in keys(tf))
end

function cosine_similarity(vec1::Dict, vec2::Dict)
    keys_union = union(keys(vec1), keys(vec2))
    dot_prod = sum([get(vec1, k, 0.0) * get(vec2, k, 0.0) for k in keys_union])
    norm1 = sqrt(sum([v^2 for v in values(vec1)]))
    norm2 = sqrt(sum([v^2 for v in values(vec2)]))
    return dot_prod / (norm1 * norm2 + 1e-8)
end
```

#### 5.2.3 SPICEå®Ÿè£…ï¼ˆå¤–éƒ¨ãƒ„ãƒ¼ãƒ«åˆ©ç”¨ï¼‰

SPICEã¯ã€**Scene Graphãƒ™ãƒ¼ã‚¹ã®è©•ä¾¡**ãªã®ã§ã€å¤–éƒ¨ãƒ„ãƒ¼ãƒ«ï¼ˆStanford Scene Graph Parserï¼‰ã‚’ä½¿ã†ã€‚

```julia
# SPICEè©•ä¾¡ï¼ˆPythonã‚¹ã‚¯ãƒªãƒ—ãƒˆçµŒç”±ï¼‰
function spice_score(candidate::String, references::Vector{String})
    # Pythonã®SPICEå®Ÿè£…ã‚’å‘¼ã³å‡ºã—
    result = read(`python spice.py --candidate "$candidate" --references $(join(references, "|"))`, String)
    return parse(Float64, result)
end
```

---

### 5.3 Zero-shotåˆ†é¡è©•ä¾¡

#### 5.3.1 ImageNetã§ã®è©•ä¾¡

CLIPã®Zero-shotåˆ†é¡ç²¾åº¦ã‚’ã€ImageNet-1kã§æ¸¬å®šã™ã‚‹ã€‚

```julia
using MLDatasets

# ImageNet-1kè©•ä¾¡
function evaluate_zero_shot_imagenet(clip, imagenet_val)
    # ImageNetã‚¯ãƒ©ã‚¹åï¼ˆ1000ã‚¯ãƒ©ã‚¹ï¼‰
    class_names = load_imagenet_class_names()

    correct = 0
    total = 0

    for (image, label) in imagenet_val
        # Zero-shotåˆ†é¡
        probs, pred = zero_shot_classify(clip, image, class_names)
        if pred == label
            correct += 1
        end
        total += 1
    end

    acc = correct / total
    println("ImageNet Zero-shot Accuracy: $(acc * 100)%")
    return acc
end
```

**CLIP-ViT-L/14ã®çµæœ** (è«–æ–‡å€¤)[^1]:
```
ImageNet Zero-shot Accuracy: 75.5%
```

---

### 5.4 Image-Text Retrievalè©•ä¾¡

#### 5.4.1 Recall@Kå®Ÿè£…

```julia
# Image-to-Text Retrieval
function image_to_text_retrieval(clip, images, texts, K=5)
    recall_at_k = 0

    for (i, img) in enumerate(images)
        # ç”»åƒåŸ‹ã‚è¾¼ã¿
        img_emb = clip.vision(unsqueeze(img, 4))[:, 1]

        # å…¨ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿
        text_embs = [clip.text(tokenize(t))[:, 1] for t in texts]

        # é¡ä¼¼åº¦è¨ˆç®—
        similarities = [dot(img_emb, t) / (norm(img_emb) * norm(t)) for t in text_embs]

        # Top-Kå–å¾—
        top_k_indices = sortperm(similarities, rev=true)[1:K]

        # æ­£è§£ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹
        if i in top_k_indices
            recall_at_k += 1
        end
    end

    return recall_at_k / length(images)
end
```

**COCO Captionsã§ã®çµæœ** (CLIPè«–æ–‡å€¤)[^1]:
```
Image-to-Text Recall@5: 88.0%
Text-to-Image Recall@5: 68.7%
```

---

### 5.5 Self-check Checklist

ä»¥ä¸‹ã®é …ç›®ã‚’ç¢ºèªã—ã¦ã€å®Ÿè£…ã¨è©•ä¾¡ãŒæ­£ã—ãè¡Œã‚ã‚ŒãŸã‹ãƒã‚§ãƒƒã‚¯ã—ã‚ˆã†ã€‚

- [ ] InfoNCE lossãŒæ­£ã—ãè¨ˆç®—ã•ã‚Œã¦ã„ã‚‹ï¼ˆå¯¾è§’æˆåˆ†ãŒæœ€å¤§ã«ãªã£ã¦ã„ã‚‹ã‹ï¼‰
- [ ] Vision Encoderã¨Text Encoderã®å‡ºåŠ›æ¬¡å…ƒãŒä¸€è‡´ã—ã¦ã„ã‚‹
- [ ] Zero-shotåˆ†é¡ã®ç²¾åº¦ãŒè«–æ–‡å€¤ã«è¿‘ã„ï¼ˆÂ±3%ä»¥å†…ï¼‰
- [ ] VQA Accuracyã®è¨ˆç®—å¼ãŒæ­£ã—ã„ï¼ˆ3äººä»¥ä¸Šã®åˆæ„ã§1ã‚¹ã‚³ã‚¢ï¼‰
- [ ] CIDErãŒn-gramã®TF-IDFã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—ã—ã¦ã„ã‚‹
- [ ] Image-Text Retrievalã§åŒæ–¹å‘ï¼ˆImageâ†’Text, Textâ†’Imageï¼‰ã‚’è©•ä¾¡ã—ã¦ã„ã‚‹
- [ ] Rustæ¨è«–ãŒJuliaã‹ã‚‰æ­£ã—ãå‘¼ã³å‡ºã›ã‚‹ï¼ˆFFIçµŒç”±ï¼‰

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®85%å®Œäº†ï¼** Zone 6ã§ã¯ã€æœ€æ–°ç ”ç©¶ã¨å…¨ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã‚’ä¿¯ç°ã™ã‚‹ã€‚
:::

---

## ğŸ“ 6. æŒ¯ã‚Šè¿”ã‚Šã¨ç™ºå±•ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨ç ”ç©¶landscape

ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ä¸–ç•Œã¯æ€¥é€Ÿã«é€²åŒ–ã—ã¦ã„ã‚‹ã€‚ã“ã“ã§ã¯ã€**7ã¤ã®ä¸»è¦ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**ã‚’ä¿¯ç°ã—ã€æœ€æ–°ç ”ç©¶ã‚’ç´¹ä»‹ã™ã‚‹ã€‚

### 6.1 Vision-Languageãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼ãƒ„ãƒªãƒ¼

```mermaid
graph TD
    Root[Vision-Language Models] --> LF[Late Fusion]
    Root --> DF[Deep Fusion]
    Root --> EF[Early Fusion]

    LF --> CLIP[CLIP 2021<br>Dual Encoder]
    LF --> ALIGN[ALIGN 2021<br>Noisy Data]
    LF --> SigLIP[SigLIP 2023<br>Sigmoid Loss]
    LF --> OpenCLIP[Open-CLIP 2023<br>LAION-5B]

    DF --> BLIP2[BLIP-2 2023<br>Q-Former]
    DF --> Flamingo[Flamingo 2022<br>Perceiver Resampler]
    DF --> LLaVA[LLaVA 2023<br>Visual Instruction]
    DF --> QwenVL[Qwen-VL 2024<br>Dynamic Resolution]
    DF --> CogVLM[CogVLM 2023<br>Visual Expert]
    DF --> SmolVLM[SmolVLM2 2024<br>256M Tiny]

    EF --> Chameleon[Chameleon 2024<br>Unified Tokens]
    EF --> Molmo[Molmo 2024<br>PixMo Dataset]

    style CLIP fill:#4CAF50
    style BLIP2 fill:#2196F3
    style SmolVLM fill:#FF9800
```

### 6.2 ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¯”è¼ƒè¡¨

| ãƒ¢ãƒ‡ãƒ« | å¹´ | Fusion | Vision Enc | Text Enc | ç‰¹å¾´ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | ä¸»è¦è«–æ–‡ |
|:-------|:---|:-------|:----------|:---------|:-----|:---------|:---------|
| **CLIP** | 2021 | Late | ViT/ResNet | Transformer | Contrastiveå­¦ç¿’ã€Zero-shot | 151M-428M | [^1] |
| **ALIGN** | 2021 | Late | EfficientNet | BERT | ãƒã‚¤ã‚ºãƒ‡ãƒ¼ã‚¿è€æ€§ | 1B | Google |
| **Flamingo** | 2022 | Deep | NFNet | Chinchilla | Perceiver Resamplerã€Few-shot | 80B | [^5] |
| **BLIP-2** | 2023 | Deep | ViT | OPT/FlanT5 | Q-Formerã€Frozen LLM | 2.7B-13B | [^4] |
| **LLaVA** | 2023 | Deep | CLIP ViT | Vicuna | Visual Instruction Tuning | 7B-13B | [^6] |
| **SigLIP** | 2023 | Late | ViT | Transformer | Sigmoid lossã€ãƒãƒƒãƒéä¾å­˜ | 149M-986M | [^12] |
| **Open-CLIP** | 2023 | Late | ViT | Transformer | LAION-5Bè¨“ç·´ã€OSS | 149M-986M | [^11] |
| **CogVLM** | 2023 | Deep | ViT | Vicuna | Visual Expertã€Deep Fusion | 17B | [^8] |
| **Qwen-VL** | 2024 | Deep | ViT | Qwen | Dynamic Resolutionã€RoPE 2D | 7B-72B | [^7] |
| **Molmo** | 2024 | Deep | ViT | OLMo | PixMo 1Mé«˜å“è³ªãƒ‡ãƒ¼ã‚¿ | 7B | [^13] |
| **SmolVLM2** | 2024 | Deep | ViT | SmolLM2 | æ¥µå°256Mã€3ãƒ¢ãƒ€ãƒªãƒ†ã‚£ | 256M-2.2B | [^9] |
| **Chameleon** | 2024 | Early | ViT | Unified | ç”»åƒ+ãƒ†ã‚­ã‚¹ãƒˆçµ±ä¸€Token | 7B-34B | Meta |

### 6.3 BLIP-2å®Œå…¨è§£å‰–

BLIP-2[^4]ã¯ã€**Q-Former**ã¨ã„ã†é©æ–°çš„ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å°å…¥ã—ãŸã€‚Frozen Vision Encoderã¨Frozen LLMã®é–“ã‚’æ©‹æ¸¡ã—ã™ã‚‹ã€**æƒ…å ±ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**ã®å½¹å‰²ã‚’æœãŸã™ã€‚

#### 6.3.1 Q-Formerã®è¨­è¨ˆåŸç†

**å‹•æ©Ÿ**: å¤§è¦æ¨¡ãªVision Encoderã¨LLMã‚’**ã‚¼ãƒ­ã‹ã‚‰è¨“ç·´**ã™ã‚‹ã®ã¯è¨ˆç®—ã‚³ã‚¹ãƒˆãŒè†¨å¤§ã€‚æ—¢å­˜ã®äº‹å‰è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã—ãŸã„ã€‚

**èª²é¡Œ**:
1. Vision Encoderã®å‡ºåŠ›ï¼ˆ196 tokensãªã©ï¼‰ã¯**é•·ã™ãã‚‹** â†’ LLMã®å…¥åŠ›ã¨ã—ã¦éåŠ¹ç‡
2. Vision Encoderã¨LLMã¯**ç‹¬ç«‹ã«è¨“ç·´**ã•ã‚Œã¦ã„ã‚‹ â†’ åŸ‹ã‚è¾¼ã¿ç©ºé–“ãŒç•°ãªã‚‹
3. LLMã‚’**Fine-tuning**ã™ã‚‹ã¨ã€å…ƒã®è¨€èªèƒ½åŠ›ãŒåŠ£åŒ–ã™ã‚‹ï¼ˆCatastrophic Forgettingï¼‰

**è§£æ±ºç­–: Q-Former**

Q-Formerã¯ã€**å­¦ç¿’å¯èƒ½ãªã‚¯ã‚¨ãƒª**ã‚’ä½¿ã£ã¦ã€ç”»åƒç‰¹å¾´ã‚’**å›ºå®šé•·**ï¼ˆ32 tokensï¼‰ã«åœ§ç¸®ã™ã‚‹ã€‚

```mermaid
graph TD
    ImgEnc[Frozen<br>Vision Encoder] --> ImgFeats[ç”»åƒç‰¹å¾´<br>N tokens]
    Queries[Learnable Queries<br>32 tokens] --> QFormer[Q-Former<br>Cross-Attention]
    ImgFeats --> QFormer
    QFormer --> VFeats[è¦–è¦šç‰¹å¾´<br>32 tokens]
    VFeats --> LLM[Frozen LLM]
    TextPrompt[ãƒ†ã‚­ã‚¹ãƒˆPrompt] --> LLM
    LLM --> Output[ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆ]
```

**Q-Formerã®å½¹å‰²**:
1. **æƒ…å ±ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**: å¯å¤‰é•·ã®ç”»åƒç‰¹å¾´ï¼ˆ196 tokensï¼‰ã‚’å›ºå®šé•·ï¼ˆ32 tokensï¼‰ã«åœ§ç¸®ã€‚
2. **Vision-Language Bridge**: Frozen Vision Encoderã¨Frozen LLMã®é–“ã‚’æ©‹æ¸¡ã—ã€‚
3. **Cross-Attention**: QueryãŒç”»åƒç‰¹å¾´ã«Cross-Attentionã—ã¦ã€é‡è¦ãªè¦–è¦šæƒ…å ±ã‚’æŠ½å‡ºã€‚

**æ•°å¼**:

$$
\mathbf{Q} = \text{LearnableQueries} \in \mathbb{R}^{d \times 32}
$$

$$
\mathbf{K} = W_K \mathbf{Z}^v, \quad \mathbf{V} = W_V \mathbf{Z}^v \quad (\mathbf{Z}^v \in \mathbb{R}^{d \times 196})
$$

$$
\mathbf{Z}_{\text{visual}} = \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) \in \mathbb{R}^{d \times 32}
$$

#### 6.3.2 Two-stage Pre-training

**Stage 1: Vision-Language Representation Learning**

3ã¤ã®æå¤±ã‚’åŒæ™‚æœ€é©åŒ–:

1. **ITC (Image-Text Contrastive)**: CLIPã¨åŒã˜InfoNCE loss
2. **ITG (Image-grounded Text Generation)**: ç”»åƒã‚’æ¡ä»¶ã¨ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
3. **ITM (Image-Text Matching)**: ç”»åƒ-ãƒ†ã‚­ã‚¹ãƒˆãƒšã‚¢ã®ãƒãƒƒãƒãƒ³ã‚°ï¼ˆBinaryåˆ†é¡ï¼‰

**Stage 2: Vision-to-Language Generative Learning**

Q-Formerã‚’Frozen LLMã«æ¥ç¶šã—ã€**Language Modeling Loss**ã§è¨“ç·´:

$$
\mathcal{L}_{\text{LM}} = -\sum_{t=1}^T \log p(w_t \mid w_{<t}, \mathbf{Z}_{\text{visual}})
$$

### 6.4 LLaVA: Visual Instruction Tuning

LLaVA[^6]ã¯ã€**Visual Instruction Tuning**ã‚’å°å…¥ã—ãŸã€‚

**ã‚³ãƒ³ã‚»ãƒ—ãƒˆ**: GPT-4ã«Image Captionã‚’è¦‹ã›ã¦ã€**Instruction-Following ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ**ã•ã›ã‚‹ã€‚

**ä¾‹**:
```
ç”»åƒ: [çŒ«ãŒã‚½ãƒ•ã‚¡ã§å¯ã¦ã„ã‚‹å†™çœŸ]
Instruction: "ã“ã®ç”»åƒã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
GPT-4ç”Ÿæˆå›ç­”: "ã“ã®ç”»åƒã«ã¯ã€ã‚°ãƒ¬ãƒ¼ã®çŒ«ãŒé’ã„ã‚½ãƒ•ã‚¡ã®ä¸Šã§ä¸¸ã¾ã£ã¦å¯ã¦ã„ã‚‹æ§˜å­ãŒæ˜ ã£ã¦ã„ã¾ã™ã€‚..."
```

ã“ã®ãƒ‡ãƒ¼ã‚¿ã§LLaVAã‚’è¨“ç·´ã™ã‚‹ã¨ã€**GPT-4ã®85.1%ã®æ€§èƒ½**ã‚’é”æˆï¼ˆåˆæˆãƒ‡ãƒ¼ã‚¿ã§ã®æ¯”è¼ƒï¼‰ã€‚

#### 6.4.1 LLaVAã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©³ç´°

```mermaid
graph LR
    Img[ç”»åƒ] --> CLIP[CLIP ViT-L/14<br>Frozen]
    CLIP --> ImgFeats[ç”»åƒç‰¹å¾´<br>256 tokens]
    ImgFeats --> Proj[Projection MLP<br>Trainable]
    Proj --> VisTokens[è¦–è¦šãƒˆãƒ¼ã‚¯ãƒ³<br>256â†’32 tokens]
    TextPrompt[ãƒ†ã‚­ã‚¹ãƒˆPrompt] --> Concat[Concatenate]
    VisTokens --> Concat
    Concat --> Vicuna[Vicuna-7B<br>Frozen or LoRA]
    Vicuna --> Output[ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆ]
```

**Projection MLPã®å½¹å‰²**:

LLaVAã¯ã€CLIP ViTã®å‡ºåŠ›ï¼ˆ256 tokensï¼‰ã‚’**å˜ç´”ãªMLP**ã§32 tokensã«åœ§ç¸®ã™ã‚‹ã€‚BLIP-2ã®Q-Formerã»ã©è¤‡é›‘ã§ã¯ãªã„ãŒã€**è¨“ç·´ãŒç°¡å˜**ã§åŠ¹æœçš„ã€‚

**æ•°å¼**:

$$
\mathbf{Z}_{\text{visual}} = \text{MLP}(\mathbf{Z}_{\text{CLIP}}) \in \mathbb{R}^{d \times 32}
$$

$$
\mathbf{Z}_{\text{input}} = [\mathbf{Z}_{\text{visual}}, \mathbf{Z}_{\text{text}}] \in \mathbb{R}^{d \times (32 + L)}
$$

#### 6.4.2 LLaVAã®è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆ2æ®µéšï¼‰

**Stage 1: Pre-training (Feature Alignment)**

- ãƒ‡ãƒ¼ã‚¿: CC3Mï¼ˆ3M image-caption pairsï¼‰
- ç›®æ¨™: è¦–è¦šãƒˆãƒ¼ã‚¯ãƒ³ã¨ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã®**åŸ‹ã‚è¾¼ã¿ç©ºé–“ã‚’æƒãˆã‚‹**
- è¨“ç·´å¯¾è±¡: Projection MLPã®ã¿ï¼ˆCLIP ViT + Vicunaã¯å‡çµï¼‰
- æå¤±: Language Modeling Loss

$$
\mathcal{L}_{\text{LM}} = -\sum_{t=1}^T \log p(w_t \mid w_{<t}, \mathbf{Z}_{\text{visual}})
$$

**Stage 2: Fine-tuning (Instruction Tuning)**

- ãƒ‡ãƒ¼ã‚¿: LLaVA-Instruct-150Kï¼ˆGPT-4ç”Ÿæˆï¼‰
- ç›®æ¨™: Instruction-Followingã‚’å­¦ç¿’
- è¨“ç·´å¯¾è±¡: Projection MLP + Vicunaï¼ˆLoRAï¼‰
- æå¤±: åŒã˜Language Modeling Loss

**LLaVA-1.5ã®æ”¹å–„ç‚¹**:
1. **é«˜è§£åƒåº¦å¯¾å¿œ**: 336Ã—336 å…¥åŠ›ï¼ˆå…ƒã¯224Ã—224ï¼‰
2. **ShareGPT4Vè¨“ç·´ãƒ‡ãƒ¼ã‚¿**: ã‚ˆã‚Šå¤šæ§˜ã§é«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
3. **Multi-turnå¯¾è©±**: è¤‡æ•°ãƒ©ã‚¦ãƒ³ãƒ‰ã®å¯¾è©±ã‚’å­¦ç¿’

#### 6.4.3 LLaVAã®Productionå®Ÿè£…ï¼ˆJuliaï¼‰

```julia
using Transformers, Flux

struct LLaVA
    clip_vit::VisionTransformer  # Frozen
    projection::Chain  # Trainable MLP
    llm::Vicuna  # Frozen or LoRA
end

function LLaVA()
    clip_vit = load_pretrained("openai/clip-vit-large-patch14")
    projection = Chain(
        Dense(1024, 4096, gelu),
        Dense(4096, 4096)
    )
    llm = load_pretrained("lmsys/vicuna-7b-v1.5")
    return LLaVA(clip_vit, projection, llm)
end

function (llava::LLaVA)(image, text_prompt)
    # ç”»åƒç‰¹å¾´æŠ½å‡ºï¼ˆFrozenï¼‰
    img_feats = llava.clip_vit(image)  # (1024, 256, B)

    # Projection
    vis_tokens = llava.projection(img_feats)  # (4096, 32, B)

    # ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    text_tokens = tokenize(text_prompt)  # (4096, L, B)

    # Concatenate
    input_tokens = cat(vis_tokens, text_tokens, dims=2)  # (4096, 32+L, B)

    # LLMæ¨è«–
    output = llava.llm(input_tokens)

    return output
end

# è¨“ç·´ï¼ˆStage 2: Instruction Tuningï¼‰
function train_llava_stage2(llava, instruct_data, epochs=3)
    # LoRAã‚’é©ç”¨
    apply_lora!(llava.llm, rank=8)

    opt = Adam(1e-4)
    ps = Flux.params(llava.projection, llava.llm)  # CLIP ViTã¯é™¤å¤–

    for epoch in 1:epochs
        for (image, prompt, answer) in instruct_data
            loss, back = Flux.pullback(ps) do
                output = llava(image, prompt)
                # Language Modeling Loss
                return Flux.logitcrossentropy(output, answer)
            end

            grads = back(1.0f0)
            Flux.update!(opt, ps, grads)
        end
    end
end
```

### 6.5 Qwen-VL: Dynamic Resolution

Qwen-VL[^7]ã¯ã€**Dynamic Resolution**ã‚’å°å…¥ã€‚

**å•é¡Œ**: å¾“æ¥ã®ViTã¯å›ºå®šè§£åƒåº¦ï¼ˆ224Ã—224ï¼‰ã«åˆ¶é™ã•ã‚Œã‚‹ãŸã‚ã€é«˜è§£åƒåº¦ç”»åƒã®è©³ç´°ãŒå¤±ã‚ã‚Œã‚‹ã€‚

**è§£æ±ºç­–**: å…¥åŠ›ç”»åƒã‚’**å¯å¤‰ã‚µã‚¤ã‚ºã®ãƒ‘ãƒƒãƒ**ã«åˆ†å‰²ã—ã€**2D RoPE** (Rotary Position Embedding) ã§ä½ç½®ã‚’è¡¨ç¾ã€‚

#### 6.5.1 2D RoPEã®æ•°å­¦çš„åŸºç¤

**1D RoPEï¼ˆå¾©ç¿’ï¼‰**: ç¬¬16å›ã§å­¦ã‚“ã Rotary Position Embeddingã¯ã€1æ¬¡å…ƒã®ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®ã‚’å›è»¢è¡Œåˆ—ã§è¡¨ç¾ã—ãŸ:

$$
\mathbf{q}_m = \begin{bmatrix} \cos(m\theta) & -\sin(m\theta) \\ \sin(m\theta) & \cos(m\theta) \end{bmatrix} \begin{bmatrix} q_0 \\ q_1 \end{bmatrix}
$$

**2D RoPEï¼ˆQwen-VLï¼‰**: ç”»åƒãƒ‘ãƒƒãƒã¯2æ¬¡å…ƒã®ä½ç½® $(x, y)$ ã‚’æŒã¤ãŸã‚ã€**2ã¤ã®ç‹¬ç«‹ãªå›è»¢**ã‚’é©ç”¨:

$$
\mathbf{e}_{\text{pos}}(x, y) = [\underbrace{\cos(x\theta_1), \sin(x\theta_1)}_{\text{xæ–¹å‘}}, \underbrace{\cos(y\theta_2), \sin(y\theta_2)}_{\text{yæ–¹å‘}}, \ldots]
$$

ã“ã“ã§ $\theta_i = 10000^{-2i/d}$ ã¯RoPEã®åŸºæœ¬å‘¨æ³¢æ•°ã€‚

**Attentionã¸ã®é©ç”¨**:

$$
\mathbf{A}_{ij} = \frac{(\mathbf{q}_i \odot \mathbf{e}_{\text{pos}}(x_i, y_i))^\top (\mathbf{k}_j \odot \mathbf{e}_{\text{pos}}(x_j, y_j))}{\sqrt{d_k}}
$$

$\odot$ ã¯è¦ç´ ã”ã¨ã®ç©ï¼ˆHadamardç©ï¼‰ã€‚

**åˆ©ç‚¹**:
1. **ä»»æ„ã®è§£åƒåº¦ã«å¯¾å¿œ**: è¨“ç·´æ™‚ã«è¦‹ã¦ã„ãªã„è§£åƒåº¦ã§ã‚‚æ¨è«–å¯èƒ½ã€‚
2. **ç›¸å¯¾ä½ç½®ã®å­¦ç¿’**: $(x_i - x_j, y_i - y_j)$ ã®ç›¸å¯¾ä½ç½®ãŒè‡ªå‹•ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚Œã‚‹ã€‚
3. **å¤–æŒ¿æ€§**: è¨“ç·´æ™‚ã‚ˆã‚Šã‚‚å¤§ããªè§£åƒåº¦ã§ã‚‚æ€§èƒ½åŠ£åŒ–ãŒå°‘ãªã„ã€‚

#### 6.5.2 Qwen2-VLã®æ”¹å–„ç‚¹ï¼ˆNaive Deduplicationï¼‰

**å•é¡Œ**: Webã‹ã‚‰åé›†ã—ãŸè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«ã¯**é‡è¤‡ç”»åƒ**ãŒå¤šã„ï¼ˆåŒã˜ç”»åƒãŒè¤‡æ•°ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã§ç™»å ´ï¼‰ã€‚

**è§£æ±ºç­–: Naive Deduplication**

1. **ç”»åƒãƒãƒƒã‚·ãƒ¥**: å„ç”»åƒã®perceptual hashï¼ˆpHashï¼‰ã‚’è¨ˆç®—
2. **é‡è¤‡æ¤œå‡º**: ãƒãƒƒã‚·ãƒ¥ãŒé¡ä¼¼ã—ã¦ã„ã‚‹ç”»åƒï¼ˆHammingè·é›¢ < 5ï¼‰ã‚’é‡è¤‡ã¨ã¿ãªã™
3. **ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³çµ±åˆ**: é‡è¤‡ç”»åƒã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’å…¨ã¦çµ±åˆã—ã€æœ€ã‚‚è©³ç´°ãªã‚‚ã®ã‚’æ®‹ã™

**åŠ¹æœ**:
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º: 500M â†’ 350Mï¼ˆ30%å‰Šæ¸›ï¼‰
- è¨“ç·´æ™‚é–“: 20%çŸ­ç¸®
- æ€§èƒ½: VQAv2 75.3% â†’ 77.8%ï¼ˆé‡è¤‡é™¤å»ã§ç²¾åº¦å‘ä¸Šï¼‰

#### 6.5.3 Qwen-VLã®å®Ÿè£…ï¼ˆJuliaï¼‰

```julia
# 2D RoPEã®å®Ÿè£…
function rope_2d(x::Int, y::Int, d::Int)
    Î¸ = [10000.0^(-2i/d) for i in 0:dÃ·4-1]

    # xæ–¹å‘ã®å›è»¢
    x_emb = vcat([cos(x*Î¸[i]) for i in 1:length(Î¸)],
                 [sin(x*Î¸[i]) for i in 1:length(Î¸)])

    # yæ–¹å‘ã®å›è»¢
    y_emb = vcat([cos(y*Î¸[i]) for i in 1:length(Î¸)],
                 [sin(y*Î¸[i]) for i in 1:length(Î¸)])

    return vcat(x_emb, y_emb)  # (d,)
end

# Dynamic Resolutionå¯¾å¿œã®Patch Embedding
function dynamic_patch_embed(img::Array{Float32, 3}, patch_size::Int=14)
    H, W, C = size(img)

    # ç”»åƒã‚’å¯å¤‰æ•°ã®ãƒ‘ãƒƒãƒã«åˆ†å‰²
    num_patches_h = H Ã· patch_size
    num_patches_w = W Ã· patch_size

    patches = []
    positions = []

    for i in 1:num_patches_h, j in 1:num_patches_w
        # ãƒ‘ãƒƒãƒåˆ‡ã‚Šå‡ºã—
        patch = img[(i-1)*patch_size+1:i*patch_size,
                    (j-1)*patch_size+1:j*patch_size, :]
        push!(patches, vec(patch))
        push!(positions, (i, j))
    end

    return hcat(patches...), positions  # (PÂ²C, N), [(1,1), (1,2), ...]
end

# Attentionã«2D RoPEã‚’é©ç”¨
function attention_with_2d_rope(Q, K, V, positions, d_k)
    N = size(Q, 2)

    # å„ãƒˆãƒ¼ã‚¯ãƒ³ã«2D RoPEã‚’é©ç”¨
    Q_rope = copy(Q)
    K_rope = copy(K)
    for (i, (x, y)) in enumerate(positions)
        rope_emb = rope_2d(x, y, size(Q, 1))
        Q_rope[:, i] .= Q[:, i] .* rope_emb
        K_rope[:, i] .= K[:, i] .* rope_emb
    end

    # Attentionè¨ˆç®—
    scores = Q_rope' * K_rope ./ sqrt(d_k)
    attn = softmax(scores, dims=2)

    output = V * attn'
    return output
end
```

#### 6.5.4 Dynamic Resolutionã®åŠ¹æœï¼ˆå®Ÿé¨“çµæœï¼‰

| è§£åƒåº¦ | å¾“æ¥ViT (å›ºå®š224Ã—224) | Qwen-VL (Dynamic) | æ”¹å–„ç‡ |
|:-------|:---------------------|:------------------|:------|
| 224Ã—224 | 72.3% | 72.5% | +0.2% |
| 336Ã—336 | 70.1% | 75.8% | **+5.7%** |
| 448Ã—448 | 65.4% | 78.2% | **+12.8%** |
| 672Ã—672 | 58.9% | 79.6% | **+20.7%** |

**è¦³å¯Ÿ**:
- å¾“æ¥ViTã¯ã€è¨“ç·´è§£åƒåº¦ï¼ˆ224Ã—224ï¼‰ã‹ã‚‰é›¢ã‚Œã‚‹ã¨æ€§èƒ½ãŒæ€¥æ¿€ã«ä½ä¸‹ã€‚
- Qwen-VLã¯ã€é«˜è§£åƒåº¦ã«ãªã‚‹ã»ã©æ€§èƒ½ãŒ**å‘ä¸Š**ï¼ˆç´°ã‹ã„è©³ç´°ã‚’æ‰ãˆã‚‰ã‚Œã‚‹ï¼‰ã€‚

### 6.6 CogVLM: Visual Expert

CogVLM[^8]ã¯ã€**Visual Expert**ã‚’å„Transformerãƒ¬ã‚¤ãƒ¤ãƒ¼ã«æŒ¿å…¥ã€‚

**é€šå¸¸ã®Transformer**:

$$
\mathbf{h}' = \mathbf{h} + \text{Attention}(\mathbf{h}) + \text{FFN}(\mathbf{h})
$$

**CogVLMã®Visual Expert**:

$$
\mathbf{h}' = \mathbf{h} + \alpha \cdot \text{Attention}_{\text{vis}}(\mathbf{h}, \mathbf{Z}^v) + \beta \cdot \text{FFN}_{\text{vis}}(\mathbf{h})
$$

$\alpha, \beta$ ã¯å­¦ç¿’å¯èƒ½ãªã‚²ãƒ¼ãƒˆã€‚é€šå¸¸ã®FFNã¨Visual FFNã‚’**ä¸¦åˆ—**ã«å®Ÿè¡Œã—ã€é‡ã¿ä»˜ãå’Œã‚’å–ã‚‹ã€‚

**åˆ©ç‚¹**: Frozen LMã®æ€§èƒ½ã‚’ä¿ã¡ã¤ã¤ã€è¦–è¦šæƒ…å ±ã‚’æ·±ãçµ±åˆã€‚

### 6.7 SmolVLM2: æ¥µå°256Mãƒ¢ãƒ‡ãƒ«

SmolVLM2[^9]ã¯ã€**256Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**ã§3ãƒ¢ãƒ€ãƒªãƒ†ã‚£ï¼ˆç”»åƒãƒ»ãƒ†ã‚­ã‚¹ãƒˆãƒ»ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªï¼‰ã‚’å®Ÿç¾ã€‚

**åŠ¹ç‡åŒ–æŠ€è¡“**:
1. **Distillation**: å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆIdefics2-8Bï¼‰ã‹ã‚‰çŸ¥è­˜ã‚’è’¸ç•™ã€‚
2. **Connectoråœ§ç¸®**: Vision Encoderã®å‡ºåŠ›ã‚’**16 tokens**ã«åœ§ç¸®ï¼ˆé€šå¸¸ã¯32-64 tokensï¼‰ã€‚
3. **Small LM**: SmolLM2-135Mï¼ˆGPT-2ã‚µã‚¤ã‚ºï¼‰ã‚’ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã«ä½¿ç”¨ã€‚

**æ€§èƒ½**: Idefics-80Bï¼ˆ17ãƒ¶æœˆå‰ï¼‰ã‚’ä¸Šå›ã‚‹ã€‚

### 6.8 æœ€æ–°ç ”ç©¶ãƒˆãƒ”ãƒƒã‚¯ï¼ˆ2024-2026ï¼‰

#### 6.8.1 Molmo & PixMo

Molmo[^13]ã¯ã€Allen AIã«ã‚ˆã‚‹**å®Œå…¨ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹VLM**ã€‚

**PixMo Dataset**:
- **PixMo-Cap**: 1Mé«˜å“è³ªã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ï¼ˆéŸ³å£°å…¥åŠ›ã§äººé–“ãŒè¨˜è¿°ï¼‰
- **PixMo-Points**: 2D Pointing annotations â€” éè¨€èªçš„ãªã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°

**é©æ–°**: ãƒ¢ãƒ‡ãƒ«ãŒ**ç”»åƒä¸Šã®åº§æ¨™ã‚’å‡ºåŠ›**ã§ãã‚‹ã€‚ã€ŒçŒ«ã¯ã©ã“ï¼Ÿã€â†’ `(342, 189)` ã®ã‚ˆã†ã«å›ç­”ã€‚

#### 6.8.2 EVA-CLIP

EVA-CLIPï¼ˆ2023ï¼‰ã¯ã€**5B Vision Encoder**ã‚’ä½¿ç”¨ã€‚

**è¨“ç·´æˆ¦ç•¥**:
1. **MIM (Masked Image Modeling)** ã§Vision Encoderã‚’äº‹å‰è¨“ç·´
2. CLIPã®Contrastiveå­¦ç¿’ã§Fine-tuning

**çµæœ**: ImageNet Zero-shot 80.4%ï¼ˆCLIP-ViT-L/14ã¯75.5%ï¼‰ã€‚

### 6.9 æ¨å¥¨æ›¸ç±ãƒ»ãƒªã‚½ãƒ¼ã‚¹

| æ›¸ç±ãƒ»ãƒªã‚½ãƒ¼ã‚¹ | è‘—è€…/æ©Ÿé–¢ | å†…å®¹ | URL |
|:-------------|:---------|:-----|:----|
| **CLIPè«–æ–‡** | Radford et al., OpenAI | CLIPã®åŸè«–æ–‡ | [arXiv:2103.00020](https://arxiv.org/abs/2103.00020) |
| **BLIP-2è«–æ–‡** | Li et al., Salesforce | Q-Formerã®è©³ç´° | [arXiv:2301.12597](https://arxiv.org/abs/2301.12597) |
| **Flamingoè«–æ–‡** | Alayrac et al., DeepMind | Perceiver Resampler | [arXiv:2204.14198](https://arxiv.org/abs/2204.14198) |
| **HuggingFace Transformers** | HuggingFace | VLMå®Ÿè£…é›† | [github.com/huggingface/transformers](https://github.com/huggingface/transformers) |
| **Open-CLIP** | LAION | CLIPã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹å®Ÿè£… | [github.com/mlfoundations/open_clip](https://github.com/mlfoundations/open_clip) |

:::details ç”¨èªé›†

| ç”¨èª | æ„å‘³ |
|:-----|:-----|
| **Dual Encoder** | ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ¥ã€…ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã§å‡¦ç†ã™ã‚‹æ§‹é€  |
| **Contrastive Learning** | æ­£ä¾‹ãƒšã‚¢ã®é¡ä¼¼åº¦ã‚’æœ€å¤§åŒ–ã€è² ä¾‹ãƒšã‚¢ã®é¡ä¼¼åº¦ã‚’æœ€å°åŒ–ã™ã‚‹å­¦ç¿’ |
| **InfoNCE Loss** | Noise Contrastive Estimationã«åŸºã¥ãå¯¾æ¯”æå¤± |
| **Q-Former** | BLIP-2ã®Query-based Transformerã€‚ç”»åƒç‰¹å¾´ã‚’å›ºå®šé•·ã«åœ§ç¸® |
| **Perceiver Resampler** | Flamingoã®å¯å¤‰é•·â†’å›ºå®šé•·å¤‰æ›ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« |
| **Visual Expert** | CogVLMã®è¦–è¦šå°‚ç”¨FFN |
| **Dynamic Resolution** | Qwen-VLã®å¯å¤‰è§£åƒåº¦å¯¾å¿œ |
| **Visual Instruction Tuning** | LLaVAã®Instruction-Followingè¨“ç·´æ‰‹æ³• |
| **Frozen LLM** | é‡ã¿ã‚’å›ºå®šã—ãŸLarge Language Model |
| **Modality Gap** | ç•°ãªã‚‹ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®åŸ‹ã‚è¾¼ã¿åˆ†å¸ƒã®ã‚®ãƒ£ãƒƒãƒ— |
| **Hard Negative** | é¡ä¼¼åº¦ãŒé«˜ã„è² ä¾‹ï¼ˆè­˜åˆ¥ãŒé›£ã—ã„ï¼‰ |
| **Zero-shotåˆ†é¡** | è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãªã—ã§ã®åˆ†é¡ |
| **VQA** | Visual Question Answering |
| **CIDEr** | Consensus-based Image Description Evaluation |
| **SPICE** | Semantic Propositional Image Caption Evaluation |
:::

### 6.10 çŸ¥è­˜ãƒãƒƒãƒ—ï¼ˆmermaidï¼‰

```mermaid
graph TD
    A[Vision-Language Models] --> B[ç†è«–åŸºç¤]
    A --> C[ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£]
    A --> D[è©•ä¾¡]

    B --> B1[Contrastive Learning]
    B --> B2[Cross-Modal Attention]
    B --> B3[Modality Gap]
    B1 --> B11[InfoNCE Loss]
    B1 --> B12[Temperature Scaling]

    C --> C1[Late Fusion]
    C --> C2[Deep Fusion]
    C --> C3[Early Fusion]
    C1 --> C11[CLIP]
    C1 --> C12[SigLIP]
    C2 --> C21[BLIP-2]
    C2 --> C22[LLaVA]
    C2 --> C23[CogVLM]
    C3 --> C31[Chameleon]

    D --> D1[VQA]
    D --> D2[Captioning]
    D --> D3[Zero-shot]
    D --> D4[Retrieval]

    style A fill:#FF6B6B
    style B fill:#4ECDC4
    style C fill:#45B7D1
    style D fill:#FFA07A
```

### 6.6 ä¸»è¦ãªå­¦ã³ï¼ˆ4ã¤ã®Takeawayï¼‰

3,000è¡Œã®é•·ã„æ—…ã ã£ãŸãŒã€ã“ã“ã¾ã§æ¥ãŸã‚ãªãŸã¯**Vision-Languageãƒ¢ãƒ‡ãƒ«ã®å…¨é ˜åŸŸ**ã‚’ç†è§£ã—ãŸã€‚

1. **ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ« = Late/Deep/Early Fusionã®3æˆ¦ç•¥**
   - Late Fusion (CLIP): ç‹¬ç«‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ + é¡ä¼¼åº¦è¨ˆç®—
   - Deep Fusion (BLIP-2): ä¸­é–“å±¤ã§Cross-Attention
   - Early Fusion (Chameleon): å…¥åŠ›ãƒ¬ãƒ™ãƒ«ã§çµ±ä¸€Token

2. **InfoNCE lossã®æœ¬è³ª = ç›¸äº’æƒ…å ±é‡ã®ä¸‹ç•Œæœ€å¤§åŒ–**
   - æ­£ä¾‹ãƒšã‚¢ $(v_i, t_i)$ ã®é¡ä¼¼åº¦ã‚’æœ€å¤§åŒ–
   - è² ä¾‹ãƒšã‚¢ $(v_i, t_j)$ ã®é¡ä¼¼åº¦ã‚’æœ€å°åŒ–
   - æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\tau$ ã§åˆ†å¸ƒã®é‹­ã•ã‚’åˆ¶å¾¡

3. **Vision Transformer = Self-Attentionã§ç”»åƒã‚’å‡¦ç†**
   - Patch Embedding: ç”»åƒã‚’ $P \times P$ ãƒ‘ãƒƒãƒã«åˆ†å‰²
   - Positional Encoding: 2Dä½ç½®æƒ…å ±ã‚’ä»˜ä¸
   - Global Attention: å…¨ãƒ‘ãƒƒãƒé–“ã§Attentionï¼ˆCNNã‚ˆã‚Šåºƒã„å—å®¹é‡ï¼‰

4. **å®Ÿè£…ã®ç¾å®Ÿ: âš¡Juliaè¨“ç·´ + ğŸ¦€Rustæ¨è«–**
   - Juliaã§CLIPè¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆInfoNCE losså®Ÿè£…ï¼‰
   - Rustã§SmolVLM2æ¨è«–ï¼ˆGGUF/Candleçµ±åˆï¼‰
   - FFIçµŒç”±ã§ç›¸äº’é‹ç”¨ï¼ˆProduction-readyï¼‰

### 6.7 FAQ

:::details Q1: CLIPã¨BLIP-2ã€ã©ã¡ã‚‰ã‚’ä½¿ã†ã¹ãï¼Ÿ

**A**: ã‚¿ã‚¹ã‚¯æ¬¡ç¬¬ã€‚

- **Zero-shotåˆ†é¡ãƒ»Retrieval**: CLIPï¼ˆLate Fusionï¼‰ãŒæœ€é©ã€‚è¨“ç·´ãŒç°¡å˜ã§ã€æ¨è«–ã‚‚é€Ÿã„ã€‚
- **VQAãƒ»Captioning**: BLIP-2ï¼ˆDeep Fusionï¼‰ãŒæœ€é©ã€‚Q-FormerãŒç”»åƒã®è©³ç´°ã‚’æ‰ãˆã‚‹ã€‚
- **Instruction-Following**: LLaVAã€CogVLMï¼ˆDeep Fusion + Frozen LLMï¼‰ãŒæœ€é©ã€‚

**ã‚³ã‚¹ãƒˆ vs æ€§èƒ½**:
- CLIP: è¨“ç·´ã‚³ã‚¹ãƒˆä½ã€æ¨è«–é€Ÿåº¦é€Ÿã€æ€§èƒ½ä¸­
- BLIP-2: è¨“ç·´ã‚³ã‚¹ãƒˆä¸­ã€æ¨è«–é€Ÿåº¦ä¸­ã€æ€§èƒ½é«˜
- CogVLM: è¨“ç·´ã‚³ã‚¹ãƒˆé«˜ã€æ¨è«–é€Ÿåº¦é…ã€æ€§èƒ½æœ€é«˜
:::

:::details Q2: InfoNCE lossã®æ¸©åº¦ $\tau$ ã‚’ã©ã†æ±ºã‚ã‚‹ï¼Ÿ

**A**: å®Ÿé¨“çš„ã«æ±ºå®šã™ã‚‹ã®ãŒä¸€èˆ¬çš„ã€‚

**çµŒé¨“å‰‡**:
- $\tau = 0.07$: CLIPã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã€‚ã»ã¨ã‚“ã©ã®ã‚±ãƒ¼ã‚¹ã§ã“ã‚Œã§OKã€‚
- $\tau$ ãŒå°ã•ã„ï¼ˆ0.01ã€œ0.05ï¼‰: Hard Negativeã‚’å¼·ãç½°ã™ã‚‹ã€‚ãƒ‡ãƒ¼ã‚¿ãŒå¤šæ§˜ãªã‚‰æœ‰åŠ¹ã€‚
- $\tau$ ãŒå¤§ãã„ï¼ˆ0.1ã€œ0.5ï¼‰: åˆ†å¸ƒãŒãªã ã‚‰ã‹ã€‚ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„å ´åˆã«éå­¦ç¿’ã‚’é˜²ãã€‚

**è‡ªå‹•èª¿æ•´**: $\tau$ ã‚’å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã—ã¦ã€è¨“ç·´ä¸­ã«æœ€é©åŒ–ã™ã‚‹æ‰‹æ³•ã‚‚ã‚ã‚‹ï¼ˆCLIPè«–æ–‡ã§ã¯å›ºå®šï¼‰ã€‚
:::

:::details Q3: SmolVLM2-256Mã¯å®Ÿç”¨çš„ï¼Ÿ

**A**: ç”¨é€”æ¬¡ç¬¬ã ãŒã€**ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹**ã§ã¯éå¸¸ã«æœ‰åŠ¹ã€‚

**åˆ©ç‚¹**:
- æ¨è«–ãŒè¶…é«˜é€Ÿï¼ˆ1ç”»åƒ<100ms on CPUï¼‰
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå°ã•ã„ï¼ˆ<1GB RAMï¼‰
- 3ãƒ¢ãƒ€ãƒªãƒ†ã‚£å¯¾å¿œï¼ˆç”»åƒãƒ»ãƒ†ã‚­ã‚¹ãƒˆãƒ»ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªï¼‰

**æ¬ ç‚¹**:
- è¤‡é›‘ãªæ¨è«–ã‚¿ã‚¹ã‚¯ã§ã¯å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã«åŠ£ã‚‹
- Fine-tuningã®ä½™åœ°ãŒé™å®šçš„ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå°‘ãªã„ï¼‰

**æ¨å¥¨ç”¨é€”**: ãƒ¢ãƒã‚¤ãƒ«ã‚¢ãƒ—ãƒªã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”»åƒèªè­˜ã€IoTãƒ‡ãƒã‚¤ã‚¹ã€‚
:::

:::details Q4: Rustã§VLMè¨“ç·´ã¯ã§ããªã„ï¼Ÿ

**A**: æŠ€è¡“çš„ã«ã¯å¯èƒ½ã ãŒã€**ç¾æ™‚ç‚¹ã§ã¯éæ¨å¥¨**ã€‚

**ç†ç”±**:
1. **è‡ªå‹•å¾®åˆ†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®æœªæˆç†Ÿ**: PyTorchã‚„JAXã«æ¯”ã¹ã€Rustã®è‡ªå‹•å¾®åˆ†ï¼ˆburn, dfdxï¼‰ã¯ã¾ã ç™ºå±•é€”ä¸Šã€‚
2. **ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã®æ¬ å¦‚**: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã€ã‚ªãƒ¼ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã€åˆ†æ•£è¨“ç·´ãƒ„ãƒ¼ãƒ«ãŒä¸è¶³ã€‚
3. **é–‹ç™ºé€Ÿåº¦**: Rustã¯å‹å®‰å…¨ã ãŒã€å®Ÿé¨“ã®åå¾©é€Ÿåº¦ã¯Juliaã‚„Pythonã«åŠ£ã‚‹ã€‚

**Rustã®å½¹å‰²**: è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®**æ¨è«–**ã«ç‰¹åŒ–ã€‚GGUF/Candleã§é«˜é€Ÿæ¨è«–ã‚’å®Ÿç¾ã€‚
:::

:::details Q5: ç¬¬23å›ï¼ˆFine-tuningï¼‰ã§å­¦ã¶ã“ã¨ã¯ï¼Ÿ

**A**: LoRAã€QLoRAã€Adapterãªã©ã®PEFTæŠ€è¡“ã€‚

**äºˆç¿’ãƒã‚¤ãƒ³ãƒˆ**:
- LoRAã®æ•°å¼: ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—åˆ†è§£ $W' = W + AB$ ï¼ˆ$A \in \mathbb{R}^{d \times r}$, $B \in \mathbb{R}^{r \times d}$ï¼‰
- QLoRAã®é‡å­åŒ–: 4-bité‡å­åŒ–ã§ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
- Adapterã®æŒ¿å…¥ä½ç½®: ã©ã“ã«Adapterå±¤ã‚’å…¥ã‚Œã‚‹ã‹

ç¬¬23å›ã§ã¯ã€ã“ã‚Œã‚‰ã‚’âš¡Juliaã§å®Ÿè£…ã—ã€CLIPã‚„LLaVAã‚’Fine-tuningã™ã‚‹ã€‚
:::

### 6.8 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ1é€±é–“ãƒ—ãƒ©ãƒ³ï¼‰

| æ—¥ | ã‚¿ã‚¹ã‚¯ | æ™‚é–“ | å†…å®¹ |
|:---|:------|:-----|:-----|
| **Day 1** | Zone 0-2 | 1æ™‚é–“ | Quick Start + ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ã€‚ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã®æ¦‚è¦ã‚’æ´ã‚€ |
| **Day 2** | Zone 3.1-3.2 | 2æ™‚é–“ | ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åŸºç¤ + ViTç†è«–ã€‚æ•°å¼ã‚’ç´™ã«æ›¸ããªãŒã‚‰ç†è§£ |
| **Day 3** | Zone 3.3-3.4 | 2æ™‚é–“ | Cross-Modal Attention + InfoNCE losså°å‡ºï¼ˆBoss Battleï¼‰ |
| **Day 4** | Zone 4.1-4.2 | 2æ™‚é–“ | Julia CLIPå®Ÿè£… + ViTå®Ÿè£…ã€‚å®Ÿéš›ã«ã‚³ãƒ¼ãƒ‰ã‚’å‹•ã‹ã™ |
| **Day 5** | Zone 4.3 | 1.5æ™‚é–“ | Rust SmolVLM2æ¨è«– + FFIçµ±åˆ |
| **Day 6** | Zone 5 | 2æ™‚é–“ | è©•ä¾¡å®Ÿè£…ï¼ˆVQA/Captioning/Zero-shot/Retrievalï¼‰ |
| **Day 7** | Zone 6 | 1.5æ™‚é–“ | æŒ¯ã‚Šè¿”ã‚Š + æœ€æ–°ç ”ç©¶ã€‚å…¨ä½“ã‚’ä¿¯ç° |

**Total**: 12æ™‚é–“

### 6.9 æ¬¡ã®è¬›ç¾©ã¸ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼

**ç¬¬23å›: Fine-tuning & PEFT** ã§ã¯ã€ä»¥ä¸‹ã‚’å­¦ã¶:

1. **LoRA (Low-Rank Adaptation)**
   - ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—åˆ†è§£ã§ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’1%ã«å‰Šæ¸›
   - CLIPã®Vision Encoderã«LoRAã‚’é©ç”¨

2. **QLoRA (Quantized LoRA)**
   - 4-bité‡å­åŒ–ã§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’75%å‰Šæ¸›
   - LLaVA-7Bã‚’QLoRAã§Fine-tuning

3. **Adapter**
   - å„Transformerãƒ¬ã‚¤ãƒ¤ãƒ¼ã«Adapterå±¤ã‚’æŒ¿å…¥
   - Frozen LMã‚’ä¿ã¡ã¤ã¤ã€ã‚¿ã‚¹ã‚¯ç‰¹åŒ–

4. **DreamBooth**
   - ã€ŒSksã¨ã„ã†çŒ«ã€ã‚’å­¦ç¿’ã•ã›ã‚‹ï¼ˆFew-shot Personalizationï¼‰

**å®Ÿè£…è¨€èª**: âš¡Julia (LoRA/QLoRAè¨“ç·´) + ğŸ¦€Rust (é‡å­åŒ–æ¨è«–)

æº–å‚™ã¯ã„ã„ã‹ï¼Ÿ æ¬¡å›ã‚‚æ¥½ã—ã¿ã«ã—ã¦ã„ã¦ã»ã—ã„ã€‚

### 6.10 é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼ï¼ˆJuliaå®Ÿè£…ï¼‰

```julia
# ç¬¬22å›ã®é€²æ—ã‚’è¨˜éŒ²
struct Progress
    lecture_num::Int
    zones_completed::Vector{String}
    implementations::Dict{String, Bool}
    evaluations::Dict{String, Float64}
end

function track_progress()
    progress = Progress(
        22,
        ["Zone 0", "Zone 1", "Zone 2", "Zone 3", "Zone 4", "Zone 5", "Zone 6", "Zone 7"],
        Dict(
            "CLIP Julia" => true,
            "ViT Julia" => true,
            "SmolVLM2 Rust" => true,
            "InfoNCE Loss" => true,
            "VQA Eval" => true,
            "Captioning Eval" => true,
            "Zero-shot Eval" => true,
            "Retrieval Eval" => true
        ),
        Dict(
            "InfoNCE Lossç†è§£åº¦" => 0.95,
            "CLIPå®Ÿè£…å®Œæˆåº¦" => 0.90,
            "Rustæ¨è«–æˆåŠŸç‡" => 0.88,
            "è©•ä¾¡å®Ÿè£…å®Œæˆåº¦" => 0.85
        )
    )

    println("=== ç¬¬$(progress.lecture_num)å›é€²æ— ===")
    println("å®Œäº†Zone: $(join(progress.zones_completed, ", "))")
    println("\nå®Ÿè£…çŠ¶æ³:")
    for (impl, status) in progress.implementations
        println("  $impl: $(status ? "âœ“" : "âœ—")")
    end
    println("\nè©•ä¾¡æŒ‡æ¨™:")
    for (metric, score) in progress.evaluations
        println("  $metric: $(round(score * 100, digits=1))%")
    end

    overall = mean(values(progress.evaluations))
    println("\nç·åˆç†è§£åº¦: $(round(overall * 100, digits=1))%")

    return progress
end

# å®Ÿè¡Œ
track_progress()
```

**å‡ºåŠ›ä¾‹**:
```
=== ç¬¬22å›é€²æ— ===
å®Œäº†Zone: Zone 0, Zone 1, Zone 2, Zone 3, Zone 4, Zone 5, Zone 6, Zone 7

å®Ÿè£…çŠ¶æ³:
  CLIP Julia: âœ“
  ViT Julia: âœ“
  SmolVLM2 Rust: âœ“
  InfoNCE Loss: âœ“
  VQA Eval: âœ“
  Captioning Eval: âœ“
  Zero-shot Eval: âœ“
  Retrieval Eval: âœ“

è©•ä¾¡æŒ‡æ¨™:
  InfoNCE Lossç†è§£åº¦: 95.0%
  CLIPå®Ÿè£…å®Œæˆåº¦: 90.0%
  Rustæ¨è«–æˆåŠŸç‡: 88.0%
  è©•ä¾¡å®Ÿè£…å®Œæˆåº¦: 85.0%

ç·åˆç†è§£åº¦: 89.5%
```

### 6.11 ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

**å•ã„**: ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã¯ã€Œå¿œç”¨æŠ€è¡“ã€ã§ã¯ãªãã€Œæ¨™æº–ã€ã§ã¯ï¼Ÿ

**èƒŒæ™¯**:
æˆ‘ã€…ã¯é•·ã„é–“ã€ã€Œãƒ†ã‚­ã‚¹ãƒˆã®AIã€ã€Œç”»åƒã®AIã€ã€ŒéŸ³å£°ã®AIã€ã‚’**åˆ¥ã€…ã®æŠ€è¡“**ã¨ã—ã¦æ‰±ã£ã¦ããŸã€‚ã—ã‹ã—ã€äººé–“ã®çŸ¥èƒ½ã¯**æœ¬è³ªçš„ã«ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«**ã ã€‚

- å­ä¾›ã¯ã€Œã‚Šã‚“ã”ã€ã¨ã„ã†å˜èªã‚’å­¦ã¶ã¨ãã€**å®Ÿç‰©ã‚’è¦‹ãªãŒã‚‰**èãã€‚
- æ–™ç†ã®ãƒ¬ã‚·ãƒ”ã‚’èª­ã‚€ã¨ãã€**å†™çœŸã‚’è¦‹ãªãŒã‚‰**æ‰‹é †ã‚’ç†è§£ã™ã‚‹ã€‚
- éŸ³æ¥½ã‚’è´ãã¨ãã€**æ­Œè©ã‚’èª­ã¿ãªãŒã‚‰**æ„Ÿæƒ…ã‚’æ·±ã‚ã‚‹ã€‚

ã§ã¯ã€ãªãœAIã¯ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’**åˆ†é›¢**ã—ã¦ããŸã®ã‹ï¼Ÿ

**ç­”ãˆ**: **æŠ€è¡“çš„åˆ¶ç´„**ãŒã‚ã£ãŸã‹ã‚‰ã€‚

- 1950-1990å¹´ä»£: è¨ˆç®—è³‡æºã®åˆ¶ç´„ã§ã€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã”ã¨ã«ç‰¹åŒ–ã—ãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é–‹ç™ºã€‚
- 2000-2010å¹´ä»£: Deep Learningã®å°é ­ã§ã€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã”ã¨ã«å°‚ç”¨ãƒ¢ãƒ‡ãƒ«ï¼ˆCNN for Vision, RNN for Textï¼‰ã€‚
- 2020å¹´ä»£: Transformerã®ç™»å ´ã§ã€**çµ±ä¸€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**ãŒå¯èƒ½ã«ã€‚

**ä»Šå¾Œã®æ–¹å‘æ€§**:

1. **ãƒã‚¤ãƒ†ã‚£ãƒ–ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãŒæ¨™æº–ã«ãªã‚‹** â€” å˜ä¸€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ãƒ¢ãƒ‡ãƒ«ã¯ã€Œç‰¹æ®Šç”¨é€”ã€ã«ã€‚
2. **å…¨ã¦ã®AIãŒãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã«** â€” LLMã«ã€Œç›®ã€ã€Œè€³ã€ã€Œæ‰‹ã€ãŒä»˜ãï¼ˆGPT-4o, Gemini Ultraã®æ–¹å‘æ€§ï¼‰ã€‚
3. **æ–°ã—ã„ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®çµ±åˆ** â€” è§¦è¦šã€å—…è¦šã€å‘³è¦šã‚‚AIã®å…¥åŠ›ã«ï¼Ÿ

**è­°è«–ãƒã‚¤ãƒ³ãƒˆ**:

- ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãŒæ¨™æº–ã«ãªã‚‹ã¨ã€**ã©ã‚“ãªæ–°ã—ã„ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³**ãŒç”Ÿã¾ã‚Œã‚‹ã‹ï¼Ÿ
- å˜ä¸€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ãƒ¢ãƒ‡ãƒ«ã®**å­˜åœ¨æ„ç¾©**ã¯æ®‹ã‚‹ã‹ï¼Ÿï¼ˆä¾‹: ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã®LLMï¼‰
- ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AIã®**å€«ç†çš„èª²é¡Œ**ã¯ï¼Ÿï¼ˆDeepfakeã€ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ï¼‰

**æ­´å²çš„æ–‡è„ˆ**:

20ä¸–ç´€åˆé ­ã€**ãƒ©ã‚¸ã‚ª**ãŒç™»å ´ã—ãŸã¨ãã€äººã€…ã¯ã€ŒéŸ³å£°ã ã‘ã§ååˆ†ã€ã¨è€ƒãˆãŸã€‚ã—ã‹ã—ã€**ãƒ†ãƒ¬ãƒ“**ãŒç™»å ´ã™ã‚‹ã¨ã€æ˜ åƒã¨éŸ³å£°ã®çµ„ã¿åˆã‚ã›ãŒ**æ¨™æº–**ã«ãªã£ãŸã€‚ä»Šã€AIã‚‚åŒã˜è»¢æ›ç‚¹ã«ã„ã‚‹ã€‚

:::message
**é€²æ—: 100% å®Œäº†** ğŸ‰ ç¬¬22å›ã€Œãƒã‚¤ãƒ†ã‚£ãƒ–ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å®Œå…¨ç‰ˆã€å®Œèµ°ï¼ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®çµ±åˆã‚’å®Œå…¨ã«ãƒã‚¹ã‚¿ãƒ¼ã—ãŸã€‚æ¬¡ã¯Fine-tuningã§ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¿ã‚¹ã‚¯ã«é©å¿œã•ã›ã‚‹ã€‚
:::

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., & Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision. *International Conference on Machine Learning (ICML)*.
@[card](https://arxiv.org/abs/2103.00020)

[^2]: van den Oord, A., Li, Y., & Vinyals, O. (2018). Representation Learning with Contrastive Predictive Coding. *arXiv preprint*.
@[card](https://arxiv.org/abs/1807.03748)

[^3]: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., & Houlsby, N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. *International Conference on Learning Representations (ICLR) 2021*.
@[card](https://arxiv.org/abs/2010.11929)

[^4]: Li, J., Li, D., Savarese, S., & Hoi, S. (2023). BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. *International Conference on Machine Learning (ICML)*.
@[card](https://arxiv.org/abs/2301.12597)

[^5]: Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, S., Brock, A., Nematzadeh, A., Sharifzadeh, S., Binkowski, M., Barreira, R., Vinyals, O., Zisserman, A., & Simonyan, K. (2022). Flamingo: a Visual Language Model for Few-Shot Learning. *Advances in Neural Information Processing Systems (NeurIPS)*.
@[card](https://arxiv.org/abs/2204.14198)

[^6]: Liu, H., Li, C., Wu, Q., & Lee, Y. J. (2023). Visual Instruction Tuning. *Advances in Neural Information Processing Systems (NeurIPS)*.
@[card](https://arxiv.org/abs/2304.08485)

[^7]: Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, C., Wang, L., Ge, Y., Song, Y., Li, H., Dang, K., Ouyang, S., Ren, X., Yan, D., Zhang, X., Qin, Y., Lin, Z., Huang, F., Liu, J., & Zhou, J. (2024). Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution. *arXiv preprint*.
@[card](https://arxiv.org/abs/2409.12191)

[^8]: Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao, L., Song, X., Xu, J., Xu, B., Li, J., Dong, Y., Ding, M., & Tang, J. (2023). CogVLM: Visual Expert for Pretrained Language Models. *arXiv preprint*.
@[card](https://arxiv.org/abs/2311.03079)

[^9]: HuggingFace (2024). SmolVLM2-256M-Instruct.
@[card](https://huggingface.co/HuggingFaceTB/SmolVLM2-256M-Instruct)

[^10]: HuggingFace (2023). Introducing IDEFICS: An Open Reproduction of State-of-the-art Visual Language Model.
@[card](https://huggingface.co/blog/idefics)

[^11]: Cherti, M., Beaumont, R., Wightman, R., Wortsman, M., Ilharco, G., Gordon, C., Schuhmann, C., Schmidt, L., & Jitsev, J. (2023). Reproducible scaling laws for contrastive language-image learning. *Computer Vision and Pattern Recognition (CVPR)*.
@[card](https://arxiv.org/abs/2212.07143)

[^12]: Zhai, X., Mustafa, B., Kolesnikov, A., & Beyer, L. (2023). Sigmoid Loss for Language Image Pre-Training. *arXiv preprint*.
@[card](https://arxiv.org/abs/2303.15343)

[^13]: Deitke, M., Clark, C., Lee, S., Tripathi, R., Yang, Y., Park, J. S., Salehi, M., Muennighoff, N., Lo, K., Soldaini, L., Lu, J., Anderson, T., Bransom, E., Ehsani, K., Ngo, H., Chen, Y. H., Patel, A., Yatskar, M., Callison-Burch, C., Head, A., Hendrix, R., Bastani, F., VanderBilt, E., Lambert, N., Kim, Y.-J., Choudhury, S., Chasins, S., & Farhadi, A. (2024). Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models. *arXiv preprint*.
@[card](https://arxiv.org/abs/2409.17146)

[^14]: Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., & Parikh, D. (2017). Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering. *Computer Vision and Pattern Recognition (CVPR)*.

[^15]: Anderson, P., Fernando, B., Johnson, M., & Gould, S. (2016). SPICE: Semantic Propositional Image Caption Evaluation. *European Conference on Computer Vision (ECCV)*.
@[card](https://panderson.me/spice/)

### æ•™ç§‘æ›¸

- Murphy, K. P. (2022). *Probabilistic Machine Learning: Advanced Topics*. MIT Press. [https://probml.github.io/pml-book/book2.html](https://probml.github.io/pml-book/book2.html)
- Prince, S. J. D. (2023). *Understanding Deep Learning*. MIT Press. [https://udlbook.github.io/udlbook/](https://udlbook.github.io/udlbook/)
- Zhang, A., Lipton, Z. C., Li, M., & Smola, A. J. (2023). *Dive into Deep Learning*. Cambridge University Press. [https://d2l.ai/](https://d2l.ai/)

---

## è¨˜æ³•è¦ç´„

æœ¬è¬›ç¾©ã§ä½¿ç”¨ã—ãŸæ•°å­¦è¨˜å·ã®çµ±ä¸€è¦ç´„ã€‚

| è¨˜å· | æ„å‘³ | å‚™è€ƒ |
|:-----|:-----|:-----|
| $\mathbf{x}^v$ | ç”»åƒå…¥åŠ› | $(H \times W \times C)$ |
| $\mathbf{x}^t$ | ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ› | ãƒˆãƒ¼ã‚¯ãƒ³åˆ— $(L \times d_{\text{tok}})$ |
| $f_v$ | Vision Encoder | ç”»åƒ â†’ åŸ‹ã‚è¾¼ã¿ |
| $f_t$ | Text Encoder | ãƒ†ã‚­ã‚¹ãƒˆ â†’ åŸ‹ã‚è¾¼ã¿ |
| $\mathbf{v}$ | ç”»åƒåŸ‹ã‚è¾¼ã¿ | $(d,)$ |
| $\mathbf{t}$ | ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ | $(d,)$ |
| $d$ | åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ | é€šå¸¸512, 768, 1024 |
| $N$ | ãƒãƒƒãƒã‚µã‚¤ã‚º or ãƒ‘ãƒƒãƒæ•° | æ–‡è„ˆä¾å­˜ |
| $\tau$ | æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | InfoNCE lossã®ã‚¹ã‚±ãƒ¼ãƒ« |
| $\mathbf{Q}, \mathbf{K}, \mathbf{V}$ | Query, Key, Value | Attentionæ©Ÿæ§‹ |
| $\mathbf{A}$ | Attention weights | Softmaxå¾Œã®ç¢ºç‡åˆ†å¸ƒ |
| $P$ | ãƒ‘ãƒƒãƒã‚µã‚¤ã‚º | ViTã®å…¥åŠ›åˆ†å‰²å˜ä½ï¼ˆé€šå¸¸16 or 32ï¼‰ |
| $\mathbf{z}_p$ | ãƒ‘ãƒƒãƒ $p$ ã®åŸ‹ã‚è¾¼ã¿ | Patch Embeddingå¾Œ |
| $\mathbf{e}_{\text{pos}}$ | Positional Encoding | ä½ç½®æƒ…å ±ãƒ™ã‚¯ãƒˆãƒ« |
| $s_{ij}$ | é¡ä¼¼åº¦ | $\cos(\mathbf{v}_i, \mathbf{t}_j)$ |
| $\mathcal{L}$ | æå¤±é–¢æ•° | InfoNCE loss |
| $\mathbf{Z}^v$ | ç”»åƒç‰¹å¾´é‡åˆ— | $(d \times N)$ |
| $\mathbf{Z}^t$ | ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡åˆ— | $(d \times L)$ |

---

**ç¬¬22å›å®Œ**

æ¬¡å›ã€**ç¬¬23å›: Fine-tuning & PEFT** ã§ã¾ãŸä¼šãŠã†ã€‚LoRAã€QLoRAã€Adapterã®ä¸–ç•Œã¸ã‚ˆã†ã“ãã€‚

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
