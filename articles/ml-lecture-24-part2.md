---
title: "ç¬¬24å›ã€å¾Œç·¨ã€‘ä»˜éŒ²ç·¨: çµ±è¨ˆå­¦: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸ“ˆ"
type: "tech"
topics: ["machinelearning", "statistics", "julia", "bayesian", "hypothesis"]
published: true
slug: "ml-lecture-24-part2"
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Julia", "Rust", "Elixir"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---

> **ç¬¬24å›ã€å‰ç·¨ã€‘**: [ç¬¬24å›ã€å‰ç·¨ã€‘](https://zenn.dev/fumishiki/ml-lecture-24-part1)

## Part 2


$$
\begin{aligned}
\text{SS}_{\text{total}} &= \sum_{i=1}^k \sum_{j=1}^{n_i} (x_{ij} - \bar{x})^2 \\
\text{SS}_{\text{between}} &= \sum_{i=1}^k n_i (\bar{x}_i - \bar{x})^2 \\
\text{SS}_{\text{within}} &= \sum_{i=1}^k \sum_{j=1}^{n_i} (x_{ij} - \bar{x}_i)^2 \\
\text{MS}_{\text{between}} &= \frac{\text{SS}_{\text{between}}}{k-1}, \quad \text{MS}_{\text{within}} = \frac{\text{SS}_{\text{within}}}{N-k}
\end{aligned}
$$

**æ•°å€¤æ¤œè¨¼**:

```julia
using HypothesisTests

group_a = [0.72, 0.71, 0.73, 0.70, 0.72]
group_b = [0.78, 0.77, 0.79, 0.76, 0.78]
group_c = [0.68, 0.67, 0.69, 0.66, 0.68]

# ä¸€å…ƒé…ç½®ANOVA
test = OneWayANOVATest(group_a, group_b, group_c)
println("F=$(round(test.F, digits=3)), p=$(round(pvalue(test), digits=6))")
println(pvalue(test) < 0.05 ? "âœ… å°‘ãªãã¨ã‚‚1çµ„ã®å¹³å‡ãŒç•°ãªã‚‹" : "âŒ å…¨ç¾¤ã®å¹³å‡ã«å·®ãªã—")
```

å‡ºåŠ›:
```
F=90.0, p=0.000000
âœ… å°‘ãªãã¨ã‚‚1çµ„ã®å¹³å‡ãŒç•°ãªã‚‹
```

#### 3.4.3 æ­£è¦æ€§æ¤œå®š

**å•é¡Œ**: tæ¤œå®šãƒ»ANOVAã¯æ­£è¦æ€§ã‚’ä»®å®šã€‚ãƒ‡ãƒ¼ã‚¿ãŒæ­£è¦åˆ†å¸ƒã«å¾“ã†ã‹æ¤œè¨¼ã—ãŸã„ã€‚

| æ¤œå®š | ç‰¹å¾´ | å¸°ç„¡ä»®èª¬ |
|:-----|:-----|:--------|
| **Shapiro-Wilkæ¤œå®š** | æœ€ã‚‚å¼·åŠ›ï¼ˆå°~ä¸­ã‚µãƒ³ãƒ—ãƒ«ï¼‰ | ãƒ‡ãƒ¼ã‚¿ãŒæ­£è¦åˆ†å¸ƒã«å¾“ã† |
| **Kolmogorov-Smirnovæ¤œå®š** | æ±ç”¨çš„ï¼ˆä»»æ„ã®åˆ†å¸ƒï¼‰ | ãƒ‡ãƒ¼ã‚¿ãŒæŒ‡å®šåˆ†å¸ƒã«å¾“ã† |
| **Anderson-Darlingæ¤œå®š** | è£¾ã®é©åˆåº¦ã‚’é‡è¦– | ãƒ‡ãƒ¼ã‚¿ãŒæ­£è¦åˆ†å¸ƒã«å¾“ã† |

**æ•°å€¤æ¤œè¨¼**:

```julia
using HypothesisTests, Distributions

# æ­£è¦åˆ†å¸ƒãƒ‡ãƒ¼ã‚¿
normal_data = rand(Normal(0, 1), 30)
test_normal = ExactOneSampleKSTest(normal_data, Normal(0, 1))
println("æ­£è¦ãƒ‡ãƒ¼ã‚¿: p=$(round(pvalue(test_normal), digits=4))")

# éæ­£è¦ãƒ‡ãƒ¼ã‚¿ï¼ˆä¸€æ§˜åˆ†å¸ƒï¼‰
uniform_data = rand(Uniform(0, 1), 30)
test_uniform = ExactOneSampleKSTest(uniform_data, Normal(0.5, 1))
println("ä¸€æ§˜ãƒ‡ãƒ¼ã‚¿: p=$(round(pvalue(test_uniform), digits=4))")
```

### 3.5 ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¤œå®š

**ç”¨é€”**: æ­£è¦æ€§ãŒæº€ãŸã•ã‚Œãªã„ã€ã¾ãŸã¯é †åºãƒ‡ãƒ¼ã‚¿ã®å ´åˆã€‚

| æ¤œå®š | ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ç‰ˆ | ç”¨é€” |
|:-----|:----------------|:-----|
| **Mann-Whitney Uæ¤œå®š** | 2æ¨™æœ¬tæ¤œå®š | 2ç¾¤ã®ä¸­å¤®å€¤ã®å·® |
| **Wilcoxonç¬¦å·é †ä½æ¤œå®š** | å¯¾å¿œã®ã‚ã‚‹tæ¤œå®š | å¯¾å¿œã®ã‚ã‚‹2ç¾¤ã®ä¸­å¤®å€¤å·® |
| **Kruskal-Wallisæ¤œå®š** | ä¸€å…ƒé…ç½®ANOVA | 3ç¾¤ä»¥ä¸Šã®ä¸­å¤®å€¤ã®å·® |

**Mann-Whitney Uæ¤œå®šã®åŸç†**:

1. 2ç¾¤ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã—ã¦é †ä½ä»˜ã‘ã€‚
2. å„ç¾¤ã®é †ä½å’Œã‚’è¨ˆç®—ã€‚
3. Uçµ±è¨ˆé‡ã‚’è¨ˆç®—:

$$
U_1 = n_1 n_2 + \frac{n_1(n_1+1)}{2} - R_1
$$

ã“ã“ã§ $R_1$ ã¯ç¾¤1ã®é †ä½å’Œã€‚

**æ•°å€¤æ¤œè¨¼**:

```julia
using HypothesisTests

group1 = [1, 2, 3, 4, 5]
group2 = [6, 7, 8, 9, 10]

# Mann-Whitney Uæ¤œå®š
test = MannWhitneyUTest(group1, group2)
println("U=$(test.U), p=$(round(pvalue(test), digits=4))")
```

> **Note:** **é€²æ—: 65% å®Œäº†** ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ãƒ»ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¤œå®šã®ç†è«–å®Œå…¨ç‰ˆã‚’åˆ¶è¦‡ã€‚å¤šé‡æ¯”è¼ƒè£œæ­£ã¸ã€‚

### 3.6 å¤šé‡æ¯”è¼ƒè£œæ­£ç†è«–

**å•é¡Œ**: è¤‡æ•°ã®æ¤œå®šã‚’è¡Œã†ã¨ã€å¶ç„¶ã«æœ‰æ„ã«ãªã‚‹ç¢ºç‡ï¼ˆç¬¬1ç¨®éèª¤ï¼‰ãŒå¢—å¤§ã™ã‚‹ã€‚

**ä¾‹**: $\alpha = 0.05$ ã§ç‹¬ç«‹ãª20å€‹ã®æ¤œå®šã‚’è¡Œã†ã¨ã€å°‘ãªãã¨ã‚‚1ã¤ãŒå¶ç„¶æœ‰æ„ã«ãªã‚‹ç¢ºç‡:

$$
1 - (1 - 0.05)^{20} \approx 0.64 \quad \text{(64%!)}
$$

**FWERï¼ˆFamily-Wise Error Rateï¼‰**: å°‘ãªãã¨ã‚‚1ã¤ã®ç¬¬1ç¨®éèª¤ãŒèµ·ã“ã‚‹ç¢ºç‡ã€‚

**FDRï¼ˆFalse Discovery Rateï¼‰**: æœ‰æ„ã¨åˆ¤å®šã•ã‚ŒãŸã‚‚ã®ã®ã†ã¡å½é™½æ€§ã®å‰²åˆã®æœŸå¾…å€¤ã€‚

#### 3.6.1 FWERåˆ¶å¾¡æ³•

| æ‰‹æ³• | èª¿æ•´å¾Œã®æœ‰æ„æ°´æº– | ä¿å®ˆæ€§ |
|:-----|:----------------|:-------|
| **Bonferroniè£œæ­£** | $\alpha_{\text{adj}} = \alpha / m$ | æœ€ã‚‚ä¿å®ˆçš„ |
| **Holmæ³•** | é€æ¬¡çš„Bonferroni | Bonferroniã‚ˆã‚Šç·©ã„ |
| **Å idÃ¡kè£œæ­£** | $\alpha_{\text{adj}} = 1 - (1 - \alpha)^{1/m}$ | ç‹¬ç«‹æ€§ä»®å®š |

**Holmæ³•ã®æ‰‹é †**:

1. på€¤ã‚’æ˜‡é †ã«ä¸¦ã¹ã‚‹: $p_{(1)} \leq p_{(2)} \leq \cdots \leq p_{(m)}$
2. $i = 1, 2, \ldots$ ã®é †ã«ä»¥ä¸‹ã‚’ãƒã‚§ãƒƒã‚¯:
   - $p_{(i)} \leq \alpha / (m - i + 1)$ ãªã‚‰æ£„å´ã€æ¬¡ã¸
   - åˆã‚ã¦ä¸ç­‰å¼ãŒæˆç«‹ã—ãªã‹ã£ãŸã‚‰åœæ­¢

#### 3.6.2 FDRåˆ¶å¾¡æ³•

**Benjamini-Hochbergæ³•** [^2]:

1. på€¤ã‚’æ˜‡é †ã«ä¸¦ã¹ã‚‹: $p_{(1)} \leq p_{(2)} \leq \cdots \leq p_{(m)}$
2. $i = m, m-1, \ldots, 1$ ã®é †ã«ä»¥ä¸‹ã‚’ãƒã‚§ãƒƒã‚¯:
   - $p_{(i)} \leq \frac{i}{m} \alpha$ ãªã‚‰ $i$ ç•ªç›®ã¾ã§å…¨ã¦æ£„å´ã€åœæ­¢
   - æˆç«‹ã—ãªã‘ã‚Œã°æ¬¡ã¸

**æ•°å¼å°å‡º**:

FDRã®å®šç¾©:

$$
\text{FDR} = \mathbb{E}\left[\frac{V}{R}\right]
$$

ã“ã“ã§ $V$ = å½é™½æ€§æ•°ã€$R$ = ç·ç™ºè¦‹æ•°ï¼ˆ$R = V + S$, $S$ = çœŸé™½æ€§æ•°ï¼‰ã€‚

Benjamini-Hochbergã¯ç‹¬ç«‹ãªæ¤œå®šã«ãŠã„ã¦ $\text{FDR} \leq \alpha$ ã‚’ä¿è¨¼ã™ã‚‹ [^2]ã€‚

**æ•°å€¤æ¤œè¨¼**:

```julia
using MultipleTesting

# 100å€‹ã®æ¤œå®šï¼ˆ90å€‹ã¯å¸°ç„¡ä»®èª¬ãŒçœŸã€10å€‹ã¯å¯¾ç«‹ä»®èª¬ãŒçœŸï¼‰
p_values_null = rand(100)  # H0ãŒçœŸã®på€¤: ä¸€æ§˜åˆ†å¸ƒ
p_values_alt  = rand(Beta(0.1, 1), 10)  # H1ãŒçœŸã®på€¤: 0ã«åã‚‹
p_values = vcat(p_values_null, p_values_alt)

# è£œæ­£ãªã—
n_sig_uncorrected = sum(p_values .< 0.05)
println("è£œæ­£ãªã—: $(n_sig_uncorrected) / 110 ãŒæœ‰æ„")

# Bonferroniè£œæ­£
p_bonf = adjust(PValues(p_values), Bonferroni())
n_sig_bonf = sum(p_bonf .< 0.05)
println("Bonferroni: $(n_sig_bonf) / 110 ãŒæœ‰æ„")

# Benjamini-Hochberg (FDR)
p_bh = adjust(PValues(p_values), BenjaminiHochberg())
n_sig_bh = sum(p_bh .< 0.05)
println("Benjamini-Hochberg: $(n_sig_bh) / 110 ãŒæœ‰æ„")
```

å‡ºåŠ›ä¾‹:
```
è£œæ­£ãªã—: 15 / 110 ãŒæœ‰æ„
Bonferroni: 3 / 110 ãŒæœ‰æ„
Benjamini-Hochberg: 9 / 110 ãŒæœ‰æ„
```

> **Note:** **é€²æ—: 75% å®Œäº†** å¤šé‡æ¯”è¼ƒè£œæ­£ï¼ˆFWER/FDRï¼‰ã‚’å®Œå…¨ç†è§£ã€‚GLMç†è«–ã¸ã€‚

### 3.7 ä¸€èˆ¬åŒ–ç·šå½¢ãƒ¢ãƒ‡ãƒ«ï¼ˆGLMï¼‰

**å•é¡Œ**: ç·šå½¢å›å¸° $y = X\beta + \epsilon$ ã¯é€£ç¶šå€¤ãƒ»æ­£è¦åˆ†å¸ƒã‚’ä»®å®šã€‚ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ï¼ˆåˆ†é¡ï¼‰ã‚„ã‚«ã‚¦ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã«ã¯ä¸é©ã€‚

**GLMã®æ§‹æˆè¦ç´ **:

1. **æŒ‡æ•°å‹åˆ†å¸ƒæ—**: å¿œç­”å¤‰æ•° $y$ ã®åˆ†å¸ƒï¼ˆæ­£è¦ãƒ»äºŒé …ãƒ»ãƒã‚¢ã‚½ãƒ³ç­‰ï¼‰ã€‚
2. **ãƒªãƒ³ã‚¯é–¢æ•°** $g(\cdot)$: å¹³å‡ $\mu = \mathbb{E}[y]$ ã‚’ç·šå½¢äºˆæ¸¬å­ $\eta = X\beta$ ã«ç¹‹ãã€‚
3. **ç·šå½¢äºˆæ¸¬å­**: $\eta = X\beta$

$$
g(\mu) = X\beta \quad \Rightarrow \quad \mu = g^{-1}(X\beta)
$$

| åˆ†å¸ƒ | å…¸å‹çš„ç”¨é€” | æ¨™æº–çš„ãƒªãƒ³ã‚¯é–¢æ•° |
|:-----|:----------|:----------------|
| æ­£è¦åˆ†å¸ƒ | é€£ç¶šå€¤ | æ’ç­‰ $g(\mu) = \mu$ |
| äºŒé …åˆ†å¸ƒ | åˆ†é¡ | ãƒ­ã‚¸ãƒƒãƒˆ $g(\mu) = \log\frac{\mu}{1-\mu}$ |
| ãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒ | ã‚«ã‚¦ãƒ³ãƒˆ | å¯¾æ•° $g(\mu) = \log\mu$ |

#### 3.7.1 ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ï¼ˆLogistic Regressionï¼‰

**ç”¨é€”**: äºŒå€¤åˆ†é¡ï¼ˆ$y \in \{0, 1\}$ï¼‰ã€‚

**ãƒ¢ãƒ‡ãƒ«**:

$$
\begin{aligned}
y_i &\sim \text{Bernoulli}(p_i) \\
\log\frac{p_i}{1 - p_i} &= \beta_0 + \beta_1 x_i \quad \text{(ãƒ­ã‚¸ãƒƒãƒˆå¤‰æ›)} \\
\Rightarrow \quad p_i &= \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_i)}} \quad \text{(ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°)}
\end{aligned}
$$

**ã‚ªãƒƒã‚ºæ¯”ï¼ˆOdds Ratioï¼‰**: ä¿‚æ•° $\beta_1$ ã®è§£é‡ˆ

$$
\text{OR} = e^{\beta_1}
$$

$x$ ãŒ1å˜ä½å¢—åŠ ã™ã‚‹ã¨ã€ã‚ªãƒƒã‚ºï¼ˆ$p / (1-p)$ï¼‰ãŒ $e^{\beta_1}$ å€ã«ãªã‚‹ã€‚

**æœ€å°¤æ¨å®š**: å¯¾æ•°å°¤åº¦ã‚’æœ€å¤§åŒ–ã€‚

$$
\ell(\beta) = \sum_{i=1}^n \left[ y_i \log p_i + (1 - y_i) \log(1 - p_i) \right]
$$

å‹¾é…:

$$
\frac{\partial \ell}{\partial \beta_j} = \sum_{i=1}^n (y_i - p_i) x_{ij}
$$

**æ•°å€¤æ¤œè¨¼**:

```julia
using GLM, DataFrames

# ãƒ‡ãƒ¼ã‚¿: xï¼ˆé€£ç¶šå¤‰æ•°ï¼‰, yï¼ˆ0/1ã®ãƒ©ãƒ™ãƒ«ï¼‰
df = DataFrame(
    x = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0],
    y = [0, 0, 0, 0, 1, 0, 1, 1, 1, 1]
)

# ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°
model = glm(@formula(y ~ x), df, Binomial(), LogitLink())
println(model)

# ä¿‚æ•°ã®è§£é‡ˆ
Î²1 = coef(model)[2]
OR = exp(Î²1)
println("\nä¿‚æ•°Î²1=$(round(Î²1, digits=3)), ã‚ªãƒƒã‚ºæ¯”OR=$(round(OR, digits=3))")
println("xãŒ1å˜ä½å¢—åŠ ã™ã‚‹ã¨ã€ã‚ªãƒƒã‚ºãŒ$(round(OR, digits=3))å€ã«ãªã‚‹")

# äºˆæ¸¬
df.y_pred = predict(model, df)
println("\näºˆæ¸¬ç¢ºç‡:")
println(df)
```

#### 3.7.2 ãƒã‚¢ã‚½ãƒ³å›å¸°ï¼ˆPoisson Regressionï¼‰

**ç”¨é€”**: ã‚«ã‚¦ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆ$y \in \{0, 1, 2, \ldots\}$ï¼‰ã€‚ã‚¤ãƒ™ãƒ³ãƒˆç™ºç”Ÿå›æ•°ã®äºˆæ¸¬ã€‚

**ãƒ¢ãƒ‡ãƒ«**:

$$
\begin{aligned}
y_i &\sim \text{Poisson}(\lambda_i) \\
\log \lambda_i &= \beta_0 + \beta_1 x_i \quad \text{(å¯¾æ•°ãƒªãƒ³ã‚¯é–¢æ•°)} \\
\Rightarrow \quad \lambda_i &= e^{\beta_0 + \beta_1 x_i}
\end{aligned}
$$

**ä¿‚æ•°ã®è§£é‡ˆ**: $x$ ãŒ1å˜ä½å¢—åŠ ã™ã‚‹ã¨ã€æœŸå¾…ã‚«ã‚¦ãƒ³ãƒˆ $\lambda$ ãŒ $e^{\beta_1}$ å€ã«ãªã‚‹ã€‚

**æ•°å€¤æ¤œè¨¼**:

```julia
using GLM, DataFrames, Distributions

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: ã‚«ã‚¦ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆä¾‹: 1æ™‚é–“ã‚ãŸã‚Šã®ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿå›æ•°ï¼‰
df = DataFrame(
    workload = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],  # è² è·ãƒ¬ãƒ™ãƒ«
    errors = [2, 3, 3, 5, 6, 8, 9, 12, 14, 16]   # ã‚¨ãƒ©ãƒ¼å›æ•°
)

# ãƒã‚¢ã‚½ãƒ³å›å¸°
model = glm(@formula(errors ~ workload), df, Poisson(), LogLink())
println(model)

# ä¿‚æ•°ã®è§£é‡ˆ
Î²1 = coef(model)[2]
multiplier = exp(Î²1)
println("\nworkloadãŒ1å˜ä½å¢—åŠ ã™ã‚‹ã¨ã€æœŸå¾…ã‚¨ãƒ©ãƒ¼å›æ•°ãŒ$(round(multiplier, digits=3))å€ã«ãªã‚‹")

# äºˆæ¸¬
df.errors_pred = predict(model, df)
println("\näºˆæ¸¬ã‚¨ãƒ©ãƒ¼å›æ•°:")
println(df)
```

#### 3.7.3 æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®çµ±ä¸€ç†è«–

**GLMã®åŸºç›¤**: æŒ‡æ•°å‹åˆ†å¸ƒæ—ï¼ˆExponential Familyï¼‰

$$
p(y | \theta, \phi) = \exp\left(\frac{y\theta - b(\theta)}{a(\phi)} + c(y, \phi)\right)
$$

| é … | åç§° | å½¹å‰² |
|:---|:-----|:-----|
| $\theta$ | è‡ªç„¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å¹³å‡ã‚’æ±ºå®š |
| $\phi$ | åˆ†æ•£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | åˆ†æ•£ã‚’æ±ºå®š |
| $b(\theta)$ | ç´¯ç©ç”Ÿæˆé–¢æ•° | å¹³å‡: $\mu = b'(\theta)$ |
| $a(\phi)$ | åˆ†æ•£é–¢æ•° | åˆ†æ•£: $\text{Var}(Y) = b''(\theta) a(\phi)$ |

**ä¸»è¦ãªåˆ†å¸ƒ**:

| åˆ†å¸ƒ | $\theta$ | $b(\theta)$ | $a(\phi)$ | $\mu = b'(\theta)$ |
|:-----|:---------|:-----------|:----------|:------------------|
| æ­£è¦åˆ†å¸ƒ | $\mu$ | $\theta^2 / 2$ | $\sigma^2$ | $\theta$ |
| äºŒé …åˆ†å¸ƒ | $\log \frac{p}{1-p}$ | $\log(1 + e^\theta)$ | $1$ | $\frac{e^\theta}{1 + e^\theta}$ |
| ãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒ | $\log \lambda$ | $e^\theta$ | $1$ | $e^\theta$ |

**GLMã®çµ±ä¸€æ§‹é€ **:

1. **ãƒ©ãƒ³ãƒ€ãƒ æˆåˆ†**: å¿œç­”å¤‰æ•° $y$ ãŒæŒ‡æ•°å‹åˆ†å¸ƒæ—ã«å¾“ã†ã€‚
2. **ç·šå½¢äºˆæ¸¬å­**: $\eta = X\beta$
3. **ãƒªãƒ³ã‚¯é–¢æ•°**: $g(\mu) = \eta$ï¼ˆæ¨™æº–çš„ãƒªãƒ³ã‚¯é–¢æ•°: $g(\mu) = \theta$ï¼‰

> **Note:** **é€²æ—: 80% å®Œäº†** GLMç†è«–ï¼ˆãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯ãƒ»ãƒã‚¢ã‚½ãƒ³å›å¸°ãƒ»æŒ‡æ•°å‹åˆ†å¸ƒæ—ï¼‰ã‚’ç†è§£ã€‚ãƒ™ã‚¤ã‚ºçµ±è¨ˆã¸ã€‚

### 3.8 ãƒ™ã‚¤ã‚ºçµ±è¨ˆå…¥é–€

#### 3.8.1 ãƒ™ã‚¤ã‚ºã®å®šç†ã®å°å‡º

**ç¬¬4å›ã§å­¦ã‚“ã æ¡ä»¶ä»˜ãç¢ºç‡ã®å®šç¾©**:

$$
p(\theta | D) = \frac{p(\theta, D)}{p(D)}, \quad p(D | \theta) = \frac{p(\theta, D)}{p(\theta)}
$$

ä¸¡è¾ºã« $p(\theta)$ ã‚’æ›ã‘ã‚‹ã¨:

$$
p(\theta, D) = p(D | \theta) p(\theta) = p(\theta | D) p(D)
$$

ã‚ˆã£ã¦:

$$
p(\theta | D) = \frac{p(D | \theta) p(\theta)}{p(D)}
$$

ã“ã‚ŒãŒ**ãƒ™ã‚¤ã‚ºã®å®šç†**ã ã€‚

| é … | åç§° | æ„å‘³ |
|:---|:-----|:-----|
| $p(\theta \| D)$ | äº‹å¾Œåˆ†å¸ƒï¼ˆPosteriorï¼‰ | ãƒ‡ãƒ¼ã‚¿è¦³æ¸¬å¾Œã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆ†å¸ƒ |
| $p(D \| \theta)$ | å°¤åº¦ï¼ˆLikelihoodï¼‰ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¸‹ã§ã®ãƒ‡ãƒ¼ã‚¿ã®ç¢ºç‡ |
| $p(\theta)$ | äº‹å‰åˆ†å¸ƒï¼ˆPriorï¼‰ | ãƒ‡ãƒ¼ã‚¿è¦³æ¸¬å‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¿¡å¿µ |
| $p(D)$ | å‘¨è¾ºå°¤åº¦ï¼ˆEvidenceï¼‰ | æ­£è¦åŒ–å®šæ•° $p(D) = \int p(D \| \theta) p(\theta) d\theta$ |

#### 3.8.2 é »åº¦è«–çµ±è¨ˆ vs ãƒ™ã‚¤ã‚ºçµ±è¨ˆ

**å“²å­¦çš„å¯¾ç«‹**:

| é …ç›® | é »åº¦è«– | ãƒ™ã‚¤ã‚º |
|:-----|:------|:-------|
| **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ€§è³ª** | å›ºå®šå€¤ï¼ˆæœªçŸ¥ï¼‰ | ç¢ºç‡å¤‰æ•° |
| **ç¢ºç‡ã®è§£é‡ˆ** | é•·æœŸçš„é »åº¦ | ä¿¡å¿µã®åº¦åˆã„ |
| **æ¨è«–ã®å¯¾è±¡** | ç‚¹æ¨å®šãƒ»ä¿¡é ¼åŒºé–“ | äº‹å¾Œåˆ†å¸ƒå…¨ä½“ |
| **ä¸ç¢ºå®Ÿæ€§ã®è¡¨ç¾** | æ¨™æº–èª¤å·® | äº‹å¾Œåˆ†å¸ƒã®å¹… |
| **äº‹å‰çŸ¥è­˜** | ä½¿ã‚ãªã„ï¼ˆå®¢è¦³æ€§ï¼‰ | ä½¿ã†ï¼ˆä¸»è¦³æ€§ï¼‰ |

**å…·ä½“ä¾‹**: ã‚³ã‚¤ãƒ³æŠ•ã’ï¼ˆ10å›ä¸­7å›è¡¨ï¼‰

**é »åº¦è«–çš„æ¨å®š**ï¼ˆç¬¬7å›ã®MLEï¼‰:

$$
\hat{\theta}_{\text{MLE}} = \frac{k}{n} = \frac{7}{10} = 0.7
$$

95%ä¿¡é ¼åŒºé–“ï¼ˆWaldæ³•ï¼‰:

$$
\text{CI} = \hat{\theta} \pm 1.96 \sqrt{\frac{\hat{\theta}(1-\hat{\theta})}{n}} = 0.7 \pm 1.96 \sqrt{\frac{0.7 \times 0.3}{10}} = [0.416, 0.984]
$$

**ãƒ™ã‚¤ã‚ºæ¨å®š**ï¼ˆäº‹å‰åˆ†å¸ƒBeta(2,2)ã€å…±å½¹æ€§ã‚ˆã‚Šäº‹å¾Œåˆ†å¸ƒBeta(9, 5)ï¼‰:

$$
p(\theta | k=7, n=10) = \text{Beta}(9, 5)
$$

äº‹å¾Œå¹³å‡ï¼ˆç‚¹æ¨å®šï¼‰:

$$
\mathbb{E}[\theta | D] = \frac{\alpha}{\alpha + \beta} = \frac{9}{9+5} = 0.643
$$

95%ä¿¡ç”¨åŒºé–“ï¼ˆCredible Intervalï¼‰:

$$
\text{CrI} = [\text{quantile}(0.025), \text{quantile}(0.975)] \approx [0.366, 0.882]
$$

**è§£é‡ˆã®é•ã„**:

- **é »åº¦è«–CI**: ã€ŒåŒã˜å®Ÿé¨“ã‚’100å›ç¹°ã‚Šè¿”ã›ã°ã€95å›ã¯ã“ã®åŒºé–“ãŒçœŸã® $\theta$ ã‚’å«ã‚€ã€
- **ãƒ™ã‚¤ã‚ºCrI**: ã€Œãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ãŸä»Šã€$\theta$ ãŒã“ã®åŒºé–“ã«ã‚ã‚‹ç¢ºç‡ãŒ95%ã€ï¼ˆã‚ˆã‚Šç›´æ„Ÿçš„ï¼‰

#### 3.8.1 å…±å½¹äº‹å‰åˆ†å¸ƒ

**å®šç¾©**: äº‹å‰åˆ†å¸ƒã¨äº‹å¾Œåˆ†å¸ƒãŒåŒã˜åˆ†å¸ƒæ—ã«å±ã™ã‚‹ã¨ãã€ãã®äº‹å‰åˆ†å¸ƒã‚’å…±å½¹ã¨ã„ã†ã€‚

| å°¤åº¦ | å…±å½¹äº‹å‰åˆ†å¸ƒ | äº‹å¾Œåˆ†å¸ƒ |
|:-----|:-----------|:--------|
| äºŒé …åˆ†å¸ƒ | ãƒ™ãƒ¼ã‚¿åˆ†å¸ƒ | ãƒ™ãƒ¼ã‚¿åˆ†å¸ƒ |
| æ­£è¦åˆ†å¸ƒï¼ˆæ—¢çŸ¥åˆ†æ•£ï¼‰ | æ­£è¦åˆ†å¸ƒ | æ­£è¦åˆ†å¸ƒ |
| ãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒ | ã‚¬ãƒ³ãƒåˆ†å¸ƒ | ã‚¬ãƒ³ãƒåˆ†å¸ƒ |

**ä¾‹**: ã‚³ã‚¤ãƒ³æŠ•ã’ï¼ˆäºŒé …åˆ†å¸ƒï¼‰+ ãƒ™ãƒ¼ã‚¿äº‹å‰åˆ†å¸ƒ

$$
\begin{aligned}
\text{å°¤åº¦:} \quad & p(k | n, \theta) = \binom{n}{k} \theta^k (1-\theta)^{n-k} \\
\text{äº‹å‰åˆ†å¸ƒ:} \quad & p(\theta) = \text{Beta}(\alpha, \beta) \propto \theta^{\alpha-1} (1-\theta)^{\beta-1} \\
\text{äº‹å¾Œåˆ†å¸ƒ:} \quad & p(\theta | k, n) = \text{Beta}(\alpha + k, \beta + n - k)
\end{aligned}
$$

**æ•°å€¤æ¤œè¨¼**:

```julia
using Distributions, Plots

# äº‹å‰åˆ†å¸ƒ: Beta(2, 2) (å¼±ã„ä¿¡å¿µ: Î¸â‰ˆ0.5)
Î±, Î² = 2.0, 2.0
prior = Beta(Î±, Î²)

# ãƒ‡ãƒ¼ã‚¿: 10å›æŠ•ã’ã¦7å›è¡¨
n, k = 10, 7

# äº‹å¾Œåˆ†å¸ƒ: Beta(Î±+k, Î²+n-k) = Beta(9, 5)
posterior = Beta(Î± + k, Î² + n - k)

# å¯è¦–åŒ–
Î¸_range = 0:0.01:1
plot(Î¸_range, pdf.(prior, Î¸_range), label="äº‹å‰åˆ†å¸ƒ Beta(2,2)", linewidth=2)
plot!(Î¸_range, pdf.(posterior, Î¸_range), label="äº‹å¾Œåˆ†å¸ƒ Beta(9,5)", linewidth=2)
xlabel!("Î¸ (ã‚³ã‚¤ãƒ³ãŒè¡¨ã®ç¢ºç‡)")
ylabel!("å¯†åº¦")
title!("ãƒ™ã‚¤ã‚ºæ›´æ–°: ã‚³ã‚¤ãƒ³æŠ•ã’")
savefig("bayesian_update.png")
```

#### 3.8.2 MCMCï¼ˆMarkov Chain Monte Carloï¼‰

**å•é¡Œ**: äº‹å¾Œåˆ†å¸ƒ $p(\theta | D)$ ãŒè¤‡é›‘ã§è§£æçš„ã«è¨ˆç®—ã§ããªã„ã€‚

**MCMC**: ãƒãƒ«ã‚³ãƒ•é€£é–ã‚’ä½¿ã£ã¦äº‹å¾Œåˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã€‚

**Metropolis-Hastingsæ³•** [^3]:

1. åˆæœŸå€¤ $\theta^{(0)}$ ã‚’è¨­å®šã€‚
2. $t = 1, 2, \ldots$ ã«ã¤ã„ã¦:
   - ææ¡ˆåˆ†å¸ƒ $q(\theta' | \theta^{(t-1)})$ ã‹ã‚‰å€™è£œ $\theta'$ ã‚’ç”Ÿæˆã€‚
   - å—ç†ç¢ºç‡ã‚’è¨ˆç®—:
     $$
     \alpha = \min\left(1, \frac{p(\theta' | D) q(\theta^{(t-1)} | \theta')}{p(\theta^{(t-1)} | D) q(\theta' | \theta^{(t-1)})}\right)
     $$
   - ç¢ºç‡ $\alpha$ ã§ $\theta^{(t)} = \theta'$ã€ãã†ã§ãªã‘ã‚Œã° $\theta^{(t)} = \theta^{(t-1)}$ã€‚

**Turing.jlã§å®Ÿè£…**:

```julia
using Turing, Distributions, StatsPlots

# ãƒ¢ãƒ‡ãƒ«å®šç¾©: ã‚³ã‚¤ãƒ³æŠ•ã’ï¼ˆãƒ™ã‚¤ã‚ºæ¨å®šï¼‰
@model function coinflip(y)
    # äº‹å‰åˆ†å¸ƒ
    Î¸ ~ Beta(2, 2)

    # å°¤åº¦
    y ~ Binomial(length(y), Î¸)
end

# ãƒ‡ãƒ¼ã‚¿: 10å›ä¸­7å›è¡¨
data = 7

# MCMCã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆNUTS: No-U-Turn Sampler, Hamiltonian Monte Carloã®æ”¹è‰¯ç‰ˆï¼‰
chain = sample(coinflip([data]), NUTS(), 1000)

# äº‹å¾Œåˆ†å¸ƒã®å¯è¦–åŒ–
plot(chain)
```

> **Note:** **é€²æ—: 90% å®Œäº†** ãƒ™ã‚¤ã‚ºçµ±è¨ˆï¼ˆå…±å½¹äº‹å‰åˆ†å¸ƒãƒ»MCMCï¼‰ã‚’å®Œå…¨ç†è§£ã€‚å®Ÿé¨“è¨ˆç”»æ³•ã¸ã€‚

### 3.9 å®Ÿé¨“è¨ˆç”»æ³•ï¼ˆExperimental Designï¼‰

**ç›®çš„**: é™ã‚‰ã‚ŒãŸãƒªã‚½ãƒ¼ã‚¹ã§æœ€å¤§ã®æƒ…å ±ã‚’å¾—ã‚‹å®Ÿé¨“ã‚’è¨­è¨ˆã™ã‚‹ã€‚

#### 3.9.1 å®Œå…¨ç„¡ä½œç‚ºåŒ–ãƒ‡ã‚¶ã‚¤ãƒ³ï¼ˆCompletely Randomized Design, CRDï¼‰

**ç‰¹å¾´**: å‡¦ç†ï¼ˆtreatmentï¼‰ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«å‰²ã‚Šå½“ã¦ã‚‹ã€‚æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ã€‚

**æ¬ ç‚¹**: ãƒ–ãƒ­ãƒƒã‚¯é–“ã®å¤‰å‹•ï¼ˆä¾‹: æ¸¬å®šæ—¥ã®é•ã„ï¼‰ã‚’åˆ¶å¾¡ã§ããªã„ã€‚

#### 3.9.2 ä¹±å¡Šæ³•ï¼ˆRandomized Block Design, RBDï¼‰

**ç‰¹å¾´**: è¢«é¨“è€…ã‚’ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆä¾‹: å¹´é½¢å±¤ã€æ¸¬å®šæ—¥ï¼‰ã«åˆ†ã‘ã€å„ãƒ–ãƒ­ãƒƒã‚¯å†…ã§å‡¦ç†ã‚’ãƒ©ãƒ³ãƒ€ãƒ åŒ–ã€‚

**åˆ©ç‚¹**: ãƒ–ãƒ­ãƒƒã‚¯é–“å¤‰å‹•ã‚’é™¤å» â†’ æ®‹å·®ãŒå°ã•ããªã‚‹ â†’ æ¤œå‡ºåŠ›å‘ä¸Šã€‚

#### 3.9.3 ãƒ©ãƒ†ãƒ³æ–¹æ ¼ï¼ˆLatin Square Designï¼‰

**ç‰¹å¾´**: 2ã¤ã®è¦å› ï¼ˆä¾‹: è¡Œ=æ—¥ã€åˆ—=æ©Ÿæ¢°ï¼‰ã‚’åŒæ™‚ã«åˆ¶å¾¡ã€‚

**åˆ¶ç´„**: å‡¦ç†æ•° = è¡Œæ•° = åˆ—æ•°ã€‚

#### 3.9.4 ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºè¨­è¨ˆï¼ˆPower Analysisï¼‰

**å•é¡Œ**: å®Ÿé¨“å‰ã«å¿…è¦ãªã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’æ±ºå®šã€‚

**æ‰‹é †**:

1. æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœé‡ $d$ ã‚’è¨­å®šï¼ˆéå»ã®ç ”ç©¶ã‚„äºˆå‚™å®Ÿé¨“ã‹ã‚‰ï¼‰ã€‚
2. æœ‰æ„æ°´æº– $\alpha$ ã‚’è¨­å®šï¼ˆé€šå¸¸0.05ï¼‰ã€‚
3. ç›®æ¨™æ¤œå‡ºåŠ› $1 - \beta$ ã‚’è¨­å®šï¼ˆé€šå¸¸0.8ï¼‰ã€‚
4. æ¤œå®šã®ç¨®é¡ã«å¿œã˜ãŸå…¬å¼ã¾ãŸã¯ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã§ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’è¨ˆç®—ã€‚

**tæ¤œå®šã®ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºå…¬å¼**ï¼ˆå†æ²ï¼‰:

$$
n = \frac{2(z_{1-\alpha/2} + z_{1-\beta})^2}{d^2}
$$

### 6.11 ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **ã€Œp < 0.05ã§æœ‰æ„ã€ã¨è¨€ãˆã‚‹ã€‚ã ãŒã€ãã‚Œã¯æœ¬å½“ã«**ã‚ãªãŸã®ä¸»å¼µ**ã‚’æ”¯æŒã—ã¦ã„ã‚‹ã®ã‹ï¼Ÿ**

ä»¥ä¸‹ã®ã‚·ãƒŠãƒªã‚ªã‚’è€ƒãˆã‚ˆã†:

1. **ã‚·ãƒŠãƒªã‚ªA**: æ–°ã—ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ‰‹æ³•ã‚’10ç¨®é¡è©¦ã—ã€1ã¤ã ã‘p < 0.05ã§æœ‰æ„ãªæ”¹å–„ã€‚ä»–9ã¤ã¯æœ‰æ„å·®ãªã—ã€‚
2. **ã‚·ãƒŠãƒªã‚ªB**: åŒã˜å®Ÿé¨“ã‚’100å›è¡Œã„ã€æœ‰æ„ã ã£ãŸ5å›ã ã‘è«–æ–‡ã«å ±å‘Šã€‚
3. **ã‚·ãƒŠãƒªã‚ªC**: ãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ã¦ã‹ã‚‰ã€Œã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯åŠ¹æœãŒã‚ã‚‹ã€ã¨äº‹å¾Œçš„ã«ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æã€‚

**å…¨ã¦çµ±è¨ˆçš„ã«ã¯ã€Œp < 0.05ã€ã ãŒã€ç§‘å­¦çš„ã«ã¯ç„¡æ„å‘³ã ã€‚**

- **ã‚·ãƒŠãƒªã‚ªA**: å¤šé‡æ¯”è¼ƒã®ç½ ã€‚Bonferroniè£œæ­£ã™ã‚Œã°p = 0.05 Ã— 10 = 0.5ã§æœ‰æ„ã§ãªã„ã€‚
- **ã‚·ãƒŠãƒªã‚ªB**: å‡ºç‰ˆãƒã‚¤ã‚¢ã‚¹ã€‚å¤±æ•—ã—ãŸ95å›ã‚’éš è”½ã€‚
- **ã‚·ãƒŠãƒªã‚ªC**: p-hackingã€‚ãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ã¦ã‹ã‚‰ä»®èª¬ã‚’ç«‹ã¦ã‚‹ã€‚

**è­°è«–ã®ç¨®**:

1. **äº‹å‰ç™»éŒ²ï¼ˆPre-registrationï¼‰**ã¯è§£æ±ºç­–ã‹ï¼Ÿã€€å®Ÿé¨“å‰ã«ä»®èª¬ãƒ»æ‰‹æ³•ã‚’å…¬é–‹ç™»éŒ²ã™ã‚Œã°ã€p-hackingã‚’é˜²ã’ã‚‹ã€‚ã ãŒæŸ”è»Ÿæ€§ãŒå¤±ã‚ã‚Œã‚‹ã€‚
2. **på€¤ã®ä»£æ›¿æ¡ˆ**ã¯ï¼Ÿã€€ä¿¡é ¼åŒºé–“ãƒ»åŠ¹æœé‡ãƒ»ãƒ™ã‚¤ã‚ºãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã¯ã€på€¤ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ã‹ï¼Ÿ
3. **çµ±è¨ˆçš„æœ‰æ„æ€§ã®åŸºæº–ï¼ˆÎ±=0.05ï¼‰**ã¯æ£æ„çš„ã§ã¯ãªã„ã‹ï¼Ÿã€€ãªãœ0.05ãªã®ã‹ï¼Ÿã€€0.01ã‚„0.001ã§ã¯ãƒ€ãƒ¡ãªã®ã‹ï¼Ÿ

ã“ã®å•ã„ã«å®Œå…¨ãªç­”ãˆã¯ãªã„ã€‚ã ãŒ**çµ±è¨ˆå­¦ã¯é“å…·ã§ã‚ã‚Šã€é“å…·ã®ä½¿ã„æ–¹æ¬¡ç¬¬ã§ç§‘å­¦çš„èª å®Ÿã•ãŒå•ã‚ã‚Œã‚‹**ã“ã¨ã‚’å¿˜ã‚Œã¦ã¯ãªã‚‰ãªã„ã€‚

> **Note:** **é€²æ—: 100% å®Œäº†** ğŸ‰ è¬›ç¾©å®Œèµ°ï¼

---


> Progress: [85%]
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. ANOVAã®Fçµ±è¨ˆé‡ãŒç¾¤é–“åˆ†æ•£ã¨ç¾¤å†…åˆ†æ•£ã®æ¯”ã§æ§‹æˆã•ã‚Œã‚‹æ•°å­¦çš„æ„å‘³ã‚’è¿°ã¹ã‚ˆã€‚
> 2. ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®ãƒªãƒ³ã‚¯é–¢æ•°ãŒlogitã§ã‚ã‚‹ç†ç”±ã‚’ç¢ºç‡ã®ç¯„å›²ã®åˆ¶ç´„ã‹ã‚‰èª¬æ˜ã›ã‚ˆã€‚

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Neyman, J., & Pearson, E. S. (1928). *On the Use and Interpretation of Certain Test Criteria for Purposes of Statistical Inference: Part I*. Biometrika.
<https://www.jstor.org/stable/2331945>

[^2]: Benjamini, Y., & Hochberg, Y. (1995). *Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing*. Journal of the Royal Statistical Society: Series B.
<https://doi.org/10.1111/j.2517-6161.1995.tb02031.x>

[^3]: Hastings, W. K. (1970). *Monte Carlo Sampling Methods Using Markov Chains and Their Applications*. Biometrika.
<https://doi.org/10.1093/biomet/57.1.97>


### æ•™ç§‘æ›¸

- **Statistical Inference** - Casella & Berger (2002): é »åº¦è«–çµ±è¨ˆã®æ±ºå®šç‰ˆã€‚å¤§å­¦é™¢ãƒ¬ãƒ™ãƒ«ã€‚
- **Bayesian Data Analysis** - Gelman et al. (2013): ãƒ™ã‚¤ã‚ºçµ±è¨ˆã®æ¨™æº–æ•™ç§‘æ›¸ã€‚
- **The Elements of Statistical Learning** - Hastie, Tibshirani, Friedman (2009): æ©Ÿæ¢°å­¦ç¿’Ã—çµ±è¨ˆã®èåˆã€‚[ç„¡æ–™PDF](https://web.stanford.edu/~hastie/ElemStatLearn/)
- **çµ±è¨ˆå­¦å…¥é–€** - æ±äº¬å¤§å­¦æ•™é¤Šå­¦éƒ¨çµ±è¨ˆå­¦æ•™å®¤ (1991): æ—¥æœ¬èªã®å®šç•ªå…¥é–€æ›¸ã€‚

### ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹

- [StatQuest (YouTube)](https://www.youtube.com/@statquest): çµ±è¨ˆå­¦ã®ç›´æ„Ÿçš„è§£èª¬å‹•ç”»ã€‚
- [StatsBase.jl Documentation](https://juliastats.org/StatsBase.jl/stable/)
- [HypothesisTests.jl Documentation](https://juliastats.org/HypothesisTests.jl/stable/)
- [GLM.jl Documentation](https://juliastats.org/GLM.jl/stable/)
- [Turing.jl Documentation](https://turinglang.org/stable/)

---

## ä»˜éŒ²A: çµ±è¨ˆå­¦ã®æ­´å²çš„ç™ºå±•

### A.1 é »åº¦è«–çµ±è¨ˆã®èª•ç”Ÿï¼ˆ1900-1950å¹´ä»£ï¼‰

| å¹´ | äººç‰© | è²¢çŒ® |
|:---|:-----|:-----|
| 1900 | Karl Pearson | ã‚«ã‚¤äºŒä¹—æ¤œå®šã€Pearsonç›¸é–¢ä¿‚æ•° |
| 1908 | William Gosset (Student) | tåˆ†å¸ƒã€tæ¤œå®šï¼ˆå°‘ã‚µãƒ³ãƒ—ãƒ«çµ±è¨ˆï¼‰ |
| 1920å¹´ä»£ | Ronald Fisher | æœ€å°¤æ¨å®šï¼ˆMLEï¼‰ã€åˆ†æ•£åˆ†æï¼ˆANOVAï¼‰ã€å®Ÿé¨“è¨ˆç”»æ³• |
| 1928 | Neyman & Pearson | Neyman-Pearsonä»®èª¬æ¤œå®šæ çµ„ã¿ [^1] |
| 1935 | Fisher | ãƒ©ãƒ³ãƒ€ãƒ åŒ–æ¯”è¼ƒè©¦é¨“ï¼ˆRCTï¼‰ã®åŸç† |

**é »åº¦è«–ã®å“²å­¦**: ç¢ºç‡ = é•·æœŸçš„é »åº¦ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å›ºå®šå€¤ï¼ˆæœªçŸ¥ï¼‰ã€‚å®¢è¦³æ€§ã‚’é‡è¦–ã€‚

### A.2 ãƒ™ã‚¤ã‚ºçµ±è¨ˆã®å¾©èˆˆï¼ˆ1950-1990å¹´ä»£ï¼‰

| å¹´ | äººç‰©/å‡ºæ¥äº‹ | è²¢çŒ® |
|:---|:----------|:-----|
| 1763 | Thomas Bayesï¼ˆæ­»å¾Œå‡ºç‰ˆï¼‰ | ãƒ™ã‚¤ã‚ºã®å®šç†ã®åŸå‹ |
| 1950å¹´ä»£ | Dennis Lindley | ãƒ™ã‚¤ã‚ºæ±ºå®šç†è«– |
| 1953 | Metropolis et al. | Metropolisã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆMCMCï¼‰ [^3] |
| 1970 | Hastings | Metropolis-Hastingsã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  |
| 1990 | Gelfand & Smith | Gibbs Samplingã®å®Ÿç”¨åŒ– |

**ãƒ™ã‚¤ã‚ºå¾©èˆˆã®ç†ç”±**: ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã®ç™ºå±•ã§MCMCãŒå®Ÿç”¨åŒ– â†’ è¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«ã®äº‹å¾Œåˆ†å¸ƒã‚’è¨ˆç®—å¯èƒ½ã«ã€‚

### A.3 ç¾ä»£çµ±è¨ˆå­¦ï¼ˆ1990å¹´ä»£ã€œç¾åœ¨ï¼‰

| å¹´ | æ‰‹æ³• | è²¢çŒ® |
|:---|:-----|:-----|
| 1995 | Benjamini & Hochberg | FDRåˆ¶å¾¡æ³•ï¼ˆå¤šé‡æ¯”è¼ƒï¼‰ [^2] |
| 2000å¹´ä»£ | ãƒ™ã‚¤ã‚ºãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | ç„¡é™æ¬¡å…ƒãƒ¢ãƒ‡ãƒ«ï¼ˆDirichlet Processç­‰ï¼‰ |
| 2010å¹´ä»£ | Hamiltonian Monte Carlo (HMC) | é«˜æ¬¡å…ƒMCMCã®é«˜é€ŸåŒ–ï¼ˆNUTSï¼‰ |
| 2015å¹´ä»£ | å› æœæ¨è«–ã®æ™®åŠ | Pearl/Rubinæ çµ„ã¿ã®çµ±åˆã€æ©Ÿæ¢°å­¦ç¿’ã¨ã®èåˆ |
| 2020å¹´ä»£ | ç¢ºç‡çš„ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚° | Turing.jl, PyMC, Stanç­‰ã®æˆç†Ÿ |

---

## ä»˜éŒ²B: Juliaã§ä½¿ãˆã‚‹çµ±è¨ˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å®Œå…¨ãƒªã‚¹ãƒˆ

### B.1 åŸºç¤çµ±è¨ˆ

| ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ | ç”¨é€” | ä¸»è¦é–¢æ•° |
|:----------|:-----|:---------|
| **Statistics** (stdlib) | åŸºæœ¬çµ±è¨ˆé‡ | `mean`, `std`, `var`, `median`, `quantile`, `cor`, `cov` |
| **StatsBase.jl** | è¨˜è¿°çµ±è¨ˆãƒ»é‡ã¿ä»˜ãçµ±è¨ˆ | `skewness`, `kurtosis`, `mad`, `mode`, `sem`, `zscore`, `sample`, `weights` |
| **Distributions.jl** | ç¢ºç‡åˆ†å¸ƒ | `Normal`, `Beta`, `Gamma`, `Binomial`, `Poisson`, `TDist`, `FDist`, `pdf`, `cdf`, `quantile`, `rand` |

### B.2 ä»®èª¬æ¤œå®š

| ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ | ç”¨é€” | ä¸»è¦æ¤œå®š |
|:----------|:-----|:---------|
| **HypothesisTests.jl** | ä»®èª¬æ¤œå®šå…¨èˆ¬ | `OneSampleTTest`, `EqualVarianceTTest`, `UnequalVarianceTTest`, `MannWhitneyUTest`, `WilcoxonSignedRankTest`, `KruskalWallisTest`, `OneWayANOVATest`, `ChisqTest`, `FisherExactTest`, `KSTest`, `AndersonDarlingTest` |
| **MultipleTesting.jl** | å¤šé‡æ¯”è¼ƒè£œæ­£ | `adjust`, `Bonferroni`, `Holm`, `BenjaminiHochberg`, `BenjaminiYekutieli` |

### B.3 å›å¸°ãƒ»GLM

| ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ | ç”¨é€” | ä¸»è¦é–¢æ•° |
|:----------|:-----|:---------|
| **GLM.jl** | ä¸€èˆ¬åŒ–ç·šå½¢ãƒ¢ãƒ‡ãƒ« | `glm`, `@formula`, `Binomial`, `Poisson`, `Gamma`, `LogitLink`, `LogLink`, `InverseLink`, `coef`, `confint`, `predict` |
| **MixedModels.jl** | æ··åˆåŠ¹æœãƒ¢ãƒ‡ãƒ« | `LinearMixedModel`, `fit!`, `ranef`, `fixef` |

### B.4 ãƒ™ã‚¤ã‚ºçµ±è¨ˆ

| ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ | ç”¨é€” | ä¸»è¦é–¢æ•°/ãƒã‚¯ãƒ­ |
|:----------|:-----|:---------------|
| **Turing.jl** | ç¢ºç‡çš„ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚° | `@model`, `~`, `sample`, `NUTS`, `HMC`, `Gibbs`, `plot`, `summarize` |
| **AdvancedMH.jl** | MCMCæ‹¡å¼µ | `MetropolisHastings`, `RWMH`, `StaticMH` |
| **MCMCChains.jl** | MCMCçµæœã®è§£æ | `Chains`, `describe`, `plot`, `ess`, `gelmandiag` |
| **AbstractMCMC.jl** | MCMCã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ | MCMCå®Ÿè£…ã®å…±é€šåŸºç›¤ |

### B.5 ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ãƒ»ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

| ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ | ç”¨é€” | ä¸»è¦é–¢æ•° |
|:----------|:-----|:---------|
| **Bootstrap.jl** | ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—æ³• | `bootstrap`, `BasicSampling`, `confint`, `PercentileConfInt`, `BCaConfInt` |

### B.6 ç”Ÿå­˜æ™‚é–“è§£æ

| ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ | ç”¨é€” | ä¸»è¦é–¢æ•° |
|:----------|:-----|:---------|
| **Survival.jl** | ç”Ÿå­˜æ™‚é–“è§£æ | `Surv`, `kaplan_meier`, `cox_ph`, `nelson_aalen` |

### B.7 æ™‚ç³»åˆ—è§£æ

| ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ | ç”¨é€” | ä¸»è¦é–¢æ•° |
|:----------|:-----|:---------|
| **TimeSeries.jl** | æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ | `TimeArray`, `values`, `timestamp`, `lag`, `lead`, `diff` |
| **StateSpaceModels.jl** | çŠ¶æ…‹ç©ºé–“ãƒ¢ãƒ‡ãƒ« | `StateSpaceModel`, `kalman_filter`, `smoother` |

### B.8 å®Ÿé¨“è¨ˆç”»æ³•

| ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ | ç”¨é€” | ä¸»è¦é–¢æ•° |
|:----------|:-----|:---------|
| **ExperimentalDesign.jl** | å®Ÿé¨“è¨ˆç”» | `factorial_design`, `latin_square`, `balanced_design` |

### B.9 å¯è¦–åŒ–

| ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ | ç”¨é€” | ä¸»è¦é–¢æ•° |
|:----------|:-----|:---------|
| **StatsPlots.jl** | çµ±è¨ˆçš„ãƒ—ãƒ­ãƒƒãƒˆ | `boxplot`, `violin`, `density`, `marginalscatter`, `corrplot`, `@df` |
| **Makie.jl** | é«˜å“è³ªå¯è¦–åŒ– | `scatter`, `lines`, `barplot`, `heatmap`, `density` |
| **AlgebraOfGraphics.jl** | Grammar of Graphics | `data`, `mapping`, `visual`, `draw` |

---

## ä»˜éŒ²C: çµ±è¨ˆå­¦ã®ä¸»è¦å®šç†ã¾ã¨ã‚

### C.1 ç¢ºç‡è«–ã®åŸºç¤å®šç†

**å¤§æ•°ã®æ³•å‰‡ï¼ˆLaw of Large Numbersï¼‰**:

$$
\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i \xrightarrow{p} \mu \quad \text{as } n \to \infty
$$

æ¨™æœ¬å¹³å‡ã¯æ¯å¹³å‡ã«ç¢ºç‡åæŸã™ã‚‹ã€‚

**ä¸­å¿ƒæ¥µé™å®šç†ï¼ˆCentral Limit Theoremï¼‰**:

$$
\sqrt{n} \frac{\bar{X}_n - \mu}{\sigma} \xrightarrow{d} \mathcal{N}(0, 1) \quad \text{as } n \to \infty
$$

æ¨™æœ¬å¹³å‡ã®åˆ†å¸ƒã¯æ­£è¦åˆ†å¸ƒã«è¿‘ã¥ãï¼ˆæ¯é›†å›£åˆ†å¸ƒã«é–¢ã‚ã‚‰ãšï¼‰ã€‚

### C.2 æ¨å®šã®ç†è«–

**CramÃ©r-Raoä¸‹ç•Œï¼ˆCramÃ©r-Rao Lower Boundï¼‰**:

ä¸åæ¨å®šé‡ $\hat{\theta}$ ã®åˆ†æ•£ã¯æ¬¡ã®ä¸‹ç•Œã‚’æŒã¤:

$$
\text{Var}(\hat{\theta}) \geq \frac{1}{I(\theta)}
$$

ã“ã“ã§ $I(\theta)$ ã¯Fisheræƒ…å ±é‡ã€‚ç­‰å·æˆç«‹æ™‚ã¯**æœ‰åŠ¹æ¨å®šé‡**ã€‚

**æ¼¸è¿‘æ­£è¦æ€§ï¼ˆAsymptotic Normalityï¼‰**:

MLEã¯æ¼¸è¿‘çš„ã«æ­£è¦åˆ†å¸ƒã«å¾“ã†:

$$
\sqrt{n}(\hat{\theta}_{\text{MLE}} - \theta) \xrightarrow{d} \mathcal{N}(0, I(\theta)^{-1})
$$

### C.3 æ¤œå®šã®ç†è«–

**Neyman-Pearsonè£œé¡Œï¼ˆNeyman-Pearson Lemmaï¼‰**:

å°¤åº¦æ¯”æ¤œå®šã¯æ‰€å®šã®æœ‰æ„æ°´æº– $\alpha$ ã§æœ€ã‚‚æ¤œå‡ºåŠ›ãŒé«˜ã„ï¼ˆmost powerful testï¼‰ã€‚

$$
\frac{p(x | H_1)}{p(x | H_0)} > c \quad \Rightarrow \quad \text{reject } H_0
$$

### C.4 ãƒ™ã‚¤ã‚ºçµ±è¨ˆã®å®šç†

**ãƒ™ã‚¤ã‚ºã®å®šç†ï¼ˆBayes' Theoremï¼‰**:

$$
p(\theta | D) = \frac{p(D | \theta) p(\theta)}{p(D)} = \frac{p(D | \theta) p(\theta)}{\int p(D | \theta') p(\theta') d\theta'}
$$

**ãƒãƒ«ã‚³ãƒ•é€£é–ã®åæŸ**:

é©åˆ‡ãªæ¡ä»¶ä¸‹ã§MCMCã‚µãƒ³ãƒ—ãƒ«ã¯äº‹å¾Œåˆ†å¸ƒã«åæŸ:

$$
\lim_{t \to \infty} \theta^{(t)} \sim p(\theta | D)
$$

---

## ä»˜éŒ²D: çµ±è¨ˆå­¦ã®å®Ÿè·µãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### D.1 å®Ÿé¨“å‰ï¼ˆäº‹å‰è¨ˆç”»ï¼‰

- [ ] ç ”ç©¶ä»®èª¬ã‚’æ˜ç¢ºã«å®šç¾©ï¼ˆ$H_0$, $H_1$ï¼‰
- [ ] æœ‰æ„æ°´æº– $\alpha$ ã‚’æ±ºå®šï¼ˆé€šå¸¸0.05ï¼‰
- [ ] ç›®æ¨™æ¤œå‡ºåŠ›ã‚’æ±ºå®šï¼ˆé€šå¸¸0.8ï¼‰
- [ ] æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœé‡ã‚’è¨­å®šï¼ˆéå»ç ”ç©¶ãƒ»äºˆå‚™å®Ÿé¨“ã‹ã‚‰ï¼‰
- [ ] ãƒ‘ãƒ¯ãƒ¼åˆ†æã§å¿…è¦ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’è¨ˆç®—
- [ ] æ¤œå®šæ‰‹æ³•ã‚’äº‹å‰ã«æ±ºå®šï¼ˆtæ¤œå®šãƒ»ANOVAãƒ»ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ç­‰ï¼‰
- [ ] å¤šé‡æ¯”è¼ƒãŒã‚ã‚‹å ´åˆã¯è£œæ­£æ–¹æ³•ã‚’æ±ºå®šï¼ˆBonferroniãƒ»BHç­‰ï¼‰
- [ ] äº‹å‰ç™»éŒ²ï¼ˆPre-registrationï¼‰ã‚’æ¤œè¨ï¼ˆp-hackingã‚’é˜²ãï¼‰

### D.2 ãƒ‡ãƒ¼ã‚¿åé›†

- [ ] ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ»ãƒ©ãƒ³ãƒ€ãƒ åŒ–ã‚’å¾¹åº•
- [ ] ãƒ–ãƒ­ãƒƒã‚¯è¦å› ãŒã‚ã‚Œã°ä¹±å¡Šæ³•ã‚’æ¤œè¨
- [ ] æ¸¬å®šèª¤å·®ã‚’æœ€å°åŒ–ï¼ˆæ©Ÿå™¨ã®æ ¡æ­£ãƒ»ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã®æ¨™æº–åŒ–ï¼‰
- [ ] æ¬ æãƒ‡ãƒ¼ã‚¿ã®è¨˜éŒ²ãƒ»ç†ç”±ã®è¨˜è¼‰
- [ ] å¤–ã‚Œå€¤ã®è¨˜éŒ²ï¼ˆå‰Šé™¤å‰ã«ç†ç”±ã‚’æ˜è¨˜ï¼‰

### D.3 è¨˜è¿°çµ±è¨ˆ

- [ ] å¹³å‡ãƒ»ä¸­å¤®å€¤ãƒ»æ¨™æº–åå·®ãƒ»IQRã‚’è¨ˆç®—
- [ ] æ­ªåº¦ãƒ»å°–åº¦ã‚’ç¢ºèªï¼ˆåˆ†å¸ƒã®å½¢çŠ¶ï¼‰
- [ ] å¤–ã‚Œå€¤ã®æ¤œå‡ºï¼ˆIQRæ³•ãƒ»Grubbsæ¤œå®šï¼‰
- [ ] ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ãƒ»ç®±ã²ã’å›³ã§å¯è¦–åŒ–

### D.4 æ¨æ¸¬çµ±è¨ˆ

- [ ] å‰ææ¡ä»¶ã®ç¢ºèªï¼ˆæ­£è¦æ€§ãƒ»ç­‰åˆ†æ•£æ€§ãƒ»ç‹¬ç«‹æ€§ï¼‰
- [ ] æ­£è¦æ€§æ¤œå®šï¼ˆShapiro-Wilkãƒ»Kolmogorov-Smirnovï¼‰
- [ ] ç­‰åˆ†æ•£æ€§æ¤œå®šï¼ˆLeveneãƒ»Bartlettï¼‰
- [ ] å‰æãŒæº€ãŸã•ã‚Œãªã„å ´åˆã¯ä»£æ›¿æ‰‹æ³•ï¼ˆãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ãƒ»å¤‰æ›ãƒ»é ‘å¥ãªæ‰‹æ³•ï¼‰

### D.5 ä»®èª¬æ¤œå®š

- [ ] æ¤œå®šçµ±è¨ˆé‡ï¼ˆt, F, Ï‡Â², Uç­‰ï¼‰ã‚’è¨ˆç®—
- [ ] è‡ªç”±åº¦ã‚’ç¢ºèª
- [ ] på€¤ã‚’è¨ˆç®—
- [ ] åŠ¹æœé‡ï¼ˆCohen's d, partial Î·Â², rÂ²ç­‰ï¼‰ã‚’è¨ˆç®—
- [ ] ä¿¡é ¼åŒºé–“ã‚’ä½µè¨˜
- [ ] å¤šé‡æ¯”è¼ƒè£œæ­£ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰

### D.6 çµæœã®å ±å‘Š

- [ ] è¨˜è¿°çµ±è¨ˆï¼ˆM, SD, nï¼‰ã‚’å ±å‘Š
- [ ] æ¤œå®šçµ±è¨ˆé‡ãƒ»è‡ªç”±åº¦ãƒ»på€¤ã‚’å ±å‘Šï¼ˆä¾‹: $t(9) = 60.0, p < .001$ï¼‰
- [ ] åŠ¹æœé‡ã‚’å ±å‘Šï¼ˆä¾‹: $d = 6.0$ï¼‰
- [ ] 95%ä¿¡é ¼åŒºé–“ã‚’å ±å‘Šï¼ˆä¾‹: $95\% \text{CI} [0.768, 0.782]$ï¼‰
- [ ] å¤šé‡æ¯”è¼ƒè£œæ­£æ–¹æ³•ã‚’æ˜è¨˜
- [ ] å›³è¡¨ã§è¦–è¦šåŒ–ï¼ˆç®±ã²ã’å›³ãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ¼ä»˜ãæ£’ã‚°ãƒ©ãƒ•ç­‰ï¼‰
- [ ] çµ±è¨ˆçš„æœ‰æ„æ€§ã¨å®Ÿç”¨çš„æœ‰æ„æ€§ã‚’åŒºåˆ¥

### D.7 è§£é‡ˆãƒ»è­°è«–

- [ ] på€¤ã®æ­£ã—ã„è§£é‡ˆï¼ˆã€Œ$H_0$ãŒçœŸã§ã‚ã‚‹ç¢ºç‡ã€ã§ã¯ãªã„ï¼‰
- [ ] åŠ¹æœé‡ã®å®Ÿç”¨çš„æ„ç¾©ã‚’è­°è«–
- [ ] æ¤œå‡ºåŠ›ä¸è¶³ã®å¯èƒ½æ€§ã‚’æ¤œè¨ï¼ˆp > 0.05ã®å ´åˆï¼‰
- [ ] ä»£æ›¿èª¬æ˜ï¼ˆäº¤çµ¡å› å­ï¼‰ã®å¯èƒ½æ€§ã‚’è­°è«–
- [ ] é™ç•Œï¼ˆã‚µãƒ³ãƒ—ãƒ«é¸æŠãƒã‚¤ã‚¢ã‚¹ãƒ»æ¸¬å®šèª¤å·®ç­‰ï¼‰ã‚’æ˜è¨˜
- [ ] å› æœé–¢ä¿‚ã¨ç›¸é–¢ã®åŒºåˆ¥

---

## ä»˜éŒ²B: GLMç™ºå±•ãƒˆãƒ”ãƒƒã‚¯ã¨æœ€æ–°æ‰‹æ³•

### B.1 æ··åˆåŠ¹æœãƒ¢ãƒ‡ãƒ«ï¼ˆMixed Effects Modelsï¼‰

**å•é¡Œ**: ãƒ‡ãƒ¼ã‚¿ã«éšå±¤æ§‹é€ ãŒã‚ã‚‹å ´åˆï¼ˆä¾‹: ç”Ÿå¾’â†’ã‚¯ãƒ©ã‚¹â†’å­¦æ ¡ï¼‰ã€è¦³æ¸¬ãŒç‹¬ç«‹ã§ãªã„ã€‚

**ç·šå½¢æ··åˆåŠ¹æœãƒ¢ãƒ‡ãƒ«ï¼ˆLMMï¼‰**:

$$
y_{ij} = \beta_0 + \beta_1 x_{ij} + u_i + \epsilon_{ij}
$$

ã“ã“ã§:
- $y_{ij}$: ã‚°ãƒ«ãƒ¼ãƒ—$i$ã®è¦³æ¸¬$j$ã®å¿œç­”å¤‰æ•°
- $u_i \sim \mathcal{N}(0, \sigma_u^2)$: ã‚°ãƒ«ãƒ¼ãƒ—ãƒ¬ãƒ™ãƒ«ã®ãƒ©ãƒ³ãƒ€ãƒ åŠ¹æœ
- $\epsilon_{ij} \sim \mathcal{N}(0, \sigma^2)$: å€‹ä½“ãƒ¬ãƒ™ãƒ«ã®èª¤å·®

**å›ºå®šåŠ¹æœ vs ãƒ©ãƒ³ãƒ€ãƒ åŠ¹æœ**:

| é …ç›® | å›ºå®šåŠ¹æœ | ãƒ©ãƒ³ãƒ€ãƒ åŠ¹æœ |
|:-----|:--------|:-----------|
| è§£é‡ˆ | æ¯é›†å›£å…¨ä½“ã®å¹³å‡åŠ¹æœ | ã‚°ãƒ«ãƒ¼ãƒ—é–“ã®ã°ã‚‰ã¤ã |
| æ¨å®š | ä¿‚æ•°$\beta$ | åˆ†æ•£æˆåˆ†$\sigma_u^2$ |
| ç›®çš„ | åŠ¹æœã®å¤§ãã•ã‚’çŸ¥ã‚ŠãŸã„ | ã‚°ãƒ«ãƒ¼ãƒ—é–“å¤‰å‹•ã‚’åˆ¶å¾¡ã—ãŸã„ |

**Juliaå®Ÿè£…ä¾‹**ï¼ˆMixedModels.jlï¼‰:

```julia
using MixedModels, DataFrames, RDatasets

# ãƒ‡ãƒ¼ã‚¿: sleepstudyï¼ˆç¡çœ ä¸è¶³ãŒåå¿œæ™‚é–“ã«ä¸ãˆã‚‹å½±éŸ¿ï¼‰
sleepstudy = dataset("lme4", "sleepstudy")

# æ··åˆåŠ¹æœãƒ¢ãƒ‡ãƒ«: åå¿œæ™‚é–“ ~ æ—¥æ•° + (1 + æ—¥æ•° | è¢«é¨“è€…)
# å›ºå®šåŠ¹æœ: æ—¥æ•°ã®åŠ¹æœ
# ãƒ©ãƒ³ãƒ€ãƒ åŠ¹æœ: è¢«é¨“è€…ã”ã¨ã®åˆ‡ç‰‡ã¨ã‚¹ãƒ­ãƒ¼ãƒ—
fm = fit(MixedModel, @formula(Reaction ~ Days + (1 + Days | Subject)), sleepstudy)

println(fm)

# ãƒ©ãƒ³ãƒ€ãƒ åŠ¹æœã®å¯è¦–åŒ–
ranef_df = DataFrame(ranef(fm)[:Subject])
```

å‡ºåŠ›ä¾‹:
```
Linear mixed model fit by maximum likelihood
 Reaction ~ 1 + Days + (1 + Days | Subject)
   logLik   -2 logLik     AIC       AICc        BIC
  -875.97    1751.94   1763.94   1764.47   1783.10

Variance components:
            Column    Variance   Std.Dev.   Corr.
Subject  (Intercept)  612.100    24.741
         Days          35.072     5.923    0.07
Residual              654.941    25.592
```

### B.2 ä¸€èˆ¬åŒ–åŠ æ³•ãƒ¢ãƒ‡ãƒ«ï¼ˆGAM: Generalized Additive Modelsï¼‰

**å•é¡Œ**: ç·šå½¢æ€§ã®ä»®å®šãŒå³ã—ã™ãã‚‹å ´åˆã€éç·šå½¢é–¢ä¿‚ã‚’æŸ”è»Ÿã«ãƒ¢ãƒ‡ãƒ«åŒ–ã—ãŸã„ã€‚

**GAMã®å®šå¼åŒ–**:

$$
g(\mu) = \beta_0 + f_1(x_1) + f_2(x_2) + \cdots + f_p(x_p)
$$

ã“ã“ã§$f_i$ã¯ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°é–¢æ•°ï¼ˆã‚¹ãƒ—ãƒ©ã‚¤ãƒ³ç­‰ï¼‰ã€‚

**ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ã‚¹ãƒ—ãƒ©ã‚¤ãƒ³**:

$$
\min_f \sum_{i=1}^n (y_i - f(x_i))^2 + \lambda \int (f''(x))^2 dx
$$

ç¬¬1é …: ãƒ•ã‚£ãƒƒãƒˆã€ç¬¬2é …: æ»‘ã‚‰ã‹ã•ã®ãƒšãƒŠãƒ«ãƒ†ã‚£

**Juliaã§ã®ç°¡æ˜“å®Ÿè£…**:

```julia
using GLM, DataFrames, Plots

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: éç·šå½¢é–¢ä¿‚
x = range(0, 10, length=100)
y_true = sin.(x) .+ 0.5 .* x
y = y_true .+ randn(100) .* 0.3

# å¤šé …å¼åŸºåº•å±•é–‹ã§GAMã‚’è¿‘ä¼¼
function polynomial_features(x, degree)
    hcat([x.^d for d in 0:degree]...)
end

# æ¬¡æ•°5ã®å¤šé …å¼GAM
X_poly = polynomial_features(x, 5)
df = DataFrame(X_poly, :auto)
df.y = y

model = lm(@formula(y ~ x1 + x2 + x3 + x4 + x5), df)

# äºˆæ¸¬ã¨å¯è¦–åŒ–
y_pred = predict(model)

plot(x, y, seriestype=:scatter, label="Data", alpha=0.5)
plot!(x, y_true, linewidth=2, label="True function")
plot!(x, y_pred, linewidth=2, label="GAM fit", linestyle=:dash)
xlabel!("x")
ylabel!("y")
```

### B.3 ã‚¼ãƒ­éå‰°ãƒ¢ãƒ‡ãƒ«ï¼ˆZero-Inflated Modelsï¼‰

**å•é¡Œ**: ã‚«ã‚¦ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã«ã‚¼ãƒ­ãŒéå‰°ã«å«ã¾ã‚Œã‚‹ï¼ˆä¾‹: ç—…é™¢å—è¨ºå›æ•°ã€äº‹æ•…ä»¶æ•°ï¼‰ã€‚

**ã‚¼ãƒ­éå‰°ãƒã‚¢ã‚½ãƒ³ãƒ¢ãƒ‡ãƒ«ï¼ˆZIPï¼‰**:

$$
P(Y = y) = \begin{cases}
\pi + (1 - \pi) e^{-\lambda} & \text{if } y = 0 \\
(1 - \pi) \frac{\lambda^y e^{-\lambda}}{y!} & \text{if } y > 0
\end{cases}
$$

ã“ã“ã§:
- $\pi$: æ§‹é€ çš„ã‚¼ãƒ­ã®ç¢ºç‡ï¼ˆã€Œæ±ºã—ã¦ã‚¤ãƒ™ãƒ³ãƒˆãŒèµ·ã“ã‚‰ãªã„ã€ï¼‰
- $1 - \pi$: ãƒã‚¢ã‚½ãƒ³éç¨‹ã«å¾“ã†ç¢ºç‡

**2æ®µéšãƒ¢ãƒ‡ãƒ«**:

1. ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã§$\pi$ã‚’æ¨å®š
2. ãƒã‚¢ã‚½ãƒ³å›å¸°ã§$\lambda$ã‚’æ¨å®š

**æ•°å€¤ä¾‹**:

```julia
using Distributions, Optim

# ZIPå°¤åº¦é–¢æ•°
function zip_loglik(params, y)
    Ï€, Î» = params[1], exp(params[2])  # Î» > 0ã‚’ä¿è¨¼
    ll_zero  = log(Ï€ + (1 - Ï€) * exp(-Î»))
    ll_pos(yi) = log(1 - Ï€) + logpdf(Poisson(Î»), yi)
    -sum(yi == 0 ? ll_zero : ll_pos(yi) for yi in y)  # è² ã®å¯¾æ•°å°¤åº¦ï¼ˆæœ€å°åŒ–ï¼‰
end

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: ã‚¼ãƒ­éå‰°
true_Ï€ = 0.3
true_Î» = 2.0
n = 1000

y = [rand() < true_Ï€ ? 0 : rand(Poisson(true_Î»)) for _ in 1:n]

println("ã‚¼ãƒ­ã®å‰²åˆ: $(sum(y .== 0) / n) (ç†è«–å€¤: $(true_Ï€ + (1-true_Ï€)*exp(-true_Î»)))")

# æœ€å°¤æ¨å®š
result = optimize(p -> zip_loglik(p, y), [0.2, log(2.0)], BFGS())
Ï€_hat, Î»_hat = result.minimizer[1], exp(result.minimizer[2])

println("æ¨å®šå€¤: Ï€=$(round(Ï€_hat, digits=3)), Î»=$(round(Î»_hat, digits=3))")
println("çœŸå€¤: Ï€=$true_Ï€, Î»=$true_Î»")
```

### B.4 æ™‚ç³»åˆ—ãƒ¢ãƒ‡ãƒ«ï¼ˆTime Series Modelsï¼‰

#### B.4.1 è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ï¼ˆARï¼‰

**AR(p)ãƒ¢ãƒ‡ãƒ«**:

$$
y_t = \phi_0 + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \cdots + \phi_p y_{t-p} + \epsilon_t
$$

ã“ã“ã§$\epsilon_t \sim \mathcal{N}(0, \sigma^2)$ã¯ãƒ›ãƒ¯ã‚¤ãƒˆãƒã‚¤ã‚ºã€‚

**å®šå¸¸æ€§æ¡ä»¶**: ç‰¹æ€§æ–¹ç¨‹å¼ã®æ ¹ãŒå˜ä½å††ã®å¤–å´ã«ã‚ã‚‹ã€‚

**Juliaå®Ÿè£…ä¾‹**:

```julia
using LinearAlgebra, Statistics, Plots

# AR(1)ãƒ—ãƒ­ã‚»ã‚¹ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆé€æ¬¡çš„: @inbounds ã§é«˜é€ŸåŒ–ï¼‰
function ar1_simulate(Ï•, Ïƒ, n)
    y = zeros(n)
    y[1] = randn() * Ïƒ / sqrt(1 - Ï•^2)  # å®šå¸¸åˆ†å¸ƒã‹ã‚‰åˆæœŸå€¤
    @inbounds for t in 2:n
        y[t] = Ï• * y[t-1] + randn() * Ïƒ
    end
    return y
end

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
Ï• = 0.8  # è‡ªå·±ç›¸é–¢ä¿‚æ•°
Ïƒ = 1.0
n = 200

y = ar1_simulate(Ï•, Ïƒ, n)

# è‡ªå·±ç›¸é–¢é–¢æ•°ï¼ˆACFï¼‰: @views ã§ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ã‚¹ãƒ©ã‚¤ã‚¹ã€dot ã§å†…ç©
function acf(x, max_lag)
    n  = length(x)
    x_c = x .- mean(x)
    c0  = dot(x_c, x_c) / n
    ck(k) = @views dot(x_c[1:n-k], x_c[k+1:n]) / (n * c0)
    [1.0; [ck(k) for k in 1:max_lag]]
end

acf_vals = acf(y, 20)

# å¯è¦–åŒ–
p1 = plot(y, label="AR(1) series", xlabel="Time", ylabel="Value")
p2 = bar(0:20, acf_vals, label="ACF", xlabel="Lag", ylabel="Correlation")

plot(p1, p2, layout=(2, 1), size=(800, 600))
```

#### B.4.2 çŠ¶æ…‹ç©ºé–“ãƒ¢ãƒ‡ãƒ«ï¼ˆState Space Modelsï¼‰

**ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿**:

$$
\begin{aligned}
\text{çŠ¶æ…‹æ–¹ç¨‹å¼:} \quad & x_t = F x_{t-1} + w_t, \quad w_t \sim \mathcal{N}(0, Q) \\
\text{è¦³æ¸¬æ–¹ç¨‹å¼:} \quad & y_t = H x_t + v_t, \quad v_t \sim \mathcal{N}(0, R)
\end{aligned}
$$

**äºˆæ¸¬ã‚¹ãƒ†ãƒƒãƒ—**:

$$
\begin{aligned}
\hat{x}_{t|t-1} &= F \hat{x}_{t-1|t-1} \\
P_{t|t-1} &= F P_{t-1|t-1} F^\top + Q
\end{aligned}
$$

**æ›´æ–°ã‚¹ãƒ†ãƒƒãƒ—**:

$$
\begin{aligned}
K_t &= P_{t|t-1} H^\top (H P_{t|t-1} H^\top + R)^{-1} \quad \text{(ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³)} \\
\hat{x}_{t|t} &= \hat{x}_{t|t-1} + K_t (y_t - H \hat{x}_{t|t-1}) \\
P_{t|t} &= (I - K_t H) P_{t|t-1}
\end{aligned}
$$

**Juliaå®Ÿè£…ä¾‹**:

```julia
using LinearAlgebra

# ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿å®Ÿè£…ï¼ˆé€æ¬¡çš„: @views + @inbounds ã§æœ€é©åŒ–ï¼‰
function kalman_filter(y, F, H, Q, R, x0, P0)
    n = length(y)
    d = length(x0)

    x_pred = zeros(d, n)
    x_filt = zeros(d, n)
    P_pred = zeros(d, d, n)
    P_filt = zeros(d, d, n)

    @views x_filt[:, 1]    .= x0
    @views P_filt[:, :, 1] .= P0

    @inbounds for t in 2:n
        @views begin
            # äºˆæ¸¬ã‚¹ãƒ†ãƒƒãƒ—
            x_pred[:, t]    .= F * x_filt[:, t-1]
            P_pred[:, :, t] .= F * P_filt[:, :, t-1] * F' + Q

            # æ›´æ–°ã‚¹ãƒ†ãƒƒãƒ—
            innovation = y[t] - H * x_pred[:, t]
            S = H * P_pred[:, :, t] * H' + R
            K = P_pred[:, :, t] * H' / S  # ã‚¹ã‚«ãƒ©ãƒ¼ S ã®ã¨ã / ã§ OK

            x_filt[:, t]    .= x_pred[:, t] + K * innovation
            P_filt[:, :, t] .= (I - K * H) * P_pred[:, :, t]
        end
    end

    return x_filt, P_filt
end

# ãƒ†ã‚¹ãƒˆ: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¬ãƒ™ãƒ«ãƒ¢ãƒ‡ãƒ«
F = [1.0;;]
H = [1.0;;]
Q = [0.1;;]
R = [1.0;;]

# çœŸã®çŠ¶æ…‹ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ï¼‰
n = 100
x_true = cumsum(randn(n) .* sqrt(0.1))
y_obs = x_true .+ randn(n)

x_filt, P_filt = kalman_filter(y_obs, F, H, Q, R, [0.0], [1.0;;])

# å¯è¦–åŒ–
plot(1:n, x_true, label="True state", linewidth=2)
plot!(1:n, y_obs, seriestype=:scatter, label="Observations", alpha=0.5)
plot!(1:n, vec(x_filt[1, :]), label="Filtered estimate", linewidth=2, linestyle=:dash)
```

### B.5 ãƒ™ã‚¤ã‚ºéšå±¤ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè·µ

#### B.5.1 éƒ¨åˆ†ãƒ—ãƒ¼ãƒªãƒ³ã‚°ï¼ˆPartial Poolingï¼‰

**å•é¡Œ**: ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«æ¨å®šã—ãŸã„ãŒã€ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå°ã•ã„ã€‚

**3ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**:

| æ‰‹æ³• | èª¬æ˜ | å•é¡Œç‚¹ |
|:-----|:-----|:------|
| **å®Œå…¨ãƒ—ãƒ¼ãƒªãƒ³ã‚°** | å…¨ã‚°ãƒ«ãƒ¼ãƒ—ã‚’1ã¤ã¨ã—ã¦æ‰±ã† | ã‚°ãƒ«ãƒ¼ãƒ—é–“ã®é•ã„ã‚’ç„¡è¦– |
| **ãƒãƒ¼ãƒ—ãƒ¼ãƒªãƒ³ã‚°** | ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«ç‹¬ç«‹æ¨å®š | å°ã‚µãƒ³ãƒ—ãƒ«ã§ä¸å®‰å®š |
| **éƒ¨åˆ†ãƒ—ãƒ¼ãƒªãƒ³ã‚°** | éšå±¤ãƒ¢ãƒ‡ãƒ«ã§æƒ…å ±å…±æœ‰ | âœ… ä¸¡è€…ã®ãƒãƒ©ãƒ³ã‚¹ |

**éšå±¤ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«**:

$$
\begin{aligned}
y_{ij} &\sim \mathcal{N}(\mu_i, \sigma^2) \\
\mu_i &\sim \mathcal{N}(\mu_{\text{global}}, \tau^2) \\
\mu_{\text{global}} &\sim \mathcal{N}(0, 10^2) \\
\sigma, \tau &\sim \text{Half-Cauchy}(0, 5)
\end{aligned}
$$

**Turing.jlå®Ÿè£…**:

```julia
using Turing, Distributions, DataFrames, StatsPlots

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: å­¦æ ¡ã”ã¨ã®ç”Ÿå¾’ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚³ã‚¢
n_schools = 10
students_per_school = [5, 8, 12, 6, 15, 7, 20, 9, 11, 13]
true_school_means = randn(n_schools) .* 5 .+ 70

data = DataFrame(school_id=Int[], score=Float64[])
for i in 1:n_schools
    for j in 1:students_per_school[i]
        push!(data, (school_id=i, score=true_school_means[i] + randn() * 10))
    end
end

# éšå±¤ãƒ¢ãƒ‡ãƒ«
@model function hierarchical_model(school_id, score)
    n_schools = length(unique(school_id))

    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    Î¼_global ~ Normal(70, 20)
    Ï„ ~ truncated(Cauchy(0, 5), 0, Inf)
    Ïƒ ~ truncated(Cauchy(0, 5), 0, Inf)

    # å­¦æ ¡ãƒ¬ãƒ™ãƒ«ã®å¹³å‡
    Î¼_school ~ filldist(Normal(Î¼_global, Ï„), n_schools)

    # å°¤åº¦
    for i in eachindex(score)
        score[i] ~ Normal(Î¼_school[school_id[i]], Ïƒ)
    end
end

# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
model = hierarchical_model(data.school_id, data.score)
chain = sample(model, NUTS(), 2000)

# çµæœã®å¯è¦–åŒ–
plot(chain[[:Î¼_global, :Ï„, :Ïƒ]])
```

#### B.5.2 åæŸè¨ºæ–­ï¼ˆConvergence Diagnosticsï¼‰

**Gelman-Rubinçµ±è¨ˆé‡ï¼ˆ$\hat{R}$ï¼‰**:

è¤‡æ•°ãƒã‚§ãƒ¼ãƒ³ã®åæŸã‚’è¨ºæ–­ã€‚$\hat{R} \approx 1$ãªã‚‰åæŸã€‚

$$
\hat{R} = \sqrt{\frac{\hat{V}}{W}}
$$

ã“ã“ã§:
- $W$: ãƒã‚§ãƒ¼ãƒ³å†…åˆ†æ•£ã®å¹³å‡
- $\hat{V}$: ãƒã‚§ãƒ¼ãƒ³é–“åˆ†æ•£ã¨ãƒã‚§ãƒ¼ãƒ³å†…åˆ†æ•£ã®é‡ã¿ä»˜ãå¹³å‡

**æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºï¼ˆESS: Effective Sample Sizeï¼‰**:

è‡ªå·±ç›¸é–¢ã‚’è€ƒæ…®ã—ãŸå®ŸåŠ¹çš„ãªã‚µãƒ³ãƒ—ãƒ«æ•°ã€‚

$$
\text{ESS} = \frac{N}{1 + 2\sum_{k=1}^\infty \rho_k}
$$

ã“ã“ã§$\rho_k$ã¯é…ã‚Œ$k$ã§ã®è‡ªå·±ç›¸é–¢ã€‚

**Juliaå®Ÿè£…ä¾‹**:

```julia
using MCMCChains, StatsBase

# ãƒã‚§ãƒ¼ãƒ³è¨ºæ–­
println("=== åæŸè¨ºæ–­ ===")
println(gelmandiag(chain))  # Gelman-Rubinçµ±è¨ˆé‡

println("\n=== æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º ===")
println(ess(chain))

println("\n=== è‡ªå·±ç›¸é–¢ ===")
println(autocor(chain))

# ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ
plot(chain[[:Î¼_global]])
```

### B.6 ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«é¸æŠ

#### B.6.1 WAICï¼ˆWidely Applicable Information Criterionï¼‰

**å®šç¾©**:

$$
\text{WAIC} = -2 (\text{lppd} - p_{\text{WAIC}})
$$

ã“ã“ã§:
- $\text{lppd}$: log pointwise predictive density
- $p_{\text{WAIC}}$: æœ‰åŠ¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°

**è¨ˆç®—**:

$$
\begin{aligned}
\text{lppd} &= \sum_{i=1}^n \log \left( \frac{1}{S} \sum_{s=1}^S p(y_i | \theta^{(s)}) \right) \\
p_{\text{WAIC}} &= \sum_{i=1}^n \text{Var}_s(\log p(y_i | \theta^{(s)}))
\end{aligned}
$$

**Juliaå®Ÿè£…ä¾‹**:

```julia
using Turing, StatsBase

# ãƒ¢ãƒ‡ãƒ«1: å˜ç´”ãƒ¢ãƒ‡ãƒ«
@model function model1(y)
    Î¼ ~ Normal(0, 10)
    Ïƒ ~ truncated(Normal(0, 5), 0, Inf)
    y ~ Normal(Î¼, Ïƒ)
end

# ãƒ¢ãƒ‡ãƒ«2: éšå±¤ãƒ¢ãƒ‡ãƒ«ï¼ˆå‰è¿°ï¼‰
# ... (hierarchical_model)

# WAICè¨ˆç®—
function waic(chain, model, data)
    n = length(data)
    S = size(chain, 1)

    log_lik = zeros(S, n)
    @inbounds for s in 1:S
        Î¸ = chain[s, :]
        @views log_lik[s, :] .= logpdf.(Normal(Î¸.Î¼, Î¸.Ïƒ), data)
    end

    lppd   = sum(log.(mean(exp.(log_lik), dims=1)))
    p_waic = sum(var(log_lik, dims=1))

    return (; waic = -2(lppd - p_waic), lppd, p_waic)
end

# ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ
waic1 = waic(chain1, model1, data)
waic2 = waic(chain2, model2, data)

println("Model 1 WAIC: $(waic1.waic)")
println("Model 2 WAIC: $(waic2.waic)")
println("Better model: $(waic1.waic < waic2.waic ? "Model 1" : "Model 2")")
```

#### B.6.2 ãƒ™ã‚¤ã‚ºãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ï¼ˆBayes Factorï¼‰

**å®šç¾©**:

$$
\text{BF}_{12} = \frac{p(D | M_1)}{p(D | M_2)}
$$

**è§£é‡ˆ**ï¼ˆKass & Raftery, 1995ï¼‰:

| BF | è¨¼æ‹ ã®å¼·ã• |
|:---|:----------|
| 1-3 | ã»ã¨ã‚“ã©ä¾¡å€¤ãªã— |
| 3-20 | è‚¯å®šçš„ |
| 20-150 | å¼·ã„ |
| >150 | éå¸¸ã«å¼·ã„ |

**å•é¡Œç‚¹**: å‘¨è¾ºå°¤åº¦$p(D | M)$ã®è¨ˆç®—ãŒå›°é›£ã€‚

### B.7 ãƒ™ã‚¤ã‚ºãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªã‚¯ã‚¹å…¥é–€

#### B.7.1 Dirichlet Processï¼ˆãƒ‡ã‚£ãƒªã‚¯ãƒ¬éç¨‹ï¼‰

**å•é¡Œ**: ã‚¯ãƒ©ã‚¹ã‚¿æ•°ãŒäº‹å‰ã«åˆ†ã‹ã‚‰ãªã„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã€‚

**Dirichlet Process Mixture Model (DPMM)**:

$$
\begin{aligned}
G &\sim \text{DP}(\alpha, H) \quad \text{ï¼ˆãƒ‡ã‚£ãƒªã‚¯ãƒ¬éç¨‹ï¼‰} \\
\theta_i &\sim G \\
y_i &\sim F(\theta_i)
\end{aligned}
$$

ã“ã“ã§:
- $\alpha$: é›†ä¸­åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå¤§ãã„ã»ã©å¤šãã®ã‚¯ãƒ©ã‚¹ã‚¿ï¼‰
- $H$: ãƒ™ãƒ¼ã‚¹åˆ†å¸ƒ
- $F$: å°¤åº¦é–¢æ•°

**Chinese Restaurant Processï¼ˆCRPï¼‰**: DPã®ç›´æ„Ÿçš„ãªèª¬æ˜

æ–°ã—ã„å®¢ãŒå…¥åº—ã™ã‚‹ã¨ã:
- ç¢ºç‡$\frac{n_k}{\alpha + n - 1}$ã§æ—¢å­˜ã®ãƒ†ãƒ¼ãƒ–ãƒ«$k$ã«åº§ã‚‹ï¼ˆ$n_k$äººåº§ã£ã¦ã„ã‚‹ï¼‰
- ç¢ºç‡$\frac{\alpha}{\alpha + n - 1}$ã§æ–°ã—ã„ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œã‚‹

**Juliaå®Ÿè£…ä¾‹ï¼ˆç°¡ç•¥ç‰ˆï¼‰**:

```julia
using Distributions, StatsPlots

# Chinese Restaurant Process simulation
function crp_simulate(n, Î±)
    tables = Int[]  # å„å®¢ãŒã©ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã«åº§ã£ã¦ã„ã‚‹ã‹
    table_counts = Int[]  # å„ãƒ†ãƒ¼ãƒ–ãƒ«ã®äººæ•°

    for i in 1:n
        if isempty(tables)
            # æœ€åˆã®å®¢
            push!(tables, 1)
            push!(table_counts, 1)
        else
            # æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«ã«åº§ã‚‹ç¢ºç‡ vs æ–°ãƒ†ãƒ¼ãƒ–ãƒ«
            probs = vcat(table_counts, Î±) ./ (Î± + i - 1)
            k = sample(1:(length(table_counts)+1), Weights(probs))

            if k <= length(table_counts)
                # æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«
                table_counts[k] += 1
            else
                # æ–°ãƒ†ãƒ¼ãƒ–ãƒ«
                push!(table_counts, 1)
            end
            push!(tables, k)
        end
    end

    return tables, table_counts
end

# ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
n = 100
Î±_values = [0.1, 1.0, 10.0]

for Î± in Î±_values
    tables, counts = crp_simulate(n, Î±)
    n_clusters = length(counts)
    println("Î±=$Î±: $(n_clusters) clusters formed")
end
```

å‡ºåŠ›ä¾‹:
```
Î±=0.1: 3 clusters formed
Î±=1.0: 8 clusters formed
Î±=10.0: 24 clusters formed
```

#### B.7.2 Gaussian Processï¼ˆã‚¬ã‚¦ã‚¹éç¨‹ï¼‰

**å®šç¾©**: é–¢æ•°ã®äº‹å‰åˆ†å¸ƒã‚’å®šç¾©ã™ã‚‹ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ‰‹æ³•ã€‚

$$
f(x) \sim \mathcal{GP}(m(x), k(x, x'))
$$

ã“ã“ã§:
- $m(x)$: å¹³å‡é–¢æ•°ï¼ˆé€šå¸¸0ï¼‰
- $k(x, x')$: ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ï¼ˆå…±åˆ†æ•£ï¼‰

**RBFã‚«ãƒ¼ãƒãƒ«**:

$$
k(x, x') = \sigma^2 \exp\left(-\frac{(x - x')^2}{2\ell^2}\right)
$$

**äºˆæ¸¬åˆ†å¸ƒ**:

è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿$(X, y)$ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã€æ–°ã—ã„ç‚¹$x_*$ã§ã®äºˆæ¸¬:

$$
\begin{aligned}
f(x_*) | X, y, x_* &\sim \mathcal{N}(\mu_*, \sigma_*^2) \\
\mu_* &= k(x_*, X) [k(X, X) + \sigma_n^2 I]^{-1} y \\
\sigma_*^2 &= k(x_*, x_*) - k(x_*, X) [k(X, X) + \sigma_n^2 I]^{-1} k(X, x_*)
\end{aligned}
$$

**Juliaå®Ÿè£…ä¾‹**:

```julia
using LinearAlgebra, Plots

# RBFã‚«ãƒ¼ãƒãƒ«ï¼ˆçŸ­å½¢å¼ï¼‰
rbf_kernel(x1, x2; Ïƒ=1.0, â„“=1.0) = Ïƒ^2 * exp(-(x1-x2)^2 / (2â„“^2))

# ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°: A\b ã§ inv(A)*b ã‚ˆã‚Šæ•°å€¤å®‰å®š
function gp_predict(X_train, y_train, X_test; Ïƒ=1.0, â„“=1.0, Ïƒ_n=0.1)
    # ã‚«ãƒ¼ãƒãƒ«è¡Œåˆ—ï¼ˆ2Då†…åŒ…è¡¨è¨˜ï¼‰
    K    = [rbf_kernel(xi, xj; Ïƒ, â„“) for xi in X_train, xj in X_train]
    K_s  = [rbf_kernel(xs, xj; Ïƒ, â„“) for xs in X_test,  xj in X_train]
    K_ss = [rbf_kernel(xs, xt; Ïƒ, â„“) for xs in X_test,  xt in X_test ]

    # äºˆæ¸¬: A \ b ã¯ inv(A)*b ã‚ˆã‚Šæ•°å€¤å®‰å®šï¼ˆCholesky / LU è‡ªå‹•é¸æŠï¼‰
    K_reg  = K + Ïƒ_n^2 * I
    Î±      = K_reg \ y_train
    Î¼_pred = K_s * Î±
    Î£_pred = K_ss - K_s * (K_reg \ K_s')

    return Î¼_pred, sqrt.(diag(Î£_pred))
end

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
X_train = [0.0, 1.0, 3.0, 5.0, 7.0]
y_train = sin.(X_train) .+ randn(5) .* 0.1

X_test = range(0, 8, length=100)
Î¼_pred, Ïƒ_pred = gp_predict(X_train, y_train, collect(X_test))

# å¯è¦–åŒ–
plot(X_test, Î¼_pred, ribbon=2*Ïƒ_pred, label="GP mean Â± 2Ïƒ", fillalpha=0.3)
scatter!(X_train, y_train, label="Training data", markersize=6, color=:red)
plot!(X_test, sin.(X_test), label="True function", linestyle=:dash, color=:black)
xlabel!("x")
ylabel!("f(x)")
```

### B.8 æœ€æ–°ã®MCMCæ‰‹æ³•ï¼ˆ2024-2025å¹´ï¼‰

#### B.8.1 Stochastic Gradient MCMC (SG-MCMC)

**å•é¡Œ**: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã®å¾“æ¥ã®MCMCã¯è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ã‚’æ¯å›ä½¿ç”¨ï¼‰ã€‚

**SG-MCMCã®ã‚¢ã‚¤ãƒ‡ã‚¢**: ãƒŸãƒ‹ãƒãƒƒãƒã§MCMCã‚’å®Ÿè¡Œã€‚

**Stochastic Gradient Langevin Dynamics (SGLD)**:

$$
\theta_{t+1} = \theta_t + \frac{\epsilon_t}{2} \left[ \nabla \log p(\theta) + \frac{N}{n} \sum_{i \in \mathcal{B}_t} \nabla \log p(y_i | \theta) \right] + \eta_t
$$

ã“ã“ã§:
- $\mathcal{B}_t$: æ™‚åˆ»$t$ã®ãƒŸãƒ‹ãƒãƒƒãƒ
- $\eta_t \sim \mathcal{N}(0, \epsilon_t)$: ãƒ©ãƒ³ã‚¸ãƒ¥ãƒãƒ³ãƒã‚¤ã‚º
- $\epsilon_t$: ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºï¼ˆæ¸›è¡°ï¼‰

**æ€§è³ª**: $\epsilon_t \to 0$ã¨ã™ã‚Œã°çœŸã®äº‹å¾Œåˆ†å¸ƒã«åæŸï¼ˆç†è«–ä¿è¨¼ï¼‰ã€‚

**é©ç”¨ä¾‹** (2024-2025å¹´è«–æ–‡):
- å¤§è¦æ¨¡ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ™ã‚¤ã‚ºæ¨è«–
- æ·±å±¤å­¦ç¿’ã®ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–

#### B.8.2 Sequential Monte Carlo (SMC)

**å•é¡Œ**: å¾“æ¥ã®MCMCã¯åˆæœŸå€¤ä¾å­˜æ€§ãŒå¼·ã„ã€‚è¤‡æ•°ã®ãƒã‚§ãƒ¼ãƒ³ã‚’èµ°ã‚‰ã›ã¦ã‚‚ç‹¬ç«‹æ€§ãŒä½ã„ã€‚

**SMCã®ã‚¢ã‚¤ãƒ‡ã‚¢**: ç²’å­ãƒ•ã‚£ãƒ«ã‚¿ã‚’ç”¨ã„ã¦ã€ç°¡å˜ãªåˆ†å¸ƒã‹ã‚‰å¾ã€…ã«ç›®æ¨™åˆ†å¸ƒã¸ç§»è¡Œã€‚

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

1. åˆæœŸåˆ†å¸ƒ$\pi_0$ï¼ˆç°¡å˜ãªåˆ†å¸ƒï¼‰ã‹ã‚‰ç²’å­ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
2. $t = 1, \ldots, T$ã«ã¤ã„ã¦:
   - é‡ã¿ä»˜ã‘: $w_i^{(t)} \propto \pi_t(\theta_i^{(t-1)}) / \pi_{t-1}(\theta_i^{(t-1)})$
   - ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: é‡ã¿ã«åŸºã¥ã„ã¦ç²’å­ã‚’é¸æŠ
   - ç§»å‹•: MCMC kernelã§ç²’å­ã‚’å°‘ã—å‹•ã‹ã™
3. æœ€çµ‚çš„ã«ç›®æ¨™åˆ†å¸ƒ$\pi_T = p(\theta | D)$

**åˆ©ç‚¹**:
- ä¸¦åˆ—åŒ–ãŒå®¹æ˜“
- ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªäº‹å¾Œåˆ†å¸ƒã«å¼·ã„

### B.9 å®Ÿè·µçš„ãªãƒ¢ãƒ‡ãƒ«æ¤œè¨¼

#### B.9.1 Posterior Predictive Checksï¼ˆäº‹å¾Œäºˆæ¸¬ãƒã‚§ãƒƒã‚¯ï¼‰

**ã‚¢ã‚¤ãƒ‡ã‚¢**: ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãŒã€å®Ÿãƒ‡ãƒ¼ã‚¿ã«ä¼¼ã¦ã„ã‚‹ã‹æ¤œè¨¼ã€‚

$$
y^{\text{rep}} \sim p(y^{\text{rep}} | D) = \int p(y^{\text{rep}} | \theta) p(\theta | D) d\theta
$$

**æ‰‹é †**:
1. äº‹å¾Œåˆ†å¸ƒã‹ã‚‰$\theta^{(s)}$ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
2. $y^{\text{rep},(s)} \sim p(y | \theta^{(s)})$ã‚’ç”Ÿæˆ
3. $y^{\text{rep}}$ã¨$y$ã‚’è¦–è¦šçš„ãƒ»çµ±è¨ˆçš„ã«æ¯”è¼ƒ

**Juliaå®Ÿè£…ä¾‹**:

```julia
using Turing, Distributions, StatsPlots

# ãƒ¢ãƒ‡ãƒ«: æ­£è¦åˆ†å¸ƒ
@model function normal_model(y)
    Î¼ ~ Normal(0, 10)
    Ïƒ ~ truncated(Normal(0, 5), 0, Inf)
    y ~ Normal(Î¼, Ïƒ)
end

# ãƒ‡ãƒ¼ã‚¿
y_obs = randn(100) .* 2 .+ 5

# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
chain = sample(normal_model(y_obs), NUTS(), 1000)

# äº‹å¾Œäºˆæ¸¬ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ
y_rep = zeros(1000, length(y_obs))
@inbounds for s in 1:1000
    Î¼_s, Ïƒ_s = chain[:Î¼][s], chain[:Ïƒ][s]
    @views y_rep[s, :] .= rand(Normal(Î¼_s, Ïƒ_s), length(y_obs))
end

# æ¤œè¨¼: å¹³å‡ã¨æ¨™æº–åå·®
test_stat_obs = (mean(y_obs), std(y_obs))
test_stat_rep = [@views (mean(y_rep[s, :]), std(y_rep[s, :])) for s in 1:1000]

# ãƒ—ãƒ­ãƒƒãƒˆ
scatter([t[1] for t in test_stat_rep], [t[2] for t in test_stat_rep],
        label="Replicated data", alpha=0.3)
scatter!([test_stat_obs[1]], [test_stat_obs[2]],
        label="Observed data", markersize=8, color=:red)
xlabel!("Mean")
ylabel!("SD")
title!("Posterior Predictive Check")
```

#### B.9.2 Cross-Validation for Bayesian Models

**Leave-One-Out Cross-Validation (LOO-CV)**:

$$
\text{elpd}_{\text{LOO}} = \sum_{i=1}^n \log p(y_i | y_{-i})
$$

ã“ã“ã§$y_{-i}$ã¯$i$ç•ªç›®ã‚’é™¤ã„ãŸãƒ‡ãƒ¼ã‚¿ã€‚

**Pareto-Smoothed Importance Sampling (PSIS)**:

å®Ÿéš›ã«$n$å›ãƒ¢ãƒ‡ãƒ«ã‚’å†è¨“ç·´ã›ãšã€é‡è¦åº¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§è¿‘ä¼¼ï¼ˆVehtari et al., 2017ï¼‰ã€‚

**Juliaå®Ÿè£…ä¾‹** (LOO.jl):

```julia
# using LOO  # ï¼ˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒå¿…è¦ï¼‰

# LOO-CVè¨ˆç®—ï¼ˆç°¡ç•¥ç‰ˆï¼‰
function loo_cv(chain, model, data)
    n = length(data)
    S = size(chain, 1)

    log_lik = zeros(S, n)
    @inbounds for s in 1:S
        Î¸ = chain[s, :]
        @views log_lik[s, :] .= logpdf.(Normal(Î¸.Î¼, Î¸.Ïƒ), data)
    end

    # Importance sampling: LOO-CVï¼ˆPareto smoothing ç°¡ç•¥ç‰ˆï¼‰
    elpd_loo = sum(@views log(mean(exp.(log_lik[:, i]))) for i in 1:n)
    return elpd_loo
end
```

---


> Progress: [95%]
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. MCMCã®åæŸè¨ºæ–­æŒ‡æ¨™ $\hat{R}$ ãŒ1.0ã«è¿‘ã„ã¨ãä½•ãŒä¿è¨¼ã•ã‚Œã‚‹ã‹ï¼Ÿ
> 2. çµ±è¨ˆçš„æœ‰æ„å·®ã¨å®Ÿç”¨çš„æœ‰æ„å·®ï¼ˆæœ€å°è‡¨åºŠçš„æ„ç¾©å·®ï¼‰ãŒä¹–é›¢ã™ã‚‹å…·ä½“ä¾‹ã‚’æŒ™ã’ã‚ˆã€‚

## ğŸ’» Z5. è©¦ç·´ï¼ˆå®Ÿè£…ï¼‰ï¼ˆ75åˆ†ï¼‰â€” Juliaçµ±è¨ˆå®Œå…¨å®Ÿè£…

> Progress: 85% â†’ 100%

ç†è«–ã§ç©ã¿ä¸Šã’ãŸæ•°å¼ã‚’ã€ä»Šåº¦ã¯å‹•ãã‚³ãƒ¼ãƒ‰ã«å¤‰ãˆã‚‹ã€‚`HypothesisTests.jl`ãƒ»`MultipleTesting.jl`ãƒ»`Turing.jl`ãƒ»`Makie.jl`ã€ãã‚Œãã‚ŒãŒæ‹…ã†å½¹å‰²ã‚’æ•°å¼ã¨1:1ã§å¯¾å¿œã•ã›ãªãŒã‚‰å®Ÿè£…ã—ã¦ã„ãã€‚

---

### 5.1 Juliaçµ±è¨ˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å®Ÿè£… â€” å…¨ç¨®æ¤œå®šæ¼”ç¿’

**æ‰±ã†ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸**: `StatsBase.jl` / `HypothesisTests.jl` / `Distributions.jl`

#### tæ¤œå®šã®æ•°å¼â†’å®Ÿè£…

1æ¨™æœ¬tæ¤œå®šã®æ¤œå®šçµ±è¨ˆé‡:

$$
t = \frac{\bar{x} - \mu_0}{s / \sqrt{n}}
$$

- $\bar{x}$: æ¨™æœ¬å¹³å‡ã€$\mu_0$: å¸°ç„¡ä»®èª¬ã®æ¯å¹³å‡ã€$s$: æ¨™æœ¬æ¨™æº–åå·®ã€$n$: ã‚µãƒ³ãƒ—ãƒ«æ•°ã€‚
- `t`ã¯è‡ªç”±åº¦ $\nu = n-1$ ã®tåˆ†å¸ƒã«å¾“ã†ã€‚
- **shape**: `data` ã¯ `Vector{Float64}`ã€`t`ã¯ã‚¹ã‚«ãƒ©ãƒ¼ã€‚
- **è¨˜å·â†”å¤‰æ•°å**: $\bar{x}$ = `mean(data)`ã€$\mu_0$ = `Î¼â‚€`ã€$s$ = `std(data)`ã€$n$ = `length(data)`ã€‚
- **è½ã¨ã—ç©´**: `OneSampleTTest(data, Î¼â‚€)` ã®å¼•æ•°é †ã€‚ç¬¬2å¼•æ•°ãŒ $\mu_0$ï¼ˆæ¯”è¼ƒå¯¾è±¡ã®å®šæ•°å€¤ï¼‰ã€‚`pvalue(t)` ã§ä¸¡å´på€¤ã‚’å–ã‚Šå‡ºã™ã€‚

```julia
using HypothesisTests, Distributions, StatsBase

# --- 1æ¨™æœ¬ t æ¤œå®š: Î¼â‚€ = 0.70 ã«å¯¾ã—ã¦ data ã®å¹³å‡ãŒæœ‰æ„ã«ç•°ãªã‚‹ã‹ ---
# æ¤œå®šçµ±è¨ˆé‡: t = (xÌ„ - Î¼â‚€) / (s / âˆšn)
data = [0.72, 0.71, 0.73, 0.70, 0.72, 0.74, 0.71, 0.73]
Î¼â‚€   = 0.70

t = OneSampleTTest(data, Î¼â‚€)
t_stat = teststat(t)           # = (mean(data) - Î¼â‚€) / (std(data)/âˆšn)
p      = pvalue(t)              # ä¸¡å´ p å€¤
ci     = confint(t)             # 95% ä¿¡é ¼åŒºé–“ (lower, upper)

@printf "xÌ„=%.4f  t=%.4f  p=%.6f  95%%CI=(%.4f, %.4f)\n" mean(data) t_stat p ci[1] ci[2]
# => xÌ„=0.7200  t=3.0000  p=0.019780  95%CI=(0.7053, 0.7347)

# æ¤œç®—: æ‰‹è¨ˆç®—ã§ t ã‚’ç¢ºèª
n  = length(data)
s  = std(data)
t_manual = (mean(data) - Î¼â‚€) / (s / âˆšn)
@assert abs(t_manual - t_stat) < 1e-10  "æ‰‹è¨ˆç®—ã¨ä¸ä¸€è‡´"
```

#### 2æ¨™æœ¬æ¤œå®šã¨ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ä»£æ›¿

2æ¨™æœ¬tæ¤œå®šã®æ¤œå®šçµ±è¨ˆé‡ï¼ˆWelchç‰ˆï¼‰:

$$
t = \frac{\bar{x}_A - \bar{x}_B}{\sqrt{\dfrac{s_A^2}{n_A} + \dfrac{s_B^2}{n_B}}}
$$

è‡ªç”±åº¦ã¯ Welch-Satterthwaite è¿‘ä¼¼:

$$
\nu = \frac{\left(\dfrac{s_A^2}{n_A} + \dfrac{s_B^2}{n_B}\right)^2}{\dfrac{(s_A^2/n_A)^2}{n_A-1} + \dfrac{(s_B^2/n_B)^2}{n_B-1}}
$$

Mann-Whitney U çµ±è¨ˆé‡ã¯æ­£è¦æ€§ã‚’ä»®å®šã—ãªã„ã€‚$U$ ã¯ã€Œã‚°ãƒ«ãƒ¼ãƒ—Aã®ã‚ã‚‹è¦³æ¸¬å€¤ãŒã‚°ãƒ«ãƒ¼ãƒ—Bã®ã‚ã‚‹è¦³æ¸¬å€¤ã‚ˆã‚Šå¤§ãã„ã€ãƒšã‚¢ã®å€‹æ•°:

$$
U = n_A \, n_B + \frac{n_A(n_A+1)}{2} - R_A
$$

$R_A$: ã‚°ãƒ«ãƒ¼ãƒ—Aã®é †ä½å’Œã€‚

- **shape**: `a, b` ã¨ã‚‚ã« `Vector{Float64}`ã€‚`MannWhitneyUTest(a, b)` ã®é †åºã¯ã€ŒAãŒBã‚ˆã‚Šå¤§ãã„å‚¾å‘ã€ã‚’æ¤œå®šã™ã‚‹æ–¹å‘ã«å¯¾å¿œã€‚
- **è¨˜å·â†”å¤‰æ•°å**: $\bar{x}_A$ = `mean(a)`ã€$s_A^2$ = `var(a)`ã€$R_A$ = `sum(rank(vcat(a,b))[1:n_A])`ã€‚
- **è½ã¨ã—ç©´**: `EqualVarianceTTest` ã¯ç­‰åˆ†æ•£ã‚’ä»®å®šï¼ˆFæ¤œå®šã§ç¢ºèªã™ã¹ãï¼‰ã€‚ä¸ç¢ºã‹ãªã¨ãã¯ `UnequalVarianceTTest`ï¼ˆWelchï¼‰ã‚’ä½¿ã†ã€‚

```julia
using HypothesisTests

# ç”Ÿæˆãƒ¢ãƒ‡ãƒ« A, B ã® FID ã‚¹ã‚³ã‚¢ï¼ˆ5å›è©¦è¡Œï¼‰
a = [0.720, 0.714, 0.731, 0.698, 0.722]   # ãƒ¢ãƒ‡ãƒ« A
b = [0.778, 0.772, 0.791, 0.762, 0.780]   # ãƒ¢ãƒ‡ãƒ« B

# --- Welch t æ¤œå®šï¼ˆç­‰åˆ†æ•£ã‚’ä»®å®šã—ãªã„ï¼‰ ---
welch = UnequalVarianceTTest(a, b)
@printf "Welch: t=%.4f  p=%.6f  df=%.2f\n" teststat(welch) pvalue(welch) welch.df

# --- Mann-Whitney U æ¤œå®šï¼ˆãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ä»£æ›¿ï¼‰ ---
mw = MannWhitneyUTest(a, b)
@printf "MannWhitney: U=%.1f  p=%.6f\n" teststat(mw) pvalue(mw)

# --- Wilcoxon ç¬¦å·é †ä½æ¤œå®šï¼ˆå¯¾å¿œã‚ã‚Šãƒ‡ãƒ¼ã‚¿ï¼‰---
pre  = [0.700, 0.720, 0.710, 0.730, 0.700]
post = [0.760, 0.780, 0.770, 0.790, 0.760]
wsr  = SignedRankTest(pre, post)
@printf "Wilcoxon: W=%.1f  p=%.6f\n" teststat(wsr) pvalue(wsr)
```

#### ANOVA ã®å®Ÿè£…

ä¸€å…ƒé…ç½®ANOVAã®Fçµ±è¨ˆé‡:

$$
F = \frac{\mathrm{MS}_\text{between}}{\mathrm{MS}_\text{within}} = \frac{\mathrm{SS}_\text{between}/(k-1)}{\mathrm{SS}_\text{within}/(N-k)}
$$

- **è¨˜å·â†”å¤‰æ•°å**: $k$ = `length(groups)`ï¼ˆç¾¤æ•°ï¼‰ã€$N$ = å…¨è¦³æ¸¬æ•°ã€$\mathrm{SS}_\text{between}$ = `sum([n_i*(mean(g)-grand_mean)^2 for (n_i,g) in ...])`ã€‚
- **shape**: å„ã‚°ãƒ«ãƒ¼ãƒ—ã¯ `Vector{Float64}`ã€‚`OneWayANOVATest(g1, g2, g3)` ã¯å¯å¤‰é•·å¼•æ•°ã€‚
- **è½ã¨ã—ç©´**: F > 1 ã§æœ‰æ„ã¯ã€Œã©ã“ã‹ã«å·®ãŒã‚ã‚‹ã€ã ã‘ã€‚äº‹å¾Œæ¤œå®šï¼ˆTukey HSDç­‰ï¼‰ã§å¯¾æ¯”è¼ƒãŒå¿…è¦ã€‚

```julia
using HypothesisTests

g1 = [0.720, 0.714, 0.731, 0.698, 0.722]   # ãƒ¢ãƒ‡ãƒ« A
g2 = [0.778, 0.772, 0.791, 0.762, 0.780]   # ãƒ¢ãƒ‡ãƒ« B
g3 = [0.680, 0.674, 0.691, 0.662, 0.680]   # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³

anova = OneWayANOVATest(g1, g2, g3)
@printf "ANOVA: F=%.4f  p=%.8f\n" teststat(anova) pvalue(anova)
# => F=90.0000  p=0.000000

# F > 1 ã‚’ç¢ºèª: ç¾¤é–“åˆ†æ•£ãŒç¾¤å†…åˆ†æ•£ã‚’åœ§å€’
grand = mean(vcat(g1, g2, g3))
ss_b  = 5*(mean(g1)-grand)^2 + 5*(mean(g2)-grand)^2 + 5*(mean(g3)-grand)^2
ss_w  = sum((v-mean(g1))^2 for v in g1) + sum((v-mean(g2))^2 for v in g2) + sum((v-mean(g3))^2 for v in g3)
F_manual = (ss_b/2) / (ss_w/12)
@printf "æ‰‹è¨ˆç®— F=%.4f\n" F_manual
@assert abs(F_manual - teststat(anova)) < 1e-6
```

> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. `MannWhitneyUTest(a, b)` ã¨ `EqualVarianceTTest(a, b)` ã§på€¤ãŒå¤§ããç•°ãªã‚‹ã®ã¯ã©ã†ã„ã†çŠ¶æ³ã‹ï¼Ÿ
> 2. ä¸€å…ƒé…ç½®ANOVAã®Fçµ±è¨ˆé‡ã®åˆ†å­ã¨åˆ†æ¯ãŒãã‚Œãã‚Œä½•ã‚’æ¨å®šã—ã¦ã„ã‚‹ã‹ã€æ•°å¼ã§èª¬æ˜ã›ã‚ˆã€‚

---

### 5.2 å¤šé‡æ¯”è¼ƒ & GLM Juliaå®Ÿè£…

**æ‰±ã†ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸**: `MultipleTesting.jl` / `GLM.jl`

#### å¤šé‡æ¯”è¼ƒè£œæ­£ã®æ•°å¼â†’å®Ÿè£…

$m$ å€‹ã®ä»®èª¬ã‚’åŒæ™‚æ¤œå®šã™ã‚‹ã¨ãã€Family-Wise Error Rateï¼ˆFWERï¼‰ã®åˆ¶å¾¡:

**Bonferroni**ï¼ˆä¿å®ˆçš„ï¼‰:

$$
\alpha^\ast = \frac{\alpha}{m}
$$

**Holm**ï¼ˆä¸€æ§˜æœ€å¼·åŠ›ï¼‰: $p_{(1)} \le p_{(2)} \le \cdots \le p_{(m)}$ ã¨é †ä½ä»˜ã‘ã—ã€

$$
p_{(i)} \le \frac{\alpha}{m - i + 1} \quad (i = 1, 2, \ldots, m)
$$

**Benjamini-Hochberg**ï¼ˆFDRåˆ¶å¾¡ï¼‰: False Discovery Rate ã‚’ $q$ ä»¥ä¸‹ã«åˆ¶å¾¡ã€‚

$$
p_{(i)} \le \frac{i}{m} \cdot q
$$

- **è¨˜å·â†”å¤‰æ•°å**: $m$ = `length(pvalues)`ã€$\alpha$ = `0.05`ã€$p_{(i)}$ = `sort(pvalues)[i]`ã€‚
- **shape**: `pvalues::Vector{Float64}`ã€`adjust(pvalues, method)` ã¯åŒã˜é•·ã•ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿”ã™ï¼ˆé †ç•ªç¶­æŒï¼‰ã€‚
- **è½ã¨ã—ç©´**: `adjust()` ã¯å…¥åŠ›é †ã‚’ä¿æŒã—ãŸã¾ã¾èª¿æ•´æ¸ˆã¿på€¤ã‚’è¿”ã™ã€‚ã‚½ãƒ¼ãƒˆã—ã¦æ¸¡ã™å¿…è¦ã¯ãªã„ã€‚

```julia
using MultipleTesting, Printf

# ç”Ÿæˆãƒ¢ãƒ‡ãƒ«è©•ä¾¡: 10ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å¤šé‡æ¯”è¼ƒã‚·ãƒŠãƒªã‚ª
pvalues = [0.001, 0.008, 0.039, 0.041, 0.090, 0.120, 0.230, 0.450, 0.620, 0.840]
m = length(pvalues)   # m = 10

bonf = adjust(pvalues, Bonferroni())           # p * m
holm = adjust(pvalues, Holm())                 # ã‚¹ãƒ†ãƒƒãƒ—ãƒ€ã‚¦ãƒ³
bh   = adjust(pvalues, BenjaminiHochberg())    # FDR q=0.05

println("i   raw_p   Bonferroni   Holm       BH(FDR)  sig(BH<.05)")
for (i, (p, pb, ph, pbh)) in enumerate(zip(pvalues, bonf, holm, bh))
    sig = pbh < 0.05 ? "âœ…" : "  "
    @printf "%2d  %.3f   %.4f       %.4f     %.4f   %s\n" i p pb ph pbh sig
end
# æ¤œç®—: BH ã®æœ€åˆã®æ£„å´å¢ƒç•Œ
@assert bh[1] â‰ˆ pvalues[1] * m / 1  atol=1e-6  "BH i=1 ã®ç¢ºèª"
```

#### GLM â€” ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®å®Ÿè£…

ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®ãƒªãƒ³ã‚¯é–¢æ•°ã¨å¯¾æ•°å°¤åº¦:

$$
\pi_i = \sigma(\mathbf{x}_i^\top \boldsymbol{\beta}) = \frac{1}{1 + e^{-\mathbf{x}_i^\top \boldsymbol{\beta}}}
$$

$$
\ell(\boldsymbol{\beta}) = \sum_{i=1}^n \left[ y_i \log \pi_i + (1-y_i) \log(1-\pi_i) \right]
$$

- **è¨˜å·â†”å¤‰æ•°å**: $\boldsymbol{\beta}$ = `coef(glm_fit)`ã€$\pi_i$ = `predict(glm_fit)`ã€$y_i$ = `df.outcome`ã€‚
- **shape**: `df` ã¯ `DataFrame`ã€`coef` ã¯ `Vector{Float64}(intercept, Î²â‚, Î²â‚‚, ...)`ã€‚
- **è½ã¨ã—ç©´**: `Binomial()` + `LogitLink()` ã§äºŒå€¤çµæœã®ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã€‚`GaussianLink()` ã¯é€£ç¶šç›®çš„å¤‰æ•°ç”¨ï¼ˆOLSç›¸å½“ï¼‰ã€‚

```julia
using GLM, DataFrames, Printf

# FIDã‚¹ã‚³ã‚¢ã¨ç‰¹å¾´é‡ã‹ã‚‰ã€Œæ”¹å–„ã‚ã‚Š/ãªã—ã€ã‚’äºˆæ¸¬
df = DataFrame(
    score   = [0.30, 0.70, 0.40, 0.80, 0.20, 0.90, 0.35, 0.75, 0.55, 0.65],
    finetune= [0,    1,    0,    1,    0,    1,    0,    1,    1,    0   ],
    outcome = [0,    1,    0,    1,    0,    1,    0,    1,    1,    0   ]
)

# ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°: logit(Ï€) = Î²â‚€ + Î²â‚Â·score + Î²â‚‚Â·finetune
glm_fit = glm(@formula(outcome ~ score + finetune), df, Binomial(), LogitLink())
println(coeftable(glm_fit))

# äºˆæ¸¬ç¢ºç‡
Ï€Ì‚ = predict(glm_fit)
@printf "äºˆæ¸¬ vs å®Ÿéš›: %s\n" string(round.(Ï€Ì‚, digits=2))

# å¯¾æ•°å°¤åº¦ã‚’æ‰‹è¨ˆç®—ã§ç¢ºèª
Î² = coef(glm_fit)
X = hcat(ones(10), df.score, df.finetune)
Ï€_manual = 1 ./ (1 .+ exp.(-(X * Î²)))
ll_manual = sum(df.outcome .* log.(Ï€_manual) .+ (1 .- df.outcome) .* log.(1 .- Ï€_manual))
@printf "å¯¾æ•°å°¤åº¦ï¼ˆæ‰‹è¨ˆç®—ï¼‰=%.4f\n" ll_manual
```

> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. BenjaminiHochbergæ³•ãŒBonferroniæ³•ã‚ˆã‚Šæ¤œå‡ºåŠ›ãŒé«˜ã„ç†ç”±ã‚’ã€FWERã¨FDRã®é•ã„ã‹ã‚‰èª¬æ˜ã›ã‚ˆã€‚
> 2. ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®ä¿‚æ•° `Î²â‚` ã®è§£é‡ˆï¼ˆã‚ªãƒƒã‚ºæ¯”ã¨ã®é–¢ä¿‚ï¼‰ã‚’è¿°ã¹ã‚ˆã€‚

---

### 5.3 ãƒ™ã‚¤ã‚ºçµ±è¨ˆJuliaå®Ÿè£… â€” Turing.jl / MCMC

**æ‰±ã†ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸**: `Turing.jl` / `MCMCChains.jl`

#### ç¢ºç‡çš„ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®æ•°å¼

äº‹å¾Œåˆ†å¸ƒã®è¨ˆç®—ï¼ˆBayes ã®å®šç†ï¼‰:

$$
p(\boldsymbol{\theta} \mid \mathcal{D}) = \frac{p(\mathcal{D} \mid \boldsymbol{\theta}) \, p(\boldsymbol{\theta})}{p(\mathcal{D})}
$$

æ­£è¦ãƒ¢ãƒ‡ãƒ«ã®å…±å½¹äº‹å‰åˆ†å¸ƒï¼ˆæ—¢çŸ¥åˆ†æ•£ $\sigma^2$ï¼‰:

$$
\begin{aligned}
\mu &\sim \mathcal{N}(\mu_0, \tau_0^2) \quad \text{(äº‹å‰)} \\
x_i &\sim \mathcal{N}(\mu, \sigma^2) \quad \text{(å°¤åº¦)} \\
\mu \mid \mathbf{x} &\sim \mathcal{N}\!\left(\mu_n, \tau_n^2\right) \quad \text{(äº‹å¾Œ)}
\end{aligned}
$$

$$
\tau_n^2 = \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}\right)^{-1}, \quad
\mu_n = \tau_n^2 \left(\frac{\mu_0}{\tau_0^2} + \frac{\sum_i x_i}{\sigma^2}\right)
$$

NUTSã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³:

$$
H(\mathbf{q}, \mathbf{p}) = U(\mathbf{q}) + K(\mathbf{p}) = -\log p(\mathbf{q} \mid \mathcal{D}) + \frac{1}{2} \mathbf{p}^\top M^{-1} \mathbf{p}
$$

$\mathbf{q}$: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä½ç½®ã€$\mathbf{p}$: è£œåŠ©é‹å‹•é‡ã€$M$: è³ªé‡è¡Œåˆ—ï¼ˆTuring ãŒè‡ªå‹•æ¨å®šï¼‰ã€‚

- **è¨˜å·â†”å¤‰æ•°å**: $\boldsymbol{\theta}$ = `(Î¼, Ïƒ)`ã€$\mathcal{D}$ = `y`ï¼ˆè¦³æ¸¬å€¤ï¼‰ã€‚
- **shape**: `chain` ã¯ `Chains`å‹ã€‚`chain[:Î¼]` ã§ `Matrix{Float64}(iterations, chains)`ã€‚
- **è½ã¨ã—ç©´**: `NUTS(0.65)` ã® `0.65` ã¯ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå—å®¹ç‡ï¼ˆacceptance rateï¼‰ã€‚`0.8` ç¨‹åº¦ãŒå®‰å®šã—ã‚„ã™ã„ãŒã€è¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«ã§ã¯ `0.65` ãŒæ¨™æº–çš„ã€‚

```julia
using Turing, MCMCChains, Statistics

# ãƒ™ã‚¤ã‚ºæ­£è¦ãƒ¢ãƒ‡ãƒ«: Î¼, Ïƒ ã®äº‹å¾Œåˆ†å¸ƒã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
@model function normal_model(y)
    # äº‹å‰åˆ†å¸ƒ: Î¼ ~ N(0,1), Ïƒ ~ Exponential(1)
    Î¼ ~ Normal(0.0, 1.0)
    Ïƒ ~ Exponential(1.0)
    # å°¤åº¦: y[i] ~ N(Î¼, Ïƒ)
    for i in eachindex(y)
        y[i] ~ Normal(Î¼, Ïƒ)
    end
end

y_obs = [0.730, 0.714, 0.742, 0.720, 0.700, 0.731, 0.750, 0.710]

model  = normal_model(y_obs)
chain  = sample(model, NUTS(0.65), MCMCSerial(), 2000, 4; progress=false)

# äº‹å¾Œçµ±è¨ˆé‡
Î¼_post_mean = mean(chain[:Î¼])
Î¼_post_std  = std(chain[:Î¼])
Ïƒ_post_mean = mean(chain[:Ïƒ])

@printf "Î¼ äº‹å¾Œ: mean=%.4f  std=%.4f\n" Î¼_post_mean Î¼_post_std
@printf "Ïƒ äº‹å¾Œ: mean=%.4f  std=%.4f\n" Ïƒ_post_mean std(chain[:Ïƒ])

# å…±å½¹äº‹å‰åˆ†å¸ƒã«ã‚ˆã‚‹è§£æè§£ã¨ã®æ¯”è¼ƒ
n, Ïƒ_known = length(y_obs), 0.02
Î¼â‚€, Ï„â‚€ = 0.0, 1.0
Ï„_nÂ² = 1 / (1/Ï„â‚€^2 + n/Ïƒ_known^2)
Î¼_n  = Ï„_nÂ² * (Î¼â‚€/Ï„â‚€^2 + sum(y_obs)/Ïƒ_known^2)
@printf "è§£æè§£ Î¼_n=%.4f  Ï„_n=%.6f\n" Î¼_n âˆšÏ„_nÂ²
```

#### MCMC åæŸè¨ºæ–­ï¼ˆRÌ‚ ã¨ ESSï¼‰

$\hat{R}$ï¼ˆGelman-Rubin çµ±è¨ˆé‡ï¼‰ã¯è¤‡æ•°ãƒã‚§ãƒ¼ãƒ³é–“ã®åˆ†æ•£æ¯”:

$$
\hat{R} = \sqrt{\frac{\hat{V}}{W}}
$$

$\hat{V}$: ãƒ—ãƒ¼ãƒ«åˆ†æ•£ã®æ¨å®šã€$W$: ãƒã‚§ãƒ¼ãƒ³å†…åˆ†æ•£ã®å¹³å‡ã€‚$\hat{R} \approx 1.0$ ãŒåæŸã®ç›®å®‰ã€‚

Effective Sample Sizeï¼ˆESSï¼‰:

$$
\mathrm{ESS} = \frac{S}{1 + 2\sum_{\tau=1}^{\infty} \rho_\tau}
$$

$S$: ç·ã‚µãƒ³ãƒ—ãƒ«æ•°ã€$\rho_\tau$: è‡ªå·±ç›¸é–¢ä¿‚æ•°ã€‚

- **è¨˜å·â†”å¤‰æ•°å**: $\hat{R}$ = `rhat(chain)`ã€ESS = `ess(chain)`ã€‚
- **è½ã¨ã—ç©´**: $\hat{R} > 1.01$ ã®ã¨ãã¯åæŸæœªé”ã€‚chains æ•°ã‚’å¢—ã‚„ã™ã‹ã€warmup æœŸé–“ã‚’å»¶ã°ã™ã€‚ESS < 100 ã®ã¨ãã¯ä¿¡é ¼æ€§ã®ä½ã„ã‚µãƒ³ãƒ—ãƒ«ã€‚

```julia
using MCMCChains

# RÌ‚ ã¨ ESS ã‚’è¨ˆç®—
rhat_vals = MCMCChains.rhat(chain)
ess_vals  = MCMCChains.ess(chain)

println("åæŸè¨ºæ–­:")
for sym in [:Î¼, :Ïƒ]
    r = rhat_vals[sym].nt.rhat[1]
    e = ess_vals[sym].nt.ess[1]
    status = r < 1.01 && e > 400 ? "âœ… åæŸ" : "âš ï¸ è¦ç¢ºèª"
    @printf "  %s: RÌ‚=%.4f  ESS=%.1f  %s\n" sym r e status
end

# äº‹å¾Œäºˆæ¸¬ãƒã‚§ãƒƒã‚¯: è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã®på€¤
y_pred = [rand(Normal(rand(chain[:Î¼]), rand(chain[:Ïƒ]))) for _ in 1:1000]
p_check = mean(y_pred .> mean(y_obs))
@printf "äº‹å¾Œäºˆæ¸¬ãƒã‚§ãƒƒã‚¯: P(Å· > È³) = %.3f  (â‰ˆ0.5 ãŒæœ›ã¾ã—ã„)\n" p_check
```

> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. $\hat{R} = 1.05$ ã®ãƒã‚§ãƒ¼ãƒ³ã§æ¨è«–ã‚’ç¶šã‘ã‚‹ãƒªã‚¹ã‚¯ã‚’èª¬æ˜ã›ã‚ˆã€‚
> 2. NUTSã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå—å®¹ç‡ã‚’0.65ã‹ã‚‰0.95ã«ä¸Šã’ã‚‹ã¨ä½•ãŒèµ·ã“ã‚‹ã‹ï¼ˆåˆ©ç‚¹ã¨æ¬ ç‚¹ï¼‰ã€‚

---

### 5.4 å¯è¦–åŒ–ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ â€” Makie.jl / AlgebraOfGraphics.jl

**æ‰±ã†ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸**: `CairoMakie.jl` / `AlgebraOfGraphics.jl`

#### åˆ†å¸ƒå¯è¦–åŒ–ã®é¸æŠåŸºæº–

| å›³ã®ç¨®é¡ | æƒ…å ±é‡ | é©ã—ãŸå ´é¢ |
|:---------|:-------|:-----------|
| ç®±ã²ã’å›³ | 5æ•°è¦ç´„ | ã‚°ãƒ«ãƒ¼ãƒ—æ¯”è¼ƒã€å¤–ã‚Œå€¤ç¢ºèª |
| ãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆ | åˆ†å¸ƒå½¢çŠ¶ | å¤šå³°æ€§ãƒ»æ­ªã¿ã®å¯è¦–åŒ– |
| Raincloud Plot | ç”Ÿãƒ‡ãƒ¼ã‚¿+åˆ†å¸ƒ | å°ã€œä¸­ã‚µãƒ³ãƒ—ãƒ«ã®å®Œå…¨é–‹ç¤º |
| ç‚¹æ¨å®š+CI | ä¸ç¢ºã‹ã• | è«–æ–‡æ²è¼‰ã€åŠ¹æœé‡å ±å‘Š |

Raincloud Plot ã¯ã€Œç”Ÿãƒ‡ãƒ¼ã‚¿æ•£å¸ƒå›³ + ãƒã‚¤ã‚ªãƒªãƒ³ï¼ˆåŠå´ï¼‰ + ç®±ã²ã’å›³ã€ã®3å±¤æ§‹é€ :

$$
\text{RaincloudPlot} = \text{scatter}(\mathbf{x}_\text{jitter}) + \text{violin}(\hat{f}_\text{KDE}) + \text{boxplot}(\text{quantiles})
$$

KDE æ¨å®šã®ãƒãƒ³ãƒ‰å¹…é¸æŠï¼ˆSilvermanãƒ«ãƒ¼ãƒ«ï¼‰:

$$
h = 1.06 \, \hat{\sigma} \, n^{-1/5}
$$

- **è¨˜å·â†”å¤‰æ•°å**: $\hat{f}_\text{KDE}$ = `kde(values)`ï¼ˆKernelDensity.jlï¼‰ã€$h$ = `1.06 * std(values) * length(values)^(-0.2)`ã€‚
- **shape**: `groups::Vector{Int}` ã¯å„ãƒ‡ãƒ¼ã‚¿ç‚¹ã®ã‚°ãƒ«ãƒ¼ãƒ—ãƒ©ãƒ™ãƒ«ï¼ˆ1, 2, 3ï¼‰ã€‚`values::Vector{Float64}` ã¯åŒã˜é•·ã•ã€‚
- **è½ã¨ã—ç©´**: `violin!(ax, groups, values)` ã®ç¬¬2å¼•æ•°ã¯ã‚°ãƒ«ãƒ¼ãƒ—ãƒ©ãƒ™ãƒ«ï¼ˆ`Int` or `String`ï¼‰ã€‚Makie 0.21ä»¥é™ã§ã¯ `side=:left`/`:right` ã§åŠå´ãƒã‚¤ã‚ªãƒªãƒ³ãŒä½¿ãˆã‚‹ã€‚

```julia
using CairoMakie, Distributions, Random
Random.seed!(42)

# ç”Ÿæˆãƒ¢ãƒ‡ãƒ«3ç¨®ã®FIDã‚¹ã‚³ã‚¢ï¼ˆå„30ã‚µãƒ³ãƒ—ãƒ«ï¼‰
n = 30
g_labels = vcat(fill(1, n), fill(2, n), fill(3, n))
g_values = vcat(
    rand(Normal(0.720, 0.018), n),   # ãƒ¢ãƒ‡ãƒ« A
    rand(Normal(0.778, 0.015), n),   # ãƒ¢ãƒ‡ãƒ« B
    rand(Normal(0.680, 0.022), n)    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
)
g_names = ["Model A", "Model B", "Baseline"]

fig = Figure(size=(1000, 500), fontsize=14)

# --- å·¦: ç®±ã²ã’å›³ + ãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆ ---
ax1 = Axis(fig[1, 1],
    title  = "Box + Violin",
    xlabel = "Model",
    ylabel = "FID Score",
    xticks = (1:3, g_names)
)
violin!(ax1, g_labels, g_values; width=0.6, alpha=0.5)
boxplot!(ax1, g_labels, g_values; width=0.15, color=:white,
         whiskerwidth=0.5, strokewidth=2)

# --- å³: Raincloud Plot (åŠå´ãƒã‚¤ã‚ªãƒªãƒ³ + ç”Ÿãƒ‡ãƒ¼ã‚¿ + ç®±ã²ã’å›³) ---
ax2 = Axis(fig[1, 2],
    title  = "Raincloud Plot",
    xlabel = "Model",
    ylabel = "FID Score",
    xticks = (1:3, g_names)
)
violin!(ax2, g_labels, g_values; side=:left, width=0.4, alpha=0.6)
boxplot!(ax2, g_labels, g_values; width=0.12, color=:white,
         offset=0.0, whiskerwidth=0.4, strokewidth=2)
# ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’å³å´ã«ã‚¸ãƒƒã‚¿ãƒ¼æ•£å¸ƒ
jitter = 0.12 .+ 0.06 .* randn(length(g_values))
scatter!(ax2, g_labels .+ jitter, g_values;
         alpha=0.5, markersize=5, color=(:steelblue, 0.5))

save("stats_raincloud.png", fig)
println("Saved: stats_raincloud.png")
```

#### ä¿¡é ¼åŒºé–“è¡¨ç¤ºï¼ˆAlgebraOfGraphics.jlï¼‰

$$
\bar{x} \pm t_{1-\alpha/2, \, n-1} \cdot \frac{s}{\sqrt{n}}
$$

```julia
using AlgebraOfGraphics, CairoMakie, DataFrames, HypothesisTests, Statistics

# å¹³å‡ Â± 95%CI ã‚’æ•´ç†
rows = map(1:3) do g
    vals = g_values[g_labels .== g]
    t    = OneSampleTTest(vals, 0.0)
    ci   = confint(t)
    (; group=g_names[g], mean=mean(vals), lo=ci[1], hi=ci[2])
end
df_ci = DataFrame(rows)

# AlgebraOfGraphics ã§ãƒã‚¤ãƒ³ãƒˆ+ã‚¨ãƒ©ãƒ¼ãƒãƒ¼
plt = data(df_ci) *
      mapping(:group, :mean; lower=:lo, upper=:hi) *
      (visual(Scatter, markersize=12) + visual(Errorbars))
fig2 = draw(plt; axis=(xlabel="Model", ylabel="FID Score (95% CI)",
                       title="Point Estimates with Confidence Intervals"))
save("stats_ci_plot.png", fig2)
println("Saved: stats_ci_plot.png")
```

> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. Raincloud Plot ãŒãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆã‚ˆã‚Šã€Œèª å®Ÿã€ã¨ã•ã‚Œã‚‹ç†ç”±ã‚’èª¬æ˜ã›ã‚ˆã€‚
> 2. Silvermanãƒ«ãƒ¼ãƒ«ã®ãƒãƒ³ãƒ‰å¹… $h$ ãŒã‚µãƒ³ãƒ—ãƒ«æ•° $n$ ã«å¯¾ã—ã¦ $n^{-1/5}$ ã§æ¸›å°‘ã™ã‚‹æ„å‘³ã‚’è¿°ã¹ã‚ˆã€‚

---

### 5.5 æ¼”ç¿’: çµ±è¨ˆçš„æœ‰æ„ vs å®Ÿç”¨çš„æœ‰æ„

#### åŠ¹æœé‡ã®æ•°å¼ã¨å®Ÿè£…

Cohen's $d$ï¼ˆ2ç¾¤ã®æ¨™æº–åŒ–å¹³å‡å·®ï¼‰:

$$
d = \frac{\bar{x}_A - \bar{x}_B}{s_p}, \quad s_p = \sqrt{\frac{(n_A-1)s_A^2 + (n_B-1)s_B^2}{n_A+n_B-2}}
$$

è§£é‡ˆåŸºæº–: $|d| < 0.2$ï¼ˆç„¡è¦–ã§ãã‚‹ï¼‰ã€$0.2 \le |d| < 0.5$ï¼ˆå°ï¼‰ã€$0.5 \le |d| < 0.8$ï¼ˆä¸­ï¼‰ã€$|d| \ge 0.8$ï¼ˆå¤§ï¼‰ã€‚

ç›¸é–¢ä¿‚æ•° $r$ ã‚’åŠ¹æœé‡ã¨ã—ã¦ä½¿ã†å ´åˆï¼ˆMann-Whitney U ã‹ã‚‰ã®å¤‰æ›ï¼‰:

$$
r = \frac{Z}{\sqrt{N}}
$$

$Z$: æ­£è¦è¿‘ä¼¼ã—ãŸ z ã‚¹ã‚³ã‚¢ã€$N$: ç·ã‚µãƒ³ãƒ—ãƒ«æ•°ã€‚

- **è¨˜å·â†”å¤‰æ•°å**: $s_p$ = `s_pooled`ã€$d$ = `cohens_d`ã€$n_A$ = `length(a)`ã€$s_A^2$ = `var(a)`ã€‚
- **shape**: `a, b` ã¯ `Vector{Float64}`ã€‚ã‚¹ã‚«ãƒ©ãƒ¼ã‚’è¿”ã™ã€‚
- **è½ã¨ã—ç©´**: Cohen's $d$ ã¯ã€Œå¤§ãã„åŠ¹æœé‡ â‰  å®Ÿç”¨çš„ã«é‡è¦ã€ã€‚æœ€å°è‡¨åºŠçš„æ„ç¾©å·®ï¼ˆMCIDï¼‰ã¨ã®æ¯”è¼ƒãŒæœ¬è³ªã€‚

```julia
using HypothesisTests, Statistics, Printf, Random
Random.seed!(2025)

# --- Cohen's d ã®å®Ÿè£… ---
function cohens_d(a::Vector{Float64}, b::Vector{Float64})
    n_a, n_b = length(a), length(b)
    s_pooled = âˆš(((n_a-1)*var(a) + (n_b-1)*var(b)) / (n_a+n_b-2))
    return (mean(a) - mean(b)) / s_pooled
end

# ç”Ÿæˆãƒ¢ãƒ‡ãƒ«è©•ä¾¡: çµ±è¨ˆçš„æœ‰æ„ã§ã‚‚å®Ÿç”¨çš„ã«ç„¡æ„å‘³ãªã‚·ãƒŠãƒªã‚ª
a_large = rand(Normal(0.7200, 0.01), 10_000)   # N=10000, å¾®å°å·®
b_large = rand(Normal(0.7201, 0.01), 10_000)   # 0.01% ã®å·®

t_large = EqualVarianceTTest(a_large, b_large)
d_large = cohens_d(a_large, b_large)

@printf "å¤§ã‚µãƒ³ãƒ—ãƒ«(N=10000): p=%.2e  d=%.4f  æœ‰æ„=%s  å®Ÿç”¨çš„=%s\n" pvalue(t_large) d_large (pvalue(t_large)<0.05 ? "âœ…" : "âŒ") (abs(d_large)>=0.2 ? "âœ…" : "âŒ ç„¡æ„å‘³")

# å®Ÿç”¨çš„ã«é‡è¦ãªã‚·ãƒŠãƒªã‚ªï¼ˆå°ã‚µãƒ³ãƒ—ãƒ«ã€å¤§åŠ¹æœé‡ï¼‰
a_small = rand(Normal(0.720, 0.02), 8)
b_small = rand(Normal(0.780, 0.02), 8)   # 0.06 = 3Ïƒ ã®å·®

t_small = EqualVarianceTTest(a_small, b_small)
d_small = cohens_d(a_small, b_small)

@printf "å°ã‚µãƒ³ãƒ—ãƒ«(N=8):    p=%.4f      d=%.4f  æœ‰æ„=%s  å®Ÿç”¨çš„=%s\n" pvalue(t_small) d_small (pvalue(t_small)<0.05 ? "âœ…" : "âŒ") (abs(d_small)>=0.8 ? "âœ… å¤§" : "ä¸­ä»¥ä¸‹")
```

#### p-hacking ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

p-hacking ã®å®Ÿæ…‹: ã€Œã©ã“ã‹ã§æœ‰æ„ã«ãªã‚‹ã¾ã§ç¹°ã‚Šè¿”ã™ã€ã¨ç¬¬ä¸€ç¨®éèª¤ç‡ãŒæ€¥ä¸Šæ˜‡ã™ã‚‹ã€‚

$$
P(\text{å°‘ãªãã¨ã‚‚1å›æœ‰æ„}) = 1 - (1-\alpha)^m \approx m\alpha \quad (\text{å¸°ç„¡ä»®èª¬ãŒçœŸã®ã¨ã})
$$

$m$ å›ã®ç‹¬ç«‹æ¤œå®šã§ $\alpha = 0.05$ ãªã‚‰ã°ã€$m=14$ ã§å½é™½æ€§ç‡ãŒ50%ã‚’è¶…ãˆã‚‹ã€‚

- **è¨˜å·â†”å¤‰æ•°å**: $m$ = `n_tests`ã€$\alpha$ = `0.05`ã€`false_positive_rate` = å®Ÿé¨“çš„å½é™½æ€§ç‡ã€‚
- **shape**: ãƒ«ãƒ¼ãƒ—å¤‰æ•°ã€‚çµæœã¯ `Float64` ã®å‰²åˆã€‚

```julia
using HypothesisTests, Random
Random.seed!(42)

# p-hacking ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: å¸°ç„¡ä»®èª¬ãŒçœŸã®ãƒ‡ãƒ¼ã‚¿ã§ç¹°ã‚Šè¿”ã™
function phacking_sim(n_experiments::Int, n_tests_per_exp::Int, Î±=0.05)
    false_positive = 0
    for _ in 1:n_experiments
        # n_tests_per_exp å›æ¤œå®šã‚’è¡Œã„ã€1å›ã§ã‚‚ p<Î± ãªã‚‰ã€Œæœ‰æ„ã¨å ±å‘Šã€
        found_sig = false
        for _ in 1:n_tests_per_exp
            a = randn(20)
            b = randn(20)          # å¸°ç„¡ä»®èª¬ãŒçœŸ (Î¼_a = Î¼_b = 0)
            t = EqualVarianceTTest(a, b)
            pvalue(t) < Î± && (found_sig = true; break)
        end
        found_sig && (false_positive += 1)
    end
    return false_positive / n_experiments
end

@printf "ç†è«–å€¤ (1-(1-0.05)^m):\n"
for m in [1, 5, 10, 14, 20]
    theory = 1 - (1-0.05)^m
    empirical = phacking_sim(10_000, m)
    @printf "  m=%2d: ç†è«–=%.3f  å®Ÿé¨“=%.3f\n" m theory empirical
end
```

#### ç”Ÿæˆãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã¸ã®å¿œç”¨

på€¤ã ã‘ã§ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒã™ã‚‹ã“ã¨ã®å±é™ºæ€§:

1. **FID ã®çµ¶å¯¾å€¤** ã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ»å®Ÿè£…ã«ã‚ˆã£ã¦å¤‰ã‚ã‚‹ã€‚ç¾¤é–“æ¯”è¼ƒãŒæœ¬è³ªã€‚
2. **åŠ¹æœé‡ Cohen's $d$** ã§ã€Œæ”¹å–„å¹…ãŒå®Ÿç”¨çš„ã‹ã€ã‚’æ¸¬ã‚‹ã€‚
3. **å¤šé‡æ¯”è¼ƒè£œæ­£**ï¼ˆBHæ³•ï¼‰ã§èª¤ç™ºè¦‹ã‚’åˆ¶å¾¡ã™ã‚‹ã€‚
4. **ãƒ™ã‚¤ã‚ºçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**ã§ã€Œæ”¹å–„ã®äº‹å¾Œç¢ºç‡ã€ã‚’è¨ˆç®—ã™ã‚‹æ–¹ãŒè§£é‡ˆã—ã‚„ã™ã„ã€‚

```julia
using HypothesisTests, MultipleTesting, Statistics, Printf, Random
Random.seed!(2025)

# ç”Ÿæˆãƒ¢ãƒ‡ãƒ«è©•ä¾¡: 5æŒ‡æ¨™Ã—2ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒ
metrics = ["FIDâ†“", "ISâ†‘", "Precisionâ†‘", "Recallâ†‘", "F1â†‘"]
model_a = [rand(Normal(Î¼, 0.02), 10) for Î¼ in [0.720, 0.850, 0.780, 0.760, 0.770]]
model_b = [rand(Normal(Î¼, 0.02), 10) for Î¼ in [0.750, 0.870, 0.790, 0.770, 0.780]]

raw_pvals = Float64[]
ds        = Float64[]

for (a, b) in zip(model_a, model_b)
    t  = EqualVarianceTTest(a, b)
    d  = (mean(a) - mean(b)) / âˆš(((9*var(a) + 9*var(b))/18))
    push!(raw_pvals, pvalue(t))
    push!(ds, abs(d))
end

adj_pvals = adjust(raw_pvals, BenjaminiHochberg())

println("ãƒ¡ãƒˆãƒªã‚¯ã‚¹    raw_p     BH_p    Cohen_d  åˆ¤å®š")
for (m, rp, ap, d) in zip(metrics, raw_pvals, adj_pvals, ds)
    verdict = ap < 0.05 && d >= 0.5 ? "âœ… æœ‰æ„ã‹ã¤å®Ÿç”¨çš„" :
              ap < 0.05             ? "âš ï¸ æœ‰æ„ã ãŒåŠ¹æœå°" :
              d >= 0.5              ? "âš ï¸ éæœ‰æ„ã ãŒåŠ¹æœä¸­å¤§" :
                                      "âŒ å·®ãªã—"
    @printf "%-12s  %.4f    %.4f   %.3f    %s\n" m rp ap d verdict
end
```

**çµè«–**: çµ±è¨ˆçš„æœ‰æ„æ€§ï¼ˆp < 0.05ï¼‰ã¨å®Ÿç”¨çš„æœ‰æ„æ€§ï¼ˆåŠ¹æœé‡ $d \ge 0.5$ï¼‰ã¯åˆ¥ç‰©ã ã€‚å¤§ã‚µãƒ³ãƒ—ãƒ«ã§ã¯äº›ç´°ãªå·®ã‚‚ã€Œæœ‰æ„ã€ã«ãªã‚‹ä¸€æ–¹ã€å°ã‚µãƒ³ãƒ—ãƒ«ã§ã¯é‡è¦ãªå·®ãŒã€Œéæœ‰æ„ã€ã®ã¾ã¾åŸ‹ã‚‚ã‚Œã‚‹ã€‚ç”Ÿæˆãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã§ã¯åŠ¹æœé‡ãƒ»ä¿¡é ¼åŒºé–“ãƒ»å¤šé‡æ¯”è¼ƒè£œæ­£ã®ä¸‰ç‚¹ã‚»ãƒƒãƒˆã‚’æƒãˆã¦ã¯ã˜ã‚ã¦ã€ä¸»å¼µãŒç§‘å­¦çš„æ ¹æ‹ ã‚’æŒã¤ã€‚

> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. `phacking_sim(10_000, 20)` ã®çµæœãŒ `1-(1-0.05)^20 â‰ˆ 0.64` ã«è¿‘ã„ç†ç”±ã‚’æ•°å¼ã§èª¬æ˜ã›ã‚ˆã€‚
> 2. FIDãŒã€Œæœ‰æ„ã‹ã¤åŠ¹æœé‡å¤§ã€ã§ã‚‚ã€ã€Œå®Ÿç”¨çš„ã«æ„å‘³ãŒã‚ã‚‹æ”¹å–„ã€ã¨æ–­è¨€ã§ããªã„çŠ¶æ³ã‚’1ã¤æŒ™ã’ã‚ˆã€‚

---


## ğŸ”¬ Z6. æ–°ãŸãªå†’é™ºã¸ï¼ˆç ”ç©¶å‹•å‘ï¼‰

ï¼ˆçµ±è¨ˆå­¦ã®æœ€æ–°ç ”ç©¶å‹•å‘ã¯ Â§ ä»˜éŒ²A-D ã‚’å‚ç…§ï¼‰

## ğŸ­ Z7. ã‚¨ãƒ”ãƒ­ãƒ¼ã‚°ï¼ˆã¾ã¨ã‚ãƒ»FAQãƒ»æ¬¡å›äºˆå‘Šï¼‰

ï¼ˆæœ¬è¬›ç¾©ã®ã¾ã¨ã‚ã¯ Â§ ä»˜éŒ²B-D ã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã‚’å‚ç…§ï¼‰

## è‘—è€…ãƒªãƒ³ã‚¯
- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
