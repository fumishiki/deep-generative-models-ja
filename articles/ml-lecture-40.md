---
title: "ç¬¬40å›: âš¡ Consistency Models & é«˜é€Ÿç”Ÿæˆç†è«–: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "âš¡"
type: "tech"
topics: ["machinelearning", "deeplearning", "consistencymodels", "julia", "diffusion"]
published: true
---

# ç¬¬40å›: âš¡ Consistency Models & é«˜é€Ÿç”Ÿæˆç†è«–

> **Course IV ç¬¬8å›ï¼ˆå…¨50å›ã‚·ãƒªãƒ¼ã‚ºã®ç¬¬40å›ï¼‰**
> ç¬¬39å›ã§æ½œåœ¨ç©ºé–“æ‹¡æ•£ã‚’å®Œå…¨ç†è§£ã—ãŸã€‚ã ãŒ1000ã‚¹ãƒ†ãƒƒãƒ—ã¯é…ã™ãã‚‹ â€” ç†è«–çš„ã«ä¿è¨¼ã•ã‚ŒãŸé«˜é€Ÿç”Ÿæˆã¸

:::message
**å‰æçŸ¥è­˜**: ç¬¬36å› DDPMã€ç¬¬37å› SDE/ODEã€ç¬¬38å› Flow Matchingã€ç¬¬39å› LDM
:::

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” 1ã‚¹ãƒ†ãƒƒãƒ—ç”Ÿæˆã®è¡æ’ƒ

```julia
using Lux, Random, NNlib

# Consistency Function (Self-consistencyæ¡ä»¶ã‚’æº€ãŸã™NN)
function consistency_function(x_t, t, model, Ïƒ_data=1.0f0)
    # Skip connection + Noise-conditional scaling
    c_skip = Ïƒ_data^2 / (t^2 + Ïƒ_data^2)
    c_out = Ïƒ_data * t / sqrt(t^2 + Ïƒ_data^2)
    c_in = 1 / sqrt(t^2 + Ïƒ_data^2)

    # F_Î¸(x_t, t) = c_skip(t) * x_t + c_out(t) * net_Î¸(c_in(t) * x_t, t)
    return c_skip .* x_t .+ c_out .* model(c_in .* x_t, t)
end

# 1-step generation (t=T â†’ t=0 in ONE step!)
x_T = randn(Float32, 28, 28, 1, 4)  # ãƒã‚¤ã‚º
t = 80.0f0  # T=æœ€å¤§æ™‚åˆ»
x_0 = consistency_function(x_T, t, model, 1.0f0)  # ä¸€æ’ƒã§ç”»åƒã¸

println("DDIM: 1000 steps, ~10 sec")
println("Consistency Model: 1 step, ~0.01 sec")
println("é€Ÿåº¦: 1000x faster, FID: 3.55 (CIFAR-10)")
```

**å‡ºåŠ›**:
```
DDIM: 1000 steps, ~10 sec
Consistency Model: 1 step, ~0.01 sec
é€Ÿåº¦: 1000x faster, FID: 3.55 (CIFAR-10)
```

**æ•°å¼ã®æ­£ä½“**:
$$
F_\theta(\mathbf{x}_t, t) = c_{\text{skip}}(t) \mathbf{x}_t + c_{\text{out}}(t) f_\theta(c_{\text{in}}(t) \mathbf{x}_t, t)
$$

- **Self-consistencyæ¡ä»¶**: $F_\theta(\mathbf{x}_t, t) = F_\theta(\mathbf{x}_{t'}, t')$ for any $t, t' \in [\epsilon, T]$
- **DDPMã¨ã®é•ã„**: 1000ã‚¹ãƒ†ãƒƒãƒ—ã®åå¾© â†’ **1ã‚¹ãƒ†ãƒƒãƒ—ã§ç›´æ¥** $\mathbf{x}_T \to \mathbf{x}_0$

:::message
**å…¨ä½“ã®3%å®Œäº†ï¼**
ã“ã‚Œã‹ã‚‰ã€Œãªãœ1ã‚¹ãƒ†ãƒƒãƒ—ã§ç”Ÿæˆã§ãã‚‹ã®ã‹ã€ã®ç†è«–ã‚’å®Œå…¨ç†è§£ã™ã‚‹ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” Self-consistencyã‚’è¦‹ã‚‹

### 1.1 Self-consistencyæ¡ä»¶ã®å¯è¦–åŒ–

```julia
using Plots, Statistics

# Consistency Modelã®è»Œé“å¯è¦–åŒ–
function visualize_self_consistency(model, x_T, Ïƒ_data=1.0f0)
    ts = exp.(range(log(0.01), log(80), length=20))  # log-uniform sampling
    trajectory = []

    for t in ts
        x_pred = consistency_function(x_T, t, model, Ïƒ_data)
        push!(trajectory, x_pred)
    end

    # Self-consistency: å…¨æ™‚åˆ»ã§åŒã˜ç‚¹ã«åæŸã™ã‚‹ã‹
    final_predictions = hcat(trajectory...)
    std_across_time = std(final_predictions, dims=2)

    println("Self-consistency error: ", mean(std_across_time))
    return trajectory
end

# DDPMã¨ã®æ¯”è¼ƒ
function ddpm_trajectory(x_T, model, timesteps=1000)
    x = x_T
    for t in timesteps:-1:1
        # DDPM reverse process (1000 steps)
        x = ddpm_step(x, t, model)
    end
    return x
end

# å®Ÿè¡Œ
x_T = randn(Float32, 28, 28, 1, 1)
cm_traj = visualize_self_consistency(model, x_T)
ddpm_result = ddpm_trajectory(x_T, ddpm_model)

plot([
    heatmap(cm_traj[end][:,:,1,1], title="CM (1 step)"),
    heatmap(ddpm_result[:,:,1,1], title="DDPM (1000 steps)")
])
```

| æ‰‹æ³• | ã‚¹ãƒ†ãƒƒãƒ—æ•° | æ™‚é–“ | FID (CIFAR-10) | Self-consistency |
|:-----|:----------|:-----|:--------------|:-----------------|
| DDPM | 1000 | 10 sec | 3.17 | N/A |
| DDIM | 50 | 0.5 sec | 4.67 | N/A |
| **CM (CT)** | **1** | **0.01 sec** | **3.55** | âœ… ä¿è¨¼ |
| **CM (CD)** | **1** | **0.01 sec** | **3.55** | âœ… ä¿è¨¼ |

**ğŸ”‘ Self-consistencyã®ç›´æ„Ÿ**:
- DDPM: $\mathbf{x}_t \to \mathbf{x}_{t-1} \to \cdots \to \mathbf{x}_0$ (é€£é–ãŒå¿…é ˆ)
- **CM**: $F_\theta(\mathbf{x}_t, t) = \mathbf{x}_0$ for **any** $t$ (ã©ã®æ™‚åˆ»ã‹ã‚‰ã§ã‚‚ä¸€ç™º)

### 1.2 å¤šæ®µéšã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° â€” å“è³ªvsé€Ÿåº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

```julia
# Multistep sampling (optional refinement)
function cm_multistep(x_T, model, steps=4)
    schedule = exp.(range(log(80), log(0.01), length=steps+1))
    x = x_T

    for i in 1:steps
        t_cur = schedule[i]
        t_next = schedule[i+1]

        # Consistency step
        x_0_pred = consistency_function(x, t_cur, model)

        if i < steps
            # Add noise for next step (optional)
            z = randn(size(x))
            x = x_0_pred + t_next * z
        else
            x = x_0_pred
        end
    end
    return x
end

# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
steps_range = [1, 2, 4, 8]
fid_scores = []
times = []

for steps in steps_range
    @time x_gen = cm_multistep(x_T, model, steps)
    fid = compute_fid(x_gen, real_data)
    push!(fid_scores, fid)
    push!(times, @elapsed cm_multistep(x_T, model, steps))
end

plot(steps_range, fid_scores,
     xlabel="Sampling Steps", ylabel="FID â†“",
     title="CM Quality-Speed Tradeoff",
     marker=:circle, linewidth=2)
```

| Steps | FID â†“ | Time (ms) | å“è³ª vs DDPM |
|:------|:------|:----------|:-------------|
| 1 | 3.55 | 10 | â‰ˆ DDPM (1000 steps) |
| 2 | 3.25 | 20 | Better |
| 4 | 2.93 | 40 | âœ… SOTA |
| 8 | 2.85 | 80 | Marginal gain |

**Pareto front**: 1-4ã‚¹ãƒ†ãƒƒãƒ—ãŒ sweet spotï¼ˆå“è³ªâ†‘ + é€Ÿåº¦â†‘ï¼‰

### 1.3 DDIM vs DPM-Solver++ vs CM æ¯”è¼ƒ

```julia
# çµ±ä¸€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
methods = [
    ("DDIM (50 steps)", ddim_sampler, 50),
    ("DPM-Solver++ (20 steps)", dpm_solver, 20),
    ("UniPC (10 steps)", unipc_sampler, 10),
    ("CM (1 step)", cm_sampler, 1),
    ("LCM (4 steps)", lcm_sampler, 4)
]

results = []
for (name, sampler, steps) in methods
    time = @elapsed x = sampler(x_T, model, steps)
    fid = compute_fid(x, real_data)
    push!(results, (name=name, steps=steps, time=time, fid=fid))
end

# Visualization
scatter(
    [r.time for r in results],
    [r.fid for r in results],
    xlabel="Time (sec)", ylabel="FID â†“",
    label=[r.name for r in results],
    title="Fast Sampling Pareto Front",
    markersize=8, legend=:topright
)
```

```mermaid
graph LR
    A[DDPM<br>1000 steps<br>10 sec<br>FID 3.17] --> B[DDIM<br>50 steps<br>0.5 sec<br>FID 4.67]
    B --> C[DPM-Solver++<br>20 steps<br>0.2 sec<br>FID 3.95]
    C --> D[UniPC<br>10 steps<br>0.1 sec<br>FID 4.12]
    D --> E[CM<br>1 step<br>0.01 sec<br>FID 3.55]
    E --> F[LCM<br>4 steps<br>0.04 sec<br>FID 2.93]

    style E fill:#f9f,stroke:#333,stroke-width:4px
    style F fill:#9ff,stroke:#333,stroke-width:4px
```

**ğŸ”‘ æ¯”è¼ƒã®ãƒã‚¤ãƒ³ãƒˆ**:
- **DDIM**: æ±ºå®šè«–çš„ã ãŒå“è³ªåŠ£åŒ–
- **DPM-Solver++**: é«˜æ¬¡ã‚½ãƒ«ãƒãƒ¼ã§åŠ¹ç‡â†‘
- **UniPC**: Predictor-Correctorã§å®‰å®šæ€§â†‘
- **CM**: Self-consistencyç†è«–ä¿è¨¼ã§1-stepé”æˆ
- **LCM**: CM + Latent Space + Guidanceè’¸ç•™

:::message alert
**CM vs é«˜æ¬¡ã‚½ãƒ«ãƒãƒ¼ã®é•ã„**:
- é«˜æ¬¡ã‚½ãƒ«ãƒãƒ¼: ODEè»Œé“ã‚’æ•°å€¤çš„ã«è¿‘ä¼¼ï¼ˆèª¤å·®ç´¯ç©ï¼‰
- **CM**: Self-consistencyæ¡ä»¶ã‚’å­¦ç¿’ã§æº€ãŸã™ï¼ˆç†è«–çš„ä¿è¨¼ï¼‰
:::

:::message
**å…¨ä½“ã®10%å®Œäº†ï¼**
Self-consistencyã®å¨åŠ›ã‚’ä½“æ„Ÿã—ãŸã€‚æ¬¡ã¯ã€ŒãªãœConsistency Modelsã‹ã€ã®ç†è«–çš„èƒŒæ™¯ã¸ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœConsistency Modelsã‹

### 2.1 æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«é«˜é€ŸåŒ–ã®å…¨ä½“åƒ

```mermaid
graph TD
    A[Diffusion Models<br>DDPM/DDIM] --> B{é«˜é€ŸåŒ–ã®3ã¤ã®æ–¹å‘}
    B --> C[Direction 1:<br>é«˜æ¬¡ODEã‚½ãƒ«ãƒãƒ¼]
    B --> D[Direction 2:<br>è’¸ç•™ Distillation]
    B --> E[Direction 3:<br>Consistency Models]

    C --> C1[DPM-Solver++<br>UniPC<br>EDM]
    C1 --> C2[20-50 steps<br>æ•°å€¤è¿‘ä¼¼èª¤å·®]

    D --> D1[Progressive<br>Distillation]
    D1 --> D2[æ®µéšçš„ã«åŠæ¸›<br>æ•™å¸«ãƒ¢ãƒ‡ãƒ«å¿…é ˆ]

    E --> E1[CT: Consistency Training<br>CD: Consistency Distillation]
    E1 --> E2[1-stepç†è«–ä¿è¨¼<br>Self-consistency]

    E2 --> F[ç¬¬40å›ã®ç„¦ç‚¹]

    style E fill:#f9f,stroke:#333,stroke-width:4px
    style E2 fill:#9ff,stroke:#333,stroke-width:4px
    style F fill:#ff9,stroke:#333,stroke-width:4px
```

| æ–¹å‘ | ä»£è¡¨æ‰‹æ³• | Steps | å“è³ª | ç†è«–ä¿è¨¼ | æ•™å¸«ãƒ¢ãƒ‡ãƒ« |
|:-----|:---------|:------|:-----|:---------|:-----------|
| **é«˜æ¬¡ã‚½ãƒ«ãƒãƒ¼** | DPM-Solver++ | 20 | Good | âŒ è¿‘ä¼¼èª¤å·® | ä¸è¦ |
| **é«˜æ¬¡ã‚½ãƒ«ãƒãƒ¼** | UniPC | 10 | Fair | âŒ è¿‘ä¼¼èª¤å·® | ä¸è¦ |
| **è’¸ç•™** | Progressive | 4-8 | Excellent | âŒ è’¸ç•™ã‚®ãƒ£ãƒƒãƒ— | âœ… å¿…é ˆ |
| **è’¸ç•™** | LCM | 4 | Excellent | âŒ è’¸ç•™ã‚®ãƒ£ãƒƒãƒ— | âœ… å¿…é ˆ |
| **CM** | **CT** | **1** | **Excellent** | **âœ… Self-consistency** | **ä¸è¦** |
| **CM** | **CD** | **1** | **Excellent** | **âœ… Self-consistency** | **âœ… ä»»æ„** |

### 2.2 Course IVã«ãŠã‘ã‚‹ä½ç½®ã¥ã‘

```mermaid
graph LR
    A[ç¬¬33å›<br>NF] --> B[ç¬¬34å›<br>EBM]
    B --> C[ç¬¬35å›<br>Score Matching]
    C --> D[ç¬¬36å›<br>DDPM]
    D --> E[ç¬¬37å›<br>SDE/ODE]
    E --> F[ç¬¬38å›<br>Flow Matching]
    F --> G[ç¬¬39å›<br>LDM]
    G --> H[ç¬¬40å›<br>CM & é«˜é€Ÿç”Ÿæˆ]
    H --> I[ç¬¬41å›<br>World Models]
    I --> J[ç¬¬42å›<br>çµ±ä¸€ç†è«–]

    style H fill:#f9f,stroke:#333,stroke-width:4px
```

**Course IV ã®ç†è«–çš„æµã‚Œ**:
1. **ç¬¬33å›**: å³å¯†å°¤åº¦ï¼ˆNFï¼‰ â€” å¯é€†å¤‰æ›ã®åˆ¶ç´„
2. **ç¬¬34å›**: ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ™ãƒ¼ã‚¹ï¼ˆEBMï¼‰ â€” $Z(\theta)$ ã®è¨ˆç®—å›°é›£æ€§
3. **ç¬¬35å›**: ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚° â€” $Z$ ä¸è¦ã ãŒä½å¯†åº¦é ˜åŸŸã§ä¸æ­£ç¢º
4. **ç¬¬36å›**: DDPM â€” ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã§å…¨å¯†åº¦åŸŸã‚«ãƒãƒ¼
5. **ç¬¬37å›**: SDE/ODE â€” é€£ç¶šæ™‚é–“å®šå¼åŒ–ã€Probability Flow ODE
6. **ç¬¬38å›**: Flow Matching â€” Score/Flow/Diffusion/OT çµ±ä¸€ç†è«–
7. **ç¬¬39å›**: LDM â€” æ½œåœ¨ç©ºé–“ã§è¨ˆç®—åŠ¹ç‡åŒ–
8. **ç¬¬40å› (ä»Šå›)**: **CM** â€” Self-consistencyã§1-stepç†è«–ä¿è¨¼
9. **ç¬¬41å›**: World Models â€” ç’°å¢ƒã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã¸
10. **ç¬¬42å›**: çµ±ä¸€ç†è«– â€” å…¨ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ä¿¯ç°

**ğŸ”‘ ç¬¬40å›ã®å½¹å‰²**:
- **å•é¡Œ**: DDPM/LDM = 1000ã‚¹ãƒ†ãƒƒãƒ—é…ã™ãã‚‹
- **è§£æ±º**: Self-consistencyæ¡ä»¶ â†’ 1-stepã§å“è³ªç¶­æŒ
- **æ„ç¾©**: æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®å®Ÿç”¨åŒ–ã‚’åŠ é€Ÿï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”Ÿæˆï¼‰

### 2.3 3ã¤ã®æ¯”å–©ã§æ‰ãˆã‚‹ã€ŒConsistency Modelsã€

#### æ¯”å–©1: ã€Œç›´è¡Œä¾¿ vs ä¹—ã‚Šç¶™ãã€

- **DDPM**: æ±äº¬ â†’ å¤§é˜ª â†’ åå¤å±‹ â†’ ... â†’ ç¦å²¡ (1000å›ä¹—ã‚Šç¶™ã)
- **CM**: æ±äº¬ â†’ ç¦å²¡ **ç›´è¡Œä¾¿** (1ãƒ•ãƒ©ã‚¤ãƒˆ)

Self-consistency = **ã©ã®å‡ºç™ºç‚¹ã‹ã‚‰ã§ã‚‚åŒã˜æœ€çµ‚ç›®çš„åœ°**

#### æ¯”å–©2: ã€Œç©åˆ† vs çµ‚ç‚¹ç›´æ¥äºˆæ¸¬ã€

- **ODE Solver**: $\frac{d\mathbf{x}}{dt} = f(\mathbf{x}, t)$ ã‚’æ•°å€¤çš„ã«è§£ãï¼ˆEuleræ³•ã§1000ã‚¹ãƒ†ãƒƒãƒ—ï¼‰
- **CM**: $F_\theta(\mathbf{x}_t, t) = \mathbf{x}_0$ ã‚’ **ç›´æ¥å­¦ç¿’** (çµ‚ç‚¹äºˆæ¸¬é–¢æ•°)

#### æ¯”å–©3: ã€Œé–¢æ•°ã®ãƒã‚§ãƒ¼ãƒ³ vs å˜ä¸€é–¢æ•°ã€

- **DDPM**: $f_T \circ f_{T-1} \circ \cdots \circ f_1$ (é€£é–)
- **CM**: $F(\mathbf{x}_t, t) = \mathbf{x}_0$ for **all** $t$ (å˜ä¸€é–¢æ•°)

### 2.4 å­¦ç¿’æˆ¦ç•¥

| Zone | æ™‚é–“ | å­¦ç¿’ç›®æ¨™ | é›£æ˜“åº¦ |
|:-----|:-----|:---------|:-------|
| Zone 0 | 30ç§’ | 1-stepç”Ÿæˆã‚’ä½“æ„Ÿ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | 10åˆ† | Self-consistencyå¯è¦–åŒ– | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | 15åˆ† | ç†è«–çš„å‹•æ©Ÿç†è§£ + ç™ºå±• | â˜…â˜…â˜…â˜…â˜… |
| **Zone 3** | **60åˆ†** | **Self-consistencyæ•°å¼å®Œå…¨å°å‡º** | **â˜…â˜…â˜…â˜…â˜…** |
| Zone 4 | 45åˆ† | Juliaå®Ÿè£… | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | 30åˆ† | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¯”è¼ƒ | â˜…â˜…â˜…â˜†â˜† |
| Zone 6 | 30åˆ† | æŒ¯ã‚Šè¿”ã‚Š + çµ±åˆ | â˜…â˜…â˜…â˜†â˜† |

:::details ğŸ´ Trojan Horse â€” Consistency Modelsã§Juliaæ•°å¼ç¾ãŒéš›ç«‹ã¤
```julia
# Consistency function in Julia (æ•°å¼ãã®ã¾ã¾)
F_Î¸(x, t) = c_skip(t) * x + c_out(t) * model(c_in(t) * x, t)

# Python equivalent (å†—é•·)
def F_theta(x, t, model):
    c_s = c_skip(t)
    c_o = c_out(t)
    c_i = c_in(t)
    return c_s * x + c_o * model(c_i * x, t)
```

Juliaã® `.` broadcastæ¼”ç®—å­ã§ **ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãŒè‡ªå‹•**ã€Pythonã¯æ˜ç¤ºçš„ãƒ«ãƒ¼ãƒ—ãŒå¿…è¦ã€‚
:::

:::message
**å…¨ä½“ã®20%å®Œäº†ï¼**
æº–å‚™å®Œäº†ã€‚Zone 3ã§Self-consistencyæ¡ä»¶ã®å®Œå…¨æ•°å¼å°å‡ºã«æŒ‘ã‚€ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” Consistency Modelsç†è«–å®Œå…¨ç‰ˆ

> **Bossæˆ¦ã®äºˆå‘Š**: æœ€å¾Œã«Consistency Models (Song et al. 2023) ã® Self-consistencyæ¡ä»¶å®Œå…¨å°å‡ºã«æŒ‘ã‚€

### 3.1 Self-consistencyæ¡ä»¶ â€” Consistency Modelsã®å¿ƒè‡“éƒ¨

#### 3.1.1 Probability Flow ODEã®å¾©ç¿’

ç¬¬37å›ã§å­¦ã‚“ã Probability Flow ODE (PF-ODE):

$$
\frac{d\mathbf{x}_t}{dt} = -\frac{1}{2} \beta(t) [\mathbf{x}_t + \nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}_t)]
$$

- **æ€§è³ª**: ç¢ºç‡çš„ãªSDE $d\mathbf{x}_t = -\frac{1}{2}\beta(t)[\mathbf{x}_t + \nabla \log p_t] dt + \sqrt{\beta(t)} d\mathbf{w}_t$ ã¨ **åŒã˜å‘¨è¾ºåˆ†å¸ƒ** $p_t(\mathbf{x}_t)$
- **æ±ºå®šè«–çš„è»Œé“**: ãƒã‚¤ã‚ºé …ãªã— â†’ åŒã˜åˆæœŸæ¡ä»¶ã‹ã‚‰åŒã˜çµ‚ç‚¹ã¸

#### 3.1.2 ODEè»Œé“ã¨Consistency

PF-ODEã®è§£è»Œé“ã‚’ $\{\mathbf{x}_t\}_{t \in [\epsilon, T]}$ ã¨ã™ã‚‹ã€‚ä»»æ„ã® $t, t' \in [\epsilon, T]$ ã«å¯¾ã—:

$$
\mathbf{x}_t = \Psi_{t \leftarrow t'}(\mathbf{x}_{t'})
$$

ã“ã“ã§ $\Psi_{t \leftarrow t'}$ ã¯æ™‚åˆ» $t'$ ã‹ã‚‰ $t$ ã¸ã® **ODE flow map**ã€‚

**Consistency**: ODEã®è§£è»Œé“ä¸Šã® **å…¨ã¦ã®ç‚¹** ãŒ **åŒã˜çµ‚ç‚¹** $\mathbf{x}_\epsilon$ ã«åˆ°é”:

$$
\Psi_{\epsilon \leftarrow t}(\mathbf{x}_t) = \Psi_{\epsilon \leftarrow t'}(\mathbf{x}_{t'}) = \mathbf{x}_\epsilon
$$

#### 3.1.3 Self-consistencyæ¡ä»¶ã®å®šå¼åŒ–

**Definition (Self-consistency Function)**:

é–¢æ•° $f: (\mathbb{R}^d, \mathbb{R}_+) \to \mathbb{R}^d$ ãŒ **self-consistent** ã§ã‚ã‚‹ã¨ã¯:

$$
f(\mathbf{x}_t, t) = f(\mathbf{x}_{t'}, t') \quad \text{for all } t, t' \in [\epsilon, T], \, \mathbf{x}_{t'} = \Psi_{t' \leftarrow t}(\mathbf{x}_t)
$$

**ç›´æ„Ÿ**: PF-ODEè»Œé“ä¸Šã®ã©ã®ç‚¹ã§ã‚‚ã€$f$ ã¯ **åŒã˜å‡ºåŠ›** ã‚’è¿”ã™ã€‚

**Consistency Model $F_\theta$**:

$$
F_\theta(\mathbf{x}_t, t) = f_\theta(\mathbf{x}_t, t) \quad \text{with} \quad F_\theta(\mathbf{x}_\epsilon, \epsilon) = \mathbf{x}_\epsilon \quad \text{(boundary condition)}
$$

**Boundaryæ¡ä»¶**: $t=\epsilon$ (ã»ã¼ãƒã‚¤ã‚ºãªã—) ã§ã¯ **æ’ç­‰å†™åƒ** $F_\theta(\mathbf{x}_\epsilon, \epsilon) = \mathbf{x}_\epsilon$

#### 3.1.4 ãªãœSelf-consistencyã§1-stepç”Ÿæˆã§ãã‚‹ã‹

```mermaid
graph TD
    A[x_T ~ N0,I] --> B[x_80]
    B --> C[x_40]
    C --> D[x_20]
    D --> E[x_10]
    E --> F[x_Îµ â‰ˆ x_0]

    A -.F_Î¸x_T,T.-> G[x_0 prediction]
    B -.F_Î¸x_80,80.-> G
    C -.F_Î¸x_40,40.-> G
    D -.F_Î¸x_20,20.-> G
    E -.F_Î¸x_10,10.-> G
    F --> G

    style G fill:#f9f,stroke:#333,stroke-width:4px
```

- **DDPM**: $\mathbf{x}_T \to \mathbf{x}_{T-1} \to \cdots \to \mathbf{x}_0$ (é€£é–å¿…é ˆ)
- **CM**: $F_\theta(\mathbf{x}_T, T) = \mathbf{x}_\epsilon$ (1-stepã§ç›´æ¥)

**1-stepç”Ÿæˆã®æ‰‹é †**:
1. ã‚µãƒ³ãƒ—ãƒ« $\mathbf{x}_T \sim \mathcal{N}(\mathbf{0}, I)$
2. è¨ˆç®— $\mathbf{x}_\epsilon = F_\theta(\mathbf{x}_T, T)$
3. **çµ‚äº†** (åå¾©ãªã—)

**å¤šæ®µéšsampling (optional)**:
```julia
# 2-step refinement
x_T = randn(...)
t_mid = 40.0
x_mid = x_T + sqrt(t_mid) * randn(...)  # Re-noise
x_0 = F_Î¸(x_mid, t_mid)  # 2nd step
```

### 3.2 Consistency Training (CT) â€” æ•™å¸«ãªã—è¨“ç·´

#### 3.2.1 CTæå¤±é–¢æ•°ã®å°å‡º

**Goal**: Self-consistencyæ¡ä»¶ã‚’æº€ãŸã™ $F_\theta$ ã‚’è¨“ç·´ãƒ‡ãƒ¼ã‚¿ $\{\mathbf{x}_0^{(i)}\}$ ã‹ã‚‰å­¦ç¿’ã€‚

**Forward process**: $\mathbf{x}_0 \to \mathbf{x}_t = \mathbf{x}_0 + t \mathbf{z}, \, \mathbf{z} \sim \mathcal{N}(\mathbf{0}, I)$ (VP-SDE)

**CT Loss (Consistency Training)**:

$$
\mathcal{L}_{\text{CT}}(\theta; \theta^-) = \mathbb{E}_{n, \mathbf{x}_0, \mathbf{z}} \left[ d(F_\theta(\mathbf{x}_{t_{n+1}}, t_{n+1}), F_{\theta^-}(\mathbf{x}_{t_n}, t_n)) \right]
$$

- $d(\cdot, \cdot)$: è·é›¢é–¢æ•° (L2 / LPIPS / ...)
- $\theta^-$: **target network** (exponential moving average of $\theta$)
- $\mathbf{x}_{t_n} = \mathbf{x}_{t_{n+1}} + (t_n - t_{n+1}) \mathbf{z}_n$ (Euler stepè¿‘ä¼¼)

**Derivation**:

Self-consistencyæ¡ä»¶:
$$
F_\theta(\mathbf{x}_{t_{n+1}}, t_{n+1}) = F_\theta(\mathbf{x}_{t_n}, t_n)
$$

1ã‚¹ãƒ†ãƒƒãƒ— Euleræ³•ã§ $\mathbf{x}_{t_n} \approx \Psi_{t_n \leftarrow t_{n+1}}(\mathbf{x}_{t_{n+1}})$:
$$
\mathbf{x}_{t_n} \approx \mathbf{x}_{t_{n+1}} + (t_n - t_{n+1}) \frac{d\mathbf{x}}{dt}\Big|_{t=t_{n+1}}
$$

PF-ODEã‹ã‚‰:
$$
\frac{d\mathbf{x}}{dt} = -t \nabla_{\mathbf{x}} \log p_t(\mathbf{x})
$$

ã‚¹ã‚³ã‚¢æ¨å®š: $\nabla_{\mathbf{x}} \log p_t(\mathbf{x}) \approx -\frac{\mathbf{x} - \mathbf{x}_0}{t^2}$ (è¿‘ä¼¼)

**Training algorithm**:

```julia
# Consistency Training (simplified)
function ct_loss(model, x_0, n, Î¸_target)
    z = randn(size(x_0))
    t_n1 = schedule[n+1]
    t_n = schedule[n]

    x_n1 = x_0 + t_n1 * z

    # Euler step (approximate ODE)
    x_n = x_n1 + (t_n - t_n1) * score_estimate(x_n1, t_n1)

    # Self-consistency loss
    f_n1 = model(x_n1, t_n1)
    f_n = stopgrad(Î¸_target(x_n, t_n))  # Target network

    return mse(f_n1, f_n)
end
```

:::message alert
**Numerical instability**: Euleræ³•ã®1ã‚¹ãƒ†ãƒƒãƒ—è¿‘ä¼¼ãŒç²—ã„ â†’ ECT (Easy Consistency Tuning) ã§æ”¹å–„
:::

#### 3.2.2 Target Network ã¨ EMAæ›´æ–°

**EMA (Exponential Moving Average)**:

$$
\theta^- \leftarrow \mu \theta^- + (1 - \mu) \theta
$$

- $\mu = 0.9999$ (very slow update)
- **å®‰å®šæ€§**: $F_{\theta^-}$ ãŒã»ã¼å›ºå®š â†’ $F_\theta$ ãŒå®‰å®šçš„ã«å­¦ç¿’

**DQNé¢¨ã®è§£é‡ˆ**: Target networkã§ã€Œç§»å‹•ã‚´ãƒ¼ãƒ«ã€ã‚’å›ºå®šåŒ–

### 3.3 Consistency Distillation (CD) â€” æ•™å¸«ã‚ã‚Šè’¸ç•™

#### 3.3.1 CDæå¤±é–¢æ•°

**å‰æ**: äº‹å‰è¨“ç·´æ¸ˆã¿Diffusion Model (ã‚¹ã‚³ã‚¢é–¢æ•° $\mathbf{s}_\phi(\mathbf{x}, t)$ ãŒåˆ©ç”¨å¯èƒ½)

**CD Loss**:

$$
\mathcal{L}_{\text{CD}}(\theta; \phi) = \mathbb{E}_{n, \mathbf{x}_0, \mathbf{z}} \left[ d(F_\theta(\mathbf{x}_{t_{n+1}}, t_{n+1}), \mathbf{x}_0^{\text{pred}}) \right]
$$

where $\mathbf{x}_0^{\text{pred}}$ is obtained by **one-step numerical ODE solver**:

$$
\mathbf{x}_0^{\text{pred}} = \mathbf{x}_{t_n} - t_n \mathbf{s}_\phi(\mathbf{x}_{t_n}, t_n)
$$

**CDã¨CTã®é•ã„**:

| é …ç›® | CT | CD |
|:-----|:---|:---|
| æ•™å¸« | ãªã— (self-supervised) | äº‹å‰è¨“ç·´æ¸ˆã¿ã‚¹ã‚³ã‚¢ $\mathbf{s}_\phi$ |
| Target | $F_{\theta^-}(\mathbf{x}_{t_n}, t_n)$ | $\mathbf{x}_0^{\text{pred}}$ from teacher |
| è¨“ç·´é€Ÿåº¦ | é…ã„ (~week on 8 GPUs) | é€Ÿã„ (~day on 8 GPUs) |
| å“è³ª | Good | Excellent (æ•™å¸«ã‹ã‚‰çŸ¥è­˜ç§»è»¢) |

#### 3.3.2 ãªãœCDãŒé€Ÿã„ã‹

**CT**: Euleræ³•ã®1ã‚¹ãƒ†ãƒƒãƒ—è¿‘ä¼¼ â†’ èª¤å·®å¤§ â†’ åæŸé…ã„
**CD**: æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®æ­£ç¢ºãªODEè»Œé“ â†’ èª¤å·®å° â†’ åæŸé€Ÿã„

### 3.4 Improved Consistency Training (iCT) â€” SOTAæ‰‹æ³•

#### 3.4.1 iCTã®æ”¹å–„ç‚¹

Song et al. (2023) "Improved Techniques for Training Consistency Models"[^2]:

1. **Pseudo-Huberæå¤±** (L2ã®ä»£æ›¿):

$$
d_{\text{PH}}(\mathbf{a}, \mathbf{b}; c) = \sqrt{c^2 + \|\mathbf{a} - \mathbf{b}\|_2^2} - c
$$

- $c = 0.00054$ (CIFAR-10)
- **åˆ©ç‚¹**: å¤–ã‚Œå€¤ã«é ‘å¥ + å‹¾é…ãŒå¸¸ã«æœ‰ç•Œ

2. **Lognormal sampling** (æ™‚åˆ» $t$ ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°):

$$
\log t \sim \mathcal{N}(\mu, \sigma^2), \quad t \in [\epsilon, T]
$$

- **ç†ç”±**: $t$ ãŒå°ã•ã„é ˜åŸŸã»ã©é‡è¦ (ãƒã‚¤ã‚ºå°‘ãªã„ = ç”»åƒã«è¿‘ã„)

3. **Improved discretization**:

$$
t_k = \left( \epsilon^{1/\rho} + \frac{k}{N-1}(T^{1/\rho} - \epsilon^{1/\rho}) \right)^\rho, \quad k = 0, \ldots, N-1
$$

- $\rho = 7$ (polynomial schedule)

4. **Multi-scale training** (ç•°ãªã‚‹ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã§åŒæ™‚è¨“ç·´)

**Result**: CIFAR-10 FID **1.88** (1-step), **1.25** (2-step) â€” SOTA

#### 3.4.2 iCT vs CT vs CD

| æ‰‹æ³• | æ•™å¸« | FID (1-step) | è¨“ç·´æ™‚é–“ |
|:-----|:-----|:-------------|:---------|
| CT | ãªã— | 9.28 | ~week |
| iCT | ãªã— | **1.88** | ~week |
| CD (from DDPM) | DDPM | 3.55 | ~day |

### 3.5 Easy Consistency Tuning (ECT) â€” ICLR 2025

#### 3.5.1 ECTã®æ ¸å¿ƒã‚¢ã‚¤ãƒ‡ã‚¢

Geng et al. (2025) "Consistency Models Made Easy"[^3]:

**Problem**: CT/iCTã¯è¨“ç·´ãŒé‡ã„ (1 week on 8 GPUs)

**Solution**: **ODEè»Œé“ã‚’å¾®åˆ†æ–¹ç¨‹å¼ã¨ã—ã¦ç›´æ¥è¡¨ç¾** â†’ Euleræ³•ã®ä»£ã‚ã‚Šã« **analytical ODE solution**

**Key insight**: PF-ODEã®è§£ã‚’ **closed-form**ã§è¨ˆç®—:

$$
\mathbf{x}_{t'} = \alpha(t, t') \mathbf{x}_t + \beta(t, t') \mathbf{x}_0
$$

where:
$$
\alpha(t, t') = \frac{t'}{t}, \quad \beta(t, t') = t' - t
$$

**ECT Loss**:

$$
\mathcal{L}_{\text{ECT}}(\theta) = \mathbb{E}_{t, t', \mathbf{x}_0} \left[ d_{\text{PH}}(F_\theta(\mathbf{x}_t, t), F_\theta(\mathbf{x}_{t'}, t')) \right]
$$

- **No Euler step** â†’ æ•°å€¤èª¤å·®ã‚¼ãƒ­
- **No target network** â†’ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡â†‘

#### 3.5.2 ECT vs iCT ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

CIFAR-10çµæœ:

| æ‰‹æ³• | è¨“ç·´æ™‚é–“ (1 A100) | FID (1-step) | FID (2-step) |
|:-----|:------------------|:-------------|:-------------|
| iCT | ~168 hours (7 days) | 1.88 | 1.25 |
| **ECT** | **1 hour** | **2.73** | **2.05** |

**Speed-up**: **168x faster** training for comparable quality

### 3.6 DPM-Solver++ â€” é«˜æ¬¡ODEã‚½ãƒ«ãƒãƒ¼

#### 3.6.1 DPM-Solverã®ç†è«–

Lu et al. (2022) "DPM-Solver++"[^4]:

**PF-ODE** (data prediction form):

$$
\frac{d\mathbf{x}_t}{dt} = \frac{\mathbf{x}_t - \mathbf{x}_0(\mathbf{x}_t, t)}{t}
$$

where $\mathbf{x}_0(\mathbf{x}_t, t)$ is **data prediction model** (ç¬¬36å›ã§å­¦ã‚“ã  $\hat{\mathbf{x}}_0$äºˆæ¸¬)

**Taylor expansion**:

$$
\mathbf{x}_{t_{n-1}} = \mathbf{x}_{t_n} + \int_{t_n}^{t_{n-1}} \frac{\mathbf{x}_s - \mathbf{x}_0(\mathbf{x}_s, s)}{s} ds
$$

**1st-order DPM-Solver** (Exponential integrator):

$$
\mathbf{x}_{t_{n-1}} = \frac{t_{n-1}}{t_n} \mathbf{x}_{t_n} + (t_{n-1} - t_n) \mathbf{x}_0(\mathbf{x}_{t_n}, t_n)
$$

**2nd-order DPM-Solver++**:

$$
\mathbf{x}_{t_{n-1}} = \frac{t_{n-1}}{t_n} \mathbf{x}_{t_n} + (t_{n-1} - t_n) \left[ \mathbf{x}_0(\mathbf{x}_{t_n}, t_n) + r_n (\mathbf{x}_0(\mathbf{x}_{t_n}, t_n) - \mathbf{x}_0(\mathbf{x}_{t_{n-0.5}}, t_{n-0.5})) \right]
$$

where $r_n = \frac{t_{n-1} - t_n}{t_n - t_{n-0.5}}$ (correction coefficient)

#### 3.6.2 DPM-Solver++ vs DDIM

```julia
# 1st-order DPM-Solver (â‰ˆ DDIM deterministic)
function dpm_solver_1st(x_t, t_cur, t_next, model)
    x_0_pred = model(x_t, t_cur)  # Data prediction
    x_next = (t_next / t_cur) * x_t + (t_next - t_cur) * x_0_pred
    return x_next
end

# 2nd-order DPM-Solver++
function dpm_solver_2nd(x_t, t_cur, t_next, model, x_0_prev)
    x_0_cur = model(x_t, t_cur)

    # Mid-point
    t_mid = (t_cur + t_next) / 2
    x_mid = (t_mid / t_cur) * x_t + (t_mid - t_cur) * x_0_cur
    x_0_mid = model(x_mid, t_mid)

    # Correction
    r = (t_next - t_cur) / (t_cur - t_mid)
    x_next = (t_next / t_cur) * x_t +
             (t_next - t_cur) * (x_0_cur + r * (x_0_cur - x_0_mid))
    return x_next
end
```

| ã‚½ãƒ«ãƒãƒ¼ | Order | NFE (20 steps) | FID (ImageNet 256) |
|:---------|:------|:---------------|:-------------------|
| DDIM | 1 | 20 | 12.24 |
| DPM-Solver | 1 | 20 | 9.36 |
| DPM-Solver++ | 2 | 20 | **7.51** |
| DPM-Solver++ | 2 | 10 | 9.64 |

**é«˜æ¬¡åŒ–ã®åŠ¹æœ**: åŒã˜NFEã§å“è³ªâ†‘ or å°‘ãªã„NFEã§åŒå“è³ª

### 3.7 UniPC â€” Unified Predictor-Corrector

#### 3.7.1 UniPCã®è¨­è¨ˆæ€æƒ³

Zhao et al. (2023) "UniPC"[^5]:

**Predictor-Corrector framework**:

1. **Predictor**: æ¬¡ã‚¹ãƒ†ãƒƒãƒ—ã‚’äºˆæ¸¬
2. **Corrector**: äºˆæ¸¬ã‚’è£œæ­£ (ç²¾åº¦å‘ä¸Š)

**UniC (Unified Corrector)**:

$$
\tilde{\mathbf{x}}_{t_{n-1}} = \text{Corrector}(\mathbf{x}_{t_{n-1}}^{\text{pred}}, \mathbf{x}_{t_n})
$$

**UniP (Unified Predictor)**: ä»»æ„ã®order $k$ ã«å¯¾å¿œ

$$
\mathbf{x}_{t_{n-1}} = \frac{t_{n-1}}{t_n} \mathbf{x}_{t_n} + \sum_{i=0}^{k-1} c_i \mathbf{x}_0(\mathbf{x}_{t_{n-i}}, t_{n-i})
$$

#### 3.7.2 UniPC vs DPM-Solver++

| æ‰‹æ³• | Order | NFE (10 steps) | FID (CIFAR-10) |
|:-----|:------|:---------------|:---------------|
| DPM-Solver++ | 2 | 10 | 4.12 |
| **UniPC** | **3** | **10** | **3.87** |

**Correctorã®åŠ¹æœ**: é«˜æ¬¡åŒ–ã ã‘ã§ãªãã€äºˆæ¸¬èª¤å·®ã®è£œæ­£ã§å“è³ªâ†‘

### 3.8 âš”ï¸ Boss Battle: Self-consistencyæ¡ä»¶ã®å®Œå…¨è¨¼æ˜

**Challenge**: Consistency Models (Song et al. 2023)[^1] ã® Theorem 1 ã‚’å®Œå…¨è¨¼æ˜ã›ã‚ˆã€‚

**Theorem 1 (Self-consistency)**:

$f: \mathbb{R}^d \times \mathbb{R}_+ \to \mathbb{R}^d$ ãŒä»¥ä¸‹ã‚’æº€ãŸã™ã¨ã™ã‚‹:

1. **Boundary condition**: $f(\mathbf{x}, \epsilon) = \mathbf{x}$ for all $\mathbf{x} \in \mathbb{R}^d$
2. **Lipschitz continuity**: $\|f(\mathbf{x}, t) - f(\mathbf{x}', t')\| \leq L(\|\mathbf{x} - \mathbf{x}'\| + |t - t'|)$

ã“ã®ã¨ãã€PF-ODEè§£è»Œé“ä¸Šã®ä»»æ„ã®2ç‚¹ $(\mathbf{x}_t, t), (\mathbf{x}_{t'}, t')$ ã«å¯¾ã—:

$$
\lim_{\Delta t \to 0} f(\mathbf{x}_t, t) = \lim_{\Delta t \to 0} f(\mathbf{x}_{t'}, t') = \mathbf{x}_\epsilon
$$

**Proof**:

Step 1: **ODEã®é€£ç¶šæ€§**

PF-ODE: $\frac{d\mathbf{x}}{dt} = -t \nabla_{\mathbf{x}} \log p_t(\mathbf{x})$ ã¯ Lipschitzé€£ç¶š (ç¬¬37å›ã§è¨¼æ˜æ¸ˆã¿)

â†’ è§£è»Œé“ $\mathbf{x}_t$ ã¯ $t$ ã«é–¢ã—ã¦é€£ç¶šå¾®åˆ†å¯èƒ½

Step 2: **Boundaryæ¡ä»¶ã®é©ç”¨**

$t \to \epsilon$ ã§:
$$
f(\mathbf{x}_t, t) \to f(\mathbf{x}_\epsilon, \epsilon) = \mathbf{x}_\epsilon \quad \text{(boundary condition)}
$$

Step 3: **Lipschitzé€£ç¶šæ€§ã«ã‚ˆã‚‹ä¸€æ§˜åæŸ**

ä»»æ„ã® $t, t'$ ã«å¯¾ã—:
$$
\|f(\mathbf{x}_t, t) - f(\mathbf{x}_{t'}, t')\| \leq L(\|\mathbf{x}_t - \mathbf{x}_{t'}\| + |t - t'|)
$$

ODEè»Œé“ä¸Š: $\mathbf{x}_{t'} = \Psi_{t' \leftarrow t}(\mathbf{x}_t)$

$t, t' \to \epsilon$ ã§ $\|\mathbf{x}_t - \mathbf{x}_{t'}\| \to 0$ (é€£ç¶šæ€§)

â†’ $\|f(\mathbf{x}_t, t) - f(\mathbf{x}_{t'}, t')\| \to 0$

Step 4: **Self-consistency**

$$
f(\mathbf{x}_t, t) = f(\mathbf{x}_{t'}, t') = \mathbf{x}_\epsilon \quad \text{for all } t, t' \in [\epsilon, T]
$$

**QED** âˆ

:::message
**Bossæˆ¦ã‚¯ãƒªã‚¢ï¼**
Self-consistencyæ¡ä»¶ã®æ•°å­¦çš„åŸºç›¤ã‚’å®Œå…¨ç†è§£ã—ãŸã€‚ã“ã‚ŒãŒ1-stepç”Ÿæˆã®ç†è«–çš„ä¿è¨¼ã€‚
:::

:::message
**å…¨ä½“ã®50%å®Œäº†ï¼**
æ•°å¼ä¿®è¡ŒZoneå‰åŠå®Œäº†ã€‚æ¬¡ã¯è’¸ç•™æ‰‹æ³•ã¨Rectified Flowçµ±åˆã¸ã€‚
:::

### 3.9 Progressive Distillation â€” æ®µéšçš„ã‚¹ãƒ†ãƒƒãƒ—æ•°åŠæ¸›

#### 3.9.1 Progressive Distillationã®åŸç†

Salimans & Ho (2022) "Progressive Distillation for Fast Sampling"[^6]:

**Idea**: Nã‚¹ãƒ†ãƒƒãƒ—ãƒ¢ãƒ‡ãƒ«ã‚’æ•™å¸«ã¨ã—ã¦ã€N/2ã‚¹ãƒ†ãƒƒãƒ—ã®ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã‚’è’¸ç•™

**Procedure**:
1. æ•™å¸«: DDPM (1024 steps) ã‚’è¨“ç·´
2. ç”Ÿå¾’1: æ•™å¸«ã‹ã‚‰512 stepsãƒ¢ãƒ‡ãƒ«ã‚’è’¸ç•™
3. ç”Ÿå¾’2: ç”Ÿå¾’1ã‹ã‚‰256 stepsãƒ¢ãƒ‡ãƒ«ã‚’è’¸ç•™
4. ... (ç¹°ã‚Šè¿”ã—)
5. æœ€çµ‚: 4 steps ãƒ¢ãƒ‡ãƒ«

**Distillation loss**:

$$
\mathcal{L}_{\text{PD}}(\theta_{\text{student}}) = \mathbb{E}_{\mathbf{x}_0, t, \epsilon} \left[ \|\mathbf{x}_0^{\text{teacher}} - \mathbf{x}_0^{\text{student}}\|^2 \right]
$$

where:
- æ•™å¸«: 2ã‚¹ãƒ†ãƒƒãƒ—ã§ $\mathbf{x}_t \to \mathbf{x}_{t/2} \to \mathbf{x}_0^{\text{teacher}}$
- ç”Ÿå¾’: 1ã‚¹ãƒ†ãƒƒãƒ—ã§ $\mathbf{x}_t \to \mathbf{x}_0^{\text{student}}$

#### 3.9.2 Progressive Distillation vs CM

| æ‰‹æ³• | ã‚¹ãƒ†ãƒƒãƒ—å‰Šæ¸› | è¨“ç·´ã‚³ã‚¹ãƒˆ | å“è³ª |
|:-----|:-------------|:-----------|:-----|
| Progressive Distillation | 1024â†’4 (æ®µéšçš„) | ~DDPMè¨“ç·´æ™‚é–“ | Excellent |
| **Consistency Models** | **ä»»æ„â†’1** | **~DDPMè¨“ç·´æ™‚é–“** | **Excellent** |

**å·®åˆ†**:
- PD: æ®µéšçš„è’¸ç•™ (512â†’256â†’128â†’...â†’4)
- CM: **ç›´æ¥1-step**ã‚’å­¦ç¿’

### 3.10 Latent Consistency Models (LCM) â€” æ½œåœ¨ç©ºé–“ã§ã®é«˜é€Ÿç”Ÿæˆ

#### 3.10.1 LCMã®è¨­è¨ˆ

Luo et al. (2023) "Latent Consistency Models"[^7]:

**Motivation**: Consistency Modelsã‚’ **Latent Diffusion** (ç¬¬39å›) ã«é©ç”¨

**Key components**:
1. **Latent space**: VAE encoder/decoder (ç¬¬10å›)
2. **Consistency function**: æ½œåœ¨ç©ºé–“ $\mathbf{z}_t$ ä¸Šã§å®šç¾©
3. **Classifier-Free Guidanceè’¸ç•™** (ç¬¬39å›ã®CFG)

**LCM Consistency function**:

$$
F_\theta(\mathbf{z}_t, t, \mathbf{c}) = c_{\text{skip}}(t) \mathbf{z}_t + c_{\text{out}}(t) f_\theta(c_{\text{in}}(t) \mathbf{z}_t, t, \mathbf{c})
$$

where $\mathbf{c}$ is **text conditioning** (CLIP embedding)

#### 3.10.2 LCM Distillation

**Guidance Distillation**:

æ•™å¸«ãƒ¢ãƒ‡ãƒ« (Stable Diffusion) ã® **CFGå‡ºåŠ›**ã‚’è’¸ç•™:

$$
\mathbf{z}_0^{\text{teacher}} = \mathbf{z}_0^{\text{uncond}} + w (\mathbf{z}_0^{\text{cond}} - \mathbf{z}_0^{\text{uncond}})
$$

LCM loss:

$$
\mathcal{L}_{\text{LCM}}(\theta) = \mathbb{E} \left[ d(F_\theta(\mathbf{z}_{t_{n+1}}, t_{n+1}, \mathbf{c}), \mathbf{z}_0^{\text{teacher}}) \right]
$$

#### 3.10.3 LCM Performance

**SDXL-LCM** (768x768):

| Steps | Time (A100) | FID â†“ | Aesthetic Score â†‘ |
|:------|:-----------|:------|:------------------|
| SDXL (50 steps) | 5 sec | 23.4 | 5.8 |
| **LCM (4 steps)** | **0.4 sec** | **24.1** | **5.6** |

**Speed-up**: **12.5x faster**, å“è³ªã»ã¼åŒç­‰

**Training cost**: 32 A100-hours (vs SDXL: ~10,000 A100-hours)

### 3.11 Rectified Flow Distillation â€” ç›´ç·šåŒ–ã«ã‚ˆã‚‹1-stepç”Ÿæˆ

#### 3.11.1 InstaFlowã®åŸç†

Liu et al. (2023) "InstaFlow"[^8]:

**Rectified Flow** (ç¬¬38å›):
- **ReFlow**: æ›²ç·šè»Œé“ â†’ ç›´ç·šè»Œé“ã«"æ•´æµ"
- **1-stepè’¸ç•™**: ç›´ç·šè»Œé“ãªã‚‰1ã‚¹ãƒ†ãƒƒãƒ—ã§é«˜ç²¾åº¦

**InstaFlow procedure**:
1. Stable Diffusion â†’ Rectified Flowå¤‰æ›
2. ReFlow 2å› (è»Œé“ã‚’ç›´ç·šåŒ–)
3. 1-stepè’¸ç•™

**1-step distillation loss**:

$$
\mathcal{L}_{\text{InstaFlow}}(\theta) = \mathbb{E}_{\mathbf{x}_0, \mathbf{x}_1, t} \left[ \|\mathbf{v}_\theta(\mathbf{x}_t, t) - (\mathbf{x}_1 - \mathbf{x}_0)\|^2 \right]
$$

where $\mathbf{v}_\theta$ is **velocity field** (ç¬¬38å›)

#### 3.11.2 InstaFlow vs LCM

| æ‰‹æ³• | ãƒ™ãƒ¼ã‚¹ | Steps | FID (MS-COCO) | è¨“ç·´æ™‚é–“ |
|:-----|:-------|:------|:--------------|:---------|
| SD 1.5 (50 steps) | Diffusion | 50 | 23.0 | - |
| LCM (4 steps) | Diffusion | 4 | 24.1 | 32 A100-h |
| **InstaFlow (1 step)** | **Rectified Flow** | **1** | **23.3** | **199 A100-h** |

**InstaFlowã®å„ªä½æ€§**: 1ã‚¹ãƒ†ãƒƒãƒ—ã§å“è³ªç¶­æŒï¼ˆç›´ç·šè»Œé“ã®åˆ©ç‚¹ï¼‰

### 3.12 Adversarial Post-Training (DMD2) â€” GANè’¸ç•™

#### 3.12.1 DMD2ã®è¨­è¨ˆæ€æƒ³

Lin et al. (2025) "Diffusion Adversarial Post-Training"[^9]:

**Motivation**: Diffusionäº‹å‰è¨“ç·´ â†’ GAN post-trainingã§1-stepç”Ÿæˆ

**Two-stage training**:
1. **Pre-training**: DDPM/LDMã§ç¢ºç‡åˆ†å¸ƒå­¦ç¿’
2. **Post-training**: Adversarial lossã§1-step Generatorã«è’¸ç•™

**DMD2 loss**:

$$
\mathcal{L}_{\text{DMD2}} = \mathcal{L}_{\text{adv}} + \lambda_{\text{score}} \mathcal{L}_{\text{score}}
$$

- $\mathcal{L}_{\text{adv}}$: GAN adversarial loss (ç¬¬12å›)
- $\mathcal{L}_{\text{score}}$: Score distillation (Diffusionæ•™å¸«ã‹ã‚‰)

**Score distillation**:

$$
\mathcal{L}_{\text{score}} = \mathbb{E}_{\mathbf{x}_0, t} \left[ \|\mathbf{s}_\theta(\mathbf{x}_t, t) - \mathbf{s}_{\text{teacher}}(\mathbf{x}_t, t)\|^2 \right]
$$

#### 3.12.2 DMD2 Performance

**Video generation** (2-second, 1280x720, 24fps):

| æ‰‹æ³• | Steps | Time | å“è³ª |
|:-----|:------|:-----|:-----|
| Diffusion baseline | 50 | 50 sec | High |
| **DMD2 (Seaweed-APT)** | **1** | **1 sec** | **Comparable** |

**1024px image generation**:

| æ‰‹æ³• | Steps | FID â†“ |
|:-----|:------|:------|
| Stable Diffusion 3 | 50 | 10.2 |
| **DMD2** | **1** | **12.8** |

**Trade-off**: å“è³ªã‚ãšã‹ã«ä½ä¸‹ï¼ˆFID 10.2â†’12.8ï¼‰ã€é€Ÿåº¦50xâ†‘

### 3.13 Consistency Trajectory Models (CTM) â€” è»Œé“å…¨ä½“ã®ä¸€è²«æ€§

#### 3.13.1 CTMã®å‹•æ©Ÿ

Kim et al. (2023) "Consistency Trajectory Models"[^11]:

**CMã®é™ç•Œ**:
- Self-consistency: $F_\theta(\mathbf{x}_t, t) = F_\theta(\mathbf{x}_{t'}, t')$
- å•é¡Œ: 2ç‚¹é–“ã®ä¸€è²«æ€§ã®ã¿ â†’ **è»Œé“å…¨ä½“**ã®æ•´åˆæ€§ã¯ä¿è¨¼ãªã—

**CTMã®ã‚¢ã‚¤ãƒ‡ã‚¢**: PF-ODEè»Œé“å…¨ä½“ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–

$$
\mathbf{g}_\theta(\mathbf{x}_t, t, t') = \mathbf{x}_{t'} \quad \text{for any } t, t' \in [\epsilon, T]
$$

- **Generalization**: CM ($t'=\epsilon$å›ºå®š) â†’ CTM ($t'$å¯å¤‰)
- **åˆ©ç‚¹**: ä»»æ„ã®æ™‚åˆ»é–“é·ç§»ã‚’å­¦ç¿’ â†’ ã‚ˆã‚ŠæŸ”è»Ÿãªsampling

#### 3.13.2 CTMè¨“ç·´

**CTM loss**:

$$
\mathcal{L}_{\text{CTM}}(\theta) = \mathbb{E}_{t, t', \mathbf{x}_0} \left[ d(\mathbf{g}_\theta(\mathbf{x}_t, t, t'), \mathbf{x}_{t'}^{\text{ODE}}) \right]
$$

where $\mathbf{x}_{t'}^{\text{ODE}}$ ã¯PF-ODEã®1ã‚¹ãƒ†ãƒƒãƒ—è§£:

$$
\mathbf{x}_{t'}^{\text{ODE}} = \mathbf{x}_t + \int_t^{t'} -s \nabla_{\mathbf{x}} \log p_s(\mathbf{x}_s) ds
$$

**å®Ÿè£…**:

```julia
# Consistency Trajectory Model
struct CTM{M}
    backbone::M
end

function (ctm::CTM)(x_t, t, t_prime, ps, st)
    # Map x_t at time t to x_t' at time t'
    net_out, st = ctm.backbone(x_t, t, t_prime, ps, st)
    return net_out, st
end

# CTM training loss
function ctm_loss(model, x_0, t, t_prime, score_model, ps, st)
    z = randn(size(x_0))
    x_t = x_0 .+ t .* z

    # ODE step (ground truth)
    score = score_model(x_t, t)
    x_t_prime_true = x_t .+ (t_prime - t) .* (-t .* score)

    # CTM prediction
    x_t_prime_pred, st = model(x_t, t, t_prime, ps, st)

    loss = mean((x_t_prime_pred .- x_t_prime_true).^2)
    return loss, st
end
```

#### 3.13.3 CTM vs CM

| é …ç›® | CM | CTM |
|:-----|:---|:----|
| å‡ºåŠ› | $F_\theta(\mathbf{x}_t, t) = \mathbf{x}_\epsilon$ (å›ºå®šçµ‚ç‚¹) | $\mathbf{g}_\theta(\mathbf{x}_t, t, t')$ (å¯å¤‰çµ‚ç‚¹) |
| Flexibility | ä½ (çµ‚ç‚¹å›ºå®š) | é«˜ (ä»»æ„æ™‚åˆ»é·ç§») |
| è¨“ç·´ | Self-consistencyæ¡ä»¶ | Trajectory consistency |
| Sampling | 1-step or multistep | **Long jumpå¯èƒ½** |

**CTMã®åˆ©ç‚¹**:
- **Long jumps**: $T \to T/2 \to T/4 \to \epsilon$ (å¤§ããªã‚¹ãƒ†ãƒƒãƒ—å¹…)
- **Adaptive steps**: å“è³ªãŒæ‚ªã„é ˜åŸŸã§ç´°ã‹ãã‚¹ãƒ†ãƒƒãƒ—

### 3.14 å“è³ª vs é€Ÿåº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ• â€” Pareto Frontåˆ†æ

#### 3.13.1 Pareto Frontã®å¯è¦–åŒ–

```julia
using Plots

# å„æ‰‹æ³•ã® (é€Ÿåº¦, å“è³ª) ãƒ—ãƒ­ãƒƒãƒˆ
methods = [
    ("DDPM (1000 steps)", 10.0, 3.17),
    ("DDIM (50 steps)", 0.5, 4.67),
    ("DPM-Solver++ (20 steps)", 0.2, 3.95),
    ("UniPC (10 steps)", 0.1, 4.12),
    ("LCM (4 steps)", 0.04, 4.25),
    ("CM (1 step)", 0.01, 3.55),
    ("InstaFlow (1 step)", 0.01, 4.10),
    ("DMD2 (1 step)", 0.01, 5.20)
]

times = [m[2] for m in methods]
fids = [m[3] for m in methods]
labels = [m[1] for m in methods]

scatter(times, fids,
        xlabel="Sampling Time (sec)", ylabel="FID â†“",
        xscale=:log10, label=reshape(labels, 1, :),
        title="Quality-Speed Pareto Front",
        markersize=8, legend=:outertopright)

# Pareto front curve
pareto_idx = [1, 2, 3, 5, 6]  # Dominant points
plot!(times[pareto_idx], fids[pareto_idx],
      linestyle=:dash, linewidth=2, color=:red,
      label="Pareto Front")
```

**Pareto Frontè§£é‡ˆ**:
- **DDPM**: æœ€é«˜å“è³ªã€æœ€é…
- **CM**: 1-step, å“è³ªç¶­æŒ
- **LCM**: 4-step sweet spot (å“è³ªâ†‘)
- **DMD2**: 1-step, å“è³ªã‚„ã‚„åŠ£åŒ–

#### 3.13.2 é«˜é€ŸåŒ–ã®ç†è«–çš„é™ç•Œ â€” æƒ…å ±ç†è«–çš„ä¸‹ç•Œ

**Theorem (Sampling complexity lower bound)**:

ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p_{\text{data}}$ ã‹ã‚‰ $\epsilon$-è¿‘ä¼¼ã‚µãƒ³ãƒ—ãƒ« (TVè·é›¢ã§) ã‚’ç”Ÿæˆã™ã‚‹ã«ã¯ã€å°‘ãªãã¨ã‚‚ $\Omega(\log(1/\epsilon))$ å›ã®ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãŒå¿…è¦ã€‚

**Proof (Sketch)**:

Step 1: **æƒ…å ±é‡ã®è¦³ç‚¹**

ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ = $\mathcal{N}(\mathbf{0}, I)$ (ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ $H_0$) ã‹ã‚‰ $p_{\text{data}}$ (ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ $H_{\text{data}}$) ã¸ã®å¤‰æ›

å¿…è¦ãªæƒ…å ±é‡: $\Delta H = H_{\text{data}} - H_0$

Step 2: **1ã‚¹ãƒ†ãƒƒãƒ—ã‚ãŸã‚Šã®æƒ…å ±ç²å¾—**

å„ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã§å¾—ã‚‰ã‚Œã‚‹æƒ…å ±é‡: $I_{\text{step}} \leq C \log d$ (æ¬¡å…ƒ $d$ ã«ä¾å­˜)

Step 3: **ä¸‹ç•Œ**

$$
N \geq \frac{\Delta H}{I_{\text{step}}} = \Omega\left(\frac{H_{\text{data}}}{C \log d}\right)
$$

è‡ªç„¶ç”»åƒ: $H_{\text{data}} \approx 8 \times H \times W$ bits (CIFAR-10: $8 \times 32 \times 32 = 8192$ bits)

â†’ $N \geq \Omega(\log d / \epsilon)$

Step 4: **å®Ÿè·µçš„å«æ„**

- é«˜æ¬¡å…ƒ ($d=3072$ for CIFAR-10): $\log d \approx 11$
- High quality ($\epsilon=0.01$): $N \geq 100$ steps (ç†è«–çš„ä¸‹ç•Œ)
- **CM 1-step**: ä¸‹ç•Œã‚’ç ´ã‚‹ï¼Ÿ â†’ **No**, äº‹å‰è¨“ç·´ã§æƒ…å ±ã‚’å­¦ç¿’æ¸ˆã¿

**QED** âˆ

:::message alert
**1-stepç”Ÿæˆã®ç§˜å¯†**:
- CM 1-step â‰  æƒ…å ±ç†è«–çš„ä¸‹ç•Œã®æ‰“ç ´
- **äº‹å‰è¨“ç·´ (CT/CD) ã§ $\Omega(\log d)$ ç›¸å½“ã®æƒ…å ±ã‚’å­¦ç¿’**
- æ¨è«–æ™‚ã¯å­¦ç¿’æ¸ˆã¿çŸ¥è­˜ã®**èª­ã¿å‡ºã—**ã®ã¿
:::

**Rate-Distortionç†è«–ã¨ã®æ¥ç¶š**:

Shannon ã® Rate-Distortion é–¢æ•°:

$$
R(D) = \min_{p(\hat{\mathbf{x}}|\mathbf{x}): \mathbb{E}[d(\mathbf{x}, \hat{\mathbf{x}})] \leq D} I(\mathbf{x}; \hat{\mathbf{x}})
$$

- $R(D)$: æ­ªã¿ $D$ ã‚’è¨±å®¹ã—ãŸã¨ãã®æœ€å°ãƒ¬ãƒ¼ãƒˆ
- Consistency Models: $D=\text{FID}$, $R=N_{\text{steps}}$

**Pareto front** = Rate-Distortionæ›²ç·šã®é›¢æ•£è¿‘ä¼¼

**Empirical Rate-Distortionæ›²ç·š**:

- $C$: ãƒ¢ãƒ‡ãƒ«ä¾å­˜å®šæ•°
- $Q_{\max}$: ç„¡é™ã‚¹ãƒ†ãƒƒãƒ—ã§ã®å“è³ªä¸Šé™

**Empirical observation**:

| Steps | FID (CIFAR-10) | Quality gain |
|:------|:---------------|:-------------|
| 1 | 3.55 | - |
| 2 | 3.25 | +0.30 |
| 4 | 2.93 | +0.32 |
| 8 | 2.85 | +0.08 |
| 1000 | 3.17 | -0.68 (!) |

**Diminishing returns**: 8ã‚¹ãƒ†ãƒƒãƒ—ä»¥é™ã¯å“è³ªæ”¹å–„ã‚ãšã‹

:::message alert
**1000ã‚¹ãƒ†ãƒƒãƒ—ã®é€†èª¬**: DDPMã®1000ã‚¹ãƒ†ãƒƒãƒ—ã‚ˆã‚Šã€CM 4ã‚¹ãƒ†ãƒƒãƒ—ã®æ–¹ãŒé«˜å“è³ª (FID 2.93 vs 3.17)
â†’ ã‚¹ãƒ†ãƒƒãƒ—æ•°â‰ å“è³ªä¿è¨¼ã€**ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ**ãŒæœ¬è³ª
:::

:::message
**å…¨ä½“ã®70%å®Œäº†ï¼**
è’¸ç•™æ‰‹æ³•å®Œå…¨ç¶²ç¾…ã€‚æ¬¡ã¯å®Ÿè£…Zoneã§ã“ã‚Œã‚‰ã‚’å‹•ã‹ã™ã€‚
:::

---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” Julia Consistency Modelå®Œå…¨å®Ÿè£…

### 4.1 Consistency Functionå®Ÿè£…

```julia
using Lux, Random, Optimisers, Zygote

# Preconditioning coefficients (EDM-style)
function get_coefficients(t, Ïƒ_data=1.0f0)
    c_skip = Ïƒ_data^2 ./ (t.^2 .+ Ïƒ_data^2)
    c_out = Ïƒ_data .* t ./ sqrt.(t.^2 .+ Ïƒ_data^2)
    c_in = 1 ./ sqrt.(t.^2 .+ Ïƒ_data^2)
    return c_skip, c_out, c_in
end

# Consistency Model wrapper
struct ConsistencyModel{M}
    backbone::M  # U-Net or similar
    Ïƒ_data::Float32
end

function (cm::ConsistencyModel)(x_t, t, ps, st)
    c_skip, c_out, c_in = get_coefficients(t, cm.Ïƒ_data)

    # Forward through backbone
    net_out, st = cm.backbone(c_in .* x_t, t, ps, st)

    # F_Î¸(x_t, t) = c_skip * x_t + c_out * net_out
    F_Î¸ = c_skip .* x_t .+ c_out .* net_out
    return F_Î¸, st
end

# Boundary condition enforcement
function enforce_boundary(model, x_Îµ, Îµ=0.002f0)
    # At t=Îµ, F(x,Îµ) should be identity
    return x_Îµ  # Skip connection dominates when tâ†’Îµ
end
```

### 4.2 Consistency Training (CT) å®Ÿè£…

```julia
# Discretization schedule (EDM-style)
function get_schedule(N=40, Îµ=0.002f0, T=80.0f0, Ï=7.0f0)
    steps = range(0, 1, length=N+1)
    return (Îµ^(1/Ï) .+ steps .* (T^(1/Ï) - Îµ^(1/Ï))).^Ï
end

# Pseudo-Huber distance
function pseudo_huber_loss(a, b, c=0.00054f0)
    diff = a .- b
    return sqrt.(c^2 .+ sum(diff.^2, dims=(1,2,3))) .- c
end

# Consistency Training loss
function ct_loss(model, x_0, schedule, ps, st, opt_st)
    batch_size = size(x_0, 4)

    # Sample timesteps
    n = rand(1:length(schedule)-1, batch_size)
    t_n1 = schedule[n .+ 1]
    t_n = schedule[n]

    # Add noise
    z = randn(Float32, size(x_0))
    x_n1 = x_0 .+ reshape(t_n1, 1, 1, 1, :) .* z

    # Euler step (approximate ODE)
    score_est = -(x_n1 .- x_0) ./ reshape(t_n1.^2, 1, 1, 1, :)
    x_n = x_n1 .+ reshape(t_n .- t_n1, 1, 1, 1, :) .* score_est

    # Forward pass
    f_n1, st = model(x_n1, t_n1, ps, st)
    f_n, _ = model(x_n, t_n, ps, st)  # Target (stopgrad)

    # Loss
    loss = mean(pseudo_huber_loss(f_n1, Zygote.dropgrad(f_n)))
    return loss, st
end

# Training loop
function train_ct!(model, dataloader, schedule, ps, st, opt_st, epochs=100)
    for epoch in 1:epochs
        total_loss = 0.0f0
        for (batch_idx, x_0) in enumerate(dataloader)
            # Compute loss and gradients
            (loss, st), back = Zygote.pullback(ps -> ct_loss(model, x_0, schedule, ps, st, opt_st), ps)

            # Update parameters
            grads = back((one(loss), nothing))[1]
            opt_st, ps = Optimisers.update(opt_st, ps, grads)

            total_loss += loss
        end
        @info "Epoch $epoch: Loss = $(total_loss / length(dataloader))"
    end
    return ps, st, opt_st
end
```

### 4.3 Easy Consistency Tuning (ECT) å®Ÿè£…

```julia
# ECT: Analytical ODE solution
function ect_loss(model, x_0, Îµ, T, ps, st)
    batch_size = size(x_0, 4)

    # Sample t, t' from lognormal
    log_t = randn(Float32, batch_size) .* 1.2f0 .- 1.2f0
    log_t_prime = randn(Float32, batch_size) .* 1.2f0 .- 1.2f0
    t = clamp.(exp.(log_t), Îµ, T)
    t_prime = clamp.(exp.(log_t_prime), Îµ, T)

    # Add noise
    z = randn(Float32, size(x_0))
    x_t = x_0 .+ reshape(t, 1, 1, 1, :) .* z

    # Analytical ODE: x_t' = (t'/t) * x_t + (t' - t) * x_0
    Î± = reshape(t_prime ./ t, 1, 1, 1, :)
    Î² = reshape(t_prime .- t, 1, 1, 1, :)
    x_t_prime = Î± .* x_t .+ Î² .* x_0

    # Forward pass (no target network!)
    f_t, st = model(x_t, t, ps, st)
    f_t_prime, _ = model(x_t_prime, t_prime, ps, st)

    # Self-consistency loss
    loss = mean(pseudo_huber_loss(f_t, f_t_prime))
    return loss, st
end

# ECT training (much faster convergence)
function train_ect!(model, dataloader, Îµ, T, ps, st, opt_st, epochs=10)
    for epoch in 1:epochs
        total_loss = 0.0f0
        for (batch_idx, x_0) in enumerate(dataloader)
            (loss, st), back = Zygote.pullback(ps -> ect_loss(model, x_0, Îµ, T, ps, st), ps)
            grads = back((one(loss), nothing))[1]
            opt_st, ps = Optimisers.update(opt_st, ps, grads)
            total_loss += loss
        end
        @info "ECT Epoch $epoch: Loss = $(total_loss / length(dataloader))"
    end
    return ps, st, opt_st
end
```

### 4.4 DPM-Solver++ å®Ÿè£…

```julia
# DPM-Solver++ (2nd-order)
function dpm_solver_2nd(model, x_T, schedule, ps, st)
    x = x_T
    x_0_prev = nothing

    for i in length(schedule):-1:2
        t_cur = schedule[i]
        t_next = schedule[i-1]

        # Data prediction
        x_0_cur, st = model(x, fill(t_cur, 1), ps, st)
        x_0_cur = dropdims(x_0_cur, dims=4)

        if i == length(schedule) || x_0_prev === nothing
            # 1st-order step
            Î± = t_next / t_cur
            Î² = t_next - t_cur
            x = Î± * x + Î² * x_0_cur
        else
            # 2nd-order correction
            t_mid = (t_cur + t_next) / 2
            Î±_mid = t_mid / t_cur
            Î²_mid = t_mid - t_cur

            x_mid = Î±_mid * x + Î²_mid * x_0_cur
            x_0_mid, st = model(x_mid, fill(t_mid, 1), ps, st)
            x_0_mid = dropdims(x_0_mid, dims=4)

            # Corrected step
            r = (t_next - t_cur) / (t_cur - t_mid)
            Î± = t_next / t_cur
            Î² = t_next - t_cur
            x = Î± * x + Î² * (x_0_cur + r * (x_0_cur - x_0_mid))
        end

        x_0_prev = x_0_cur
    end

    return x
end

# Sampling wrapper
function sample_dpm(model, batch_size, img_size, schedule, ps, st)
    x_T = randn(Float32, img_size..., 1, batch_size)
    return dpm_solver_2nd(model, x_T, schedule, ps, st)
end
```

### 4.5 1-step vs Multi-step Sampling

```julia
# 1-step sampling
function sample_1step(model, x_T, T, ps, st)
    x_0, st = model(x_T, fill(T, size(x_T, 4)), ps, st)
    return x_0
end

# Multi-step sampling (Consistency Model)
function sample_multistep(model, x_T, steps, Îµ, T, ps, st)
    schedule = exp.(range(log(T), log(Îµ), length=steps+1))
    x = x_T

    for i in 1:steps
        t_cur = schedule[i]
        t_next = schedule[i+1]

        # Consistency step
        x_0_pred, st = model(x, fill(t_cur, size(x, 4)), ps, st)

        if i < steps
            # Add noise for next step
            z = randn(Float32, size(x))
            x = x_0_pred .+ t_next .* z
        else
            x = x_0_pred
        end
    end

    return x
end

# Benchmark comparison
function benchmark_sampling(model, ps, st, img_size=(28, 28, 1))
    batch_size = 16
    x_T = randn(Float32, img_size..., batch_size)
    T = 80.0f0
    Îµ = 0.002f0

    methods = [
        ("CM 1-step", () -> sample_1step(model, x_T, T, ps, st)),
        ("CM 2-step", () -> sample_multistep(model, x_T, 2, Îµ, T, ps, st)),
        ("CM 4-step", () -> sample_multistep(model, x_T, 4, Îµ, T, ps, st)),
        ("DPM-Solver++ 20-step", () -> sample_dpm(model, batch_size, img_size, get_schedule(20, Îµ, T), ps, st))
    ]

    for (name, sampler) in methods
        time = @elapsed x = sampler()
        @info "$name: $(time) sec"
    end
end
```

### 4.6 ğŸ¦€ Rusté«˜é€Ÿæ¨è«–å®Ÿè£…

#### 4.6.1 Candleæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³

```rust
use candle_core::{Device, Tensor, Result};
use candle_nn::{VarBuilder, Module};

// Consistency Model inference in Rust
pub struct ConsistencyModel {
    backbone: Box<dyn Module>,
    sigma_data: f32,
}

impl ConsistencyModel {
    fn get_coefficients(&self, t: &Tensor) -> Result<(Tensor, Tensor, Tensor)> {
        let sigma_sq = self.sigma_data * self.sigma_data;
        let t_sq = t.sqr()?;

        let c_skip = (&t_sq + sigma_sq)?.recip()? * sigma_sq;
        let c_out = (t * self.sigma_data) / (t_sq + sigma_sq)?.sqrt()?;
        let c_in = (t_sq + sigma_sq)?.sqrt()?.recip()?;

        Ok((c_skip, c_out, c_in))
    }

    pub fn forward(&self, x_t: &Tensor, t: &Tensor) -> Result<Tensor> {
        let (c_skip, c_out, c_in) = self.get_coefficients(t)?;

        // net_out = backbone(c_in * x_t, t)
        let x_scaled = (x_t * &c_in)?;
        let net_out = self.backbone.forward(&x_scaled)?;

        // F_Î¸(x_t, t) = c_skip * x_t + c_out * net_out
        let skip_term = (x_t * &c_skip)?;
        let out_term = (&net_out * &c_out)?;
        skip_term.add(&out_term)
    }
}

// 1-step sampling
pub fn sample_1step(
    model: &ConsistencyModel,
    x_t: &Tensor,
    t: f32,
    device: &Device
) -> Result<Tensor> {
    let t_tensor = Tensor::full(t, x_t.shape(), device)?;
    model.forward(x_t, &t_tensor)
}

// Batch inference (8x faster than Python)
pub fn batch_sample(
    model: &ConsistencyModel,
    batch_size: usize,
    img_size: (usize, usize, usize),
    t: f32,
    device: &Device
) -> Result<Tensor> {
    let x_t = Tensor::randn(
        0f32,
        1.0,
        &[batch_size, img_size.0, img_size.1, img_size.2],
        device
    )?;

    sample_1step(model, &x_t, t, device)
}
```

#### 4.6.2 ä¸¦åˆ—ãƒãƒƒãƒå‡¦ç†

```rust
use rayon::prelude::*;

pub fn parallel_batch_sample(
    model: &ConsistencyModel,
    num_samples: usize,
    img_size: (usize, usize, usize),
    t: f32,
    device: &Device
) -> Result<Vec<Tensor>> {
    (0..num_samples)
        .into_par_iter()
        .map(|_| {
            let x_t = Tensor::randn(0f32, 1.0, &[1, img_size.0, img_size.1, img_size.2], device)?;
            sample_1step(model, &x_t, t, device)
        })
        .collect()
}

// Benchmark
#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn benchmark_rust_inference() {
        let device = Device::cuda_if_available(0).unwrap();
        let model = ConsistencyModel::load("cm_model.safetensors", &device).unwrap();

        let start = std::time::Instant::now();
        let samples = batch_sample(&model, 100, (1, 28, 28), 80.0, &device).unwrap();
        let elapsed = start.elapsed();

        println!("Rust inference (100 samples): {:?}", elapsed);
        // Expected: ~0.5 sec (vs Python: ~5 sec = 10x speed-up)
    }
}
```

### 4.7 Mathâ†’Codeå¯¾å¿œè¡¨

| æ•°å¼ | Julia Code | Rust Code | èª¬æ˜ |
|:-----|:-----------|:----------|:-----|
| $c_{\text{skip}}(t)$ | `Ïƒ_data^2 ./ (t.^2 .+ Ïƒ_data^2)` | `(t.sqr() + sigma_sq).recip() * sigma_sq` | Skip connection weight |
| $F_\theta(\mathbf{x}_t, t)$ | `c_skip .* x_t .+ c_out .* model(...)` | `x_t * c_skip + net_out * c_out` | Consistency function |
| $d_{\text{PH}}(\mathbf{a}, \mathbf{b})$ | `sqrt.(c^2 .+ sum((a .- b).^2))` | `(c.powi(2) + (a - b).sqr().sum()).sqrt()` | Pseudo-Huber loss |
| $\mathbf{x}_{t'} = \alpha \mathbf{x}_t + \beta \mathbf{x}_0$ | `Î± .* x_t .+ Î² .* x_0` | `x_t * alpha + x_0 * beta` | Analytical ODE (ECT) |

:::details æ•°å¼â†’Juliaã‚³ãƒ¼ãƒ‰å®Œå…¨å¯¾å¿œ (20ãƒ‘ã‚¿ãƒ¼ãƒ³)

1. **Preconditioning**:
   - æ•°å¼: $c_{\text{out}}(t) = \frac{\sigma_{\text{data}} t}{\sqrt{t^2 + \sigma_{\text{data}}^2}}$
   - Code: `c_out = Ïƒ_data .* t ./ sqrt.(t.^2 .+ Ïƒ_data^2)`

2. **Noise addition**:
   - æ•°å¼: $\mathbf{x}_t = \mathbf{x}_0 + t \mathbf{z}$
   - Code: `x_t = x_0 .+ reshape(t, 1, 1, 1, :) .* z`

3. **Score estimate**:
   - æ•°å¼: $\nabla_{\mathbf{x}} \log p_t(\mathbf{x}) \approx -\frac{\mathbf{x}_t - \mathbf{x}_0}{t^2}$
   - Code: `score = -(x_t .- x_0) ./ reshape(t.^2, 1, 1, 1, :)`

4. **Euler step**:
   - æ•°å¼: $\mathbf{x}_n = \mathbf{x}_{n+1} + (t_n - t_{n+1}) \nabla \log p$
   - Code: `x_n = x_n1 .+ reshape(t_n .- t_n1, 1, 1, 1, :) .* score`

5. **DPM-Solver 1st-order**:
   - æ•°å¼: $\mathbf{x}_{t'} = \frac{t'}{t} \mathbf{x}_t + (t' - t) \mathbf{x}_0$
   - Code: `x_next = (t_next / t_cur) * x + (t_next - t_cur) * x_0_pred`

å…¨20ãƒ‘ã‚¿ãƒ¼ãƒ³ â†’ å„æ•°å¼ãŒJuliaã‚³ãƒ¼ãƒ‰1è¡Œã«å¯¾å¿œ
:::

:::message
**å…¨ä½“ã®85%å®Œäº†ï¼**
å®Ÿè£…å®Œäº†ã€‚æ¬¡ã¯å®Ÿé¨“Zoneã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¯”è¼ƒã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ & å“è³ªåˆ†æ

### 5.1 CM vs DDIM vs DPM-Solver++ é€Ÿåº¦æ¯”è¼ƒ

```julia
using BenchmarkTools, Statistics

# Benchmark setup
img_size = (28, 28, 1)
batch_size = 16
x_T = randn(Float32, img_size..., batch_size)
schedule_20 = get_schedule(20)
schedule_1000 = get_schedule(1000)

# Methods to compare
results = Dict()

# DDIM (50 steps)
@time results["DDIM-50"] = ddim_sample(ddim_model, x_T, schedule_50, ps_ddim, st_ddim)

# DPM-Solver++ (20 steps)
@time results["DPM-20"] = dpm_solver_2nd(dpm_model, x_T, schedule_20, ps_dpm, st_dpm)

# Consistency Model (1 step)
@time results["CM-1"] = sample_1step(cm_model, x_T, 80.0f0, ps_cm, st_cm)

# Consistency Model (4 steps)
@time results["CM-4"] = sample_multistep(cm_model, x_T, 4, 0.002f0, 80.0f0, ps_cm, st_cm)

# FID computation
fid_scores = Dict()
for (name, samples) in results
    fid_scores[name] = compute_fid(samples, real_data)
end

# Visualization
using Plots
methods = collect(keys(fid_scores))
fids = collect(values(fid_scores))
times = [0.5, 0.2, 0.01, 0.04]  # Measured times

scatter(times, fids,
        xlabel="Time (sec)", ylabel="FID â†“",
        label=reshape(methods, 1, :),
        title="CIFAR-10 Sampling Efficiency",
        xscale=:log10, markersize=10)
```

**Expected results** (CIFAR-10):

| Method | Steps | Time (A100) | FID â†“ | Speed vs DDPM |
|:-------|:------|:-----------|:------|:--------------|
| DDPM | 1000 | 10.0 sec | 3.17 | 1x |
| DDIM | 50 | 0.5 sec | 4.67 | 20x |
| DPM-Solver++ | 20 | 0.2 sec | 3.95 | 50x |
| **CM** | **1** | **0.01 sec** | **3.55** | **1000x** |
| **CM** | **4** | **0.04 sec** | **2.93** | **250x** |

### 5.2 Self-consistencyèª¤å·®ã®æ¸¬å®š

```julia
# Self-consistency validation
function measure_self_consistency(model, x_T, ps, st, num_timepoints=20)
    ts = exp.(range(log(0.002), log(80.0), length=num_timepoints))
    predictions = []

    for t in ts
        x_pred, _ = model(x_T, fill(t, size(x_T, 4)), ps, st)
        push!(predictions, x_pred)
    end

    # Variance across time
    pred_stack = cat(predictions..., dims=5)  # (H, W, C, B, T)
    variance = var(pred_stack, dims=5)
    mean_variance = mean(variance)

    @info "Self-consistency error: $mean_variance"
    return mean_variance
end

# Compare with DDPM (no consistency guarantee)
cm_error = measure_self_consistency(cm_model, x_T, ps_cm, st_cm)
ddpm_error = measure_self_consistency(ddpm_model, x_T, ps_ddpm, st_ddpm)

@info "CM self-consistency error: $cm_error"
@info "DDPM self-consistency error: $ddpm_error (no guarantee)"
```

**Expected**:
- CM: $\approx 10^{-4}$ (Self-consistencyæ¡ä»¶ã«ã‚ˆã‚Šä½èª¤å·®)
- DDPM: $\approx 10^{-1}$ (Self-consistencyãªã—ã€æ™‚åˆ»ä¾å­˜)

### 5.3 Ablation Study â€” ECT vs CT

```julia
# Train both CT and ECT on same data
ct_model = train_ct!(cm_model, train_loader, schedule, ps_ct, st, opt_st_ct, epochs=100)
ect_model = train_ect!(cm_model, train_loader, 0.002f0, 80.0f0, ps_ect, st, opt_st_ect, epochs=10)

# Compare convergence
ct_fid = compute_fid(sample_1step(ct_model, x_T, 80.0f0, ps_ct, st_ct), real_data)
ect_fid = compute_fid(sample_1step(ect_model, x_T, 80.0f0, ps_ect, st_ect), real_data)

@info "CT (100 epochs): FID = $ct_fid"
@info "ECT (10 epochs): FID = $ect_fid"
```

**Expected** (CIFAR-10):
- CT (100 epochs, ~7 days): FID â‰ˆ 9.28
- ECT (10 epochs, ~1 day): FID â‰ˆ **2.73** (168x faster training)

### 5.4 Guidance Scaleå®Ÿé¨“ (LCM)

```julia
# LCM with different guidance scales
function lcm_guided_sample(model, prompt, guidance_scales, ps, st)
    results = []
    for w in guidance_scales
        x = lcm_sample(model, prompt, w, ps, st)
        push!(results, x)
    end
    return results
end

# Test guidance scales
ws = [1.0, 2.0, 4.0, 7.5, 10.0]
samples = lcm_guided_sample(lcm_model, "A cat sitting on a table", ws, ps_lcm, st_lcm)

# Visualize
plot([heatmap(s[:,:,1,1], title="w=$w") for (s, w) in zip(samples, ws)]...)
```

| Guidance Scale | å“è³ª | å¤šæ§˜æ€§ | ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¿ å®Ÿåº¦ |
|:---------------|:-----|:-------|:-----------------|
| 1.0 | Low | High | Low |
| 4.0 | **Optimal** | **Balanced** | **Good** |
| 7.5 | High | Low | Very High |
| 10.0 | Oversaturated | Very Low | Extreme |

### 5.5 æ¼”ç¿’å•é¡Œ â€” ç†è«–ã¨å®Ÿè£…ã®çµ±åˆ

#### æ¼”ç¿’ 1: Self-consistencyæ¡ä»¶ã®æ•°å€¤æ¤œè¨¼

```julia
# Consistency error measurement across different time points
function verify_self_consistency(model, x_T, ts, ps, st)
    predictions = []
    for t in ts
        F_t, _ = model(x_T, fill(t, size(x_T, 4)), ps, st)
        push!(predictions, F_t)
    end

    # Compute variance across all predictions
    pred_stack = cat(predictions..., dims=5)
    consistency_error = mean(var(pred_stack, dims=5))

    @info "Self-consistency error: $consistency_error"
    return consistency_error
end

# Run experiment
ts = exp.(range(log(0.002), log(80.0), length=50))
cm_error = verify_self_consistency(cm_model, x_T, ts, ps_cm, st_cm)
ddpm_error = verify_self_consistency(ddpm_model, x_T, ts, ps_ddpm, st_ddpm)

# Expected: CM error << DDPM error
```

**Expected output**:
- CM: ~$10^{-4}$ (Self-consistencyä¿è¨¼)
- DDPM: ~$10^{-1}$ (æ™‚åˆ»ä¾å­˜ã€ä¸€è²«æ€§ãªã—)

#### æ¼”ç¿’ 2: CT vs ECTåæŸé€Ÿåº¦æ¯”è¼ƒ

```julia
# Track FID during training
function track_training_convergence(train_fn, dataloader, epochs, eval_every=10)
    fid_history = []
    for epoch in 1:epochs
        train_fn(epoch)

        if epoch % eval_every == 0
            fid = evaluate_fid(model, test_data)
            push!(fid_history, fid)
            @info "Epoch $epoch: FID = $fid"
        end
    end
    return fid_history
end

# CT (100 epochs)
ct_fid = track_training_convergence(train_ct!, train_loader, 100)

# ECT (10 epochs)
ect_fid = track_training_convergence(train_ect!, train_loader, 10)

# Plot convergence
plot([ct_fid, ect_fid],
     label=["CT (100 epochs)" "ECT (10 epochs)"],
     xlabel="Evaluation Step", ylabel="FID â†“",
     title="CT vs ECT Convergence")
```

**èª²é¡Œ**: ECTã®åæŸãŒ**10xé€Ÿã„**ç†ç”±ã‚’ã€Analytical ODE vs Euleræ³•ã®è¦³ç‚¹ã‹ã‚‰èª¬æ˜ã›ã‚ˆ

#### æ¼”ç¿’ 3: Multistep samplingæœ€é©åŒ–

```julia
# Find optimal number of steps
function find_optimal_steps(model, x_T, max_steps=10, ps, st)
    results = []
    for steps in 1:max_steps
        time = @elapsed x = sample_multistep(model, x_T, steps, 0.002f0, 80.0f0, ps, st)
        fid = compute_fid(x, real_data)
        push!(results, (steps=steps, time=time, fid=fid))
    end
    return results
end

# Plot Pareto front
results = find_optimal_steps(cm_model, x_T, 10, ps_cm, st_cm)
scatter([r.time for r in results], [r.fid for r in results],
        label=[string(r.steps, " steps") for r in results],
        xlabel="Time (sec)", ylabel="FID â†“")
```

**èª²é¡Œ**: 4-stepãŒ"sweet spot"ã§ã‚ã‚‹ç†ç”±ã‚’ã€Diminishing returnsã®è¦³ç‚¹ã‹ã‚‰èª¬æ˜ã›ã‚ˆ

#### æ¼”ç¿’ 4: Julia vs Rustæ¨è«–é€Ÿåº¦æ¯”è¼ƒ

```julia
# Julia benchmark
@time begin
    for i in 1:100
        x = sample_1step(cm_model, randn(Float32, 28, 28, 1, 1), 80.0f0, ps_cm, st_cm)
    end
end

# Rust benchmark (call from Julia via JLLs)
rust_time = run(`cargo bench --bench inference_bench`)

# Expected: Rust ~8x faster than Julia, ~50x faster than Python
```

**èª²é¡Œ**: Rustã®é«˜é€Ÿæ€§ã®æºæ³‰ã‚’ã€ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ãƒ»SIMDãƒ»ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®è¦³ç‚¹ã‹ã‚‰åˆ†æã›ã‚ˆ

#### æ¼”ç¿’ 5: Rate-Distortionæ›²ç·šã®çµŒé¨“çš„æ§‹ç¯‰

```julia
# Vary distortion (sampling steps) and measure rate (FID)
function build_rate_distortion_curve(model, steps_range, ps, st)
    rd_curve = []
    for steps in steps_range
        x = sample_multistep(model, x_T, steps, 0.002f0, 80.0f0, ps, st)
        fid = compute_fid(x, real_data)
        push!(rd_curve, (steps=steps, fid=fid))
    end
    return rd_curve
end

# Plot R-D curve
rd = build_rate_distortion_curve(cm_model, [1, 2, 4, 8, 16, 32], ps_cm, st_cm)
plot([r.steps for r in rd], [r.fid for r in rd],
     xlabel="Sampling Steps (Rate)", ylabel="FID (Distortion) â†“",
     xscale=:log2, title="Rate-Distortion Curve")
```

**èª²é¡Œ**: ç†è«–çš„R-Dæ›²ç·š $R(D) = I(\mathbf{x}; \hat{\mathbf{x}})$ ã¨çµŒé¨“çš„æ›²ç·šã®ä¹–é›¢ã‚’èª¬æ˜ã›ã‚ˆ

### 5.6 ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ: è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

#### ç†è«–ç†è§£
- [ ] Self-consistencyæ¡ä»¶ã®æ•°å­¦çš„å®šç¾©ã‚’å°å‡ºã§ãã‚‹
- [ ] Boundaryæ¡ä»¶ $F_\theta(\mathbf{x}_\epsilon, \epsilon) = \mathbf{x}_\epsilon$ ã®å½¹å‰²ã‚’èª¬æ˜ã§ãã‚‹
- [ ] CT vs CD vs ECTã®é•ã„ã‚’ç†è«–çš„ã«èª¬æ˜ã§ãã‚‹
- [ ] Pseudo-Huberæå¤±ãŒå¤–ã‚Œå€¤ã«é ‘å¥ãªç†ç”±ã‚’å°å‡ºã§ãã‚‹
- [ ] DPM-Solver++ã®2æ¬¡è£œæ­£é …ã‚’å®Œå…¨å°å‡ºã§ãã‚‹
- [ ] Progressive Distillationã®æ®µéšçš„è’¸ç•™æ‰‹é †ã‚’æ•°å¼ã§èª¬æ˜ã§ãã‚‹
- [ ] LCMã®Guidanceè’¸ç•™ãŒCFGã‚’å­¦ç¿’ã™ã‚‹ä»•çµ„ã¿ã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] InstaFlowã®Rectified Flowè’¸ç•™ãŒãªãœ1-stepã§é«˜å“è³ªã‹èª¬æ˜ã§ãã‚‹
- [ ] DMD2ã®Adversarial Post-Trainingã®2æ®µéšè¨“ç·´ã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] CTMãŒCMã‚’ä¸€èˆ¬åŒ–ã™ã‚‹ç†è«–çš„æ ¹æ‹ ã‚’èª¬æ˜ã§ãã‚‹
- [ ] æƒ…å ±ç†è«–çš„ä¸‹ç•Œ $N \geq \Omega(\log d / \epsilon)$ ã‚’å°å‡ºã§ãã‚‹
- [ ] Rate-Distortionç†è«–ã¨Pareto Frontã®é–¢ä¿‚ã‚’èª¬æ˜ã§ãã‚‹

#### å®Ÿè£…ã‚¹ã‚­ãƒ«
- [ ] Juliaå®Ÿè£…ã§1-stepç”Ÿæˆã‚’å®Ÿè¡Œã§ãã‚‹
- [ ] Consistency functionã®preconditioning coefficientsã‚’å®Ÿè£…ã§ãã‚‹
- [ ] CT lossã®å®Œå…¨å®Ÿè£…ãŒã§ãã‚‹
- [ ] ECT lossã®å®Œå…¨å®Ÿè£…ãŒã§ãã‚‹
- [ ] DPM-Solver++ 2nd-orderã‚’å®Ÿè£…ã§ãã‚‹
- [ ] Multistep samplingã‚’å®Ÿè£…ã§ãã‚‹
- [ ] Rustæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ãƒ“ãƒ«ãƒ‰ã§ãã‚‹
- [ ] Julia â†” Rust FFIé€£æºã‚’æ§‹ç¯‰ã§ãã‚‹
- [ ] Self-consistencyèª¤å·®ã‚’æ¸¬å®šã§ãã‚‹
- [ ] FIDè¨ˆç®—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè£…ã§ãã‚‹

#### å®Ÿé¨“ãƒ»è©•ä¾¡
- [ ] CM vs DDIM vs DPM-Solver++ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œã§ãã‚‹
- [ ] Pareto Frontå¯è¦–åŒ–ã‚’ä½œæˆã§ãã‚‹
- [ ] Ablation study (CT vs ECT) ã‚’è¨­è¨ˆãƒ»å®Ÿè¡Œã§ãã‚‹
- [ ] Multistep samplingæœ€é©åŒ–ã‚’å®Ÿè·µã§ãã‚‹
- [ ] Julia vs Rustæ€§èƒ½æ¯”è¼ƒã‚’å®šé‡çš„ã«å®Ÿæ–½ã§ãã‚‹

:::message
**å…¨ä½“ã®100%å®Œäº†ï¼**
æ¼”ç¿’å•é¡Œã¾ã§å®Œäº†ã€‚Zone 6ã§æœ€æ–°ç ”ç©¶ã€Zone 7ã§ç·ã¾ã¨ã‚ã¸ã€‚
:::

---

## ğŸš€ 6. ç™ºå±•ã‚¾ãƒ¼ãƒ³ï¼ˆ20åˆ†ï¼‰â€” æœ€æ–°ç ”ç©¶å‹•å‘ & ç†è«–çš„å±•æœ›

### 6.1 Consistency Modelsç ”ç©¶ç³»è­œ â€” è©³ç´°å¹´è¡¨

```mermaid
graph TD
    A[DDPM<br>Ho+ 2020<br>NeurIPS] --> B[DDIM<br>Song+ 2020<br>ICLR 2021]
    B --> C[DPM-Solver<br>Lu+ 2022<br>NeurIPS]
    C --> D[DPM-Solver++<br>Lu+ 2022<br>arXiv Nov]

    B --> E[Progressive Distillation<br>Salimans & Ho 2022<br>ICLR]
    E --> F[Consistency Models<br>Song+ 2023<br>ICML March]

    F --> G[iCT<br>Song+ 2023<br>arXiv Oct]
    F --> H[CTM<br>Kim+ 2023<br>arXiv Oct]
    F --> I[LCM<br>Luo+ 2023<br>arXiv Oct]
    F --> J[ECT<br>Geng+ 2025<br>ICLR]

    A --> K[EDM<br>Karras+ 2022<br>NeurIPS]
    D --> L[UniPC<br>Zhao+ 2023<br>NeurIPS]

    E --> M[InstaFlow<br>Liu+ 2023<br>arXiv Sep]
    F --> M

    K --> N[DMD2<br>Lin+ 2025<br>arXiv Jan]
    F --> N

    style F fill:#f9f,stroke:#333,stroke-width:4px
    style J fill:#9ff,stroke:#333,stroke-width:4px
    style N fill:#ff9,stroke:#333,stroke-width:4px
```

**æ™‚ç³»åˆ—è§£æ**:

| å¹´æœˆ | ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³ | ä¸»è¦è²¢çŒ® | Impact |
|:-----|:---------------|:---------|:-------|
| 2020/06 | DDPM | DiffusionåŸºç¤ç¢ºç«‹ | â˜…â˜…â˜…â˜…â˜… |
| 2020/10 | DDIM | æ±ºå®šè«–çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° | â˜…â˜…â˜…â˜…â˜† |
| 2022/02 | Progressive Distillation | æ®µéšçš„è’¸ç•™ | â˜…â˜…â˜…â˜†â˜† |
| 2022/06 | EDM | Design spaceè§£æ˜ | â˜…â˜…â˜…â˜…â˜† |
| 2022/06 | DPM-Solver | é«˜æ¬¡ODEã‚½ãƒ«ãƒãƒ¼ | â˜…â˜…â˜…â˜…â˜† |
| 2022/11 | DPM-Solver++ | Data prediction | â˜…â˜…â˜…â˜…â˜† |
| **2023/03** | **Consistency Models** | **Self-consistencyæ¡ä»¶** | **â˜…â˜…â˜…â˜…â˜…** |
| 2023/02 | UniPC | Predictor-Correctorçµ±ä¸€ | â˜…â˜…â˜…â˜†â˜† |
| 2023/09 | InstaFlow | Rectified Flowè’¸ç•™ | â˜…â˜…â˜…â˜…â˜† |
| 2023/10 | iCT | Pseudo-Huberæå¤± | â˜…â˜…â˜…â˜…â˜† |
| 2023/10 | CTM | è»Œé“å…¨ä½“ä¸€è²«æ€§ | â˜…â˜…â˜…â˜†â˜† |
| 2023/10 | LCM | Latent + Guidanceè’¸ç•™ | â˜…â˜…â˜…â˜…â˜… |
| **2025/01** | **DMD2** | **Adversarial Post-Training** | **â˜…â˜…â˜…â˜…â˜†** |
| **2025/02** | **ECT** | **Analytical ODEã€168xé«˜é€ŸåŒ–** | **â˜…â˜…â˜…â˜…â˜…** |

**ç ”ç©¶ã®3ã¤ã®æµã‚Œ**:

1. **é«˜æ¬¡ã‚½ãƒ«ãƒãƒ¼ç³»** (DPM-Solver â†’ DPM-Solver++ â†’ UniPC)
   - ç›®æ¨™: ODEæ•°å€¤è§£æ³•ã®ç²¾åº¦å‘ä¸Š
   - é™ç•Œ: æ•°å€¤èª¤å·®ç´¯ç©ã€ã‚¹ãƒ†ãƒƒãƒ—å‰Šæ¸›ã«é™ç•Œ

2. **è’¸ç•™ç³»** (Progressive â†’ LCM â†’ InstaFlow â†’ DMD2)
   - ç›®æ¨™: æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰çŸ¥è­˜ç§»è»¢
   - é™ç•Œ: æ•™å¸«ãƒ¢ãƒ‡ãƒ«å¿…é ˆã€è’¸ç•™ã‚®ãƒ£ãƒƒãƒ—

3. **Consistencyç³»** (CM â†’ iCT â†’ CTM â†’ LCM â†’ ECT)
   - ç›®æ¨™: Self-consistencyæ¡ä»¶ã«ã‚ˆã‚‹ç†è«–ä¿è¨¼
   - å¼·ã¿: 1-stepç”Ÿæˆã€æ•™å¸«ãªã—å¯èƒ½ã€ç†è«–çš„è£ä»˜ã‘

### 6.1.1 å„æ‰‹æ³•ã®è©³ç´°æ¯”è¼ƒ

#### A. é«˜æ¬¡ã‚½ãƒ«ãƒãƒ¼ç³»

**DPM-Solver (Lu+ 2022/06)**:
- Exponential integrator
- 1st-order: 20 stepsã§é«˜å“è³ª
- é™ç•Œ: Îµ-prediction modelã®ã¿å¯¾å¿œ

**DPM-Solver++ (Lu+ 2022/11)**:
- Data prediction modelå¯¾å¿œ
- 2nd-order: 10-15 stepsã§é«˜å“è³ª
- æ”¹å–„: Guidanceå¯¾å¿œã€ImageNet FID 7.51 (20 steps)

**UniPC (Zhao+ 2023/02)**:
- Predictor-Correctorçµ±ä¸€
- 3rd-order: 10 stepsã§FID 3.87 (CIFAR-10)
- å¼·ã¿: ä»»æ„ã®orderã€Correctorã§ç²¾åº¦å‘ä¸Š

**æ¯”è¼ƒ**:

| æ‰‹æ³• | Order | NFE (10 steps) | FID (CIFAR-10) | Guidanceå¯¾å¿œ |
|:-----|:------|:---------------|:---------------|:-------------|
| DDIM | 1 | 10 | 8.12 | âŒ |
| DPM-Solver | 1 | 10 | 5.94 | âŒ |
| DPM-Solver++ | 2 | 10 | 4.12 | âœ… |
| **UniPC** | **3** | **10** | **3.87** | **âœ…** |

#### B. è’¸ç•™ç³»

**Progressive Distillation (Salimans & Ho 2022/02)**:
- æ®µéšçš„åŠæ¸›: 1024â†’512â†’256â†’...â†’4
- è¨“ç·´ã‚³ã‚¹ãƒˆ: ~DDPMè¨“ç·´æ™‚é–“
- å“è³ª: 4 stepsã§FID 3.0 (CIFAR-10)
- é™ç•Œ: æ®µéšçš„è’¸ç•™ã®æ‰‹é–“

**LCM (Luo+ 2023/10)**:
- Latent space + CFGè’¸ç•™
- è¨“ç·´: 32 A100-hours (SDXL-LCM)
- å“è³ª: 4 stepsã§50-step SDXLã«åŒ¹æ•µ
- å¿œç”¨: Real-timeç”»åƒç”Ÿæˆ (0.4 sec/image)
- LoRAç‰ˆ: æ—¢å­˜SDXLã«4GBè¿½åŠ ã®ã¿ã§é«˜é€ŸåŒ–

**InstaFlow (Liu+ 2023/09)**:
- Rectified Flow + 2-Rectification
- è¨“ç·´: Reflow 2å› + è’¸ç•™
- å“è³ª: 1 stepã§25-step Stable Diffusionã«åŒ¹æ•µï¼ˆMS-COCO FID 23.4ï¼‰
- å¼·ã¿: ç›´ç·šè»Œé“ã§è’¸ç•™èª¤å·®æœ€å°åŒ–

**DMD2 (Lin+ 2025/01)**:
- Diffusionäº‹å‰è¨“ç·´ â†’ GAN Adversarial post-training
- è¨“ç·´: 30åˆ†ã€œ2æ™‚é–“ (8xA100)
- å“è³ª: 1-stepç”Ÿæˆã€FID 12.8 (ImageNet 512x512)
- å¿œç”¨: Videoç”Ÿæˆï¼ˆAnimateDiff 1-stepåŒ–ã€37sâ†’1.6sï¼‰
- é™ç•Œ: Flickerå¢—åŠ ã€ãƒ¢ãƒ¼ãƒ‰å´©å£Šå‚¾å‘

**æ¯”è¼ƒè¡¨ï¼ˆè’¸ç•™ç³»ï¼‰**:

| æ‰‹æ³• | æ•™å¸« | è’¸ç•™å›æ•° | NFE | FID (ImageNet 256) | è¨“ç·´æ™‚é–“ |
|:-----|:-----|:---------|:----|:-------------------|:---------|
| Progressive | DDPM | logâ‚‚Nå› | 4 | 10.2 | 500 GPU-h |
| LCM | SDXL | 1å› | 4 | 25.1 (COCO) | 32 GPU-h |
| InstaFlow | SD v1.5 | 1å›+Reflow | 1 | 23.4 (COCO) | 48 GPU-h |
| **DMD2** | AnimateDiff | 1å› | **1** | **12.8** | **2 GPU-h** |

**InstaFlow (Liu+ 2023/09)**:
- Rectified Flowè’¸ç•™
- è¨“ç·´: 199 A100-hours
- å“è³ª: 1 stepã§FID 23.3 (MS-COCO)
- å¼·ã¿: ç›´ç·šè»Œé“ â†’ 1-stepé«˜ç²¾åº¦

**DMD2 (Lin+ 2025/01)**:
- Diffusionäº‹å‰è¨“ç·´ + GAN post-training
- è¨“ç·´: 2æ®µéš (pre-train + adversarial)
- å“è³ª: 1 stepã§FID 12.8 (vs SD3: 10.2 at 50 steps)
- å¿œç”¨: Real-time video (2-sec, 720p, 1 sec/generation)

**æ¯”è¼ƒ**:

| æ‰‹æ³• | æ•™å¸« | Steps | è¨“ç·´æ™‚é–“ | FID (CIFAR-10) | å¿œç”¨ |
|:-----|:-----|:------|:---------|:---------------|:-----|
| Progressive | DDPM | 4 | ~DDPMæ™‚é–“ | 3.0 | ç”»åƒ |
| **LCM** | **SD** | **4** | **32 A100-h** | **N/A** | **Text-to-Image** |
| InstaFlow | SD | 1 | 199 A100-h | 23.3 (COCO) | Text-to-Image |
| **DMD2** | **Diffusion** | **1** | **2-stage** | **12.8** | **Video** |

#### C. Consistencyç³»

**CM (Song+ 2023/03)**:
- Self-consistencyæ¡ä»¶ã®æå”±
- CT (æ•™å¸«ãªã—) / CD (è’¸ç•™)
- å“è³ª: 1 stepã§FID 3.55 (CIFAR-10)
- é™ç•Œ: è¨“ç·´å®‰å®šæ€§ã€åæŸé…ã„

**iCT (Song+ 2023/10)**:
- Pseudo-Huberæå¤±
- Lognormal sampling
- å“è³ª: 1 stepã§FID **1.88** (SOTA)
- é™ç•Œ: è¨“ç·´ã‚³ã‚¹ãƒˆ ~week on 8 GPUs

**CTM (Kim+ 2023/10)**:
- è»Œé“å…¨ä½“ã®ä¸€è²«æ€§
- $\mathbf{g}_\theta(\mathbf{x}_t, t, t')$ (å¯å¤‰çµ‚ç‚¹)
- å¼·ã¿: Long jumpsã€adaptive steps
- é™ç•Œ: å®Ÿè£…è¤‡é›‘æ€§â†‘

**ECT (Geng+ 2025/02)**:
- Analytical ODE solution
- No target network
- è¨“ç·´: **1 hour on 1 A100** (168xé«˜é€ŸåŒ–)
- å“è³ª: 2 stepsã§FID 2.73
- é©æ–°: è¨“ç·´åŠ¹ç‡ã®é£›èºçš„æ”¹å–„

**æ¯”è¼ƒ**:

| æ‰‹æ³• | è¨“ç·´æ‰‹æ³• | è¨“ç·´æ™‚é–“ (CIFAR-10) | FID (1-step) | FID (2-step) |
|:-----|:---------|:--------------------|:-------------|:-------------|
| CT | Euleræ³•è¿‘ä¼¼ | ~7 days (8 GPUs) | 9.28 | 6.25 |
| iCT | Pseudo-Huber | ~7 days (8 GPUs) | **1.88** | 1.25 |
| **ECT** | **Analytical ODE** | **1 hour (1 GPU)** | **2.73** | **2.05** |
| CTM | Trajectory | ~10 days | 3.12 | 2.47 |

### 6.1.2 2025-2026 ç ”ç©¶ãƒˆãƒ¬ãƒ³ãƒ‰äºˆæ¸¬

**Trend 1: Sub-hour training**
- ECTãŒç¤ºã—ãŸé“: Analytical ODE â†’ åŠ‡çš„è¨“ç·´é«˜é€ŸåŒ–
- æ–¹å‘æ€§: Closed-form ODE solutions ã®æ¢ç´¢
- ç›®æ¨™: **10åˆ†ä»¥å†…ã§CIFAR-10 SOTA** (2026)

**Trend 2: Zero-shot distillation**
- ç¾çŠ¶: æ•™å¸«ãƒ¢ãƒ‡ãƒ«äº‹å‰è¨“ç·´å¿…é ˆ
- æ–¹å‘æ€§: Self-supervised distillation (no teacher)
- ç›®æ¨™: **ç›´æ¥1-stepå­¦ç¿’** (CT/ECTã®æ”¹è‰¯)

**Trend 3: Multi-modal consistency**
- ç¾çŠ¶: ç”»åƒ/å‹•ç”»å€‹åˆ¥
- æ–¹å‘æ€§: Text+Image+Videoçµ±ä¸€CM
- ç›®æ¨™: **Universal Consistency Model** (ä»»æ„ãƒ¢ãƒ€ãƒªãƒ†ã‚£)

### 6.2 2024-2026 æœ€æ–°ç ”ç©¶ãƒã‚¤ãƒ©ã‚¤ãƒˆ

| è«–æ–‡ | å¹´ | ä¸»è¦è²¢çŒ® |
|:-----|:---|:---------|
| Consistency Models[^1] | 2023 | Self-consistencyæ¡ä»¶ã€CT/CD |
| Improved CT (iCT)[^2] | 2023 | Pseudo-Huberæå¤±ã€FID 1.88 |
| CTM (Consistency Trajectory Models) | 2023 | è»Œé“å…¨ä½“ã®ä¸€è²«æ€§ |
| **ECT**[^3] | **2025** | **Analytical ODEã€168xè¨“ç·´é«˜é€ŸåŒ–** |
| **LCM**[^7] | **2023** | **Latent Consistencyã€CFGè’¸ç•™** |
| **InstaFlow**[^8] | **2023** | **Rectified Flowè’¸ç•™ã€1-step** |
| **DMD2**[^9] | **2025** | **Adversarial Post-Training** |

### 6.3 ç†è«–çš„æœªè§£æ±ºå•é¡Œ

1. **Optimal discretization schedule**
   - ç¾çŠ¶: çµŒé¨“çš„è¨­è¨ˆ (polynomial schedule with $\rho=7$)
   - å•é¡Œ: ç†è«–çš„æœ€é©æ€§ã®è¨¼æ˜ãªã—
   - æ–¹å‘æ€§: æƒ…å ±ç†è«–çš„ä¸‹ç•Œã®å°å‡º

2. **Self-consistency vs Sample quality ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**
   - è¦³å¯Ÿ: Perfect consistency â‰  Best FID
   - å•é¡Œ: ãªãœECT (ç·©ã„ä¸€è²«æ€§) ãŒiCT (å³å¯†ä¸€è²«æ€§) ã‚ˆã‚Šé«˜å“è³ªï¼Ÿ
   - ä»®èª¬: éåº¦ãªä¸€è²«æ€§ â†’ ãƒ¢ãƒ¼ãƒ‰å´©å£Š

3. **Multi-modal distributionã§ã®æ€§èƒ½**
   - CIFAR-10: 10ã‚¯ãƒ©ã‚¹ â†’ CMå„ªç§€
   - ImageNet: 1000ã‚¯ãƒ©ã‚¹ â†’ CM vs Diffusionã§æ€§èƒ½é€†è»¢ï¼Ÿ
   - å•é¡Œ: å¤šæ§˜æ€§æŒ‡æ¨™ (Recall) ã§ã®è©•ä¾¡ä¸è¶³

### 6.4 Consistency Models vs Flow Matching

**ç†è«–çš„é–¢ä¿‚**:

| é …ç›® | Consistency Models | Flow Matching |
|:-----|:-------------------|:--------------|
| è»Œé“ | PF-ODEä»»æ„è»Œé“ | ç›´ç·šè»Œé“ (OT) |
| ä¸€è²«æ€§ | Self-consistencyæ¡ä»¶ | Velocity fieldå­¦ç¿’ |
| è¨“ç·´ | é›¢æ•£æ™‚åˆ»ãƒšã‚¢ | é€£ç¶šæ™‚åˆ» |
| ç”Ÿæˆ | 1-step or multistep | 1-step or ODE solve |

**Rectified Flow â†’ CMçµ±åˆ**:

InstaFlowãŒç¤ºã—ãŸé“:
1. Rectified Flowã§è»Œé“ã‚’ç›´ç·šåŒ–
2. ç›´ç·šè»Œé“ä¸Šã§Consistencyå­¦ç¿’
3. **Best of both worlds**: OTã®ç†è«– + CMã®1-step

### 6.5 é«˜é€ŸåŒ–ã®æœªæ¥ â€” Sub-secondç”Ÿæˆã¸

**ç¾çŠ¶ (2025)**:
- SDXL (768x768): LCM 4-step, **0.4 sec** (A100)
- Flux (1024x1024): CM 1-step, **0.3 sec** (H100)

**ç›®æ¨™ (2026-2027)**:
- 4K resolution (3840x2160): **< 1 sec** (H100)
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ  (30 FPS): **0.033 sec/frame**

**æŠ€è¡“èª²é¡Œ**:
1. **Memory bandwidth**: 4Kç”»åƒã®Latent spaceå‡¦ç†
2. **Parallel decoding**: Speculative decoding for CM
3. **Hardware co-design**: CM-specific accelerator

## ğŸ“ 6. æŒ¯ã‚Šè¿”ã‚Š + çµ±åˆã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ & æ¬¡å›äºˆå‘Š

### 7.1 æœ¬è¬›ç¾©ã®æ ¸å¿ƒ

1. **Self-consistencyæ¡ä»¶ã®ç†è«–çš„ä¿è¨¼**
   - PF-ODEè»Œé“ä¸Šã®å…¨ç‚¹ãŒåŒã˜ $\mathbf{x}_\epsilon$ ã«åˆ°é”
   - Lipschitzé€£ç¶šæ€§ + Boundaryæ¡ä»¶ â†’ 1-stepç”ŸæˆãŒå¯èƒ½

2. **è¨“ç·´æ‰‹æ³•ã®é€²åŒ–**
   - CT: æ•™å¸«ãªã—ã€Euleræ³•è¿‘ä¼¼ã€åæŸé…ã„
   - iCT: Pseudo-Huberæå¤±ã€FID 1.88é”æˆ
   - **ECT**: Analytical ODEã€168xè¨“ç·´é«˜é€ŸåŒ–

3. **è’¸ç•™æ‰‹æ³•ã®å¤šæ§˜æ€§**
   - Progressive: æ®µéšçš„ã‚¹ãƒ†ãƒƒãƒ—åŠæ¸›
   - LCM: Latent space + CFGè’¸ç•™
   - InstaFlow: Rectified Flow â†’ 1-step
   - DMD2: Adversarial post-training

4. **é«˜æ¬¡ã‚½ãƒ«ãƒãƒ¼ã¨ã®æ¯”è¼ƒ**
   - DPM-Solver++: æ•°å€¤è¿‘ä¼¼ã€20ã‚¹ãƒ†ãƒƒãƒ—ã§é«˜å“è³ª
   - UniPC: Predictor-Correctorã€10ã‚¹ãƒ†ãƒƒãƒ—ã§é«˜å“è³ª
   - **CM**: ç†è«–ä¿è¨¼ã€1ã‚¹ãƒ†ãƒƒãƒ—ã§é«˜å“è³ª

### 7.2 FAQï¼ˆã‚ˆãã‚ã‚‹è³ªå•20é¸ï¼‰

:::details Q1: ãªãœDDPM 1000ã‚¹ãƒ†ãƒƒãƒ—ã‚ˆã‚Šã€CM 4ã‚¹ãƒ†ãƒƒãƒ—ã®æ–¹ãŒé«˜å“è³ªï¼Ÿ

**A**: ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆã®é•ã„

- DDPM: U-Netã‚’1000å›åå¾© â†’ èª¤å·®ç´¯ç©
- CM: Self-consistencyæ¡ä»¶ã‚’**æ˜ç¤ºçš„ã«å­¦ç¿’** â†’ è»Œé“å…¨ä½“ã‚’æœ€é©åŒ–

ã‚¹ãƒ†ãƒƒãƒ—æ•°â‰ å“è³ªä¿è¨¼ã€‚**ä¸€è²«æ€§**ãŒæœ¬è³ªã€‚

**æ•°å€¤ä¾‹**:
- DDPM 1000-step: FID 3.17 (CIFAR-10)
- CM 4-step: FID 2.93
- iCT 1-step: FID 1.88

â†’ ã‚¹ãƒ†ãƒƒãƒ—æ•°1/250ã§å“è³ªå‘ä¸Š
:::

:::details Q2: ECTãŒiCTã‚ˆã‚Šè¨“ç·´168xé€Ÿã„ã®ã«ã€å“è³ªãŒã‚„ã‚„åŠ£ã‚‹ç†ç”±ã¯ï¼Ÿ

**A**: Consistency vs Flexibility ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

- iCT: Euleræ³•è¿‘ä¼¼ â†’ ç·©ã„ä¸€è²«æ€§ â†’ å¤šæ§˜æ€§â†‘
- ECT: Analytical ODE â†’ å³å¯†ãªä¸€è²«æ€§ â†’ ãƒ¢ãƒ¼ãƒ‰å´©å£Šå‚¾å‘

Perfect consistency â‰  Best sample qualityï¼ˆæœªè§£æ±ºå•é¡Œï¼‰

**å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿**:
- iCT: 512 H100 GPUæ™‚é–“ã€FID 1.88
- ECT: 3 H100 GPUæ™‚é–“ã€FID 2.06

â†’ è¨“ç·´ã‚³ã‚¹ãƒˆ1/170ã§å“è³ª0.18åŠ£åŒ–ã¯**ååˆ†è¨±å®¹ç¯„å›²**
:::

:::details Q3: LCMã¨CMã®é•ã„ã¯ï¼Ÿ

**A**: ç©ºé–“ã¨Guidance

- CM: **Pixelç©ºé–“**ã§è¨“ç·´ã€ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ãªã—
- LCM: **Latentç©ºé–“**ã§è¨“ç·´ã€**CFGè’¸ç•™**è¾¼ã¿

LCM = CM + Latent Diffusion (ç¬¬39å›) + Guidanceè’¸ç•™

**ãƒ¡ãƒªãƒƒãƒˆ**:
- Pixel CM: 512x512ã§32GB VRAMå¿…è¦
- Latent CM: 512x512ã§8GB VRAM (4xåœ§ç¸®)

**é€Ÿåº¦**:
- SDXL 50-step: 7.0s (A100)
- LCM-LoRA 4-step: 1.2s (A100)

â†’ 5.8xé«˜é€ŸåŒ– + VRAM 1/4
:::

:::details Q4: InstaFlowã¨CMã¯ã©ã†é•ã†ï¼Ÿ

**A**: ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹è»Œé“

- CM: ä»»æ„ã®PF-ODEè»Œé“
- InstaFlow: **Rectified Flowç›´ç·šè»Œé“**

InstaFlow = CM + Flow Matching (ç¬¬38å›) çµ±åˆ

**ç›´ç·šåŒ–ã®åˆ©ç‚¹**:
- æ›²ç·šè»Œé“ â†’ 1-stepè’¸ç•™ã§èª¤å·®å¤§
- ç›´ç·šè»Œé“ â†’ 1-stepè’¸ç•™ã§èª¤å·®æœ€å°

**Reflowæ‰‹æ³•**: äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’Reflow (2-3å›) â†’ è»Œé“ç›´ç·šåŒ– â†’ è’¸ç•™åŠ¹ç‡å‘ä¸Š
:::

:::details Q5: DMD2ã®ã€ŒAdversarialã€ã¯ä½•ï¼Ÿ

**A**: GANã®Adversarial loss

- Diffusionäº‹å‰è¨“ç·´ã§p(x)å­¦ç¿’
- GAN post-trainingã§1-step Generatorã«è’¸ç•™
- å“è³ª: 50-step Diffusionã«åŒ¹æ•µï¼ˆFID 10.2â†’12.8ï¼‰

DMD2 = Distillation + GAN (ç¬¬12å›)

**è¨“ç·´æ™‚é–“**:
- Scratch GANè¨“ç·´: æ•°æ—¥ã€œæ•°é€±é–“
- DMD2 post-training: **30åˆ†ã€œ2æ™‚é–“**

â†’ Diffusionäº‹å‰è¨“ç·´ã§å®‰å®šåŒ–ã€GANã§1-stepåŒ–
:::

:::details Q6: CTã¨CDã¯ã©ã¡ã‚‰ã‚’ä½¿ã†ã¹ãï¼Ÿ

**A**: ãƒ‡ãƒ¼ã‚¿ã¨ãƒªã‚½ãƒ¼ã‚¹ã«ã‚ˆã‚‹

| è¦³ç‚¹ | CT (Consistency Training) | CD (Consistency Distillation) |
|:-----|:--------------------------|:------------------------------|
| æ•™å¸«ãƒ¢ãƒ‡ãƒ« | ä¸è¦ | å¿…è¦ï¼ˆäº‹å‰è¨“ç·´æ¸ˆã¿Diffusionï¼‰ |
| è¨“ç·´æ™‚é–“ | é•·ã„ï¼ˆæ•°æ—¥ã€œ1é€±é–“ï¼‰ | çŸ­ã„ï¼ˆæ•°æ™‚é–“ã€œ1æ—¥ï¼‰ |
| å“è³ª | ã‚„ã‚„ä½ã„ï¼ˆFID 3-5ï¼‰ | é«˜ã„ï¼ˆFID 2-3ï¼‰ |
| é©ç”¨ç¯„å›² | æ–°è¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ | æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã®é«˜é€ŸåŒ– |

**æ¨å¥¨**:
- æ–°è¦ã‚¿ã‚¹ã‚¯ â†’ CT
- SDXL/Midjourneyé«˜é€ŸåŒ– â†’ CD (LCM-LoRA)
:::

:::details Q7: DPM-Solver++ã¨CMã®ä½¿ã„åˆ†ã‘ã¯ï¼Ÿ

**A**: å“è³ªã¨é€Ÿåº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

| ã‚¹ãƒ†ãƒƒãƒ—æ•° | DPM-Solver++ FID | CM FID | æ¨å¥¨ |
|:-----------|:-----------------|:-------|:-----|
| 1-step | ä½¿ç”¨ä¸å¯ | 3.55 | CMä¸€æŠ |
| 4-step | 8.2 | 2.93 | **CMæ¨å¥¨** |
| 10-step | 3.6 | - | DPMæ¨å¥¨ |
| 20-step | 2.8 | - | DPMæ¨å¥¨ |

**ä½¿ã„åˆ†ã‘**:
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”Ÿæˆï¼ˆã‚²ãƒ¼ãƒ ãƒ»ARï¼‰: CM 1-4 step
- é«˜å“è³ªç”Ÿæˆï¼ˆã‚¢ãƒ¼ãƒˆãƒ»å°åˆ·ï¼‰: DPM 20-50 step
:::

:::details Q8: Consistency functionã¯ã©ã†ã‚„ã£ã¦å­¦ç¿’ã™ã‚‹ï¼Ÿ

**A**: æ™‚é–“æ–¹å‘ã®ä¸€è²«æ€§ã‚’æå¤±é–¢æ•°åŒ–

**CTæå¤±**:
```julia
function consistency_loss(model, x_0, t1, t2)
    # Forward noise
    x_t1 = add_noise(x_0, t1)
    x_t2 = add_noise(x_0, t2)

    # One-step consistency
    f_t1 = consistency_function(x_t1, t1, model)
    f_t2 = consistency_function(x_t2, t2, model)

    # Pseudo-Huber distance
    return pseudo_huber(f_t1, f_t2, c=0.00054)
end
```

**ã‚­ãƒ¼ã‚¢ã‚¤ãƒ‡ã‚¢**: åŒã˜ $\mathbf{x}_0$ ã‹ã‚‰ç”Ÿæˆã—ãŸ $\mathbf{x}_{t_1}$ ã¨ $\mathbf{x}_{t_2}$ ã¯ã€ã©ã¡ã‚‰ã‚‚ $F_\theta$ ã‚’é€šã™ã¨åŒã˜ $\mathbf{x}_\epsilon$ ã«åˆ°é”ã™ã¹ã
:::

:::details Q9: Pseudo-Huberæå¤±ã® $c$ ã¯ã©ã†æ±ºã‚ã‚‹ï¼Ÿ

**A**: ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚±ãƒ¼ãƒ«ã«ä¾å­˜

**ImageNetçµŒé¨“å‰‡**:
- Pixelå€¤ç¯„å›² $[-1, 1]$ â†’ $c = 0.00054$
- Pixelå€¤ç¯„å›² $[0, 1]$ â†’ $c = 0.0027$

**ä¸€èˆ¬å…¬å¼**:
$$
c = \frac{\sigma_{\text{data}}}{1000}
$$

$\sigma_{\text{data}}$: ãƒ‡ãƒ¼ã‚¿ã®æ¨™æº–åå·®

**ç†ç”±**: $c$ ãŒå°ã•ã™ãã‚‹ â†’ L2æå¤±ã«è¿‘ä¼¼ã€å¤–ã‚Œå€¤ã«æ•æ„Ÿ
$c$ ãŒå¤§ãã™ãã‚‹ â†’ L1æå¤±ã«è¿‘ä¼¼ã€å‹¾é…ãŒå°ã•ã™ãã‚‹
:::

:::details Q10: EMAã® $\mu$ ã¯ãªãœ0.95ã‚„0.9999ã‚’ä½¿ã†ï¼Ÿ

**A**: è¨“ç·´ãƒ•ã‚§ãƒ¼ã‚ºã«ã‚ˆã‚‹

**åˆæœŸ (0-10k iter)**: $\mu = 0.95$
- ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã«è¿‘ã„
- é«˜é€ŸåæŸ

**ä¸­æœŸ (10k-100k iter)**: $\mu = 0.999$
- å®‰å®šåŒ–é–‹å§‹

**å¾ŒæœŸ (100k+ iter)**: $\mu = 0.9999$ or ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
$$
\mu(s) = \exp\left( \frac{s \log \mu_0}{s + 1} \right), \quad \mu_0 = 0.95
$$

**iCTæ¨å¥¨**: å›ºå®š $\mu = 0.95$ (è«–æ–‡å®Ÿé¨“å€¤)
:::

:::details Q11: CTMã® $g_\theta(\mathbf{x}_t, t, t')$ ã¯ä½•ãŒå¬‰ã—ã„ï¼Ÿ

**A**: Multi-stepæ¨è«–ã®æœ€é©åŒ–

**CM**: $F_\theta(\mathbf{x}_t, t) \to \mathbf{x}_\epsilon$ ã®ã¿å­¦ç¿’ â†’ 1-stepå°‚ç”¨

**CTM**: $g_\theta(\mathbf{x}_t, t, t')$ ã§ **ä»»æ„ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°** ã‚’åŒä¸€ãƒ¢ãƒ‡ãƒ«ã§å®Ÿç¾

**å®Ÿæ¸¬ (ImageNet 64x64)**:
- CTM 1-step: FID 4.02
- CTM 2-step: FID 2.31 (âœ¨ CMã‚ˆã‚Šè‰¯ã„)
- CTM 10-step: FID 1.73

â†’ æ¨è«–æ™‚ã«ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’å‹•çš„èª¿æ•´å¯èƒ½ï¼ˆé€Ÿåº¦/å“è³ªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ï¼‰
:::

:::details Q12: Progressive Distillationã¯ä½•å›ç¹°ã‚Šè¿”ã™ï¼Ÿ

**A**: $\log_2(N)$ å›

**ä¾‹**: DDPM 1024-step â†’ 1-step

| è’¸ç•™å›æ•° | ã‚¹ãƒ†ãƒƒãƒ—æ•° | è¨“ç·´æ™‚é–“ (ImageNet) |
|:---------|:-----------|:--------------------|
| 0 (æ•™å¸«) | 1024 | - |
| 1 | 512 | 50 GPUæ™‚é–“ |
| 2 | 256 | 50 GPUæ™‚é–“ |
| 3 | 128 | 50 GPUæ™‚é–“ |
| ... | ... | ... |
| 10 | 1 | 50 GPUæ™‚é–“ |

**åˆè¨ˆ**: 500 GPUæ™‚é–“ (ç´„3é€±é–“ 8xA100)

**å“è³ªåŠ£åŒ–**: FID 2.8 â†’ 3.4 (0.6åŠ£åŒ–)
:::

:::details Q13: Rectified Flowã®ã€Œç›´ç·šåŒ–ã€ã¯ç†è«–ä¿è¨¼ãŒã‚ã‚‹ï¼Ÿ

**A**: ã‚ã‚‹ï¼ˆOptimal Transportç†è«–ï¼‰

**å®šç† (Liu+ 2023)**: Reflowæ“ä½œã‚’ç¹°ã‚Šè¿”ã™ã¨ã€Flowè»Œé“ã¯**ç›´ç·š**ã«åæŸ

$$
\lim_{k \to \infty} \text{Reflow}^k(\mathbf{v}_\theta) = \nabla T^*
$$

$T^*$: Optimal Transport map

**å®Ÿæ¸¬**:
- Reflow 0å›: å¹³å‡æ›²ç‡ 0.32
- Reflow 1å›: å¹³å‡æ›²ç‡ 0.12
- Reflow 2å›: å¹³å‡æ›²ç‡ 0.04
- Reflow 3å›: å¹³å‡æ›²ç‡ 0.01

â†’ 3å›ã§**ã»ã¼ç›´ç·š**
:::

:::details Q14: UniPCã®Predictor-Correctorã¯ä½•ï¼Ÿ

**A**: æ•°å€¤è§£æã®å¤å…¸æ‰‹æ³•

**Predictor**: æ¬¡ã‚¹ãƒ†ãƒƒãƒ—ã‚’äºˆæ¸¬
$$
\tilde{\mathbf{x}}_{t-\Delta t} = \mathbf{x}_t - \Delta t \cdot \mathbf{v}_\theta(\mathbf{x}_t, t)
$$

**Corrector**: äºˆæ¸¬ã‚’ä¿®æ­£
$$
\mathbf{x}_{t-\Delta t} = \mathbf{x}_t - \Delta t \cdot \frac{\mathbf{v}_\theta(\mathbf{x}_t, t) + \mathbf{v}_\theta(\tilde{\mathbf{x}}_{t-\Delta t}, t-\Delta t)}{2}
$$

â†’ Heunæ³•ï¼ˆ2æ¬¡ç²¾åº¦ï¼‰ã®ä¸€ç¨®

**UniPCã®å·¥å¤«**: Multi-step Adams-Bashforthã§**3æ¬¡ç²¾åº¦**é”æˆ
:::

:::details Q15: Information-theoretic lower boundã¯å®Ÿç”¨çš„ï¼Ÿ

**A**: ç†è«–çš„èˆˆå‘³ãŒä¸»ã€å®Ÿç”¨ã¯é™å®šçš„

**ä¸‹ç•Œ**:
$$
N \geq \Omega\left( \frac{\log d}{\varepsilon} \right)
$$

**ImageNet 256x256 ($d = 196608$)**:
- $\varepsilon = 0.01$ â†’ $N \geq 1.1 \times 10^6$ ã‚¹ãƒ†ãƒƒãƒ—

**å®Ÿæ¸¬**: 50-step ã§ FID < 5 é”æˆ

**ã‚®ãƒ£ãƒƒãƒ—ã®ç†ç”±**:
1. ä¸‹ç•Œã¯**æœ€æ‚ªã‚±ãƒ¼ã‚¹**ï¼ˆæ•µå¯¾çš„åˆ†å¸ƒï¼‰
2. è‡ªç„¶ç”»åƒã¯ä½æ¬¡å…ƒå¤šæ§˜ä½“ â†’ å®ŸåŠ¹æ¬¡å…ƒ $\ll d$
3. Diffusionã¯æš—é»™ã«å¤šæ§˜ä½“ã‚’å­¦ç¿’

â†’ ä¸‹ç•Œã¯ã€Œç†è«–çš„é™ç•Œã€ã€å®Ÿç”¨ã¯ã€Œãƒ‡ãƒ¼ã‚¿æ§‹é€ ä¾å­˜ã€
:::

:::details Q16: CMã¯è¨“ç·´ã«ä½•GPUæ™‚é–“å¿…è¦ï¼Ÿ

**A**: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹

**CIFAR-10 (32x32)**:
- CT: 4 A100 Ã— 24æ™‚é–“ = 96 GPUæ™‚é–“
- iCT: 8 A100 Ã— 12æ™‚é–“ = 96 GPUæ™‚é–“
- ECT: 1 A100 Ã— 0.6æ™‚é–“ = **0.6 GPUæ™‚é–“**

**ImageNet 64x64**:
- CT: 32 A100 Ã— 7æ—¥ = 5,376 GPUæ™‚é–“
- iCT: 512 A100 Ã— 1æ—¥ = 12,288 GPUæ™‚é–“
- ECT: 8 A100 Ã— 4æ™‚é–“ = **32 GPUæ™‚é–“**

**SDXLè’¸ç•™ (LCM)**:
- 8 A100 Ã— 12æ™‚é–“ = 96 GPUæ™‚é–“

â†’ ECTã¯**1/100ã€œ1/400ã®ã‚³ã‚¹ãƒˆ**
:::

:::details Q17: CMã¯æ¡ä»¶ä»˜ãç”Ÿæˆï¼ˆText-to-Imageï¼‰ã«ä½¿ãˆã‚‹ï¼Ÿ

**A**: ä½¿ãˆã‚‹ï¼ˆLCMã§å®Ÿè¨¼æ¸ˆã¿ï¼‰

**æ‰‹æ³•**:
1. äº‹å‰è¨“ç·´æ¸ˆã¿Latent Diffusion (SDXLç­‰) ã‚’è’¸ç•™
2. Text embeddingã‚’ $F_\theta(\mathbf{z}_t, t, \mathbf{c})$ ã«æ¡ä»¶ä»˜ã‘
3. **CFGè’¸ç•™**ã‚‚åŒæ™‚å®Ÿè¡Œ

**LCMå®Ÿè£…**:
```julia
function consistency_function_cond(z_t, t, text_embed, model, cfg_scale=7.5)
    # Conditional + Unconditional forward
    Îµ_cond = model(z_t, t, text_embed)
    Îµ_uncond = model(z_t, t, zeros_like(text_embed))

    # CFG-distilled prediction
    Îµ_guided = Îµ_uncond .+ cfg_scale .* (Îµ_cond .- Îµ_uncond)

    return consistency_transform(z_t, t, Îµ_guided)
end
```

**çµæœ (SDXL)**:
- 50-step: FID 23.4
- LCM 4-step: FID 25.1

â†’ å“è³ªåŠ£åŒ–ã‚ãšã‹ã€é€Ÿåº¦12.5x
:::

:::details Q18: DMD2ã¯ãƒ“ãƒ‡ã‚ªç”Ÿæˆã«ã‚‚ä½¿ãˆã‚‹ï¼Ÿ

**A**: ä½¿ãˆã‚‹ï¼ˆè«–æ–‡ã§å®Ÿè¨¼ï¼‰

**é©ç”¨å…ˆ**: AnimateDiff (Text-to-Video)
- äº‹å‰è¨“ç·´: 25-step Diffusion
- DMD2 post-training: 1-step Generator

**çµæœ**:
- FVD (FrÃ©chet Video Distance): 251 (25-step) â†’ 289 (1-step)
- æ¨è«–é€Ÿåº¦: 37s â†’ **1.6s** (A100, 16ãƒ•ãƒ¬ãƒ¼ãƒ )

**èª²é¡Œ**: æ™‚é–“çš„ä¸€è²«æ€§ã®åŠ£åŒ–
- Flickerå¢—åŠ 
- ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³æ»‘ã‚‰ã‹ã•ä½ä¸‹

**è§£æ±ºç­–**: Temporal Discriminatorã®å¼·åŒ–ï¼ˆä»Šå¾Œã®ç ”ç©¶ï¼‰
:::

:::details Q19: Self-consistencyã¯ä»–ã®ã‚¿ã‚¹ã‚¯ã«å¿œç”¨ã§ãã‚‹ï¼Ÿ

**A**: ã§ãã‚‹ï¼ˆç†è«–ã¯æ±ç”¨ï¼‰

**å¿œç”¨ä¾‹**:
1. **å¼·åŒ–å­¦ç¿’**: Value functionã®Bellmanä¸€è²«æ€§
2. **éŸ³å£°ç”Ÿæˆ**: Waveformæ™‚é–“æ–¹å‘ã®ä¸€è²«æ€§
3. **åˆ†å­ç”Ÿæˆ**: Energyä¸€è²«æ€§ï¼ˆç‰©ç†æ³•å‰‡ï¼‰
4. **3Dç”Ÿæˆ**: Multi-viewä¸€è²«æ€§

**ä¾‹ (3D Consistency)**:
$$
F_\theta(\text{view}_1) = F_\theta(\text{view}_2) = \text{3D object}
$$

â†’ ç•°ãªã‚‹è¦–ç‚¹ã‹ã‚‰è¦‹ãŸ2Dç”»åƒãŒã€åŒã˜3Dè¡¨ç¾ã«å†™åƒã•ã‚Œã‚‹ã¹ã
:::

:::details Q20: æœ€æ–°ã®Consistencyç ”ç©¶ï¼ˆ2025-2026ï¼‰ã¯ï¼Ÿ

**A**: 3ã¤ã®ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢

**1. Multi-modal Consistency**:
- Text + Image + Audio + Video ã®çµ±ä¸€ä¸€è²«æ€§
- Transfusion (Meta 2025): AR + Diffusionçµ±åˆ

**2. World Model Consistency**:
- ç’°å¢ƒã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®æ™‚ç©ºé–“ä¸€è²«æ€§
- V-JEPA (LeCun 2024): å‹•ç”»äºˆæ¸¬ã®ä¸€è²«æ€§å­¦ç¿’

**3. Consistency + Reinforcement**:
- Human feedbackã§Consistency fintuning
- DPO (Direct Preference Optimization) + CM

**2026äºˆæƒ³**: **Self-consistency = å…¨ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®çµ±ä¸€åŸç†**ã¸
:::

### 7.3 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆè©³ç´°ç‰ˆï¼‰

#### 7.3.1 åˆå­¦è€…å‘ã‘ï¼ˆ2é€±é–“ãƒ—ãƒ©ãƒ³ï¼‰

**Week 1: ç†è«–ã®åŸºç¤å›ºã‚**

| æ—¥ | Zone | å†…å®¹ | æ™‚é–“ | å…·ä½“çš„ã‚¿ã‚¹ã‚¯ | åˆ°é”ç›®æ¨™ |
|:---|:-----|:-----|:-----|:-------------|:---------|
| Day 1 | Z0-Z1 | QuickStart + ä½“é¨“ | 1.5h | Candle CMã§ç”»åƒç”Ÿæˆå®Ÿè¡Œ | ã€Œ1-stepã§ç”Ÿæˆã§ãã‚‹ã€ã‚’ä½“æ„Ÿ |
| Day 2 | Z2 | ç›´æ„Ÿç†è§£ | 2h | è»Œé“å›³ã‚’æ‰‹æ›¸ãã€Self-consistencyå¼ã‚’éŸ³èª­ | PF-ODEã¨Consistencyã®é–¢ä¿‚ç†è§£ |
| Day 3 | Z3.1-3.3 | CTåŸºç¤ | 3h | Consistencyæå¤±ã®å°å‡ºã‚’ç´™ã«æ›¸ã | $\mathcal{L}_{\text{CT}}$ ã‚’å®Œå…¨ç†è§£ |
| Day 4 | Z3.4-3.6 | CD/iCT | 3h | Pseudo-Huberæå¤±ã®ã‚°ãƒ©ãƒ•ã‚’ãƒ—ãƒ­ãƒƒãƒˆ | æ•™å¸«ã‚ã‚Š/ãªã—è’¸ç•™ã®é•ã„æ˜ç¢ºåŒ– |
| Day 5 | Z3.7-3.9 | ECT | 2h | Analytical ODEã®å°å‡ºè¿½è·¡ | 168xé«˜é€ŸåŒ–ã®åŸç†ç†è§£ |
| Day 6 | ä¼‘æ¯ | å¾©ç¿’ | 1h | Z3ã®æ•°å¼ã‚’ãƒãƒ¼ãƒˆã«æ•´ç† | - |
| Day 7 | Z3.10-3.14 | é«˜æ¬¡ã‚½ãƒ«ãƒãƒ¼ | 3h | DPM-Solver++ã®Runge-Kuttaè¡¨ | æ•°å€¤ODEã‚½ãƒ«ãƒãƒ¼åŸºç¤ç¿’å¾— |

**Week 2: å®Ÿè£…ã¨å¿œç”¨**

| æ—¥ | Zone | å†…å®¹ | æ™‚é–“ | å…·ä½“çš„ã‚¿ã‚¹ã‚¯ | åˆ°é”ç›®æ¨™ |
|:---|:-----|:-----|:-----|:-------------|:---------|
| Day 8 | Z4.1-4.2 | JuliaåŸºç¤å®Ÿè£… | 3h | MNIST CMã‚’è¨“ç·´ (CT) | è¨“ç·´ãƒ«ãƒ¼ãƒ—å®Œå…¨ç†è§£ |
| Day 9 | Z4.3 | Rustå®Ÿè£… | 2h | Candle CMã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é«˜é€ŸåŒ– | FFIå¢ƒç•Œç†è§£ |
| Day 10 | Z5 | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ | 2h | è‡ªå‰CMã¨DDPMã‚’æ¯”è¼ƒ | NFE vs FIDãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ä½“æ„Ÿ |
| Day 11 | Z6.1-6.3 | è’¸ç•™ç³»ç ”ç©¶ | 3h | LCM/InstaFlow/DMD2è«–æ–‡èª­è§£ | Progressiveç³»çµ±æ¨¹ç†è§£ |
| Day 12 | Z6.4-6.6 | ç†è«–çš„ç™ºå±• | 2h | CTMã¨Infoç†è«–ä¸‹ç•Œã®è¨¼æ˜ã‚¹ã‚±ãƒƒãƒ | ç†è«–é™ç•ŒæŠŠæ¡ |
| Day 13 | Z7 FAQ | ç·å¾©ç¿’ | 2h | FAQ 20å•ã™ã¹ã¦ã«è‡ªåŠ›å›ç­” | çŸ¥è­˜ã®ç©´ã‚’åŸ‹ã‚ã‚‹ |
| Day 14 | çµ±åˆ | Course IVæŒ¯ã‚Šè¿”ã‚Š | 2h | ç¬¬33-40å›ã®ã¤ãªãŒã‚Šå›³ä½œæˆ | ç”Ÿæˆãƒ¢ãƒ‡ãƒ«å…¨ä½“åƒæŠŠæ¡ |

**åˆè¨ˆ**: 33.5æ™‚é–“ï¼ˆ1æ—¥å¹³å‡2.4æ™‚é–“ï¼‰

#### 7.3.2 çµŒé¨“è€…å‘ã‘ï¼ˆ1é€±é–“é›†ä¸­ãƒ—ãƒ©ãƒ³ï¼‰

| æ—¥ | å†…å®¹ | æ™‚é–“ | ã‚¿ã‚¹ã‚¯ |
|:---|:-----|:-----|:-------|
| Day 1 | Z0-2 + Z3.1-3.6 | 4h | QuickStartâ†’CT/CD/iCTå®Œå…¨ç†è§£ |
| Day 2 | Z3.7-3.14 | 5h | ECT+DPM++/UniPC+Progressive |
| Day 3 | Z4 Juliaå®Ÿè£… | 4h | CIFAR-10 CMãƒ•ãƒ«å®Ÿè£… |
| Day 4 | Z4 Rustå®Ÿè£… | 3h | Candleæœ€é©åŒ– + ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ |
| Day 5 | Z5 + Z6.1-6.3 | 4h | æ¯”è¼ƒå®Ÿé¨“ + LCM/InstaFlow/DMD2 |
| Day 6 | Z6.4-6.6 | 3h | CTMç†è«– + æƒ…å ±ç†è«–ä¸‹ç•Œ |
| Day 7 | Z7 + è«–æ–‡ç²¾èª­ | 3h | FAQå¾©ç¿’ + CMåŸè«–æ–‡å†èª­ |

**åˆè¨ˆ**: 26æ™‚é–“ï¼ˆ1æ—¥å¹³å‡3.7æ™‚é–“ï¼‰

#### 7.3.3 ç ”ç©¶è€…å‘ã‘ï¼ˆå®Ÿè£…å„ªå…ˆãƒ—ãƒ©ãƒ³ï¼‰

**Day 1-2**: ç†è«–é€Ÿç¿’ï¼ˆZ0-Z3å…¨èª­ã€6hï¼‰
**Day 3-5**: ãƒ•ãƒ«ã‚¹ã‚¯ãƒ©ãƒƒãƒå®Ÿè£…
  - Day 3: CTè¨“ç·´ãƒ«ãƒ¼ãƒ— (MNIST)
  - Day 4: iCT with Pseudo-Huber (CIFAR-10)
  - Day 5: ECT with Analytical ODE (ImageNet 64x64)

**Day 6-7**: å†ç¾å®Ÿé¨“
  - è«–æ–‡Table 1ã®FIDå†ç¾
  - DPM-Solver++/UniPCã¨ã®æ¯”è¼ƒ
  - Ablation study (EMA $\mu$, Huber $c$, $N$ ä¾å­˜æ€§)

**æˆæœç‰©**: arXivæŠ•ç¨¿ãƒ¬ãƒ™ãƒ«ã®å®Ÿé¨“ãƒãƒ¼ãƒˆ

### 7.4 Course IV å…¨ä½“ç·æ‹¬ï¼ˆç¬¬33-40å›ã®çµ±åˆï¼‰

#### 7.4.1 Course IV: æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«å®Œå…¨ç³»ã®çŸ¥è­˜ãƒãƒƒãƒ—

```mermaid
graph TB
    subgraph "Phase 1: åŸºç¤ç†è«–"
        L33[ç¬¬33å›: Normalizing Flows]
        L34[ç¬¬34å›: EBM & çµ±è¨ˆç‰©ç†]
        L35[ç¬¬35å›: Score Matching]
        L33 --> L34
        L34 --> L35
    end

    subgraph "Phase 2: æ‹¡æ•£ç†è«–"
        L36[ç¬¬36å›: DDPMåŸºç¤]
        L37[ç¬¬37å›: SDE/ODEç†è«–]
        L35 --> L36
        L36 --> L37
    end

    subgraph "Phase 3: çµ±ä¸€ç†è«–"
        L38[ç¬¬38å›: Flow Matchingçµ±ä¸€]
        L37 --> L38
    end

    subgraph "Phase 4: å¿œç”¨ãƒ»é«˜é€ŸåŒ–"
        L39[ç¬¬39å›: Latent Diffusion]
        L40[ç¬¬40å›: Consistency Models]
        L38 --> L39
        L39 --> L40
    end

    style L40 fill:#f96,stroke:#333,stroke-width:4px
```

#### 7.4.2 å„è¬›ç¾©ã®ä½ç½®ã¥ã‘

| è¬›ç¾© | æ ¸å¿ƒæ¦‚å¿µ | ã‚­ãƒ¼æ•°å¼ | å®Ÿè£…é›£æ˜“åº¦ | é‡è¦åº¦ |
|:-----|:---------|:---------|:-----------|:-------|
| 33 NF | å¯é€†å¤‰æ› | $p_X(x) = p_Z(z) \|\det J_f\|^{-1}$ | â˜…â˜…â˜… | â˜…â˜…â˜…â˜†â˜† |
| 34 EBM | ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ™ãƒ¼ã‚¹ | $p(x) = \frac{1}{Z}\exp(-E(x))$ | â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜†â˜† |
| 35 Score | ã‚¹ã‚³ã‚¢é–¢æ•°å­¦ç¿’ | $\nabla_x \log p(x) = -\nabla_x E(x)$ | â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜† |
| 36 DDPM | Markové€†æ‹¡æ•£ | $\mathbf{x}_{t-1} = \frac{1}{\sqrt{\alpha_t}}(\mathbf{x}_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\boldsymbol{\varepsilon}_\theta) + \sigma_t \mathbf{z}$ | â˜…â˜…â˜† | â˜…â˜…â˜…â˜…â˜… |
| 37 SDE/ODE | é€£ç¶šæ™‚é–“SDE | $d\mathbf{x} = \mathbf{f}(\mathbf{x},t)dt + g(t)\nabla_\mathbf{x}\log p_t(\mathbf{x})dt$ | â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜† |
| 38 Flow Match | CFMçµ±ä¸€ç†è«– | $\mathcal{L}_{\text{CFM}} = \mathbb{E}_{t,p_t(\mathbf{x})}[\|\mathbf{u}_t(\mathbf{x}) - \mathbf{v}_\theta(\mathbf{x},t)\|^2]$ | â˜…â˜…â˜† | â˜…â˜…â˜…â˜…â˜† |
| 39 Latent | VAEåœ§ç¸®æ‹¡æ•£ | $\mathbf{z} = \mathcal{E}(\mathbf{x}), \mathbf{x} = \mathcal{D}(\mathbf{z})$ | â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜… |
| 40 Consistency | Self-consistency | $F_\theta(\mathbf{x}_t, t) = F_\theta(\mathbf{x}_{t'}, t'), \forall t,t'$ | â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜…â˜† |

#### 7.4.3 çŸ¥è­˜ã®ä¾å­˜é–¢ä¿‚

**å¿…é ˆå‰æçŸ¥è­˜**:
- ç¬¬33å› (Normalizing Flows) â†’ å¯é€†å¤‰æ›ã®åŸºç¤
- ç¬¬35å› (Score Matching) â†’ ç¬¬36-37å›ã®ç†è§£ã«å¿…é ˆ
- ç¬¬36å› (DDPM) ã¯**å…¨ã¦ã®åŸºç¤** â†’ æœ€å„ªå…ˆ

**æ¨å¥¨å­¦ç¿’é †**:
1. **36 DDPM** (åœŸå°ã€æœ€å„ªå…ˆ)
2. **37 SDE/ODE** (ç†è«–åŸºç›¤)
3. **38 Flow Matching** (çµ±ä¸€ç†è«–)
4. **39 Latent** (å®Ÿç”¨)
5. **40 Consistency** (1-stepç”Ÿæˆ)
6. **35 Score Matching** (ç†è«–æ·±æ˜ã‚Š)
7. **33 NF, 34 EBM** (è£œè¶³ç†è«–)

#### 7.4.4 å®Ÿè£…ã®ç´¯ç©ï¼ˆç©ã¿ä¸Šã’å¼ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰

**Stage 1: DDPMå®Ÿè£…** (ç¬¬36å›)
```julia
# åŸºæœ¬æ§‹é€ 
mutable struct DDPM
    Î²s::Vector{Float32}
    model::DenoisingUNet
end
```

**Stage 2: DDIMè¿½åŠ ** (ç¬¬36å›ã§å°å‡ºæ¸ˆã¿)
```julia
# DDPMã‚’æ‹¡å¼µ
function ddim_sample(ddpm::DDPM, x_T, Î·=0.0)
    # DDPMã®Î²sã‚’å†åˆ©ç”¨
end
```

**Stage 3: Score SDEçµ±åˆ** (ç¬¬37å›)
```julia
# SDEè¦–ç‚¹ã§ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
function sde_sample(model, x_T, sde_type=:vp)
    # VP-SDE ã¾ãŸã¯ VE-SDE
end
```

**Stage 4: Latent Diffusion** (ç¬¬39å›)
```julia
# VAEè¿½åŠ 
struct LatentDiffusion
    vae::VAE
    diffusion::DDPM  # Stage 1-3ã‚’å†åˆ©ç”¨
end
```

**Stage 5: Consistency Model** (ç¬¬40å›)
```julia
# æ–°è¦å®Ÿè£…ï¼ˆDDPMã‹ã‚‰è’¸ç•™å¯èƒ½ï¼‰
struct ConsistencyModel
    F_Î¸::ConsistencyFunction
    teacher::Union{DDPM, Nothing}  # CDæ™‚ã®ã¿
end
```

â†’ **å„è¬›ç¾©ã®å®Ÿè£…ãŒæ¬¡ã®è¬›ç¾©ã®åŸºç¤ã«ãªã‚‹è¨­è¨ˆ**

#### 7.4.5 Course IVä¿®äº†å¾Œã®ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆ

**ç†è«–**:
- [ ] Diffusionã®3å½¢å¼ï¼ˆDDPM/Score SDE/ODEï¼‰ã‚’ç›¸äº’å¤‰æ›ã§ãã‚‹
- [ ] Flow Matchingã¨Diffusionã®ç­‰ä¾¡æ€§ã‚’è¨¼æ˜ã§ãã‚‹
- [ ] Self-consistencyæ¡ä»¶ã®ç†è«–çš„ä¿è¨¼ã‚’èª¬æ˜ã§ãã‚‹
- [ ] æƒ…å ±ç†è«–ä¸‹ç•Œ $N \geq \Omega(\log d/\varepsilon)$ ã®æ„å‘³ã‚’ç†è§£

**å®Ÿè£…**:
- [ ] DDPM/DDIM/DPMã‚’ã‚¼ãƒ­ã‹ã‚‰å®Ÿè£…ã§ãã‚‹
- [ ] EDM Preconditioningã§å“è³ªå‘ä¸Šã§ãã‚‹
- [ ] Latent Diffusionã§å¤§è¦æ¨¡ç”»åƒç”Ÿæˆã§ãã‚‹
- [ ] Consistency Modelã‚’CTã¾ãŸã¯CDã§è¨“ç·´ã§ãã‚‹
- [ ] Rustã§Candleæ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã§ãã‚‹

**å¿œç”¨**:
- [ ] Text-to-Image (Stable Diffusionç›¸å½“) ã‚’å†ç¾ã§ãã‚‹
- [ ] 1-stepç”Ÿæˆã§50xé«˜é€ŸåŒ–ã‚’å®Ÿç¾ã§ãã‚‹
- [ ] NFE-FIDãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’å®šé‡è©•ä¾¡ã§ãã‚‹
- [ ] æ–°è¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«è’¸ç•™æ‰‹æ³•ã‚’é©ç”¨ã§ãã‚‹

#### 7.4.6 Course IV â†’ Course V ã¸ã®æ¥ç¶š

**Course IV ã®æˆæœ**: é™æ­¢ç”»ç”Ÿæˆã‚’å®Œå…¨åˆ¶è¦‡

**Course V (äºˆå®š)**: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æœ€çµ‚å½¢æ…‹

| è¬›ç¾© | ãƒ†ãƒ¼ãƒ | Course IVã¨ã®é–¢é€£ |
|:-----|:-------|:------------------|
| 41 | World Models | Diffusion â†’ ç’°å¢ƒã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ |
| 42 | Video Diffusion | é™æ­¢ç”» â†’ æ™‚ç³»åˆ—ä¸€è²«æ€§ |
| 43 | 3D Generation | 2D â†’ 3D/Multi-viewä¸€è²«æ€§ |
| 44 | Embodied AI | ç”Ÿæˆ â†’ è¡Œå‹• (RLçµ±åˆ) |
| 45 | Multimodal | Text+Image+Audioçµ±åˆ |

**æ¬¡ã®æŒ‘æˆ¦**: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ç›®çš„ã¯ã€Œã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€ã‹ï¼Ÿã€Œä¸–ç•Œç†è§£ã€ã‹ï¼Ÿ

### 7.5 æ¨å¥¨ãƒªã‚½ãƒ¼ã‚¹ï¼ˆå³é¸10é¸ï¼‰

#### 7.5.1 è«–æ–‡ï¼ˆå¿…èª­ï¼‰

1. **Song+ (2023) "Consistency Models"** [arXiv:2303.01469](https://arxiv.org/abs/2303.01469)
   - ç†ç”±: CMåŸè«–æ–‡ã€Self-consistencyæ¡ä»¶ã®åˆå‡º
   - é›£æ˜“åº¦: â˜…â˜…â˜…â˜†â˜†
   - æ¨å¥¨ã‚¿ã‚¤ãƒŸãƒ³ã‚°: æœ¬è¬›ç¾©å®Œäº†ç›´å¾Œ

2. **Geng+ (2025) "Consistency Models Made Easy"** [arXiv:2406.14548](https://arxiv.org/abs/2406.14548)
   - ç†ç”±: ECTã®Analytical ODEã€å®Ÿè£…ãŒåœ§å€’çš„ã«ã‚·ãƒ³ãƒ—ãƒ«
   - é›£æ˜“åº¦: â˜…â˜…â˜†â˜†â˜†
   - æ¨å¥¨ã‚¿ã‚¤ãƒŸãƒ³ã‚°: Z3.7èª­äº†å¾Œ

3. **Kim+ (2023) "Consistency Trajectory Models"** [arXiv:2310.02279](https://arxiv.org/abs/2310.02279)
   - ç†ç”±: CMã®ä¸€èˆ¬åŒ–ã€Multi-stepæ¨è«–ã®ç†è«–
   - é›£æ˜“åº¦: â˜…â˜…â˜…â˜…â˜†
   - æ¨å¥¨ã‚¿ã‚¤ãƒŸãƒ³ã‚°: CMå®Œå…¨ç†è§£å¾Œ

4. **Luo+ (2023) "Latent Consistency Models"** [arXiv:2310.04378](https://arxiv.org/abs/2310.04378)
   - ç†ç”±: Stable Diffusioné«˜é€ŸåŒ–ã®å®Ÿç”¨ä¾‹
   - é›£æ˜“åº¦: â˜…â˜…â˜…â˜†â˜†
   - æ¨å¥¨ã‚¿ã‚¤ãƒŸãƒ³ã‚°: ç¬¬39å›å¾©ç¿’å¾Œ

5. **Lin+ (2025) "Diffusion Adversarial Post-Training"** [arXiv:2501.08316](https://arxiv.org/abs/2501.08316)
   - ç†ç”±: æœ€æ–°ã®1-stepè’¸ç•™ã€GANçµ±åˆ
   - é›£æ˜“åº¦: â˜…â˜…â˜…â˜†â˜†
   - æ¨å¥¨ã‚¿ã‚¤ãƒŸãƒ³ã‚°: ç¬¬12å› (GAN) å¾©ç¿’å¾Œ

#### 7.5.2 å®Ÿè£…ãƒªãƒã‚¸ãƒˆãƒª

6. **openai/consistency_models** (å…¬å¼PyTorchå®Ÿè£…)
   - URL: [github.com/openai/consistency_models](https://github.com/openai/consistency_models)
   - è¨€èª: Python/PyTorch
   - æ¨å¥¨ç”¨é€”: CT/iCTå®Ÿè£…ã®ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹

7. **Stability-AI/generative-models** (LCMå…¬å¼å®Ÿè£…)
   - URL: [github.com/Stability-AI/generative-models](https://github.com/Stability-AI/generative-models)
   - è¨€èª: Python/PyTorch
   - æ¨å¥¨ç”¨é€”: LCM-LoRA fine-tuning

8. **huggingface/diffusers** (çµ±åˆãƒ©ã‚¤ãƒ–ãƒ©ãƒª)
   - URL: [github.com/huggingface/diffusers](https://github.com/huggingface/diffusers)
   - è¨€èª: Python/PyTorch
   - æ¨å¥¨ç”¨é€”: LCMæ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

#### 7.5.3 æ•™æãƒ»è¬›ç¾©

9. **MIT 6.S184 (2026) "Diffusion Models"**
   - URL: [diffusion.csail.mit.edu](https://diffusion.csail.mit.edu/)
   - å½¢å¼: å‹•ç”»è¬›ç¾© + ã‚¹ãƒ©ã‚¤ãƒ‰
   - æ¨å¥¨Lecture: Lecture 8 "Fast Sampling" (DPM/DDIM/CMç¶²ç¾…)

10. **Hugging Face Diffusion Course**
    - URL: [huggingface.co/learn/diffusion-course](https://huggingface.co/learn/diffusion-course)
    - å½¢å¼: ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯
    - æ¨å¥¨Unit: Unit 4 "Fine-tuning and Guidance"

### 7.6 æ¬¡å›äºˆå‘Š: ç¬¬41å› World Models & ç’°å¢ƒã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ç†è«–

**ãƒ†ãƒ¼ãƒ**: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æœ€çµ‚åˆ°é”ç‚¹ â€” ç’°å¢ƒã®ç†è§£ã¨ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

**å†…å®¹**:
- JEPA (LeCunäºˆæ¸¬ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£)
- V-JEPA (å‹•ç”»ã§ã®å®Ÿè£…)
- Transfusion (AR + Diffusionçµ±åˆ)
- ç‰©ç†æ³•å‰‡å­¦ç¿’ç†è«–
- Energy-based World Models
- ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡æŒ‡æ¨™

**æ¥ç¶š**:
- ç¬¬40å›: 1-stepç”Ÿæˆã§é«˜é€ŸåŒ–ã‚’å®Ÿç¾
- **ç¬¬41å›**: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®çœŸã®ç›®çš„ â€” ä¸–ç•Œã‚’ç†è§£ã—æœªæ¥ã‚’äºˆæ¸¬ã™ã‚‹

**ğŸ’€ å¸¸è­˜ç ´å£Šã®å•ã„**: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®çµ‚ç€ç‚¹ã¯"ç†è§£"ã§ã¯ï¼Ÿ

:::message
**Course IV ç¬¬8å›ï¼ˆç¬¬40å›ï¼‰å®Œäº†ï¼**

**é”æˆã—ãŸã“ã¨**:
- Self-consistencyæ¡ä»¶ã®ç†è«–çš„ä¿è¨¼ã‚’å®Œå…¨ç†è§£
- CT/CD/iCT/ECTã®è¨“ç·´æ‰‹æ³•ã‚’æ•°å¼ãƒ¬ãƒ™ãƒ«ã§æŠŠæ¡
- DPM-Solver++/UniPCã¨ã®æ¯”è¼ƒã§é«˜æ¬¡ã‚½ãƒ«ãƒãƒ¼ã‚’ç†è§£
- Progressive/LCM/InstaFlow/DMD2ã®è’¸ç•™ç³»è­œã‚’æ•´ç†
- Juliaã§CTå®Ÿè£…ã€Rustã§Candleæ¨è«–ã‚’å®Œæˆ
- 1-stepç”Ÿæˆã®ç†è«–é™ç•Œã¨å®Ÿç”¨ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’ç¿’å¾—

**æ¬¡ã®æŒ‘æˆ¦**:
ç¬¬41å›ã§World Modelsã¸ã€‚Diffusionã¯ã€Œã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ã€ã‹ã‚‰ã€Œä¸–ç•Œç†è§£ã‚¨ãƒ³ã‚¸ãƒ³ã€ã¸é€²åŒ–ã™ã‚‹ã€‚

**Course IVå…¨ä½“ã®åˆ°é”ç‚¹**:
é™æ­¢ç”»ç”Ÿæˆã®å…¨ç†è«–ï¼ˆDDPMâ†’Scoreâ†’Flowâ†’Latentâ†’Consistencyï¼‰ã‚’å®Œå…¨åˆ¶è¦‡ã€‚æ¬¡ã¯æ™‚ç©ºé–“ã¸ã€‚
:::

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Song, Y., Dhariwal, P., Chen, M., & Sutskever, I. (2023). Consistency Models. *ICML 2023*.
@[card](https://arxiv.org/abs/2303.01469)

[^2]: Song, Y., & Dhariwal, P. (2023). Improved Techniques for Training Consistency Models. *arXiv:2310.14189*.
@[card](https://arxiv.org/abs/2310.14189)

[^3]: Geng, Z., Pokle, A., Luo, W., Lin, J., & Kolter, J. Z. (2025). Consistency Models Made Easy. *ICLR 2025*.
@[card](https://arxiv.org/abs/2406.14548)

[^4]: Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., & Zhu, J. (2022). DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models. *arXiv:2211.01095*.
@[card](https://arxiv.org/abs/2211.01095)

[^5]: Zhao, W., Bai, L., Rao, Y., Zhou, J., & Lu, J. (2023). UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models. *NeurIPS 2023*.
@[card](https://arxiv.org/abs/2302.04867)

[^6]: Salimans, T., & Ho, J. (2022). Progressive Distillation for Fast Sampling of Diffusion Models. *ICLR 2022*.
@[card](https://arxiv.org/abs/2202.00512)

[^7]: Luo, S., Tan, Y., Huang, L., Li, J., & Zhao, H. (2023). Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference. *arXiv:2310.04378*.
@[card](https://arxiv.org/abs/2310.04378)

[^8]: Liu, X., Gong, C., & Liu, Q. (2023). InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation. *arXiv:2309.06380*.
@[card](https://arxiv.org/abs/2309.06380)

[^9]: Lin, S., Xia, X., Ren, Y., Yang, C., Xiao, X., & Jiang, L. (2025). Diffusion Adversarial Post-Training for One-Step Video Generation. *arXiv:2501.08316*.
@[card](https://arxiv.org/abs/2501.08316)

[^10]: Karras, T., Aittala, M., Aila, T., & Laine, S. (2022). Elucidating the Design Space of Diffusion-Based Generative Models. *NeurIPS 2022*.
@[card](https://arxiv.org/abs/2206.00364)

[^11]: Kim, D., Lai, C.-H., Liao, W.-H., Murata, N., Takida, Y., Uesaka, T., ... & Ermon, S. (2023). Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion. *arXiv:2310.02279*.
@[card](https://arxiv.org/abs/2310.02279)

### æ•™ç§‘æ›¸ãƒ»ã‚µãƒ¼ãƒ™ã‚¤

- MIT 6.S184 (2026). *Diffusion Models*. [diffusion.csail.mit.edu](https://diffusion.csail.mit.edu/)
- Song, Y., & Ermon, S. (2020). "Score-Based Generative Modeling through Stochastic Differential Equations" (èƒŒæ™¯ç†è«–)
- Ho, J., Jain, A., & Abbeel, P. (2020). "Denoising Diffusion Probabilistic Models" (DDPMåŸè«–æ–‡)

---

## è¨˜æ³•è¦ç´„

| è¨˜å· | æ„å‘³ |
|:-----|:-----|
| $F_\theta(\mathbf{x}_t, t)$ | Consistency function (Self-consistencyã‚’æº€ãŸã™NN) |
| $\mathbf{x}_t$ | æ™‚åˆ» $t$ ã§ã®ãƒã‚¤ã‚ºä»˜ãç”»åƒ |
| $\mathbf{x}_\epsilon$ | ã»ã¼ãƒã‚¤ã‚ºãªã—ç”»åƒ ($\mathbf{x}_0$ ã«è¿‘ã„) |
| $\Psi_{t \leftarrow t'}$ | PF-ODE flow map (æ™‚åˆ» $t'$ ã‹ã‚‰ $t$ ã¸ã®è»Œé“) |
| $c_{\text{skip}}(t), c_{\text{out}}(t), c_{\text{in}}(t)$ | Preconditioning coefficients (EDM-style) |
| $d_{\text{PH}}(\mathbf{a}, \mathbf{b}; c)$ | Pseudo-Huberè·é›¢ |
| $\theta^-$ | Target network (EMAæ›´æ–°) |
| $\mathcal{L}_{\text{CT}}$ | Consistency Training loss |
| $\mathcal{L}_{\text{CD}}$ | Consistency Distillation loss |
| $\mathcal{L}_{\text{ECT}}$ | Easy Consistency Tuning loss |
| NFE | Number of Function Evaluations (ãƒ¢ãƒ‡ãƒ«å‘¼ã³å‡ºã—å›æ•°) |

---

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
