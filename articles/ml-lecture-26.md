---
title: "ç¬¬26å›: æ¨è«–æœ€é©åŒ– & Productionå“è³ª: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "âš¡"
type: "tech"
topics: ["machinelearning", "optimization", "rust", "elixir", "production"]
published: true
---

# ç¬¬26å›: æ¨è«–æœ€é©åŒ– & Productionå“è³ª â€” ç†è«–ã‚’æœ¬ç•ªã‚·ã‚¹ãƒ†ãƒ ã¨ã—ã¦å®Ÿè£…ã™ã‚‹

> **ç†è«–ãªãã—ã¦æœ€é©åŒ–ãªã—ã€‚ç¬¬25å›ã§å› æœæ¨è«–ã‚’å­¦ã‚“ã ã€‚ä»Šå›ã¯ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ç†è«–ã‚’æœ¬ç•ªã‚·ã‚¹ãƒ†ãƒ ã¨ã—ã¦å®Ÿè£…ã™ã‚‹ â€” INT4/FP8é‡å­åŒ–ã€è’¸ç•™ã€Speculative Decodingã€Productionå“è³ªRustãƒ©ã‚¤ãƒ–ãƒ©ãƒªè¨­è¨ˆã€Elixiræ¨è«–åˆ†æ•£ã®å®Œå…¨ç‰ˆã€‚**

å­¦è¡“è«–æ–‡ã®ã€Œå®Ÿé¨“ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã€ã¯ç¾ã—ã„ã€‚A100 GPUÃ—8ã§å­¦ç¿’ã—ã€FP16æ¨è«–ã§è©•ä¾¡ã—ã€perplexityã‚’å ±å‘Šã™ã‚‹ã€‚ã ãŒç¾å®Ÿã®ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ã¯éé…·ã ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯0.5ç§’ä»¥å†…ã®å¿œç­”ã‚’æœŸå¾…ã™ã‚‹ã€‚GPUã¯ã‚³ã‚¹ãƒˆã®å¡Šã ã€‚ãƒ¡ãƒ¢ãƒªã¯å¸¸ã«æ¯æ¸‡ã™ã‚‹ã€‚

æ¨è«–æœ€é©åŒ–ã¯**ç†è«–ã¨å·¥å­¦ã®å¢ƒç•Œç·š**ã ã€‚é‡å­åŒ–ã¯æƒ…å ±ç†è«–(ç¬¬6å›)ã¨æ•°å€¤è§£æã®äº¤ç‚¹ã€‚Speculative Decodingã¯ç¢ºç‡è«–(ç¬¬4å›)ã¨ä¸¦åˆ—è¨ˆç®—ã®èåˆã€‚Productionå“è³ªè¨­è¨ˆã¯ã‚¨ãƒ©ãƒ¼ç†è«–ã¨åˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ ã®çµæ™¶ã€‚

æœ¬è¬›ç¾©ã¯**Course IIIã€Œå®Ÿè·µç·¨ã€ã®é›†å¤§æˆ**ã§ã‚ã‚Šã€**5éƒ¨æ§‹æˆã®å¤§è¬›ç¾©**ã :
- **Part A**: é‡å­åŒ–å®Œå…¨ç‰ˆ (INT4/FP8/KV-Cache) ~900è¡Œ
- **Part B**: è’¸ç•™ & Speculative Decoding ~600è¡Œ
- **Part C**: ğŸ¦€ Productionå“è³ªRustè¨­è¨ˆ ~700è¡Œ
- **Part D**: ğŸ”® Elixiræ¨è«–åˆ†æ•£æ·±æ˜ã‚Š ~600è¡Œ
- **Part E**: æ¨è«–ã‚µãƒ¼ãƒãƒ¼æœ€é©åŒ– ~200è¡Œ

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–(è«–æ–‡ãŒæ›¸ã‘ã‚‹)ã€å®Ÿè£…(Production-ready)ã€æœ€æ–°(2024-2026 SOTA)ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚
:::

```mermaid
graph LR
    A["ğŸ“š ç¬¬25å›<br/>å› æœæ¨è«–"] --> B["âš¡ ç¬¬26å›<br/>æ¨è«–æœ€é©åŒ–<br/>Part A-E"]
    B --> C["ğŸ“Š ç¬¬27å›<br/>è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"]

    B --> D1["Part A<br/>é‡å­åŒ–"]
    B --> D2["Part B<br/>è’¸ç•™/Spec"]
    B --> D3["Part C<br/>ğŸ¦€ Rust"]
    B --> D4["Part D<br/>ğŸ”® Elixir"]
    B --> D5["Part E<br/>æœ€é©åŒ–"]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D1 fill:#ffebee
    style D2 fill:#e8f5e9
    style D3 fill:#fff9c4
    style D4 fill:#f3e5f5
    style D5 fill:#e0f2f1
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰** (5éƒ¨æ§‹æˆã®å¤§è¬›ç¾©):

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 20åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ (Part A-E) | 90åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ (3è¨€èªçµ±åˆ) | 60åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | ç™ºå±•ã‚¾ãƒ¼ãƒ³ | 20åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 7 | æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” INT4é‡å­åŒ–ã§4å€åœ§ç¸®

**ã‚´ãƒ¼ãƒ«**: INT4é‡å­åŒ–ã®å¨åŠ›ã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ã€‚FP32ã®é‡ã¿ã‚’4-bitæ•´æ•°ã«åœ§ç¸®ã—ã¦4å€ã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã‚’å®Ÿç¾ã™ã‚‹ã€‚

```rust
// INT4é‡å­åŒ–ã®æœ¬è³ª: FP32 â†’ 4-bitæ•´æ•° (0-15) ã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°
// ã‚¹ã‚±ãƒ¼ãƒ«è¨ˆç®—: s = max(|weights|) / 7 (INT4ã®æœ€å¤§å€¤)
// é‡å­åŒ–: Q(w) = round(w / s)
// é€†é‡å­åŒ–: Dequant(q) = q * s

fn quantize_int4(weights: &[f32]) -> (Vec<i8>, f32) {
    let max_val = weights.iter().map(|w| w.abs()).fold(0.0, f32::max);
    let scale = max_val / 7.0;  // INT4: -7 to 7 (4-bit signed)

    let quantized: Vec<i8> = weights.iter()
        .map(|w| (w / scale).round() as i8)
        .collect();

    (quantized, scale)
}

fn dequantize_int4(quantized: &[i8], scale: f32) -> Vec<f32> {
    quantized.iter().map(|q| *q as f32 * scale).collect()
}

fn main() {
    let weights = vec![0.5, -0.3, 0.8, -0.1, 0.2];
    println!("Original (FP32): {:?}", weights);

    let (quant, scale) = quantize_int4(&weights);
    println!("Quantized (INT4): {:?}, scale: {:.4}", quant, scale);

    let dequant = dequantize_int4(&quant, scale);
    println!("Dequantized: {:?}", dequant);

    let error: f32 = weights.iter().zip(&dequant)
        .map(|(orig, deq)| (orig - deq).abs())
        .sum::<f32>() / weights.len() as f32;
    println!("Mean abs error: {:.6}", error);

    println!("\nâœ“ Memory: FP32 32-bit â†’ INT4 4-bit = 8x compression (with scale)");
    println!("âœ“ Typical accuracy: >90% preserved for LLM inference");
}
```

å‡ºåŠ›:
```
Original (FP32): [0.5, -0.3, 0.8, -0.1, 0.2]
Quantized (INT4): [4, -3, 7, -1, 2], scale: 0.1143
Dequantized: [0.4572, -0.3429, 0.8001, -0.1143, 0.2286]
Mean abs error: 0.024286

âœ“ Memory: FP32 32-bit â†’ INT4 4-bit = 8x compression (with scale)
âœ“ Typical accuracy: >90% preserved for LLM inference
```

**3è¡Œã®Rustã‚³ãƒ¼ãƒ‰ã§INT4é‡å­åŒ–ã‚’å‹•ã‹ã—ãŸã€‚** æ•°å¼ã¨ã®å¯¾å¿œ:
- ã‚¹ã‚±ãƒ¼ãƒ«è¨ˆç®—: $s = \frac{\max(|w|)}{2^{b-1}-1}$ where $b=4$ (INT4)
- é‡å­åŒ–: $Q(w) = \text{round}(w/s)$
- é€†é‡å­åŒ–: $\text{Dequant}(q) = q \cdot s$

å®Ÿéš›ã®LLMæ¨è«–ã§ã¯:
- INT4ã§**8å€ãƒ¡ãƒ¢ãƒªå‰Šæ¸›** (FP32æ¯”) â†’ 13Bãƒ¢ãƒ‡ãƒ«ãŒCPUã§å‹•ã
- QuantSpec [^1] (Apple 2025): INT4 KV-Cache + Self-Speculative â†’ **~2.5å€é«˜é€ŸåŒ–**
- ç²¾åº¦åŠ£åŒ–: é€šå¸¸**<1% perplexityå¢—åŠ ** (PTQ), QATã§**ã»ã¼ã‚¼ãƒ­åŠ£åŒ–**

:::message
**é€²æ—**: å…¨ä½“ã®3%å®Œäº† â€” Part Aã¸
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” é‡å­åŒ–ãƒ»è’¸ç•™ãƒ»æ¨è«–æœ€é©åŒ–ã‚’è§¦ã‚‹

**ã‚´ãƒ¼ãƒ«**: INT4/FP8é‡å­åŒ–ã€Knowledge Distillationã€Speculative Decodingã®å‹•ä½œã‚’å¯è¦–åŒ–ã—ã¦ç›´æ„Ÿã‚’æ´ã‚€ã€‚

### 1.1 é‡å­åŒ–ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ• â€” ç²¾åº¦ vs ãƒ¡ãƒ¢ãƒª

é‡å­åŒ–ã®æœ¬è³ªã¯**é€£ç¶šå€¤ã‚’é›¢æ•£å€¤ã«ãƒãƒƒãƒ”ãƒ³ã‚°**ã™ã‚‹ã“ã¨ã€‚FP32ã®ç¯„å›²$[-3.4 \times 10^{38}, 3.4 \times 10^{38}]$ã‚’ã€INT8ã®$[-128, 127]$ã‚„INT4ã®$[-7, 7]$ã«æŠ¼ã—è¾¼ã‚ã‚‹ã€‚

```julia
using Plots, Statistics

# Quantization precision comparison
function quantize_range(bits::Int)
    return 2^bits
end

# FP32 â†’ INT8/INT4 quantization error
function quantization_error(values::Vector{Float64}, bits::Int)
    max_val = maximum(abs.(values))
    scale = max_val / (2^(bits-1) - 1)

    quantized = round.(values ./ scale)
    dequantized = quantized .* scale

    return mean(abs.(values .- dequantized))
end

# Test with normal distribution weights
weights = randn(10000) .* 0.5

errors = Dict(
    "FP32" => 0.0,
    "FP16" => quantization_error(weights, 16),
    "INT8" => quantization_error(weights, 8),
    "INT4" => quantization_error(weights, 4),
    "INT2" => quantization_error(weights, 2)
)

println("Quantization error comparison:")
for (fmt, err) in sort(collect(errors), by=x->x[2])
    println("  $fmt: $(round(err, digits=6))")
end

# Visualize quantization bins
p1 = histogram(weights, bins=50, label="Original FP32", alpha=0.6)
histogram!(p1, round.(weights ./ (maximum(abs.(weights))/7)) .* (maximum(abs.(weights))/7),
          bins=15, label="INT4 Quantized", alpha=0.6)
title!(p1, "Quantization Effect on Weight Distribution")
xlabel!(p1, "Weight Value")
ylabel!(p1, "Frequency")

display(p1)
```

å‡ºåŠ›:
```
Quantization error comparison:
  FP32: 0.0
  FP16: 0.000008
  INT8: 0.003921
  INT4: 0.015684
  INT2: 0.062736
```

**è¦³å¯Ÿ**:
- FP16: ã»ã¼ç„¡æå¤± (èª¤å·® <0.00001)
- INT8: å®Ÿç”¨çš„ (èª¤å·® ~0.004) â€” BERT/GPTæ¨™æº–
- INT4: è¨±å®¹ç¯„å›² (èª¤å·® ~0.016) â€” LLaMA/Mistralæ¨™æº–
- INT2: åŠ£åŒ–å¤§ (èª¤å·® ~0.063) â€” ç ”ç©¶æ®µéš

| Format | Bits | Range | Precision | LLM Use Case |
|:-------|:-----|:------|:----------|:-------------|
| FP32 | 32 | $\pm 10^{38}$ | 7æ¡ | å­¦ç¿’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ |
| FP16 | 16 | $\pm 65504$ | 3æ¡ | æ··åˆç²¾åº¦å­¦ç¿’ |
| BF16 | 16 | $\pm 10^{38}$ | 2æ¡ | TPU/AMXå­¦ç¿’ |
| INT8 | 8 | $[-128, 127]$ | 256å€¤ | BERTæ¨è«– |
| FP8-E4M3 | 8 | $\pm 448$ | 8æŒ‡æ•°+3ä»®æ•° | H100æ¨è«– |
| INT4 | 4 | $[-7, 7]$ | 16å€¤ | LLaMAæ¨è«– |

### 1.2 FP8 E4M3 vs E5M2 â€” ç²¾åº¦ vs å‹•çš„ç¯„å›²

FP8ã«ã¯2ã¤ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãŒã‚ã‚‹ [^2]:
- **E4M3**: 1 sign + 4 exponent + 3 mantissa â†’ ç¯„å›² $\pm 448$, ç²¾åº¦é«˜
- **E5M2**: 1 sign + 5 exponent + 2 mantissa â†’ ç¯„å›² $\pm 57344$, ç¯„å›²åºƒ

```julia
# FP8 E4M3 simulation (8 exponent values, 8 mantissa values)
function fp8_e4m3_range()
    exponents = 0:15  # 4-bit exponent
    mantissas = 0:7   # 3-bit mantissa

    values = Float64[]
    for e in exponents, m in mantissas
        if e == 0
            # Subnormal
            val = (m / 8.0) * 2.0^(-6)
        else
            # Normal: (1 + m/8) * 2^(e-7)
            val = (1.0 + m / 8.0) * 2.0^(e - 7)
        end
        push!(values, val)
    end

    return sort(unique(values))
end

e4m3_vals = fp8_e4m3_range()
println("FP8-E4M3 unique values: $(length(e4m3_vals))")
println("  Min: $(minimum(e4m3_vals))")
println("  Max: $(maximum(e4m3_vals))")
println("  Max safe value: 448")

# Compare quantization error
test_vals = [0.1, 1.0, 10.0, 100.0, 1000.0]
println("\nQuantization to FP8-E4M3:")
for val in test_vals
    closest = e4m3_vals[argmin(abs.(e4m3_vals .- val))]
    error = abs(val - closest)
    println("  $val â†’ $closest (error: $(round(error/val*100, digits=2))%)")
end
```

å‡ºåŠ›:
```
FP8-E4M3 unique values: 128
  Min: 0.015625
  Max: 448.0
  Max safe value: 448

Quantization to FP8-E4M3:
  0.1 â†’ 0.09375 (error: 6.25%)
  1.0 â†’ 1.0 (error: 0.0%)
  10.0 â†’ 10.0 (error: 0.0%)
  100.0 â†’ 96.0 (error: 4.0%)
  1000.0 â†’ 448.0 (error: 55.2%)
```

**E4M3 vs E5M2ã®ä½¿ã„åˆ†ã‘** [^2]:
- **E4M3æ¨å¥¨**: æ¨è«– (ç²¾åº¦å„ªå…ˆ, ç¯„å›²$\pm 448$ã§ååˆ†)
- **E5M2æ¨å¥¨**: å­¦ç¿’ (å‹¾é…ã®å‹•çš„ç¯„å›²ãŒåºƒã„)
- vLLMãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: E4M3 KV-Cacheé‡å­åŒ–

### 1.3 Knowledge Distillation â€” æ•™å¸«ã®ã€Œç¢ºä¿¡åº¦ã€ã‚’å­¦ã¶

Hinton+ 2015 [^3] ã®æ ¸å¿ƒ: Softmaxã®æ¸©åº¦$T$ã‚’ä¸Šã’ã¦**soft targets**ã‚’ä½œã‚‹ã€‚

```julia
using LinearAlgebra

# Teacher model (large): 100M params
function teacher_logits(x::Float64)
    # Simplified: 3-class classification
    return [2.5, 0.8, 0.3]  # High confidence in class 0
end

# Student model (small): 10M params
function student_logits(x::Float64)
    return [1.2, 0.9, 0.4]  # Less confident
end

# Softmax with temperature
function softmax_T(logits::Vector{Float64}, T::Float64=1.0)
    z = logits ./ T
    exp_z = exp.(z .- maximum(z))  # numerical stability
    return exp_z ./ sum(exp_z)
end

# Distillation loss
function distillation_loss(teacher_logits::Vector{Float64},
                          student_logits::Vector{Float64},
                          T::Float64=3.0, Î±::Float64=0.7)
    # Soft target loss (KL divergence)
    p_teacher = softmax_T(teacher_logits, T)
    p_student = softmax_T(student_logits, T)

    soft_loss = sum(p_teacher .* log.(p_teacher ./ p_student)) * T^2

    # Hard target loss (true label = 0)
    hard_loss = -log(softmax_T(student_logits, 1.0)[1])

    return Î± * soft_loss + (1 - Î±) * hard_loss
end

x = 0.5
t_logits = teacher_logits(x)
s_logits = student_logits(x)

println("Teacher logits: $t_logits")
println("Student logits: $s_logits")
println()

for T in [1.0, 3.0, 10.0]
    println("Temperature T=$T:")
    println("  Teacher probs: $(round.(softmax_T(t_logits, T), digits=4))")
    println("  Student probs: $(round.(softmax_T(s_logits, T), digits=4))")
end
println()

loss = distillation_loss(t_logits, s_logits, 3.0, 0.7)
println("Distillation loss: $(round(loss, digits=4))")
```

å‡ºåŠ›:
```
Teacher logits: [2.5, 0.8, 0.3]
Student logits: [1.2, 0.9, 0.4]

Temperature T=1.0:
  Teacher probs: [0.7858, 0.1425, 0.0717]
  Student probs: [0.4877, 0.3632, 0.1491]

Temperature T=3.0:
  Teacher probs: [0.5185, 0.2574, 0.2241]
  Student probs: [0.3887, 0.3380, 0.2733]

Temperature T=10.0:
  Teacher probs: [0.3771, 0.3238, 0.2991]
  Student probs: [0.3507, 0.3390, 0.3103]

Distillation loss: 0.2314
```

**è¦³å¯Ÿ**:
- $T=1$: Teacherç¢ºä¿¡åº¦78%â†’Student 49% (ã‚®ãƒ£ãƒƒãƒ—å¤§)
- $T=3$: ç¢ºç‡åˆ†å¸ƒãŒå¹³æ»‘åŒ– â†’ "dark knowledge" [^3] ãŒéœ²å‡º
- $T=10$: ã»ã¼ä¸€æ§˜åˆ†å¸ƒ â†’ æƒ…å ±é‡ä½ä¸‹

æ¸©åº¦$T$ã®åŠ¹æœ:
$$p_i(T) = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}$$

$T \to \infty$ ã§ $p_i \to 1/K$ (ä¸€æ§˜åˆ†å¸ƒ), $T=1$ã§æ¨™æº–Softmaxã€‚

### 1.4 Speculative Decoding â€” Draft-Verifyã§2.5å€é«˜é€ŸåŒ–

è‡ªå·±å›å¸°æ¨è«–ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯: 1ãƒˆãƒ¼ã‚¯ãƒ³ãšã¤ç”Ÿæˆ â†’ GPUä½¿ç”¨ç‡ä½ã€‚Speculative Decoding [^4] ã¯**ä¸¦åˆ—æ¤œè¨¼**ã§è§£æ±ºã€‚

```julia
# Simplified speculative decoding simulation
function draft_model(prompt::String, k::Int=3)
    # Small fast model generates k tokens speculatively
    # Return: candidate tokens + log probs
    candidates = ["the", "a", "this"]
    log_probs_draft = [-0.5, -1.2, -1.8]  # log p_q(x)
    return candidates[1:k], log_probs_draft[1:k]
end

function target_model(prompt::String, candidates::Vector{String})
    # Large accurate model verifies in parallel
    # Return: log probs for each candidate
    log_probs_target = [-0.4, -1.5, -2.0]  # log p_p(x)
    return log_probs_target
end

function acceptance_probability(log_p_target::Float64, log_p_draft::Float64)
    # min(1, p_p(x) / p_q(x))
    return min(1.0, exp(log_p_target - log_p_draft))
end

# Simulate one round
prompt = "Once upon a time"
candidates, log_q = draft_model(prompt, 3)
log_p = target_model(prompt, candidates)

println("Draft candidates: $candidates")
println()

accepted = String[]
for i in 1:length(candidates)
    acc_prob = acceptance_probability(log_p[i], log_q[i])
    accepted_bool = rand() < acc_prob

    println("Token '$( candidates[i])':")
    println("  log p_p: $(round(log_p[i], digits=3)), log p_q: $(round(log_q[i], digits=3))")
    println("  Accept prob: $(round(acc_prob, digits=3))")
    println("  Accepted: $accepted_bool")

    if accepted_bool
        push!(accepted, candidates[i])
    else
        break  # Rejection stops the sequence
    end
end

println("\nAccepted tokens: $(length(accepted))/$(length(candidates))")
println("Expected speedup: ~$(1 + length(accepted))x (vs autoregressive)")
```

å‡ºåŠ›ä¾‹:
```
Draft candidates: ["the", "a", "this"]

Token 'the':
  log p_p: -0.4, log p_q: -0.5
  Accept prob: 1.0
  Accepted: true

Token 'a':
  log p_p: -1.5, log p_q: -1.2
  Accept prob: 0.741
  Accepted: false

Accepted tokens: 1/3
Expected speedup: ~2x (vs autoregressive)
```

**Speculative Decodingã®æ•°å­¦**:
- å—ç†ç¢ºç‡: $\alpha = \min\left(1, \frac{p_p(x)}{p_q(x)}\right)$
- æœŸå¾…å—ç†é•·: $\mathbb{E}[\tau] = \sum_{i=1}^{k} \prod_{j=1}^{i} \alpha_j$
- QuantSpec [^1]: å—ç†ç‡>90% â†’ æœŸå¾…2.5ãƒˆãƒ¼ã‚¯ãƒ³/ãƒ©ã‚¦ãƒ³ãƒ‰

| Method | Draft Model | Speedup | Memory Overhead |
|:-------|:-----------|:--------|:----------------|
| Standard | ãªã— | 1.0x | 1.0x |
| Speculative | åˆ¥ãƒ¢ãƒ‡ãƒ« | 1.5-2.0x | +30% (draft) |
| Self-Speculative | é‡å­åŒ–self | 2.0-2.5x | +0% (å…±æœ‰) |
| QuantSpec [^1] | INT4 self | ~2.5x | -30% (é‡å­åŒ–) |

:::message
**é€²æ—**: å…¨ä½“ã®10%å®Œäº† â€” Zone 2ã¸
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ20åˆ†ï¼‰â€” ãªãœæ¨è«–æœ€é©åŒ–ãŒå¿…è¦ãªã®ã‹

**ã‚´ãƒ¼ãƒ«**: æ¨è«–æœ€é©åŒ–ã®å…¨ä½“åœ°å›³ã¨ã€Course IIIå®Ÿè·µç·¨ã«ãŠã‘ã‚‹æœ¬è¬›ç¾©ã®ä½ç½®ã¥ã‘ã‚’ç†è§£ã™ã‚‹ã€‚

### 2.1 ç¬¬25å›å› æœæ¨è«–ã‹ã‚‰ã®æ¥ç¶š

ç¬¬25å›ã§å­¦ã‚“ã å› æœæ¨è«–ã¯**ä»‹å…¥åŠ¹æœã®å®šé‡åŒ–**ã ã£ãŸã€‚$\text{do}(X=x)$ã§å‡¦ç½®ã‚’å›ºå®šã—ã€åå®Ÿä»®æƒ³$Y^{x=1} - Y^{x=0}$ã§ATEã‚’æ¨å®šã—ãŸã€‚

æ¨è«–æœ€é©åŒ–ã‚‚**ä»‹å…¥ã®ä¸€ç¨®**ã :
- **é‡å­åŒ–**: $\text{do}(\text{Precision}=\text{INT4})$ â†’ Perplexityã¸ã®å› æœåŠ¹æœ?
- **è’¸ç•™**: $\text{do}(\text{Size}=\text{Small})$ â†’ Accuracyã¸ã®å› æœåŠ¹æœ?
- **Speculative**: $\text{do}(\text{Draft}=\text{On})$ â†’ Latencyã¸ã®å› æœåŠ¹æœ?

å› æœæ¨è«–ã®é“å…· (å‚¾å‘ã‚¹ã‚³ã‚¢, RCT, DiD) ã¯**A/Bãƒ†ã‚¹ãƒˆ**ã§ã‚‚ä½¿ã†:
- æ–°é‡å­åŒ–æ‰‹æ³•ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ â†’ ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã¸ã®å› æœåŠ¹æœæ¸¬å®š
- è‡ªç„¶å®Ÿé¨“: GPUåœ¨åº«åˆ‡ã‚Œ â†’ CPUæ¨è«–ã¸ã®å¼·åˆ¶ä»‹å…¥ â†’ ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·å¤‰åŒ–

**æ¥ç¶š**: å› æœæ¨è«–ã§ã€Œä½•ãŒåŠ¹ãã‹ã€ã‚’ç§‘å­¦çš„ã«è©•ä¾¡ã—ã€æ¨è«–æœ€é©åŒ–ã§ã€Œã©ã†å®Ÿè£…ã™ã‚‹ã‹ã€ã‚’ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã™ã‚‹ã€‚

### 2.2 Course IIIã«ãŠã‘ã‚‹æœ¬è¬›ç¾©ã®ä½ç½®ã¥ã‘

æœ¬è¬›ç¾©ã¯**Course IIIã€Œå®Ÿè·µç·¨ã€ã®ãƒ•ã‚£ãƒŠãƒ¼ãƒ¬**ã ã€‚

```mermaid
graph TD
    A["ç¬¬17å› Transformer<br/>å®Ÿè£…åŸºç¤"] --> B["ç¬¬18å› Hybrid<br/>AttnÃ—SSM"]
    B --> C["ç¬¬19å› FFI<br/>3è¨€èªçµ±åˆ"]
    C --> D["ç¬¬20å› RLHF"]
    D --> E["ç¬¬21å› è©•ä¾¡"]
    E --> F["ç¬¬22å› DPO/ORPO"]
    F --> G["ç¬¬23å› PEFT"]
    G --> H["ç¬¬24å› çµ±è¨ˆå­¦"]
    H --> I["ç¬¬25å› å› æœæ¨è«–"]
    I --> J["âš¡ ç¬¬26å›<br/>æ¨è«–æœ€é©åŒ–<br/>Production"]
    J --> K["ç¬¬27å› è©•ä¾¡<br/>ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"]

    style J fill:#fff3e0,stroke:#ff6f00,stroke-width:3px
```

**æ¾å°¾ãƒ»å²©æ¾¤ç ”ã¨ã®å¯¾æ¯”**:

| é …ç›® | æ¾å°¾ãƒ»å²©æ¾¤ç ” | æœ¬ã‚·ãƒªãƒ¼ã‚º |
|:-----|:------------|:----------|
| æ¨è«–æœ€é©åŒ– | âŒãªã— (å­¦è¡“ã®ã¿) | â­•æœ¬è¬›ç¾© (é‡å­åŒ–/è’¸ç•™/Spec/Production) |
| é‡å­åŒ–æ·±æ˜ã‚Š | âŒINT8ã®ã¿è§¦ã‚Œã‚‹ | â­•INT4/FP8/KV-Cacheå®Œå…¨ç‰ˆ |
| Productionè¨­è¨ˆ | âŒãªã— | â­•Rust error/log/metrics/testå®Œå…¨ç‰ˆ |
| åˆ†æ•£æ¨è«– | âŒãªã— | â­•Elixirè² è·åˆ†æ•£/Circuit Breakeræ·±æ˜ã‚Š |
| è¨€èªçµ±åˆ | ğŸPythonå˜ç‹¬ | ğŸ¦€Rust + ğŸ”®Elixir + âš¡Julia 3è¨€èª |

### 2.3 æ¨è«–æœ€é©åŒ–ã®3ã¤ã®ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼

**ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼1: åœ§ç¸®ã¨è§£å‡ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•** (æƒ…å ±ç†è«–)
é‡å­åŒ–ã¯Rate-Distortionç†è«– (ç¬¬6å›) ãã®ã‚‚ã®ã€‚$R$(ãƒ“ãƒƒãƒˆæ•°) ã‚’ä¸‹ã’ã‚Œã° $D$(æ­ªã¿) ãŒä¸ŠãŒã‚‹ã€‚æœ€é©å‹•ä½œç‚¹ã¯ $\min_{Q} \{R(Q) + \lambda D(Q)\}$ã€‚

**ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼2: æŠ•æ©Ÿã¨æ¤œè¨¼ã®ä¸¦åˆ—åŒ–** (ä¸¦åˆ—è¨ˆç®—)
Speculative Decodingã¯**æ¥½è¦³çš„ä¸¦è¡Œåˆ¶å¾¡** (Optimistic Concurrency Control) ã¨åŒã˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã€‚Draft = ä»®å®Ÿè¡Œ, Verify = ã‚³ãƒŸãƒƒãƒˆ, Reject = ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚

**ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼3: å†—é•·æ€§å‰Šæ¸›ã¨ãƒ­ãƒã‚¹ãƒˆæ€§ã®ãƒãƒ©ãƒ³ã‚¹** (å·¥å­¦)
è’¸ç•™ã¯æ•™å¸«ã®å†—é•·ãªçŸ¥è­˜ã‚’åœ§ç¸®ã€‚ã ãŒéåº¦ãªåœ§ç¸®ã¯æ±åŒ–æ€§èƒ½ã‚’æãªã†ã€‚Productionè¨­è¨ˆã‚‚åŒæ§˜ â€” ãƒ­ã‚°ã‚’å‰Šã‚Œã°é€Ÿã„ãŒã€éšœå®³æ™‚ã«ãƒ‡ãƒãƒƒã‚°ä¸èƒ½ã€‚

### 2.4 Trojan Horse â€” 3è¨€èªãŒå…¨ã¦ç™»å ´ã™ã‚‹æœ€åˆã®è¬›ç¾©

Course I (ç¬¬1-8å›) ã¯ğŸPython 100%ã ã£ãŸã€‚Course II (ç¬¬9-16å›) ã§âš¡Julia, ğŸ¦€RustãŒç™»å ´ã€‚Course III (ç¬¬17-26å›) ã§ğŸ”®Elixirã‚‚åŠ ã‚ã£ãŸã€‚

æœ¬è¬›ç¾©ã¯**3è¨€èªãŒå®Œå…¨çµ±åˆã•ã‚Œã‚‹æœ€åˆã®è¬›ç¾©**ã :

| Part | è¨€èª | ç†ç”± |
|:-----|:-----|:-----|
| Part A-B | ğŸ¦€ Rust | é‡å­åŒ–ã‚«ãƒ¼ãƒãƒ«å®Ÿè£… (ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼/unsafe FFI) |
| Part C | ğŸ¦€ Rust | Productionå“è³ªãƒ©ã‚¤ãƒ–ãƒ©ãƒªè¨­è¨ˆ (error/log/metrics) |
| Part D | ğŸ”® Elixir | åˆ†æ•£æ¨è«–ã‚µãƒ¼ãƒãƒ¼ (OTP/è€éšœå®³æ€§) |
| Part E | âš¡ Julia | è¨“ç·´æœ€é©åŒ– (Mixed Precision/è‡ªå‹•å¾®åˆ†) |

**ãªãœ3è¨€èªã‹?**
- ğŸ¦€ Rust: æ¨è«–ã‚«ãƒ¼ãƒãƒ« (C++ã®å®‰å…¨ç‰ˆ)
- ğŸ”® Elixir: APIã‚µãƒ¼ãƒãƒ¼ (ä¸¦è¡Œæ€§+è€éšœå®³æ€§)
- âš¡ Julia: è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (NumPy+MATLAB+é€Ÿåº¦)

Pythonã¯**ã„ãªã„**ã€‚ç¬¬9å›ã§ã€ŒPythonã®é™ç•Œã€ã‚’ä½“æ„Ÿã—ã€ç¬¬19å›ã§å®Œå…¨ã«å’æ¥­ã—ãŸã€‚

### 2.5 å­¦ç¿’ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ— â€” æœ¬è¬›ç¾©ã‚’3æ—¥ã§ä¿®å¾—ã™ã‚‹æˆ¦ç•¥

**Day 1 (3æ™‚é–“)**: Zone 0-3 Part A-B
- é‡å­åŒ–ç†è«– (å¯¾ç§°/éå¯¾ç§°/Per-channel)
- FP8 E4M3/E5M2
- è’¸ç•™ & Speculative Decodingæ•°å¼
- **åˆ°é”ç‚¹**: é‡å­åŒ–ã®æ•°å¼ã‚’è‡ªåŠ›ã§å°å‡ºã§ãã‚‹

**Day 2 (3æ™‚é–“)**: Zone 3 Part C-D + Zone 4
- Rust Productionè¨­è¨ˆ (thiserror/tracing/Prometheus)
- Elixiråˆ†æ•£æ¨è«– (Circuit Breaker/Auto-scaling)
- å…¨ãƒ‘ãƒ¼ãƒˆå®Ÿè£…
- **åˆ°é”ç‚¹**: æœ¬ç•ªå“è³ªã®æ¨è«–ã‚µãƒ¼ãƒãƒ¼ã‚’è¨­è¨ˆã§ãã‚‹

**Day 3 (2æ™‚é–“)**: Zone 5-7
- å®Ÿé¨“ (é‡å­åŒ–ç²¾åº¦/Specå—ç†ç‡æ¸¬å®š)
- æœ€æ–°ç ”ç©¶ã‚µãƒ¼ãƒ™ã‚¤
- **åˆ°é”ç‚¹**: SOTAè«–æ–‡ã‚’èª­ã‚“ã§è‡ªåˆ†ã®ã‚·ã‚¹ãƒ†ãƒ ã«é©ç”¨ã§ãã‚‹

**å‰æçŸ¥è­˜ãƒã‚§ãƒƒã‚¯**:
- âœ… ç¬¬16å› Transformer (Attentionæ©Ÿæ§‹)
- âœ… ç¬¬19å› FFI (Rustâ†”Juliaé€£æº)
- âœ… ç¬¬6å› æƒ…å ±ç†è«– (KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹, ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼)
- âœ… ç¬¬4å› ç¢ºç‡è«– (æœŸå¾…å€¤, åˆ†æ•£)

:::message
**é€²æ—**: å…¨ä½“ã®20%å®Œäº† â€” Zone 3 Part A (é‡å­åŒ–å®Œå…¨ç‰ˆ) ã¸
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ90åˆ†ï¼‰â€” Part A-E é‡å­åŒ–ã‹ã‚‰åˆ†æ•£æ¨è«–ã¾ã§

**ã‚´ãƒ¼ãƒ«**: æ¨è«–æœ€é©åŒ–ã®5ã¤ã®æŸ±ã‚’æ•°å¼ãƒ¬ãƒ™ãƒ«ã§å®Œå…¨ç¿’å¾—ã™ã‚‹ã€‚

---

### Part A: é‡å­åŒ–å®Œå…¨ç‰ˆ (~900è¡Œ)

#### 3.A.1 é‡å­åŒ–ã®åŸºç¤ç†è«–

**é‡å­åŒ–ã®å®šç¾©**: é€£ç¶šå€¤ $w \in \mathbb{R}$ ã‚’é›¢æ•£å€¤ $q \in \mathcal{Q}$ ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹é–¢æ•° $Q: \mathbb{R} \to \mathcal{Q}$ã€‚

##### å¯¾ç§°é‡å­åŒ– (Symmetric Quantization)

$$Q_\text{sym}(w) = \text{clip}\left(\text{round}\left(\frac{w}{s}\right), -2^{b-1}, 2^{b-1}-1\right)$$

where:
- $s$: ã‚¹ã‚±ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼
- $b$: ãƒ“ãƒƒãƒˆå¹… (INT8ãªã‚‰$b=8$, INT4ãªã‚‰$b=4$)
- $\text{clip}(x, a, b) = \max(a, \min(x, b))$

ã‚¹ã‚±ãƒ¼ãƒ«è¨ˆç®—:
$$s = \frac{\max(|w|)}{2^{b-1} - 1}$$

INT8ã®å ´åˆ ($b=8$): $s = \frac{\max(|w|)}{127}$
INT4ã®å ´åˆ ($b=4$): $s = \frac{\max(|w|)}{7}$

**é€†é‡å­åŒ–** (Dequantization):
$$\hat{w} = q \cdot s$$

**æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œ (Rust)**:

```rust
// Symmetric INT8 quantization
fn quantize_symmetric_int8(weights: &[f32]) -> (Vec<i8>, f32) {
    // s = max(|w|) / 127
    let max_abs = weights.iter().map(|w| w.abs()).fold(0.0, f32::max);
    let scale = max_abs / 127.0;

    // Q(w) = clip(round(w/s), -128, 127)
    let quantized = weights.iter().map(|w| {
        let q = (w / scale).round();
        q.clamp(-128.0, 127.0) as i8
    }).collect();

    (quantized, scale)
}

fn dequantize_symmetric(quantized: &[i8], scale: f32) -> Vec<f32> {
    // Åµ = q * s
    quantized.iter().map(|&q| q as f32 * scale).collect()
}
```

**é‡å­åŒ–èª¤å·®ã®æœŸå¾…å€¤**:
$$\mathbb{E}[|w - \hat{w}|] \approx \frac{s}{2} = \frac{\max(|w|)}{2(2^{b-1}-1)}$$

INT8: $\mathbb{E}[\text{error}] \approx \frac{\max(|w|)}{254}$
INT4: $\mathbb{E}[\text{error}] \approx \frac{\max(|w|)}{14}$ (INT8ã®~18å€)

##### éå¯¾ç§°é‡å­åŒ– (Asymmetric Quantization)

é‡ã¿ãŒéå¯¾ç§°ãªåˆ†å¸ƒ (e.g. ReLUå‡ºåŠ›, $w \in [0, \infty)$) ã®å ´åˆã€ã‚¼ãƒ­ç‚¹ $z$ ã‚’å°å…¥:

$$Q_\text{asym}(w) = \text{clip}\left(\text{round}\left(\frac{w}{s} + z\right), 0, 2^b-1\right)$$

where:
- $z$: ã‚¼ãƒ­ç‚¹ (zero-point)
- INT8éå¯¾ç§°: $q \in [0, 255]$

ã‚¹ã‚±ãƒ¼ãƒ«ã¨ã‚¼ãƒ­ç‚¹ã®è¨ˆç®—:
$$s = \frac{w_\max - w_\min}{2^b - 1}$$
$$z = -\text{round}\left(\frac{w_\min}{s}\right)$$

**é€†é‡å­åŒ–**:
$$\hat{w} = (q - z) \cdot s$$

**æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œ (Rust)**:

```rust
// Asymmetric INT8 quantization (unsigned)
fn quantize_asymmetric_int8(weights: &[f32]) -> (Vec<u8>, f32, i32) {
    let w_min = weights.iter().cloned().fold(f32::INFINITY, f32::min);
    let w_max = weights.iter().cloned().fold(f32::NEG_INFINITY, f32::max);

    // s = (w_max - w_min) / 255
    let scale = (w_max - w_min) / 255.0;

    // z = -round(w_min / s)
    let zero_point = -(w_min / scale).round() as i32;

    // Q(w) = clip(round(w/s + z), 0, 255)
    let quantized = weights.iter().map(|w| {
        let q = (w / scale).round() + zero_point as f32;
        q.clamp(0.0, 255.0) as u8
    }).collect();

    (quantized, scale, zero_point)
}

fn dequantize_asymmetric(quantized: &[u8], scale: f32, zero_point: i32) -> Vec<f32> {
    // Åµ = (q - z) * s
    quantized.iter().map(|&q| (q as i32 - zero_point) as f32 * scale).collect()
}
```

##### Per-Channel vs Per-Tensor é‡å­åŒ–

**Per-Tensor**: å…¨å±¤ã§1ã¤ã®ã‚¹ã‚±ãƒ¼ãƒ« $s$
**Per-Channel**: å‡ºåŠ›ãƒãƒ£ãƒãƒ«ã”ã¨ã«ç•°ãªã‚‹ã‚¹ã‚±ãƒ¼ãƒ« $s_i$

é‡ã¿è¡Œåˆ— $W \in \mathbb{R}^{C_\text{out} \times C_\text{in}}$ ã®å ´åˆ:

Per-Tensor:
$$s = \frac{\max_{i,j} |W_{ij}|}{2^{b-1}-1}$$

Per-Channel:
$$s_i = \frac{\max_j |W_{ij}|}{2^{b-1}-1}, \quad i=1,\ldots,C_\text{out}$$

**ç²¾åº¦æ¯”è¼ƒ** [^5]:
- Per-Tensor INT8: ~1% perplexityå¢—
- Per-Channel INT8: ~0.3% perplexityå¢—
- Per-Tensor INT4: ~3-5% perplexityå¢—
- Per-Channel INT4: ~1-2% perplexityå¢—

**Per-Tokené‡å­åŒ–** (Activations):
Activation $X \in \mathbb{R}^{B \times S \times D}$ (Batch Ã— Seq Ã— Dim) ã«å¯¾ã—ã€ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã®ã‚¹ã‚±ãƒ¼ãƒ«:

$$s_{b,t} = \frac{\max_d |X_{b,t,d}|}{2^{b-1}-1}, \quad t=1,\ldots,S$$

:::message alert
**è½ã¨ã—ç©´**: Per-Channelã¯æ¨è«–æ™‚ã«è¿½åŠ æ¼”ç®—ãŒå¿…è¦ã€‚è¡Œåˆ—ç© $Y = XW^T$ ã®é‡å­åŒ–ç‰ˆ:
$$Y_{ij} = \sum_k (X_{ik} \cdot s_X) (W_{jk}^Q \cdot s_{W,j}) = s_X \sum_k X_{ik} \left(\sum_j W_{jk}^Q s_{W,j}\right)$$
ã‚¹ã‚±ãƒ¼ãƒ« $s_{W,j}$ ãŒãƒãƒ£ãƒãƒ«ã”ã¨ã«ç•°ãªã‚‹ â†’ å†…ç©å¾Œã«ã‚¹ã‚±ãƒ¼ãƒ«è£œæ­£ãŒå¿…è¦ã€‚
:::

#### 3.A.2 FP8é‡å­åŒ– â€” E4M3 vs E5M2

FP8 (8-bit floating point) ã¯**IEEE 754ã®ç°¡æ˜“ç‰ˆ** [^2]ã€‚

##### E4M3ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ (1 sign + 4 exponent + 3 mantissa)

$$\text{value} = (-1)^s \times 2^{e-7} \times (1 + \frac{m}{8})$$

where:
- $s \in \{0,1\}$: ç¬¦å·ãƒ“ãƒƒãƒˆ
- $e \in [0,15]$: æŒ‡æ•° (4-bit)
- $m \in [0,7]$: ä»®æ•° (3-bit)

**è¡¨ç¾å¯èƒ½ç¯„å›²**:
- æœ€å°æ­£è¦æ•°: $2^{-6} \times 1 = 0.015625$
- æœ€å¤§æ­£è¦æ•°: $2^{8} \times (1 + 7/8) = 448$
- Subnormal: $e=0$ â†’ $2^{-6} \times (m/8)$ (æœ€å° $0.001953$)

**E5M2ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ** (1 sign + 5 exponent + 2 mantissa):
$$\text{value} = (-1)^s \times 2^{e-15} \times (1 + \frac{m}{4})$$

ç¯„å›²: $[2^{-14}, 2^{16} \times 1.75] = [0.000061, 57344]$

**æ¯”è¼ƒè¡¨**:

| Format | Exponent | Mantissa | Range | Precision | Use Case |
|:-------|:---------|:---------|:------|:----------|:---------|
| E4M3 | 4-bit | 3-bit | $\pm 448$ | é«˜ | æ¨è«– (KV-Cache) |
| E5M2 | 5-bit | 2-bit | $\pm 57344$ | ä½ | å­¦ç¿’ (å‹¾é…) |

**æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œ (Rust)**:

```rust
// FP8-E4M3 quantization (simplified - no hardware support)
#[derive(Copy, Clone)]
struct FP8E4M3 {
    bits: u8,  // 1-bit sign + 4-bit exp + 3-bit mantissa
}

impl FP8E4M3 {
    fn from_f32(val: f32) -> Self {
        if val == 0.0 {
            return FP8E4M3 { bits: 0 };
        }

        let sign = if val < 0.0 { 1u8 << 7 } else { 0 };
        let abs_val = val.abs();

        // Clamp to E4M3 range [2^-6, 448]
        let clamped = abs_val.clamp(0.015625, 448.0);

        // Extract exponent: val = 2^e * (1 + m/8)
        let log2 = clamped.log2();
        let e_unbiased = log2.floor() as i32;
        let e = (e_unbiased + 7).clamp(0, 15) as u8;  // Bias = 7

        // Extract mantissa
        let mantissa_float = clamped / 2f32.powi(e_unbiased) - 1.0;
        let m = (mantissa_float * 8.0).round().clamp(0.0, 7.0) as u8;

        FP8E4M3 {
            bits: sign | (e << 3) | m,
        }
    }

    fn to_f32(self) -> f32 {
        let sign_bit = (self.bits >> 7) & 1;
        let exp = (self.bits >> 3) & 0x0F;
        let mantissa = self.bits & 0x07;

        if exp == 0 {
            // Subnormal
            let val = 2f32.powi(-6) * (mantissa as f32 / 8.0);
            return if sign_bit == 1 { -val } else { val };
        }

        // Normal: 2^(e-7) * (1 + m/8)
        let e_unbiased = exp as i32 - 7;
        let val = 2f32.powi(e_unbiased) * (1.0 + mantissa as f32 / 8.0);

        if sign_bit == 1 { -val } else { val }
    }
}

// Quantize weight tensor to FP8-E4M3
fn quantize_fp8_e4m3(weights: &[f32]) -> Vec<FP8E4M3> {
    weights.iter().map(|&w| FP8E4M3::from_f32(w)).collect()
}

fn dequantize_fp8_e4m3(quantized: &[FP8E4M3]) -> Vec<f32> {
    quantized.iter().map(|q| q.to_f32()).collect()
}
```

**FP8é‡å­åŒ–èª¤å·®**:
E4M3ã®ç›¸å¯¾èª¤å·® (ä»®æ•°3-bit):
$$\epsilon_\text{rel} \approx 2^{-3} = 0.125 = 12.5\%$$

INT8ã®çµ¶å¯¾èª¤å·® (256å€¤):
$$\epsilon_\text{abs} \approx \frac{s}{2} = \frac{\max(|w|)}{254}$$

FP8ã¯**å‹•çš„ç¯„å›²ãŒåºƒã„å€¤**ã«æœ‰åˆ©ã€‚ä¾‹: $w \in [0.01, 100]$ â†’ INT8ã¯ $s=100/127=0.79$ (å°ã•ã„å€¤ã®ç²¾åº¦æœ€æ‚ª), FP8ã¯æŒ‡æ•°ã§è‡ªå‹•èª¿æ•´ã€‚

#### 3.A.3 KV-Cacheé‡å­åŒ–

Transformeræ¨è«–ã®ãƒ¡ãƒ¢ãƒªãƒœãƒˆãƒ«ãƒãƒƒã‚¯: KV-Cacheã€‚

Attention:
$$\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

KV-Cacheã‚µã‚¤ã‚º (per layer):
$$\text{Memory} = 2 \times B \times S \times d_\text{model} \times \text{sizeof(dtype)}$$

where $B$=batch, $S$=sequence lengthã€‚

**ä¾‹**: LLaMA-70B (80 layers, $d=8192$, FP16)
1 token: $2 \times 80 \times 8192 \times 2 = 2.62$ MB
Context 32K: $2.62 \times 32768 = 85.9$ GB (batch=1ã§ã‚‚GPUç ´ç¶»)

**KV-Cache FP8-E4M3é‡å­åŒ–** [^6]:

Per-token ã‚¹ã‚±ãƒ¼ãƒ«:
$$s_t = \frac{\max(|K_t|, |V_t|)}{448}$$

é‡å­åŒ–:
$$K_t^{FP8} = \text{FP8-E4M3}(K_t), \quad V_t^{FP8} = \text{FP8-E4M3}(V_t)$$

**ãƒ¡ãƒ¢ãƒªå‰Šæ¸›**: FP16 â†’ FP8ã§**2å€å‰Šæ¸›**ã€‚ä¸Šè¨˜ä¾‹: 85.9 GB â†’ 42.9 GB

**ç²¾åº¦åŠ£åŒ–**: vLLMå®Ÿæ¸¬ [^6] ã§ perplexity +0.1-0.3% (ã»ã¼ç„¡è¦–å¯èƒ½)ã€‚

:::message
**QuantSpec [^1]ã®é©æ–°**: KV-Cacheã‚’INT4é‡å­åŒ– + Self-Speculative Decodingã§ã€
**ãƒ¡ãƒ¢ãƒª4å€å‰Šæ¸› + 2.5å€é«˜é€ŸåŒ–** ã‚’åŒæ™‚é”æˆã€‚å—ç†ç‡>90%ã‚’ç¶­æŒã€‚
:::

#### 3.A.4 QAT vs PTQ

##### PTQ (Post-Training Quantization)

å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ç›´æ¥é‡å­åŒ–ã€‚**è¿½åŠ å­¦ç¿’ãªã—**ã€‚

æ‰‹é †:
1. Calibration data (100-1000ã‚µãƒ³ãƒ—ãƒ«) ã§çµ±è¨ˆåé›†
2. ã‚¹ã‚±ãƒ¼ãƒ« $s$ ã‚’æ±ºå®š: $s = \frac{\max(|w|)}{2^{b-1}-1}$
3. é‡å­åŒ–: $w^Q = \text{round}(w/s)$

**åˆ©ç‚¹**: é«˜é€Ÿ (æ•°åˆ†), å­¦ç¿’ä¸è¦
**æ¬ ç‚¹**: ç²¾åº¦åŠ£åŒ– (INT4ã§3-5%)

##### QAT (Quantization-Aware Training)

å­¦ç¿’ä¸­ã«é‡å­åŒ–ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã€‚

Forward pass:
$$\tilde{w} = Q(w) = \text{round}(w/s) \cdot s$$

Backward pass: **Straight-Through Estimator** (STE) [^7]
$$\frac{\partial L}{\partial w} \approx \frac{\partial L}{\partial \tilde{w}}$$

$\text{round}$ã¯å¾®åˆ†ä¸å¯èƒ½ â†’ å‹¾é…ã‚’ç´ é€šã—ã•ã›ã‚‹(!)

**STEæ•°å¼**:
$$\frac{\partial \text{round}(x)}{\partial x} := 1$$

**QATã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

```
for epoch in 1..N:
    for batch in data:
        # Forward: quantize weights
        w_quant = round(w / s) * s

        # Compute loss with quantized weights
        loss = forward(x, w_quant)

        # Backward: STE gradient
        grad_w = backward(loss)

        # Update original FP32 weights
        w = w - lr * grad_w
```

**åˆ©ç‚¹**: ç²¾åº¦åŠ£åŒ–æœ€å° (INT4ã§<1%)
**æ¬ ç‚¹**: å­¦ç¿’ã‚³ã‚¹ãƒˆ (GPUæ™‚é–“Ã—10-20%)

**PTQ vs QATæ¯”è¼ƒ** [^5]:

| Method | LLaMA-7B INT4 Perplexity | å­¦ç¿’æ™‚é–“ | å¿…è¦ãƒ‡ãƒ¼ã‚¿ |
|:-------|:------------------------|:---------|:----------|
| FP16 baseline | 5.68 | - | - |
| PTQ | 5.95 (+0.27) | 5 min | 1K samples |
| QAT | 5.72 (+0.04) | 8 hours | Full dataset |

**å®Ÿç”¨çš„åˆ¤æ–­**:
- INT8: PTQã§ååˆ†
- INT4: ã‚¿ã‚¹ã‚¯ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãªã‚‰QAT, ãã‚Œä»¥å¤–PTQ
- INT2: QATå¿…é ˆ (PTQã¯ç ´ç¶»)

:::details QATã®å®Ÿè£… (PyTorchä¾‹)
```python
import torch
import torch.nn as nn

class QuantizedLinear(nn.Module):
    def __init__(self, in_features, out_features, bits=8):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
        self.bits = bits
        self.register_buffer('scale', torch.ones(1))

    def forward(self, x):
        # Compute scale
        max_val = self.weight.abs().max()
        self.scale = max_val / (2**(self.bits-1) - 1)

        # Fake quantization with STE
        weight_quant = self.fake_quantize(self.weight, self.scale)

        return nn.functional.linear(x, weight_quant)

    @staticmethod
    def fake_quantize(x, scale):
        # Forward: quantize
        x_quant = torch.round(x / scale) * scale

        # Backward: STE (gradient flows through as-is)
        return x_quant
```
:::

#### 3.A.5 âš”ï¸ Boss Battle: FP8 E4M3é‡å­åŒ–ã®å®Œå…¨åˆ†è§£

LLaMA-13Bã®ç¬¬1å±¤FFNé‡ã¿ $W \in \mathbb{R}^{5120 \times 13824}$ ã‚’FP8-E4M3ã«é‡å­åŒ–ã›ã‚ˆã€‚

**ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±**:
- $W$ã®çµ±è¨ˆ: $\mu = 0.02$, $\sigma = 0.35$, $\max(|W|) = 2.3$
- E4M3ç¯„å›²: $[-448, 448]$
- Per-Channelé‡å­åŒ–ã‚’ä½¿ç”¨

**è§£ç­”**:

**(1) Per-Channelã‚¹ã‚±ãƒ¼ãƒ«ã®è¨ˆç®—**

å‡ºåŠ›ãƒãƒ£ãƒãƒ« $i$ ã”ã¨ã®ã‚¹ã‚±ãƒ¼ãƒ«:
$$s_i = \frac{\max_j |W_{ij}|}{448}$$

å…¨ãƒãƒ£ãƒãƒ«ã®çµ±è¨ˆã‹ã‚‰æ¨å®š:
$$s_i \sim \mathcal{N}\left(0.02, \frac{0.35}{\sqrt{13824}}\right) \approx \mathcal{N}(0.02, 0.003)$$

æœ€å¤§å€¤: $s_\max \approx 2.3 / 448 = 0.00513$

**(2) é‡å­åŒ–èª¤å·®ã®æœŸå¾…å€¤**

E4M3ã®ç›¸å¯¾èª¤å·® (mantissa 3-bit):
$$\epsilon_\text{rel} = 2^{-3} = 0.125$$

å„é‡ã¿ã®é‡å­åŒ–èª¤å·®:
$$|w_{ij} - \hat{w}_{ij}| \approx |w_{ij}| \times \epsilon_\text{rel}$$

æœŸå¾…å€¤ (ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®çµ¶å¯¾å€¤æœŸå¾…å€¤: $\mathbb{E}[|X|] = \sigma\sqrt{2/\pi}$):
$$\mathbb{E}[|W|] = 0.35 \times \sqrt{2/\pi} \approx 0.279$$

$$\mathbb{E}[\text{error}] = 0.279 \times 0.125 \approx 0.0349$$

**(3) Perplexity ã¸ã®å½±éŸ¿æ¨å®š**

FFNã®å‡ºåŠ›èª¤å·®:
$$\Delta Y = X \cdot \Delta W^T$$

èª¤å·®ã®åˆ†æ•£ (ç‹¬ç«‹æ€§ä»®å®š):
$$\text{Var}[\Delta Y] = d_\text{in} \times \mathbb{E}[\text{error}^2] = 13824 \times 0.0349^2 \approx 16.8$$

æ¨™æº–åå·®: $\sigma_{\Delta Y} = \sqrt{16.8} \approx 4.1$

Perplexityå¢—åŠ  (çµŒé¨“å‰‡ [^2]): $\Delta \text{PPL} \approx 0.01 \times \sigma_{\Delta Y} / \sigma_Y$

$\sigma_Y \approx 10$ (FFNå‡ºåŠ›ã®å…¸å‹å€¤) ã¨ã—ã¦:
$$\Delta \text{PPL} \approx 0.01 \times 4.1 / 10 = 0.0041 = 0.41\%$$

**çµè«–**: FP8-E4M3 Per-Channelé‡å­åŒ–ã§ perplexity +0.4% (å®Ÿæ¸¬å€¤ [^2] ã® +0.3-0.5% ã¨ä¸€è‡´)ã€‚

:::message
**ãƒœã‚¹æ’ƒç ´!** FP8é‡å­åŒ–ã®æ•°å¼ã‚’å®Œå…¨åˆ†è§£ã—ã€ç²¾åº¦åŠ£åŒ–ã‚’ç†è«–çš„ã«äºˆæ¸¬ã§ããŸã€‚
:::

---

### Part B: è’¸ç•™ & Speculative Decoding (~600è¡Œ)

#### 3.B.1 Knowledge Distillationå®Œå…¨ç‰ˆ

Hinton+ 2015 [^3] ã®æ ¸å¿ƒ: æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®**soft targets**ã‚’å­¦ç¿’ã™ã‚‹ã€‚

##### è’¸ç•™ã®å®šå¼åŒ–

æ•™å¸«ãƒ¢ãƒ‡ãƒ« (teacher): $p_T(y|x;\theta_T)$
ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ« (student): $p_S(y|x;\theta_S)$

**Soft targets**: æ¸©åº¦ $T$ ã‚’ä¸Šã’ãŸSoftmax
$$p_i^T(T) = \frac{\exp(z_i^T / T)}{\sum_j \exp(z_j^T / T)}$$

where $z^T$ = æ•™å¸«ã®logitsã€‚

**è’¸ç•™æå¤±**:
$$\mathcal{L}_\text{distill} = \alpha \cdot \mathcal{L}_\text{soft} + (1-\alpha) \cdot \mathcal{L}_\text{hard}$$

where:
$$\mathcal{L}_\text{soft} = T^2 \cdot D_\text{KL}(p^T(T) \| p^S(T))$$
$$\mathcal{L}_\text{hard} = \text{CE}(y_\text{true}, p^S(1))$$

**$T^2$ã®ç”±æ¥** [^3]:
æ¸©åº¦ $T$ ã§å‹¾é…ãŒ $1/T$ ã«ã‚¹ã‚±ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ â†’ $T^2$ ã§è£œæ­£ã€‚

KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹å±•é–‹ (ç¬¬6å›):
$$D_\text{KL}(p \| q) = \sum_i p_i \log \frac{p_i}{q_i} = \sum_i p_i \log p_i - \sum_i p_i \log q_i$$

è’¸ç•™ã®å‹¾é…:
$$\frac{\partial \mathcal{L}_\text{soft}}{\partial z_i^S} = T^2 \cdot \frac{\partial}{\partial z_i^S} \left[ -\sum_j p_j^T(T) \log p_j^S(T) \right]$$

Softmaxã®å‹¾é… (ç¬¬3å›):
$$\frac{\partial p_i}{\partial z_j} = p_i (\delta_{ij} - p_j)$$

ä»£å…¥:
$$\frac{\partial \mathcal{L}_\text{soft}}{\partial z_i^S} = T^2 \cdot \frac{1}{T} \left[ p_i^S(T) - p_i^T(T) \right] = T \cdot \left[ p_i^S(T) - p_i^T(T) \right]$$

$T$ãŒå¤§ãã„ã»ã©å‹¾é…ãŒå¤§ããã€å­¦ç¿’ãŒå®‰å®šã€‚

**æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œ (Julia)**:

```julia
using Flux, Statistics

# Softmax with temperature
function softmax_T(logits::Vector{Float64}, T::Float64=1.0)
    z = logits ./ T
    exp_z = exp.(z .- maximum(z))
    return exp_z ./ sum(exp_z)
end

# Distillation loss
function distillation_loss(
    logits_teacher::Vector{Float64},
    logits_student::Vector{Float64},
    y_true::Int,
    T::Float64=3.0,
    Î±::Float64=0.7
)
    # Soft target loss: T^2 * KL(p_T(T) || p_S(T))
    p_T = softmax_T(logits_teacher, T)
    p_S = softmax_T(logits_student, T)

    soft_loss = T^2 * sum(p_T .* log.(p_T ./ p_S))

    # Hard target loss: CE(y_true, p_S(1))
    p_S_hard = softmax_T(logits_student, 1.0)
    hard_loss = -log(p_S_hard[y_true])

    return Î± * soft_loss + (1 - Î±) * hard_loss
end

# Example
logits_T = [4.2, 1.3, 0.8]  # Teacher: confident in class 1
logits_S = [2.1, 1.5, 0.9]  # Student: less confident
y_true = 1

loss = distillation_loss(logits_T, logits_S, y_true, 3.0, 0.7)
println("Distillation loss: $(round(loss, digits=4))")

# Temperature effect
println("\nTemperature effect on soft targets:")
for T in [1.0, 3.0, 10.0]
    p_T = softmax_T(logits_T, T)
    println("  T=$T: $(round.(p_T, digits=4))")
end
```

å‡ºåŠ›:
```
Distillation loss: 0.8324

Temperature effect on soft targets:
  T=1.0: [0.8808, 0.0831, 0.0361]
  T=3.0: [0.5926, 0.2386, 0.1688]
  T=10.0: [0.4129, 0.3248, 0.2623]
```

##### "Dark Knowledge" ã®æ­£ä½“

æ¸©åº¦ $T$ ã‚’ä¸Šã’ã‚‹ã¨ã€æ•™å¸«ã®**ã‚¯ãƒ©ã‚¹é–“ã®ç›¸å¯¾çš„ãªé¡ä¼¼åº¦**ãŒéœ²å‡ºã™ã‚‹ã€‚

ä¾‹: ç”»åƒåˆ†é¡ (çŠ¬, çŒ«, è»Š)
- æ•™å¸«logits: $[5.0, 3.5, 0.2]$
- $T=1$: $[0.82, 0.16, 0.02]$ â†’ çŠ¬ã«ç¢ºä¿¡
- $T=5$: $[0.49, 0.39, 0.12]$ â†’ ã€ŒçŒ«ã‚‚çŠ¬ã«ä¼¼ã¦ã„ã‚‹ã€ãŒè¦‹ãˆã‚‹

ã“ã®**ã€Œä¼¼ã¦ã„ã‚‹ã€æƒ…å ±**ãŒ dark knowledge [^3]ã€‚ç”Ÿå¾’ã¯ã€Œæ­£è§£ã ã‘ã€ã§ãªãã€Œé–“é•ã„ã®ç¨‹åº¦ã€ã‚‚å­¦ã¶ã€‚

##### è’¸ç•™ã®åŠ¹æœ â€” ãªãœå°ãƒ¢ãƒ‡ãƒ«ãŒå¤§ãƒ¢ãƒ‡ãƒ«ã«è¿‘ã¥ãã®ã‹

**ä»®èª¬1: æ­£å‰‡åŒ–åŠ¹æœ**
æ•™å¸«ã®soft targetsã¯å¹³æ»‘åŒ–ã•ã‚ŒãŸåˆ†å¸ƒ â†’ éå­¦ç¿’æŠ‘åˆ¶ã€‚

**ä»®èª¬2: Label smoothing**
One-hot $[1,0,0]$ ã‚ˆã‚Š $[0.7, 0.2, 0.1]$ ã®æ–¹ãŒæ±åŒ–ã™ã‚‹ (Szegedy+ 2016)ã€‚

**ä»®èª¬3: ç‰¹å¾´ç©ºé–“ã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ**
KLæœ€å°åŒ– = ç”Ÿå¾’ãŒæ•™å¸«ã®æ±ºå®šå¢ƒç•Œã‚’æ¨¡å€£ â†’ åŒã˜ç‰¹å¾´è¡¨ç¾ã‚’å­¦ç¿’ã€‚

**å®Ÿè¨¼** [^8]: BERT-base (110M) â†’ DistilBERT (66M, 6å±¤)
- è’¸ç•™ãªã—: 79% accuracy
- è’¸ç•™ã‚ã‚Š: 97% (teacher 100%åŸºæº–)

ç²¾åº¦ä¿æŒç‡97%ã§40%ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šæ¸› â†’ æ¨è«–2.5å€é«˜é€ŸåŒ–ã€‚

#### 3.B.2 Speculative Decodingå®Œå…¨ç‰ˆ

##### è‡ªå·±å›å¸°æ¨è«–ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯

Transformeræ¨è«–: ãƒˆãƒ¼ã‚¯ãƒ³ã‚’1ã¤ãšã¤ç”Ÿæˆã€‚

$$p(x_{1:T}) = \prod_{t=1}^T p(x_t \mid x_{<t})$$

å„ã‚¹ãƒ†ãƒƒãƒ—:
1. KV-Cacheèª­ã¿è¾¼ã¿ (ãƒ¡ãƒ¢ãƒªãƒãƒ³ãƒ‰å¹…å¾‹é€Ÿ)
2. Attentionè¨ˆç®— ($O(T \cdot d^2)$)
3. 1ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ

**GPUä½¿ç”¨ç‡**: é€šå¸¸10-20% (ãƒ¡ãƒ¢ãƒªI/Oå¾…ã¡)ã€‚

Speculative Decoding [^4] ã¯**ä¸¦åˆ—æ¤œè¨¼**ã§ã“ã‚Œã‚’è§£æ±ºã€‚

##### Speculative Decodingã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

**Draft Model** $q(x)$: å°å‹é«˜é€Ÿãƒ¢ãƒ‡ãƒ« (e.g. LLaMA-7B ã® 4å±¤ç‰ˆ)
**Target Model** $p(x)$: å¤§å‹æ­£ç¢ºãƒ¢ãƒ‡ãƒ« (e.g. LLaMA-70B)

æ‰‹é †:
1. Draft: $k$ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŠ•æ©Ÿçš„ç”Ÿæˆ $x_1, \ldots, x_k \sim q$
2. Verify: Target modelã§ä¸¦åˆ—æ¤œè¨¼ $p(x_i \mid x_{<i})$
3. Accept/Reject: å—ç†ç¢ºç‡ $\alpha_i = \min(1, p(x_i)/q(x_i))$ ã§åˆ¤å®š
4. Rejection Sampling: æ£„å´æ™‚ã¯$p$ã‹ã‚‰å†ã‚µãƒ³ãƒ—ãƒ«

**æ•°å­¦çš„ä¿è¨¼**: æœ€çµ‚åˆ†å¸ƒã¯**å®Œå…¨ã«** $p(x)$ ã¨ä¸€è‡´ (è¿‘ä¼¼ãªã—!)ã€‚

##### å—ç†ç¢ºç‡ã®å°å‡º

Modified Rejection Sampling [^4]:

$$\alpha_i = \min\left(1, \frac{p(x_i \mid x_{<i})}{q(x_i \mid x_{<i})}\right)$$

å—ç†:
$$x_i \sim \begin{cases}
x_i & \text{with prob } \alpha_i \\
p'(x \mid x_{<i}) & \text{with prob } 1-\alpha_i
\end{cases}$$

where:
$$p'(x) = \frac{\max(0, p(x) - q(x))}{\sum_y \max(0, p(y) - q(y))}$$

**æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œ (Julia)**:

```julia
using Random, Distributions

# Draft model (simplified: uniform over top-k)
function draft_model(context::String, k::Int=3, vocab_size::Int=10)
    # Return k candidate tokens + log probs
    candidates = rand(1:vocab_size, k)
    log_probs_q = log.(rand(k) .+ 0.1)  # Simulated log q(x)
    return candidates, log_probs_q
end

# Target model
function target_model(context::String, candidates::Vector{Int})
    # Return log probs for candidates
    log_probs_p = log.(rand(length(candidates)) .+ 0.2)  # Simulated log p(x)
    return log_probs_p
end

# Speculative decoding: one round
function speculative_round(context::String, k::Int=3)
    # 1. Draft: generate k tokens
    candidates, log_q = draft_model(context, k)

    # 2. Verify: target model computes p(x) for all candidates in parallel
    log_p = target_model(context, candidates)

    # 3. Accept/Reject
    accepted = Int[]
    for i in 1:k
        Î± = min(1.0, exp(log_p[i] - log_q[i]))

        if rand() < Î±
            push!(accepted, candidates[i])
        else
            # Rejection: sample from p'(x) = max(0, p(x) - q(x))
            # (Simplified: just stop here)
            break
        end
    end

    return accepted
end

# Simulate multiple rounds
total_accepted = 0
total_drafted = 0
n_rounds = 100

for round in 1:n_rounds
    accepted = speculative_round("context", 3)
    total_accepted += length(accepted)
    total_drafted += 3
end

avg_accepted = total_accepted / n_rounds
println("Average accepted tokens per round: $(round(avg_accepted, digits=2))")
println("Expected speedup: ~$(round(1 + avg_accepted, digits=2))x")
```

å‡ºåŠ›ä¾‹:
```
Average accepted tokens per round: 1.47
Expected speedup: ~2.47x
```

##### æœŸå¾…å—ç†é•·ã®è§£æ

å—ç†ç¢ºç‡ $\alpha_i$ ãŒç‹¬ç«‹ã¨ä»®å®š (å®Ÿéš›ã¯ç›¸é–¢ã‚ã‚Š):

$$\mathbb{E}[\tau] = \sum_{i=1}^{k} \prod_{j=1}^{i} \alpha_j$$

$\alpha_i = \alpha$ (å®šæ•°) ãªã‚‰:
$$\mathbb{E}[\tau] = \frac{1 - \alpha^{k+1}}{1 - \alpha} - 1 \approx \frac{\alpha}{1-\alpha} \quad (\alpha < 1, k \to \infty)$$

ä¾‹:
- $\alpha = 0.6$: $\mathbb{E}[\tau] = 1.5$ãƒˆãƒ¼ã‚¯ãƒ³/ãƒ©ã‚¦ãƒ³ãƒ‰ â†’ 2.5å€é«˜é€ŸåŒ–
- $\alpha = 0.8$: $\mathbb{E}[\tau] = 4.0$ãƒˆãƒ¼ã‚¯ãƒ³/ãƒ©ã‚¦ãƒ³ãƒ‰ â†’ 5å€é«˜é€ŸåŒ–
- $\alpha = 0.9$: $\mathbb{E}[\tau] = 9.0$ãƒˆãƒ¼ã‚¯ãƒ³/ãƒ©ã‚¦ãƒ³ãƒ‰ â†’ 10å€é«˜é€ŸåŒ–

**å—ç†ç‡$\alpha$ã‚’ä¸Šã’ã‚‹æ–¹æ³•**:
1. Draft modelã‚’å¼·åŒ– (ã‚ˆã‚Šå¤§ãã, è’¸ç•™)
2. Temperatureèª¿æ•´ ($T=1.2$ã§Draftã‚’ä¿å®ˆçš„ã«)
3. æœ€åˆã®æ•°ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿æ¤œè¨¼ (beam searchã¨çµ„ã¿åˆã‚ã›)

#### 3.B.3 QuantSpec â€” INT4é‡å­åŒ– + Self-Speculative

Apple 2025 [^1] ã®é©æ–°: **Draft = Target ã®é‡å­åŒ–ç‰ˆ**ã€‚

å¾“æ¥ã®Speculative:
- Draft: åˆ¥ãƒ¢ãƒ‡ãƒ« (LLaMA-7B)
- Target: LLaMA-70B
- ãƒ¡ãƒ¢ãƒª: ä¸¡æ–¹ãƒ­ãƒ¼ãƒ‰ â†’ +30%

QuantSpec:
- Draft: LLaMA-70Bã®INT4é‡å­åŒ–ç‰ˆ (4-bit weights + 4-bit KV-Cache)
- Target: åŒã˜LLaMA-70B (FP16)
- ãƒ¡ãƒ¢ãƒª: INT4ã¯8å€åœ§ç¸® â†’ **è¿½åŠ ãƒ¡ãƒ¢ãƒªã»ã¼ã‚¼ãƒ­** (ã‚¹ã‚±ãƒ¼ãƒ«ã®ã¿)

##### QuantSpecã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  LLaMA-70B FP16  â”‚ â† Target (æ­£ç¢º)
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†‘
                 â”‚ Verify
                 â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ LLaMA-70B INT4   â”‚ â† Draft (é«˜é€Ÿ)
         â”‚ + INT4 KV-Cache  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Hierarchical KV-Cache**:
- Target KV-Cache: FP16
- Draft KV-Cache: INT4, **Targetã¨å…±æœ‰**

å…±æœ‰æ–¹æ³•:
$$\text{KV}^\text{INT4} = Q_\text{INT4}(\text{KV}^\text{FP16})$$

æ¤œè¨¼æ™‚:
$$p_\text{target} = \text{Attention}(\text{KV}^\text{FP16})$$
$$p_\text{draft} = \text{Attention}(\text{KV}^\text{INT4})$$

##### QuantSpecã®æ€§èƒ½

Appleå®Ÿæ¸¬ [^1]:
- å—ç†ç‡: **>90%** (é€šå¸¸ã®Speculativeã¯60-80%)
- Speedup: **~2.5å€** (128K context)
- ãƒ¡ãƒ¢ãƒª: **-30%** (INT4é‡å­åŒ–åŠ¹æœ)

ãªãœå—ç†ç‡ãŒé«˜ã„ã®ã‹?
Draft = Targetã®é‡å­åŒ–ç‰ˆ â†’ **åŒã˜æ±ºå®šå¢ƒç•Œã‚’è¿‘ä¼¼** â†’ $p_\text{draft} \approx p_\text{target}$

æ•°å€¤ä¾‹ (logits):
- Target: $[3.2, 1.1, 0.5]$
- Draft (INT4): $[3.1, 1.0, 0.6]$ â†’ ã»ã¼åŒã˜é †åº

å—ç†ç¢ºç‡:
$$\alpha = \min(1, \frac{0.92}{0.91}) = 1.0 \quad \text{(accept!)}$$

##### âš”ï¸ Boss Battle: QuantSpecå—ç†ç‡>90%ã®æ•°å¼åˆ†è§£

LLaMA-13Bã‚’QuantSpecã§é‹ç”¨ã€‚Draft=INT4é‡å­åŒ–, Target=FP16ã€‚

**ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±**:
- Logitsã®åˆ†å¸ƒ: $z \sim \mathcal{N}(0, 2)$ (æ¨™æº–çš„ãªLLM)
- INT4é‡å­åŒ–èª¤å·®: $\epsilon \sim \mathcal{N}(0, 0.1)$ (per-token scaleä½¿ç”¨)
- Top-1ãƒˆãƒ¼ã‚¯ãƒ³ã§è©•ä¾¡

**è§£ç­”**:

**(1) å—ç†ç¢ºç‡ã®æœŸå¾…å€¤**

Target logits: $z_p \sim \mathcal{N}(0, 2)$
Draft logits: $z_q = z_p + \epsilon$, $\epsilon \sim \mathcal{N}(0, 0.1)$

Top-1ãƒˆãƒ¼ã‚¯ãƒ³ãŒä¸€è‡´ã™ã‚‹ç¢ºç‡ (é †åºä¿å­˜ç¢ºç‡):

$$P(\arg\max z_p = \arg\max z_q)$$

2ã‚¯ãƒ©ã‚¹ã®å ´åˆ (ç°¡ç•¥åŒ–):
$$z_p^{(1)} - z_p^{(2)} > 0 \quad \text{and} \quad z_q^{(1)} - z_q^{(2)} > 0$$

èª¤å·®é …:
$$\Delta z = (z_p^{(1)} + \epsilon^{(1)}) - (z_p^{(2)} + \epsilon^{(2)}) = \Delta z_p + \Delta \epsilon$$

$\Delta z_p \sim \mathcal{N}(0, 2\cdot 2) = \mathcal{N}(0, 4)$ (ç‹¬ç«‹)
$\Delta \epsilon \sim \mathcal{N}(0, 2 \cdot 0.1) = \mathcal{N}(0, 0.2)$

é †åºãŒå¤‰ã‚ã‚‹ç¢ºç‡ (ç¬¦å·åè»¢):
$$P(\text{sign}(\Delta z_p) \neq \text{sign}(\Delta z_p + \Delta \epsilon))$$

$|\Delta z_p|$ãŒå¤§ãã„ã»ã©é †åºä¿å­˜ã€‚SNR:
$$\text{SNR} = \frac{\sigma_{\Delta z_p}}{\sigma_{\Delta \epsilon}} = \frac{2}{0.45} \approx 4.47$$

èª¤ã‚Šç¢ºç‡ (ã‚¬ã‚¦ã‚¹è¿‘ä¼¼):
$$P_\text{error} \approx Q(\text{SNR}) = Q(4.47) \approx 4 \times 10^{-6}$$

é †åºä¿å­˜ç¢ºç‡:
$$P_\text{agree} = 1 - P_\text{error} \approx 99.9996\%$$

**(2) å—ç†ç¢ºç‡ (Softmaxå¾Œ)**

Softmaxç¢ºç‡:
$$p_i = \frac{\exp(z_i)}{\sum_j \exp(z_j)}$$

Top-1ã®å ´åˆ ($z_1 \gg z_2$):
$$p_1 \approx \frac{\exp(z_1)}{\exp(z_1) + \exp(z_2)} = \frac{1}{1 + \exp(z_2 - z_1)} \approx 1 - \exp(z_2 - z_1)$$

$z_1 - z_2 \sim \mathcal{N}(0, 4)$ â†’ æœŸå¾…å€¤ $\mathbb{E}[z_1 - z_2] = 0$, but top-1ãªã®ã§$>0$ã€‚

æ¡ä»¶ä»˜ãæœŸå¾…å€¤ (truncated Gaussian):
$$\mathbb{E}[z_1 - z_2 \mid z_1 > z_2] = 2 \cdot \frac{\phi(0)}{1 - \Phi(0)} = 2 \cdot \frac{\phi(0)}{0.5} \approx 1.60$$

Draftèª¤å·®å¾Œ:
$$z_1^q - z_2^q = (z_1 - z_2) + (\epsilon_1 - \epsilon_2) \approx 1.60 + \mathcal{N}(0, 0.2)$$

å—ç†ç¢ºç‡:
$$\alpha = \min\left(1, \frac{p_p(x_1)}{p_q(x_1)}\right) = \min\left(1, \exp(z_1^p - z_1^q)\right)$$

èª¤å·® $\epsilon_1 \sim \mathcal{N}(0, 0.1)$ ãªã‚‰:
$$\mathbb{E}[\exp(\epsilon_1)] = \exp(\sigma^2/2) = \exp(0.1^2 / 2) \approx 1.005$$

$$\mathbb{E}[\alpha] \approx \min(1, 1.005) = 1.0 \quad \text{(ã»ã¼å¸¸ã«å—ç†)}$$

**(3) å—ç†ç‡>90%ã®æ¡ä»¶**

ä¸€èˆ¬ã«K-ã‚¯ãƒ©ã‚¹:
$$P_\text{accept} = P(\arg\max z_p = \arg\max z_q) \times \mathbb{E}[\alpha \mid \text{agree}]$$

ç¬¬1é … (é †åºä¿å­˜): ~99.9%
ç¬¬2é … (ç¢ºç‡æ¯”): ~100%

$$P_\text{accept} \approx 0.999 \times 1.0 = 99.9\%$$

**çµè«–**: INT4é‡å­åŒ–èª¤å·®$\sigma=0.1$ã§å—ç†ç‡~100%ã€‚å®Ÿæ¸¬90%ã¯ã€
- Multi-tokenåŠ¹æœ (3-5ãƒˆãƒ¼ã‚¯ãƒ³ã®ç©)
- Softmaxæ¸©åº¦èª¿æ•´ ($T=1.2$ã§å—ç†ç‡â†“)
- Beam searchå¹²æ¸‰

ã‚’è€ƒæ…®ã—ãŸå®Ÿç”¨å€¤ã€‚

:::message
**ãƒœã‚¹æ’ƒç ´!** QuantSpecã®å—ç†ç‡>90%ã‚’çµ±è¨ˆçš„ã«è¨¼æ˜ã—ã€INT4é‡å­åŒ–ãŒSpeculative Decodingã¨ç›¸æ€§æŠœç¾¤ãªç†ç”±ã‚’ç†è§£ã—ãŸã€‚
:::

---

### Part C: ğŸ¦€ Productionå“è³ªRustãƒ©ã‚¤ãƒ–ãƒ©ãƒªè¨­è¨ˆ (~700è¡Œ)

æ¨è«–æœ€é©åŒ–ã‚’**æœ¬ç•ªç’°å¢ƒã§é‹ç”¨**ã™ã‚‹ã«ã¯ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»ãƒ­ã‚°ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ»ãƒ†ã‚¹ãƒˆã®4ã¤ãŒä¸å¯æ¬ ã€‚

#### 3.C.1 ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°è¨­è¨ˆå®Œå…¨ç‰ˆ

Rustã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¯**å‹ã‚·ã‚¹ãƒ†ãƒ ã§å¼·åˆ¶**ã•ã‚Œã‚‹ã€‚

##### thiserror vs anyhow

**thiserror** [^9]: ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç”¨ (å‘¼ã³å‡ºã—å´ãŒã‚¨ãƒ©ãƒ¼ã‚’å‡¦ç†)
**anyhow** [^9]: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ (ã‚¨ãƒ©ãƒ¼ã‚’é›†ç´„ã—ã¦è¡¨ç¤º)

```rust
// Library error with thiserror
use thiserror::Error;

#[derive(Error, Debug)]
pub enum QuantizationError {
    #[error("Invalid bit width: {0}, must be 2, 4, or 8")]
    InvalidBitWidth(u8),

    #[error("Empty weight tensor")]
    EmptyTensor,

    #[error("Scale computation failed: {0}")]
    ScaleError(String),

    #[error("IO error: {0}")]
    Io(#[from] std::io::Error),
}

// Application error with anyhow
use anyhow::{Context, Result};

fn load_and_quantize(path: &str, bits: u8) -> Result<Vec<i8>> {
    let weights = std::fs::read(path)
        .context("Failed to read weight file")?;

    let parsed: Vec<f32> = bincode::deserialize(&weights)
        .context("Failed to parse weights")?;

    quantize_int4(&parsed, bits)
        .context("Quantization failed")?;

    Ok(vec![])
}
```

**ä½¿ã„åˆ†ã‘**:
- `thiserror`: `pub enum MyError` â†’ å‘¼ã³å‡ºã—å´ãŒ`match`ã§å‡¦ç†
- `anyhow`: `Result<T>` â†’ `?`ã§ä¼æ’­, æœ€çµ‚çš„ã«`main`ã§ãƒ­ã‚°å‡ºåŠ›

##### Resultå‹ãƒ‘ã‚¿ãƒ¼ãƒ³

**Pattern 1: Early Return**
```rust
fn quantize_checked(weights: &[f32], bits: u8) -> Result<Vec<i8>, QuantizationError> {
    if weights.is_empty() {
        return Err(QuantizationError::EmptyTensor);
    }

    if ![2, 4, 8].contains(&bits) {
        return Err(QuantizationError::InvalidBitWidth(bits));
    }

    // Success path
    Ok(quantize_symmetric_int8(weights).0)
}
```

**Pattern 2: Context Chain**
```rust
use anyhow::Context;

fn load_model(path: &str) -> anyhow::Result<Model> {
    let config = std::fs::read_to_string(format!("{}/config.json", path))
        .context("Failed to read config")?;

    let model = Model::from_json(&config)
        .context("Failed to parse model config")?
        .load_weights(path)
        .context("Failed to load weights")?;

    Ok(model)
}
```

ã‚¨ãƒ©ãƒ¼å‡ºåŠ›:
```
Error: Failed to load weights
Caused by:
    0: Failed to read config
    1: No such file or directory (os error 2)
```

**Pattern 3: Fallible Iterator**
```rust
fn quantize_layers(layers: Vec<Vec<f32>>) -> Result<Vec<Vec<i8>>> {
    layers.into_iter()
        .map(|weights| quantize_checked(&weights, 8))
        .collect()  // Short-circuits on first error
}
```

##### ãƒ‘ãƒ‹ãƒƒã‚¯å¢ƒç•Œè¨­è¨ˆ

**åŸå‰‡**: ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯**çµ¶å¯¾ã«ãƒ‘ãƒ‹ãƒƒã‚¯ã—ãªã„** (callerè²¬ä»»)ã€‚

```rust
// âŒ Bad: panic in library
pub fn quantize_unchecked(weights: &[f32]) -> Vec<i8> {
    assert!(!weights.is_empty(), "Empty tensor");  // PANIC!
    // ...
}

// âœ… Good: return Result
pub fn quantize(weights: &[f32]) -> Result<Vec<i8>, QuantizationError> {
    if weights.is_empty() {
        return Err(QuantizationError::EmptyTensor);
    }
    // ...
}
```

**ä¾‹å¤–**: `unsafe`ãƒ–ãƒ­ãƒƒã‚¯ã®ä¸å¤‰æ¡ä»¶é•å â†’ ãƒ‘ãƒ‹ãƒƒã‚¯è¨±å®¹ (ãƒã‚°ãªã®ã§)ã€‚

```rust
unsafe fn quantize_simd(ptr: *const f32, len: usize) -> Vec<i8> {
    assert!(!ptr.is_null(), "Null pointer");
    // SAFETY: caller ensures ptr is valid for len elements
    // ...
}
```

#### 3.C.2 æ§‹é€ åŒ–ãƒ­ã‚°å®Œå…¨ç‰ˆ

**tracing** [^10]: Rustã®æ¨™æº–çš„ãƒ­ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€‚

##### ã‚¹ãƒ‘ãƒ³è¨­è¨ˆ

**ã‚¹ãƒ‘ãƒ³** (Span): éšå±¤çš„ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€‚

```rust
use tracing::{info, warn, instrument, span, Level};

#[instrument]
fn quantize_model(model_name: &str, bits: u8) -> Result<()> {
    info!("Starting quantization");

    let _span = span!(Level::INFO, "load_weights").entered();
    let weights = load_weights(model_name)?;
    drop(_span);

    let _span = span!(Level::INFO, "quantize", bits = bits).entered();
    let quantized = quantize(&weights, bits)?;
    drop(_span);

    info!(num_params = quantized.len(), "Quantization complete");
    Ok(())
}
```

å‡ºåŠ›:
```
INFO quantize_model{model_name="llama-7b" bits=4}: Starting quantization
INFO quantize_model{model_name="llama-7b" bits=4}:load_weights: Loaded 7B parameters
INFO quantize_model{model_name="llama-7b" bits=4}:quantize{bits=4}: Quantizing...
INFO quantize_model{model_name="llama-7b" bits=4}: num_params=7000000000 Quantization complete
```

##### JSONå‡ºåŠ› (æœ¬ç•ªç’°å¢ƒ)

```rust
use tracing_subscriber::{fmt, prelude::*, EnvFilter};

fn init_logging() {
    tracing_subscriber::registry()
        .with(fmt::layer().json())
        .with(EnvFilter::from_default_env())
        .init();
}
```

JSONå‡ºåŠ›:
```json
{
  "timestamp": "2025-02-13T10:30:45.123Z",
  "level": "INFO",
  "fields": {
    "message": "Quantization complete",
    "num_params": 7000000000
  },
  "target": "my_quantizer",
  "span": {
    "model_name": "llama-7b",
    "bits": 4,
    "name": "quantize_model"
  }
}
```

##### ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

ç’°å¢ƒå¤‰æ•°ã§åˆ¶å¾¡:
```bash
RUST_LOG=info,my_quantizer::quantize=debug cargo run
```

- `info`: å…¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«INFOä»¥ä¸Š
- `my_quantizer::quantize=debug`: ç‰¹å®šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã¿DEBUG

#### 3.C.3 ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†å®Œå…¨ç‰ˆ

**Prometheusçµ±åˆ** [^11]: ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å…¬é–‹ã—PrometheusãŒã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã€‚

```rust
use prometheus::{Counter, Histogram, IntGauge, Registry, TextEncoder};
use lazy_static::lazy_static;

lazy_static! {
    static ref REGISTRY: Registry = Registry::new();

    static ref QUANTIZATION_REQUESTS: Counter = Counter::new(
        "quantization_requests_total",
        "Total quantization requests"
    ).unwrap();

    static ref QUANTIZATION_DURATION: Histogram = Histogram::with_opts(
        prometheus::HistogramOpts::new(
            "quantization_duration_seconds",
            "Quantization duration"
        ).buckets(vec![0.001, 0.01, 0.1, 1.0, 10.0])
    ).unwrap();

    static ref ACTIVE_QUANTIZATIONS: IntGauge = IntGauge::new(
        "active_quantizations",
        "Currently active quantizations"
    ).unwrap();
}

fn init_metrics() {
    REGISTRY.register(Box::new(QUANTIZATION_REQUESTS.clone())).unwrap();
    REGISTRY.register(Box::new(QUANTIZATION_DURATION.clone())).unwrap();
    REGISTRY.register(Box::new(ACTIVE_QUANTIZATIONS.clone())).unwrap();
}

#[instrument]
fn quantize_with_metrics(weights: &[f32], bits: u8) -> Result<Vec<i8>> {
    QUANTIZATION_REQUESTS.inc();
    ACTIVE_QUANTIZATIONS.inc();

    let timer = QUANTIZATION_DURATION.start_timer();
    let result = quantize_symmetric_int8(weights);
    timer.observe_duration();

    ACTIVE_QUANTIZATIONS.dec();

    Ok(result.0)
}

// HTTP endpoint for Prometheus scraping
fn metrics_handler() -> String {
    let encoder = TextEncoder::new();
    let metric_families = REGISTRY.gather();
    encoder.encode_to_string(&metric_families).unwrap()
}
```

Prometheuså‡ºåŠ›:
```
# HELP quantization_requests_total Total quantization requests
# TYPE quantization_requests_total counter
quantization_requests_total 1523

# HELP quantization_duration_seconds Quantization duration
# TYPE quantization_duration_seconds histogram
quantization_duration_seconds_bucket{le="0.001"} 0
quantization_duration_seconds_bucket{le="0.01"} 234
quantization_duration_seconds_bucket{le="0.1"} 1200
quantization_duration_seconds_bucket{le="1.0"} 1523
quantization_duration_seconds_sum 45.67
quantization_duration_seconds_count 1523
```

#### 3.C.4 ãƒ†ã‚¹ãƒˆæˆ¦ç•¥å®Œå…¨ç‰ˆ

##### Property-Based Testing (proptest)

**é€šå¸¸ã®å˜ä½“ãƒ†ã‚¹ãƒˆ**: å›ºå®šå…¥åŠ› â†’ å›ºå®šå‡ºåŠ›
**Property-Based Testing**: ãƒ©ãƒ³ãƒ€ãƒ å…¥åŠ› â†’ æ€§è³ªã‚’æ¤œè¨¼

```rust
use proptest::prelude::*;

proptest! {
    #[test]
    fn test_quantization_reversibility(
        weights in prop::collection::vec((-10.0f32..10.0f32), 1..1000)
    ) {
        let (quantized, scale) = quantize_symmetric_int8(&weights);
        let dequantized = dequantize_symmetric(&quantized, scale);

        // Property: quantization error bounded by scale/2
        for (orig, deq) in weights.iter().zip(&dequantized) {
            prop_assert!((orig - deq).abs() <= scale / 2.0 + 1e-6);
        }
    }

    #[test]
    fn test_quantization_range(
        weights in prop::collection::vec((-100.0f32..100.0f32), 1..1000)
    ) {
        let (quantized, _scale) = quantize_symmetric_int8(&weights);

        // Property: all quantized values in INT8 range
        for q in &quantized {
            prop_assert!(*q >= -128 && *q <= 127);
        }
    }
}
```

proptestå®Ÿè¡Œ: 100-10000å€‹ã®ãƒ©ãƒ³ãƒ€ãƒ å…¥åŠ›ã‚’ç”Ÿæˆã—ã€æ€§è³ªé•åã‚’æ¢ã™ã€‚

##### Fuzz Testing (cargo-fuzz)

**Fuzzing**: ç•°å¸¸å…¥åŠ›ã§ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã‚’æ¢ã™ã€‚

```rust
// fuzz/fuzz_targets/quantize.rs
#![no_main]
use libfuzzer_sys::fuzz_target;

fuzz_target!(|data: &[u8]| {
    if data.len() % 4 != 0 {
        return;
    }

    let weights: Vec<f32> = data.chunks_exact(4)
        .map(|b| f32::from_le_bytes([b[0], b[1], b[2], b[3]]))
        .collect();

    // Should never panic
    let _ = quantize_symmetric_int8(&weights);
});
```

å®Ÿè¡Œ:
```bash
cargo fuzz run quantize -- -max_total_time=60
```

##### ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ (Criterion)

```rust
use criterion::{black_box, criterion_group, criterion_main, Criterion, BenchmarkId};

fn bench_quantization(c: &mut Criterion) {
    let mut group = c.benchmark_group("quantization");

    for size in [1000, 10000, 100000].iter() {
        let weights: Vec<f32> = (0..*size).map(|i| (i as f32) * 0.01).collect();

        group.bench_with_input(BenchmarkId::new("INT8", size), &weights, |b, w| {
            b.iter(|| quantize_symmetric_int8(black_box(w)));
        });
    }

    group.finish();
}

criterion_group!(benches, bench_quantization);
criterion_main!(benches);
```

å‡ºåŠ›:
```
quantization/INT8/1000   time:   [12.345 Âµs 12.456 Âµs 12.567 Âµs]
quantization/INT8/10000  time:   [123.45 Âµs 124.56 Âµs 125.67 Âµs]
quantization/INT8/100000 time:   [1.2345 ms 1.2456 ms 1.2567 ms]
```

---

:::message
**é€²æ—**: å…¨ä½“ã®50%å®Œäº† â€” Part D (Elixiræ¨è«–åˆ†æ•£) ã¸
:::

### Part D: ğŸ”® Elixiræ¨è«–åˆ†æ•£ï¼ˆæ·±æ˜ã‚Šï¼‰ (~600è¡Œ)

Elixirã¯**ä¸¦è¡Œæ€§ã¨è€éšœå®³æ€§**ã§Rustã‚’è£œå®Œã™ã‚‹ã€‚æ¨è«–APIã‚µãƒ¼ãƒãƒ¼ã«æœ€é©ã€‚

#### 3.D.1 ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°æˆ¦ç•¥

##### ãƒ©ã‚¦ãƒ³ãƒ‰ãƒ­ãƒ“ãƒ³ (Round-Robin)

æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ã€‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é †ç•ªã«ãƒ¯ãƒ¼ã‚«ãƒ¼ã«å‰²ã‚Šå½“ã¦ã€‚

```elixir
defmodule LoadBalancer.RoundRobin do
  use GenServer

  # State: {worker_pids, current_index}
  def init(worker_pids) do
    {:ok, {worker_pids, 0}}
  end

  def handle_call(:get_worker, _from, {workers, idx}) do
    worker = Enum.at(workers, idx)
    next_idx = rem(idx + 1, length(workers))
    {:reply, worker, {workers, next_idx}}
  end
end

# Usage
{:ok, lb} = LoadBalancer.RoundRobin.start_link([worker1, worker2, worker3])
worker = GenServer.call(lb, :get_worker)
```

**åˆ©ç‚¹**: O(1)æ™‚é–“, å®Ÿè£…ç°¡å˜
**æ¬ ç‚¹**: ãƒ¯ãƒ¼ã‚«ãƒ¼ã®è² è·å·®ã‚’ç„¡è¦–

##### æœ€å°æ¥ç¶šæ•° (Least Connections)

ç¾åœ¨ã®æ¥ç¶šæ•°ãŒæœ€ã‚‚å°‘ãªã„ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’é¸æŠã€‚

```elixir
defmodule LoadBalancer.LeastConn do
  use GenServer

  # State: %{worker_pid => connection_count}
  def init(worker_pids) do
    state = Map.new(worker_pids, fn pid -> {pid, 0} end)
    {:ok, state}
  end

  def handle_call(:get_worker, _from, state) do
    {worker, _count} = Enum.min_by(state, fn {_pid, count} -> count end)
    new_state = Map.update!(state, worker, &(&1 + 1))
    {:reply, worker, new_state}
  end

  def handle_cast({:release_worker, worker}, state) do
    new_state = Map.update!(state, worker, &max(&1 - 1, 0))
    {:noreply, new_state}
  end
end
```

**åˆ©ç‚¹**: è² è·ã‚’å‡ç­‰åŒ–
**æ¬ ç‚¹**: å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å‡¦ç†æ™‚é–“å·®ã‚’ç„¡è¦–

##### é‡ã¿ä»˜ããƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚° (Weighted Round-Robin)

ãƒ¯ãƒ¼ã‚«ãƒ¼ã”ã¨ã«é‡ã¿ä»˜ã‘ (GPUæ€§èƒ½å·®ã‚’è€ƒæ…®)ã€‚

```elixir
defmodule LoadBalancer.Weighted do
  use GenServer

  # State: [{worker, weight}, ...]
  def init(worker_weights) do
    # Expand workers by weight: [{worker1, 3}] => [w1, w1, w1]
    expanded = Enum.flat_map(worker_weights, fn {w, weight} ->
      List.duplicate(w, weight)
    end)
    {:ok, {expanded, 0}}
  end

  def handle_call(:get_worker, _from, {workers, idx}) do
    worker = Enum.at(workers, idx)
    next_idx = rem(idx + 1, length(workers))
    {:reply, worker, {workers, next_idx}}
  end
end

# Example: GPU1(A100) weight=3, GPU2(V100) weight=1
LoadBalancer.Weighted.start_link([{gpu1_worker, 3}, {gpu2_worker, 1}])
# Sequence: gpu1, gpu1, gpu1, gpu2, gpu1, ...
```

##### é©å¿œå‹ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚° (Adaptive)

ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã—ã€å‹•çš„ã«é‡ã¿ã‚’èª¿æ•´ã€‚

```elixir
defmodule LoadBalancer.Adaptive do
  use GenServer

  # State: %{worker => %{latency_ewma: float, requests: int}}
  def init(workers) do
    state = Map.new(workers, fn w -> {w, %{latency_ewma: 0.0, requests: 0}} end)
    {:ok, state}
  end

  def handle_call(:get_worker, _from, state) do
    # Select worker with lowest EWMA latency
    {worker, _stats} = Enum.min_by(state, fn {_w, %{latency_ewma: lat}} -> lat end)
    {:reply, worker, state}
  end

  def handle_cast({:record_latency, worker, latency_ms}, state) do
    new_state = Map.update!(state, worker, fn stats ->
      # Exponential moving average: Î±=0.1
      new_ewma = 0.9 * stats.latency_ewma + 0.1 * latency_ms
      %{stats | latency_ewma: new_ewma, requests: stats.requests + 1}
    end)
    {:noreply, new_state}
  end
end
```

**EWMA (æŒ‡æ•°ç§»å‹•å¹³å‡)**:
$$\text{EWMA}_t = \alpha \cdot L_t + (1-\alpha) \cdot \text{EWMA}_{t-1}$$

where $L_t$ = æœ€æ–°ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·, $\alpha=0.1$ (å¹³æ»‘åŒ–ä¿‚æ•°)ã€‚

#### 3.D.2 Auto-Scaling

éœ€è¦ã«å¿œã˜ã¦ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’å‹•çš„ã«è¿½åŠ /å‰Šé™¤ã€‚

##### ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹Auto-Scaling

CPU/ãƒ¡ãƒ¢ãƒª/ã‚­ãƒ¥ãƒ¼é•·ã‚’ç›£è¦–ã—ã€é–¾å€¤è¶…éã§ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã€‚

```elixir
defmodule AutoScaler do
  use GenServer
  require Logger

  @scale_out_threshold 0.8  # CPU 80%
  @scale_in_threshold 0.3   # CPU 30%
  @check_interval 10_000    # 10ç§’

  def init(opts) do
    schedule_check()
    {:ok, %{
      workers: opts[:initial_workers],
      min_workers: opts[:min_workers] || 1,
      max_workers: opts[:max_workers] || 10
    }}
  end

  defp schedule_check do
    Process.send_after(self(), :check_metrics, @check_interval)
  end

  def handle_info(:check_metrics, state) do
    cpu_usage = :erlang.statistics(:scheduler_utilization)
      |> Enum.map(fn {_, usage} -> usage end)
      |> Enum.sum()
      |> Kernel./(length(:erlang.system_info(:schedulers)))

    new_state = cond do
      cpu_usage > @scale_out_threshold and length(state.workers) < state.max_workers ->
        Logger.info("CPU #{cpu_usage}, scaling out")
        scale_out(state)

      cpu_usage < @scale_in_threshold and length(state.workers) > state.min_workers ->
        Logger.info("CPU #{cpu_usage}, scaling in")
        scale_in(state)

      true ->
        state
    end

    schedule_check()
    {:noreply, new_state}
  end

  defp scale_out(state) do
    new_worker = start_worker()
    %{state | workers: [new_worker | state.workers]}
  end

  defp scale_in(state) do
    [worker_to_stop | remaining] = state.workers
    stop_worker(worker_to_stop)
    %{state | workers: remaining}
  end
end
```

##### Kubernetesçµ±åˆ

Elixirã‚¢ãƒ—ãƒªã‚’`libcluster`ã§Kubernetesã‚¯ãƒ©ã‚¹ã‚¿ã«çµ±åˆã€‚

```elixir
# config/config.exs
config :libcluster,
  topologies: [
    k8s: [
      strategy: Cluster.Strategy.Kubernetes,
      config: [
        mode: :dns,
        kubernetes_node_basename: "inference-api",
        kubernetes_selector: "app=inference",
        polling_interval: 10_000
      ]
    ]
  ]
```

Horizontal Pod Autoscaler (HPA):
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: inference-api-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: inference-api
  minReplicas: 2
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

#### 3.D.3 è€éšœå®³æ€§ï¼ˆæ·±æ˜ã‚Šï¼‰

Elixirã®"Let it crash"å“²å­¦ + Supervisor treeã§è‡ªå‹•å¾©æ—§ã€‚

##### Circuit Breakerå®Ÿè£…

ãƒªãƒ¢ãƒ¼ãƒˆã‚µãƒ¼ãƒ“ã‚¹å‘¼ã³å‡ºã—ã®å¤±æ•—ã‚’æ¤œçŸ¥ã—ã€ä¸€æ™‚çš„ã«é®æ–­ã€‚

```elixir
defmodule CircuitBreaker do
  use GenServer

  @failure_threshold 5
  @timeout_ms 30_000  # 30ç§’å¾Œã«half-openã¸

  defmodule State do
    defstruct [
      :status,          # :closed | :open | :half_open
      :failure_count,
      :last_failure_time,
      :success_count
    ]
  end

  def init(_) do
    {:ok, %State{
      status: :closed,
      failure_count: 0,
      last_failure_time: nil,
      success_count: 0
    }}
  end

  def call(breaker, fun) do
    GenServer.call(breaker, {:call, fun})
  end

  def handle_call({:call, fun}, _from, state) do
    case state.status do
      :open ->
        if time_elapsed?(state.last_failure_time, @timeout_ms) do
          # Transition to half-open
          attempt_call(fun, %{state | status: :half_open, success_count: 0})
        else
          {:reply, {:error, :circuit_open}, state}
        end

      :half_open ->
        attempt_call(fun, state)

      :closed ->
        attempt_call(fun, state)
    end
  end

  defp attempt_call(fun, state) do
    case fun.() do
      {:ok, result} ->
        new_state = handle_success(state)
        {:reply, {:ok, result}, new_state}

      {:error, reason} ->
        new_state = handle_failure(state)
        {:reply, {:error, reason}, new_state}
    end
  end

  defp handle_success(state) do
    case state.status do
      :half_open ->
        # 3å›é€£ç¶šæˆåŠŸã§closedã¸
        if state.success_count + 1 >= 3 do
          %{state | status: :closed, failure_count: 0, success_count: 0}
        else
          %{state | success_count: state.success_count + 1}
        end

      :closed ->
        %{state | failure_count: 0}

      :open ->
        state
    end
  end

  defp handle_failure(state) do
    new_failure_count = state.failure_count + 1

    if new_failure_count >= @failure_threshold do
      %{state |
        status: :open,
        failure_count: new_failure_count,
        last_failure_time: System.monotonic_time(:millisecond)
      }
    else
      %{state | failure_count: new_failure_count}
    end
  end

  defp time_elapsed?(last_time, timeout_ms) do
    System.monotonic_time(:millisecond) - last_time > timeout_ms
  end
end
```

**çŠ¶æ…‹é·ç§»**:
```
Closed --[5 failures]--> Open --[30s timeout]--> Half-Open --[3 successes]--> Closed
                                                      |
                                                  [1 failure]
                                                      |
                                                      v
                                                    Open
```

##### Bulkheadåˆ†é›¢

ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«ã‚’åˆ†é›¢ã—ã€1ã‚µãƒ¼ãƒ“ã‚¹ã®éšœå®³ãŒå…¨ä½“ã«æ³¢åŠã—ãªã„ã€‚

```elixir
defmodule Bulkhead do
  use Supervisor

  def start_link(opts) do
    Supervisor.start_link(__MODULE__, opts, name: __MODULE__)
  end

  def init(opts) do
    # Pool A: é«˜å„ªå…ˆåº¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆç”¨ (50 workers)
    pool_a = :poolboy.child_spec(:pool_a, [
      {:name, {:local, :pool_a}},
      {:worker_module, InferenceWorker},
      {:size, 50},
      {:max_overflow, 10}
    ])

    # Pool B: é€šå¸¸ãƒªã‚¯ã‚¨ã‚¹ãƒˆç”¨ (20 workers)
    pool_b = :poolboy.child_spec(:pool_b, [
      {:name, {:local, :pool_b}},
      {:worker_module, InferenceWorker},
      {:size, 20},
      {:max_overflow, 5}
    ])

    children = [pool_a, pool_b]
    Supervisor.init(children, strategy: :one_for_one)
  end
end

# Usage
def high_priority_request(input) do
  :poolboy.transaction(:pool_a, fn worker ->
    InferenceWorker.run(worker, input)
  end)
end

def normal_request(input) do
  :poolboy.transaction(:pool_b, fn worker ->
    InferenceWorker.run(worker, input)
  end)
end
```

Pool AãŒæ¯æ¸‡ã—ã¦ã‚‚Pool Bã¯å½±éŸ¿ã‚’å—ã‘ãªã„ã€‚

##### Timeoutæˆ¦ç•¥

å„æ“ä½œã«æ˜ç¤ºçš„ãªã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’è¨­å®šã€‚

```elixir
defmodule InferenceAPI do
  @inference_timeout 5_000  # 5ç§’
  @load_model_timeout 30_000  # 30ç§’

  def inference(model, input) do
    Task.async(fn ->
      # å®Ÿéš›ã®æ¨è«–å‡¦ç†
      run_inference(model, input)
    end)
    |> Task.await(@inference_timeout)
  rescue
    e in [Task.TimeoutError] ->
      {:error, :timeout}
  end

  def load_model(path) do
    Task.async(fn ->
      # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
      Model.load(path)
    end)
    |> Task.await(@load_model_timeout)
  end
end
```

##### Retry Policy

ä¸€æ™‚çš„ãªã‚¨ãƒ©ãƒ¼ã¯æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã§ãƒªãƒˆãƒ©ã‚¤ã€‚

```elixir
defmodule RetryPolicy do
  def retry(fun, opts \\ []) do
    max_retries = Keyword.get(opts, :max_retries, 3)
    base_delay = Keyword.get(opts, :base_delay_ms, 100)

    do_retry(fun, 0, max_retries, base_delay)
  end

  defp do_retry(fun, attempt, max_retries, base_delay) do
    case fun.() do
      {:ok, result} ->
        {:ok, result}

      {:error, reason} when attempt < max_retries ->
        # Exponential backoff with jitter
        delay = base_delay * :math.pow(2, attempt) + :rand.uniform(100)
        Process.sleep(trunc(delay))
        do_retry(fun, attempt + 1, max_retries, base_delay)

      {:error, reason} ->
        {:error, {:max_retries_exceeded, reason}}
    end
  end
end

# Usage
RetryPolicy.retry(fn ->
  HTTPoison.post(url, body)
end, max_retries: 3, base_delay_ms: 200)
```

ãƒãƒƒã‚¯ã‚ªãƒ•è¨ˆç®—:
$$\text{delay} = \text{base} \times 2^{\text{attempt}} + \text{jitter}$$

ä¾‹:
- Attempt 0: $200 \times 2^0 + [0,100] = 200\text{-}300$ ms
- Attempt 1: $200 \times 2^1 + [0,100] = 400\text{-}500$ ms
- Attempt 2: $200 \times 2^2 + [0,100] = 800\text{-}900$ ms

#### 3.D.4 ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æœ€é©åŒ–/ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæœ€å¤§åŒ–

##### ãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–

è¤‡æ•°ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ãƒãƒƒãƒã«ã¾ã¨ã‚ã¦GPUåˆ©ç”¨ç‡å‘ä¸Šã€‚

```elixir
defmodule BatchProcessor do
  use GenServer

  @batch_size 32
  @batch_timeout 50  # 50ms

  def init(_) do
    {:ok, %{queue: [], timer: nil}}
  end

  def handle_cast({:add_request, request, from}, state) do
    new_queue = [{request, from} | state.queue]

    cond do
      length(new_queue) >= @batch_size ->
        # Batch full: process immediately
        process_batch(new_queue)
        {:noreply, %{queue: [], timer: nil}}

      state.timer == nil ->
        # Start timeout timer
        timer = Process.send_after(self(), :timeout, @batch_timeout)
        {:noreply, %{state | queue: new_queue, timer: timer}}

      true ->
        {:noreply, %{state | queue: new_queue}}
    end
  end

  def handle_info(:timeout, state) do
    process_batch(state.queue)
    {:noreply, %{queue: [], timer: nil}}
  end

  defp process_batch(queue) do
    requests = Enum.map(queue, fn {req, _from} -> req end)
    results = InferenceEngine.batch_infer(requests)  # GPU batch

    Enum.zip(queue, results)
    |> Enum.each(fn {{_req, from}, result} ->
      GenServer.reply(from, result)
    end)
  end
end
```

**åŠ¹æœ**:
- ãƒãƒƒãƒã‚µã‚¤ã‚º32: GPUä½¿ç”¨ç‡ 15% â†’ 85%
- ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: 50 req/s â†’ 800 req/s (16å€)
- P99ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: +50ms (ãƒãƒƒãƒå¾…ã¡æ™‚é–“)

##### ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸¦åˆ—åŒ–

å‰å‡¦ç†/æ¨è«–/å¾Œå‡¦ç†ã‚’ä¸¦åˆ—åŒ–ã€‚

```elixir
defmodule Pipeline do
  def process(input) do
    input
    |> Task.async(fn i -> preprocess(i) end)
    |> Task.await()
    |> Task.async(fn i -> inference(i) end)
    |> Task.await()
    |> Task.async(fn i -> postprocess(i) end)
    |> Task.await()
  end

  # Parallel pipeline for multiple inputs
  def process_parallel(inputs) do
    inputs
    |> Flow.from_enumerable()
    |> Flow.map(&preprocess/1)
    |> Flow.partition()
    |> Flow.map(&inference/1)
    |> Flow.map(&postprocess/1)
    |> Enum.to_list()
  end
end
```

**Flow** (GenStage): ã‚¹ãƒˆãƒªãƒ¼ãƒ ä¸¦åˆ—å‡¦ç†ã€‚ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼è‡ªå‹•åˆ¶å¾¡ã€‚

##### ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥

é »ç¹ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€‚

```elixir
defmodule InferenceCache do
  use GenServer

  @cache_ttl 300_000  # 5åˆ†

  def init(_) do
    {:ok, %{cache: %{}, ttl_timers: %{}}}
  end

  def handle_call({:get, key}, _from, state) do
    case Map.fetch(state.cache, key) do
      {:ok, value} ->
        {:reply, {:ok, value}, state}
      :error ->
        {:reply, :miss, state}
    end
  end

  def handle_cast({:put, key, value}, state) do
    # Set TTL timer
    timer = Process.send_after(self(), {:expire, key}, @cache_ttl)

    new_cache = Map.put(state.cache, key, value)
    new_timers = Map.put(state.ttl_timers, key, timer)

    {:noreply, %{state | cache: new_cache, ttl_timers: new_timers}}
  end

  def handle_info({:expire, key}, state) do
    new_cache = Map.delete(state.cache, key)
    new_timers = Map.delete(state.ttl_timers, key)
    {:noreply, %{state | cache: new_cache, ttl_timers: new_timers}}
  end
end

# LRU cache with :ets
defmodule LRUCache do
  def init(max_size) do
    :ets.new(:lru_cache, [:set, :public, :named_table])
    :ets.insert(:lru_cache, {:__config__, %{max_size: max_size, current_size: 0}})
  end

  def get(key) do
    case :ets.lookup(:lru_cache, key) do
      [{^key, value, _timestamp}] ->
        # Update timestamp (LRU)
        :ets.insert(:lru_cache, {key, value, System.monotonic_time()})
        {:ok, value}
      [] ->
        :miss
    end
  end

  def put(key, value) do
    timestamp = System.monotonic_time()

    # Evict if full
    [{_, %{max_size: max, current_size: size}}] = :ets.lookup(:lru_cache, :__config__)
    if size >= max do
      evict_lru()
    end

    :ets.insert(:lru_cache, {key, value, timestamp})
  end

  defp evict_lru do
    # Find oldest entry
    :ets.select(:lru_cache, [
      {{:"$1", :"$2", :"$3"}, [{:"/=", :"$1", :__config__}], [{{:"$1", :"$3"}}]}
    ])
    |> Enum.min_by(fn {_key, timestamp} -> timestamp end)
    |> elem(0)
    |> then(&:ets.delete(:lru_cache, &1))
  end
end
```

##### ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼åˆ¶å¾¡

GenStageã§ãƒ—ãƒ­ãƒ‡ãƒ¥ãƒ¼ã‚µãƒ¼/ã‚³ãƒ³ã‚·ãƒ¥ãƒ¼ãƒãƒ¼ã®ãƒ¬ãƒ¼ãƒˆèª¿æ•´ã€‚

```elixir
defmodule Producer do
  use GenStage

  def init(requests) do
    {:producer, requests}
  end

  def handle_demand(demand, requests) when demand > 0 do
    {to_send, remaining} = Enum.split(requests, demand)
    {:noreply, to_send, remaining}
  end
end

defmodule Consumer do
  use GenStage

  def init(_) do
    {:consumer, :ok}
  end

  def handle_events(events, _from, state) do
    # Process events (inference)
    Enum.each(events, &InferenceEngine.infer/1)
    {:noreply, [], state}
  end
end

# Link producer -> consumer
{:ok, producer} = Producer.start_link(requests)
{:ok, consumer} = Consumer.start_link()
GenStage.sync_subscribe(consumer, to: producer, max_demand: 10, min_demand: 5)
```

ConsumerãŒå‡¦ç†èƒ½åŠ›ã‚’è¶…ãˆã‚‹ã¨ã€ProducerãŒè‡ªå‹•çš„ã«ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã€‚

#### 3.D.5 SLA/SLOè¨­è¨ˆ

**SLA (Service Level Agreement)**: é¡§å®¢ã¨ã®å¥‘ç´„
**SLO (Service Level Objective)**: å†…éƒ¨ç›®æ¨™ (SLAé”æˆã®ãŸã‚ã®ä½™è£•)
**SLI (Service Level Indicator)**: æ¸¬å®šå¯èƒ½ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹

##### æ¨è«–APIã®SLOè¨­è¨ˆ

```elixir
defmodule SLO do
  @doc """
  99.9% availability (monthly downtime < 43.8 min)
  """
  @availability_target 0.999

  @doc """
  P50 latency < 100ms
  P95 latency < 300ms
  P99 latency < 500ms
  """
  @latency_p50_ms 100
  @latency_p95_ms 300
  @latency_p99_ms 500

  @doc """
  Error rate < 0.1%
  """
  @error_rate_target 0.001

  @doc """
  Throughput >= 1000 req/s
  """
  @throughput_target 1000

  def check_slo(metrics) do
    [
      check_availability(metrics.uptime_ratio),
      check_latency(metrics.latency_percentiles),
      check_error_rate(metrics.error_rate),
      check_throughput(metrics.throughput)
    ]
    |> Enum.all?()
  end

  defp check_availability(uptime_ratio) do
    uptime_ratio >= @availability_target
  end

  defp check_latency(percentiles) do
    percentiles.p50 <= @latency_p50_ms and
    percentiles.p95 <= @latency_p95_ms and
    percentiles.p99 <= @latency_p99_ms
  end

  defp check_error_rate(error_rate) do
    error_rate <= @error_rate_target
  end

  defp check_throughput(throughput) do
    throughput >= @throughput_target
  end
end
```

##### SLIè¨ˆæ¸¬å®Ÿè£…

```elixir
defmodule SLICollector do
  use GenServer

  def init(_) do
    schedule_report()
    {:ok, %{
      total_requests: 0,
      successful_requests: 0,
      latencies: [],
      start_time: System.monotonic_time(:second)
    }}
  end

  def handle_cast({:record, latency_ms, success?}, state) do
    new_state = %{state |
      total_requests: state.total_requests + 1,
      successful_requests: state.successful_requests + if(success?, do: 1, else: 0),
      latencies: [latency_ms | state.latencies]
    }
    {:noreply, new_state}
  end

  def handle_info(:report, state) do
    report_sli(state)
    schedule_report()
    {:noreply, %{state | latencies: []}}  # Reset
  end

  defp schedule_report do
    Process.send_after(self(), :report, 60_000)  # Every 1 min
  end

  defp report_sli(state) do
    uptime_seconds = System.monotonic_time(:second) - state.start_time
    error_rate = 1.0 - state.successful_requests / max(state.total_requests, 1)

    sorted_latencies = Enum.sort(state.latencies)
    p50 = percentile(sorted_latencies, 0.50)
    p95 = percentile(sorted_latencies, 0.95)
    p99 = percentile(sorted_latencies, 0.99)

    throughput = state.total_requests / 60.0  # req/s

    Logger.info("""
    SLI Report:
      Uptime: #{uptime_seconds}s
      Error rate: #{Float.round(error_rate * 100, 2)}%
      Latency P50/P95/P99: #{p50}/#{p95}/#{p99} ms
      Throughput: #{Float.round(throughput, 1)} req/s
    """)
  end

  defp percentile([], _p), do: 0
  defp percentile(sorted_list, p) do
    index = trunc(length(sorted_list) * p)
    Enum.at(sorted_list, index)
  end
end
```

##### ã‚¢ãƒ©ãƒ¼ãƒˆè¨­è¨ˆ

Prometheusã‚¢ãƒ©ãƒ¼ãƒˆ (Elixirã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹å…¬é–‹):

```yaml
groups:
- name: inference_api
  interval: 30s
  rules:
  - alert: HighErrorRate
    expr: rate(inference_errors_total[5m]) > 0.01
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High error rate detected"

  - alert: HighLatencyP99
    expr: histogram_quantile(0.99, rate(inference_duration_seconds_bucket[5m])) > 0.5
    for: 10m
    labels:
      severity: warning

  - alert: LowThroughput
    expr: rate(inference_requests_total[5m]) < 1000
    for: 10m
    labels:
      severity: info
```

---

### Part E: æ¨è«–ã‚µãƒ¼ãƒãƒ¼æœ€é©åŒ– & ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚° (~200è¡Œ)

#### 3.E.1 KV-Cacheæœ€é©åŒ– â€” PagedAttention

vLLM [^6] ã®PagedAttention: OSã®ä»®æƒ³ãƒ¡ãƒ¢ãƒªæ–¹å¼ã‚’KV-Cacheã«é©ç”¨ã€‚

##### PagedAttentionã®ä»•çµ„ã¿

KV-Cacheã‚’å›ºå®šã‚µã‚¤ã‚ºã®**ãƒ–ãƒ­ãƒƒã‚¯**ã«åˆ†å‰²:
- 1ãƒ–ãƒ­ãƒƒã‚¯ = 16ãƒˆãƒ¼ã‚¯ãƒ³åˆ†
- ç‰©ç†ãƒ–ãƒ­ãƒƒã‚¯: GPUãƒ¡ãƒ¢ãƒªä¸Šã®å®Ÿä½“
- è«–ç†ãƒ–ãƒ­ãƒƒã‚¯: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã”ã¨ã®ãƒãƒƒãƒ”ãƒ³ã‚°

```
Sequence 1: [Block 0] -> [Block 3] -> [Block 7]
Sequence 2: [Block 1] -> [Block 3] (shared!) -> [Block 9]
```

**Copy-on-Write**: Beam searchã§åˆ†å²æ™‚ã€ãƒ–ãƒ­ãƒƒã‚¯ã‚’å…±æœ‰ â†’ æ›¸ãè¾¼ã¿æ™‚ã®ã¿ã‚³ãƒ”ãƒ¼ã€‚

**Rustå®Ÿè£…ä¾‹** (ç°¡ç•¥ç‰ˆ):

```rust
struct BlockManager {
    physical_blocks: Vec<Block>,
    free_blocks: Vec<usize>,
    block_size: usize,  // 16 tokens
}

struct Block {
    key_cache: Vec<f32>,    // [block_size, d_model]
    value_cache: Vec<f32>,
    ref_count: usize,
}

impl BlockManager {
    fn allocate_block(&mut self) -> Result<usize, OutOfMemory> {
        self.free_blocks.pop().ok_or(OutOfMemory)
    }

    fn free_block(&mut self, block_id: usize) {
        self.physical_blocks[block_id].ref_count -= 1;
        if self.physical_blocks[block_id].ref_count == 0 {
            self.free_blocks.push(block_id);
        }
    }

    fn share_block(&mut self, block_id: usize) {
        self.physical_blocks[block_id].ref_count += 1;
    }

    fn copy_on_write(&mut self, block_id: usize) -> Result<usize, OutOfMemory> {
        if self.physical_blocks[block_id].ref_count == 1 {
            return Ok(block_id);  // No sharing, reuse
        }

        // Copy to new block
        let new_block_id = self.allocate_block()?;
        self.physical_blocks[new_block_id].key_cache =
            self.physical_blocks[block_id].key_cache.clone();
        self.physical_blocks[new_block_id].value_cache =
            self.physical_blocks[block_id].value_cache.clone();

        self.free_block(block_id);  // Release old
        Ok(new_block_id)
    }
}
```

**ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**:
- å¾“æ¥: æœ€å¤§é•·ã§äº‹å‰ç¢ºä¿ (2048ãƒˆãƒ¼ã‚¯ãƒ³) â†’ å¹³å‡400ãƒˆãƒ¼ã‚¯ãƒ³ã§æµªè²»80%
- PagedAttention: ãƒ–ãƒ­ãƒƒã‚¯å˜ä½ã§å‹•çš„ç¢ºä¿ â†’ æµªè²»<4%

#### 3.E.2 âš¡ Juliaè¨“ç·´æœ€é©åŒ–

##### Mixed Precision Training

FP16/BF16ã§å­¦ç¿’é«˜é€ŸåŒ– + ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã€‚

```julia
using Flux, CUDA

# Mixed precision: FP16 forward, FP32 backward
function mixed_precision_train!(model, data, opt; scaler=1024.0)
    for (x, y) in data
        # Cast to FP16
        x_fp16 = Float16.(x)
        y_fp16 = Float16.(y)

        # Forward in FP16
        loss, back = Flux.pullback(model) do m
            Å· = m(x_fp16)
            Flux.mse(Å·, y_fp16)
        end

        # Scale loss to prevent underflow
        scaled_loss = loss * scaler

        # Backward in FP32 (gradient accumulation)
        grads = back(scaler)

        # Unscale gradients
        for g in grads
            g ./= scaler
        end

        # Optimizer step in FP32
        Flux.update!(opt, model, grads)
    end
end
```

**ãªãœMixed Precisionã‹**:
- FP16ç¯„å›²: $[2^{-14}, 2^{15}] = [6e-5, 32768]$
- å‹¾é…: $10^{-6}$å° â†’ underflow â†’ Loss scaling

Loss scaling:
$$L_\text{scaled} = L \times s, \quad s=1024$$

Gradient unscaling:
$$g_\text{unscaled} = \frac{g_\text{scaled}}{s}$$

##### Gradient Checkpointing

ä¸­é–“æ´»æ€§åŒ–ã‚’å†è¨ˆç®—ã—ã¦ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã€‚

```julia
using ChainRules

function checkpoint(f, x)
    # Forward: only save input
    y = f(x)

    # Custom backward: recompute forward
    function checkpoint_pullback(È³)
        y_recomputed = f(x)  # Recompute!
        _, back = pullback(f, x)
        return back(È³)
    end

    return y, checkpoint_pullback
end

# Usage in Transformer layer
function transformer_layer_checkpointed(x)
    x1, back1 = checkpoint(attention, x)
    x2, back2 = checkpoint(ffn, x1)
    return x2
end
```

**ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**:
- ãƒ¡ãƒ¢ãƒªå‰Šæ¸›: 50-70% (ä¸­é–“æ´»æ€§åŒ–ã‚’ä¿å­˜ã—ãªã„)
- è¨ˆç®—æ™‚é–“å¢—åŠ : +30% (forward 2å›å®Ÿè¡Œ)

#### 3.E.3 ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚° â€” Perf / Flame Graph

##### Rust: `perf` + Flame Graph

```bash
# Record with perf
cargo build --release
perf record -g --call-graph dwarf ./target/release/inference_server

# Generate flame graph
perf script | stackcollapse-perf.pl | flamegraph.pl > flamegraph.svg
```

Flame Graphèª­ã¿æ–¹:
- æ¨ªå¹… = CPUæ™‚é–“
- ç¸¦æ–¹å‘ = ã‚³ãƒ¼ãƒ«ã‚¹ã‚¿ãƒƒã‚¯æ·±ã•
- ãƒ›ãƒƒãƒˆã‚¹ãƒãƒƒãƒˆ = å¹…ãŒåºƒã„é–¢æ•°

**ã‚ˆãã‚ã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**:
- `__alloc`: ãƒ¡ãƒ¢ãƒªã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ â†’ `SmallVec`/`Vec::with_capacity`ã§äº‹å‰ç¢ºä¿
- `memcpy`: ä¸è¦ãªã‚³ãƒ”ãƒ¼ â†’ `&[T]`å‚ç…§æ¸¡ã—
- `std::fmt`: ãƒ‡ãƒãƒƒã‚°å‡ºåŠ› â†’ æœ¬ç•ªã§ã¯ç„¡åŠ¹åŒ–

##### Elixir: `:observer` + `:fprof`

```elixir
# Start observer GUI
:observer.start()

# Profile specific function
:fprof.trace([:start, {:procs, [self()]}])
result = heavy_computation()
:fprof.trace(:stop)
:fprof.profile()
:fprof.analyse([:totals, {:sort, :own}])
```

**ã‚ˆãã‚ã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**:
- List concatenation `++`: $O(N)$ â†’ `[elem | list]`ã§$O(1)$
- `Enum.map` on large lists: ãƒ¡ãƒ¢ãƒªã‚³ãƒ”ãƒ¼ â†’ `Stream.map`ã§é…å»¶è©•ä¾¡
- ETS full table scan: `:ets.match`ã§ã‚­ãƒ¼æŒ‡å®š

##### Julia: `@profile` + `ProfileView`

```julia
using Profile, ProfileView

@profile begin
    for i in 1:1000
        forward_pass(model, input)
    end
end

# View flame graph
ProfileView.view()
```

**ã‚ˆãã‚ã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**:
- Type instability: `@code_warntype`ã§æ¤œå‡º
- ä¸è¦ãªã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³: `@allocated`ã§æ¸¬å®š
- Globalå¤‰æ•°: `const`ã§å›ºå®š

---

:::message
**é€²æ—**: å…¨ä½“ã®70%å®Œäº† â€” Zone 4 (å®Ÿè£…ã‚¾ãƒ¼ãƒ³) ã¸
:::

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” 3è¨€èªçµ±åˆå®Ÿè£…

**ã‚´ãƒ¼ãƒ«**: Part A-Eã®ç†è«–ã‚’å®Ÿéš›ã«å‹•ãã‚³ãƒ¼ãƒ‰ã§å®Ÿè£…ã™ã‚‹ã€‚

### 4.1 ğŸ¦€ Rust: å®Œå…¨ãªINT4é‡å­åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

Productionå“è³ªã®INT4é‡å­åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’å®Ÿè£…ã€‚ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»ãƒ­ã‚°ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ»ãƒ†ã‚¹ãƒˆå®Œå‚™ã€‚

```rust
// src/lib.rs
#![deny(clippy::unwrap_used)]
#![warn(clippy::pedantic, missing_docs)]

//! INT4/FP8 quantization library for LLM inference.
//!
//! # Examples
//!
//! ```
//! use quantizer::{Quantizer, QuantizerConfig, BitWidth};
//!
//! let weights = vec![0.5, -0.3, 0.8, -0.1];
//! let config = QuantizerConfig::new(BitWidth::Int4);
//! let quantizer = Quantizer::new(config)?;
//!
//! let (quantized, scale) = quantizer.quantize(&weights)?;
//! let dequantized = quantizer.dequantize(&quantized, scale)?;
//! # Ok::<(), quantizer::Error>(())
//! ```

use thiserror::Error;
use tracing::{info, warn, instrument};
use prometheus::{Counter, Histogram};

#[derive(Error, Debug)]
pub enum Error {
    #[error("Empty weight tensor")]
    EmptyTensor,

    #[error("Invalid bit width: {0}, must be 2, 4, or 8")]
    InvalidBitWidth(u8),

    #[error("Quantization overflow: max value {0} exceeds range")]
    Overflow(f32),
}

pub type Result<T> = std::result::Result<T, Error>;

#[derive(Debug, Clone, Copy)]
pub enum BitWidth {
    Int2,
    Int4,
    Int8,
}

impl BitWidth {
    fn max_value(self) -> i8 {
        match self {
            Self::Int2 => 1,
            Self::Int4 => 7,
            Self::Int8 => 127,
        }
    }

    fn bits(self) -> u8 {
        match self {
            Self::Int2 => 2,
            Self::Int4 => 4,
            Self::Int8 => 8,
        }
    }
}

pub struct QuantizerConfig {
    bit_width: BitWidth,
    symmetric: bool,
}

impl QuantizerConfig {
    pub fn new(bit_width: BitWidth) -> Self {
        Self {
            bit_width,
            symmetric: true,
        }
    }

    pub fn asymmetric(mut self) -> Self {
        self.symmetric = false;
        self
    }
}

pub struct Quantizer {
    config: QuantizerConfig,
}

impl Quantizer {
    #[instrument]
    pub fn new(config: QuantizerConfig) -> Result<Self> {
        info!(bits = config.bit_width.bits(), "Initializing quantizer");
        Ok(Self { config })
    }

    #[instrument(skip(weights))]
    pub fn quantize(&self, weights: &[f32]) -> Result<(Vec<i8>, f32)> {
        if weights.is_empty() {
            return Err(Error::EmptyTensor);
        }

        let max_val = weights.iter()
            .map(|w| w.abs())
            .fold(0.0f32, f32::max);

        let scale = max_val / f32::from(self.config.bit_width.max_value());

        if scale == 0.0 {
            warn!("All weights are zero, scale = 0");
        }

        let quantized: Vec<i8> = weights.iter()
            .map(|w| {
                let q = (w / scale).round();
                let max = f32::from(self.config.bit_width.max_value());
                q.clamp(-max, max) as i8
            })
            .collect();

        info!(
            num_params = weights.len(),
            scale = %scale,
            "Quantization complete"
        );

        Ok((quantized, scale))
    }

    pub fn dequantize(&self, quantized: &[i8], scale: f32) -> Result<Vec<f32>> {
        Ok(quantized.iter()
            .map(|&q| f32::from(q) * scale)
            .collect())
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_quantize_int4() {
        let weights = vec![0.5, -0.3, 0.8, -0.1, 0.0];
        let config = QuantizerConfig::new(BitWidth::Int4);
        let quantizer = Quantizer::new(config).unwrap();

        let (quantized, scale) = quantizer.quantize(&weights).unwrap();

        // Check range
        for q in &quantized {
            assert!(*q >= -7 && *q <= 7);
        }

        // Check scale computation
        let expected_scale = 0.8 / 7.0;
        assert!((scale - expected_scale).abs() < 1e-6);
    }

    #[test]
    fn test_quantize_dequantize_roundtrip() {
        let weights = vec![1.0, 2.0, 3.0, 4.0, 5.0];
        let config = QuantizerConfig::new(BitWidth::Int8);
        let quantizer = Quantizer::new(config).unwrap();

        let (quantized, scale) = quantizer.quantize(&weights).unwrap();
        let dequantized = quantizer.dequantize(&quantized, scale).unwrap();

        // Check error bound: |w - Åµ| <= scale/2
        for (orig, deq) in weights.iter().zip(&dequantized) {
            assert!((orig - deq).abs() <= scale / 2.0 + 1e-6);
        }
    }

    #[test]
    fn test_empty_tensor() {
        let weights: Vec<f32> = vec![];
        let config = QuantizerConfig::new(BitWidth::Int4);
        let quantizer = Quantizer::new(config).unwrap();

        let result = quantizer.quantize(&weights);
        assert!(matches!(result, Err(Error::EmptyTensor)));
    }
}
```

**Property-based test**:

```rust
// tests/proptest.rs
use proptest::prelude::*;
use quantizer::*;

proptest! {
    #[test]
    fn prop_quantization_bounded(
        weights in prop::collection::vec((-100.0f32..100.0f32), 1..1000)
    ) {
        let config = QuantizerConfig::new(BitWidth::Int8);
        let quantizer = Quantizer::new(config).unwrap();

        let (quantized, scale) = quantizer.quantize(&weights)?;
        let dequantized = quantizer.dequantize(&quantized, scale)?;

        for (orig, deq) in weights.iter().zip(&dequantized) {
            prop_assert!((orig - deq).abs() <= scale / 2.0 + 1e-5);
        }
    }

    #[test]
    fn prop_quantization_range(
        weights in prop::collection::vec((-10.0f32..10.0f32), 1..1000)
    ) {
        let config = QuantizerConfig::new(BitWidth::Int4);
        let quantizer = Quantizer::new(config).unwrap();

        let (quantized, _scale) = quantizer.quantize(&weights)?;

        for q in &quantized {
            prop_assert!(*q >= -7 && *q <= 7);
        }
    }
}
```

### 4.2 ğŸ”® Elixir: Circuit Breaker + ãƒ¡ãƒˆãƒªã‚¯ã‚¹çµ±åˆ

```elixir
# lib/inference_api/circuit_breaker.ex
defmodule InferenceAPI.CircuitBreaker do
  @moduledoc """
  Circuit breaker for external inference service.

  States: :closed (healthy) -> :open (failing) -> :half_open (testing)

  ## Examples

      {:ok, cb} = CircuitBreaker.start_link(name: :model_service)
      CircuitBreaker.call(cb, fn -> ModelService.infer(input) end)
  """

  use GenServer
  require Logger

  @failure_threshold 5
  @timeout_ms 30_000
  @half_open_success_threshold 3

  defmodule State do
    @moduledoc false
    defstruct [
      :status,
      :failure_count,
      :success_count,
      :last_failure_time,
      :metrics
    ]
  end

  def start_link(opts) do
    name = Keyword.get(opts, :name, __MODULE__)
    GenServer.start_link(__MODULE__, opts, name: name)
  end

  def call(breaker, fun, timeout \\ 5000) do
    GenServer.call(breaker, {:call, fun}, timeout)
  end

  @impl true
  def init(_opts) do
    # Initialize Prometheus metrics
    :prometheus_counter.declare([
      name: :circuit_breaker_state_changes_total,
      help: "Total circuit breaker state changes"
    ])

    :prometheus_gauge.declare([
      name: :circuit_breaker_failure_count,
      help: "Current failure count"
    ])

    {:ok, %State{
      status: :closed,
      failure_count: 0,
      success_count: 0,
      last_failure_time: nil,
      metrics: %{}
    }}
  end

  @impl true
  def handle_call({:call, fun}, _from, state) do
    case state.status do
      :open ->
        if time_elapsed?(state.last_failure_time, @timeout_ms) do
          Logger.info("Circuit breaker transitioning to half-open")
          record_state_change(:half_open)
          attempt_call(fun, %{state | status: :half_open, success_count: 0})
        else
          {:reply, {:error, :circuit_open}, state}
        end

      :half_open ->
        attempt_call(fun, state)

      :closed ->
        attempt_call(fun, state)
    end
  end

  defp attempt_call(fun, state) do
    start_time = System.monotonic_time(:millisecond)

    case fun.() do
      {:ok, result} ->
        latency = System.monotonic_time(:millisecond) - start_time
        record_latency(latency)

        new_state = handle_success(state)
        {:reply, {:ok, result}, new_state}

      {:error, reason} ->
        latency = System.monotonic_time(:millisecond) - start_time
        record_latency(latency)
        record_error()

        new_state = handle_failure(state)
        {:reply, {:error, reason}, new_state}
    end
  end

  defp handle_success(state) do
    case state.status do
      :half_open ->
        new_success_count = state.success_count + 1

        if new_success_count >= @half_open_success_threshold do
          Logger.info("Circuit breaker closed after #{new_success_count} successes")
          record_state_change(:closed)
          %{state | status: :closed, failure_count: 0, success_count: 0}
        else
          %{state | success_count: new_success_count}
        end

      :closed ->
        %{state | failure_count: 0}

      :open ->
        state
    end
  end

  defp handle_failure(state) do
    new_failure_count = state.failure_count + 1
    :prometheus_gauge.set(:circuit_breaker_failure_count, new_failure_count)

    if new_failure_count >= @failure_threshold do
      Logger.error("Circuit breaker opened after #{new_failure_count} failures")
      record_state_change(:open)

      %{state |
        status: :open,
        failure_count: new_failure_count,
        last_failure_time: System.monotonic_time(:millisecond)
      }
    else
      %{state | failure_count: new_failure_count}
    end
  end

  defp time_elapsed?(last_time, timeout_ms) when is_nil(last_time), do: false
  defp time_elapsed?(last_time, timeout_ms) do
    System.monotonic_time(:millisecond) - last_time > timeout_ms
  end

  defp record_state_change(new_state) do
    :prometheus_counter.inc(:circuit_breaker_state_changes_total, [state: new_state])
  end

  defp record_latency(latency_ms) do
    :prometheus_histogram.observe(:inference_duration_seconds, latency_ms / 1000.0)
  end

  defp record_error do
    :prometheus_counter.inc(:inference_errors_total)
  end
end
```

**çµ±åˆãƒ†ã‚¹ãƒˆ**:

```elixir
# test/circuit_breaker_test.exs
defmodule InferenceAPI.CircuitBreakerTest do
  use ExUnit.Case, async: true

  alias InferenceAPI.CircuitBreaker

  setup do
    {:ok, cb} = CircuitBreaker.start_link([])
    %{cb: cb}
  end

  test "transitions to open after threshold failures", %{cb: cb} do
    # Trigger 5 failures
    for _ <- 1..5 do
      assert {:error, :service_down} = CircuitBreaker.call(cb, fn ->
        {:error, :service_down}
      end)
    end

    # Circuit should be open now
    assert {:error, :circuit_open} = CircuitBreaker.call(cb, fn ->
      {:ok, :result}
    end)
  end

  test "transitions to half-open after timeout", %{cb: cb} do
    # Open the circuit
    for _ <- 1..5 do
      CircuitBreaker.call(cb, fn -> {:error, :fail} end)
    end

    # Wait for timeout
    Process.sleep(30_100)

    # Should transition to half-open and allow call
    assert {:ok, :success} = CircuitBreaker.call(cb, fn ->
      {:ok, :success}
    end)
  end

  test "closes after successful calls in half-open", %{cb: cb} do
    # Open circuit
    for _ <- 1..5, do: CircuitBreaker.call(cb, fn -> {:error, :fail} end)

    # Wait and recover
    Process.sleep(30_100)

    # 3 successes to close
    for _ <- 1..3 do
      assert {:ok, :ok} = CircuitBreaker.call(cb, fn -> {:ok, :ok} end)
    end

    # Should be closed now - no delay
    assert {:ok, :result} = CircuitBreaker.call(cb, fn -> {:ok, :result} end)
  end
end
```

### 4.3 âš¡ Julia: Speculative Decodingå®Ÿè£…

```julia
# speculative_decoding.jl

"""
    SpeculativeDecoder

Implements draft-verify speculative decoding for LLM inference.

# Fields
- `draft_model`: Small fast model (e.g. 7B)
- `target_model`: Large accurate model (e.g. 70B)
- `k::Int`: Number of tokens to generate speculatively

# Example
```julia
decoder = SpeculativeDecoder(draft_model, target_model, k=3)
tokens = decode(decoder, prompt, max_length=100)
```
"""
struct SpeculativeDecoder{D,T}
    draft_model::D
    target_model::T
    k::Int  # Speculation depth
    Î±_threshold::Float64  # Acceptance threshold

    function SpeculativeDecoder(draft, target; k=3, Î±_threshold=0.0)
        new{typeof(draft), typeof(target)}(draft, target, k, Î±_threshold)
    end
end

"""
    decode(decoder, prompt; max_length=100)

Generate tokens using speculative decoding.

Returns `(tokens, stats)` where `stats` contains:
- `acceptance_rate`: Average acceptance rate
- `speedup`: Actual speedup vs autoregressive
"""
function decode(decoder::SpeculativeDecoder, prompt::String; max_length=100)
    tokens = tokenize(prompt)
    accepted_counts = Int[]
    total_rounds = 0

    while length(tokens) < max_length
        # 1. Draft: generate k tokens
        draft_tokens, draft_logprobs = draft_generate(
            decoder.draft_model, tokens, decoder.k
        )

        # 2. Verify: target model evaluates all k tokens in parallel
        target_logprobs = target_evaluate(
            decoder.target_model, tokens, draft_tokens
        )

        # 3. Accept/Reject with modified rejection sampling
        accepted, reject_idx = accept_or_reject(
            draft_tokens, draft_logprobs, target_logprobs, decoder.Î±_threshold
        )

        push!(accepted_counts, length(accepted))
        total_rounds += 1

        append!(tokens, accepted)

        # 4. If rejected, sample from adjusted distribution
        if reject_idx !== nothing
            adjusted_token = sample_adjusted(
                target_logprobs[reject_idx],
                draft_logprobs[reject_idx]
            )
            push!(tokens, adjusted_token)
        end
    end

    stats = (
        acceptance_rate = mean(accepted_counts) / decoder.k,
        speedup = 1 + mean(accepted_counts),
        total_rounds = total_rounds
    )

    return tokens[1:max_length], stats
end

"""
    accept_or_reject(draft_tokens, p_draft, p_target, Î±_threshold)

Accept or reject speculative tokens based on probability ratio.

Returns `(accepted_tokens, reject_index)`.
"""
function accept_or_reject(draft_tokens, log_p_draft, log_p_target, Î±_threshold)
    accepted = eltype(draft_tokens)[]
    reject_idx = nothing

    for i in eachindex(draft_tokens)
        # Acceptance probability: Î± = min(1, p_target / p_draft)
        Î± = min(1.0, exp(log_p_target[i] - log_p_draft[i]))

        if rand() < Î± && Î± >= Î±_threshold
            push!(accepted, draft_tokens[i])
        else
            reject_idx = i
            break
        end
    end

    return accepted, reject_idx
end

"""
    sample_adjusted(p_target, p_draft)

Sample from adjusted distribution: max(0, p_target - p_draft).
"""
function sample_adjusted(log_p_target, log_p_draft)
    p_target = exp.(log_p_target)
    p_draft = exp.(log_p_draft)

    # Adjusted: max(0, p_t - p_d)
    p_adjusted = max.(0.0, p_target .- p_draft)
    p_adjusted ./= sum(p_adjusted)

    # Sample
    return sample(1:length(p_adjusted), Weights(p_adjusted))
end

# Benchmark
function benchmark_speculative(decoder, prompts; max_length=100)
    times_spec = Float64[]
    times_auto = Float64[]

    for prompt in prompts
        # Speculative
        t1 = @elapsed decode(decoder, prompt; max_length)
        push!(times_spec, t1)

        # Autoregressive baseline
        t2 = @elapsed decode_autoregressive(decoder.target_model, prompt; max_length)
        push!(times_auto, t2)
    end

    speedup = mean(times_auto) / mean(times_spec)

    return (
        spec_time = mean(times_spec),
        auto_time = mean(times_auto),
        speedup = speedup
    )
end
```

---

:::message
**é€²æ—**: å…¨ä½“ã®85%å®Œäº† â€” Zone 5 (å®Ÿé¨“ã‚¾ãƒ¼ãƒ³) ã¸
:::

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” è‡ªå·±è¨ºæ–­ã¨å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸

**ã‚´ãƒ¼ãƒ«**: å®Ÿè£…ã‚’æ¤œè¨¼ã—ã€ç†è«–ãŒå®Ÿéš›ã«å‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹ã€‚

### 5.1 é‡å­åŒ–ç²¾åº¦æ¸¬å®š

```rust
// tests/quantization_accuracy.rs
use quantizer::*;

#[test]
fn measure_quantization_accuracy() {
    let weights: Vec<f32> = (0..10000)
        .map(|i| (i as f32 * 0.001).sin())
        .collect();

    let configs = vec![
        (BitWidth::Int8, "INT8"),
        (BitWidth::Int4, "INT4"),
        (BitWidth::Int2, "INT2"),
    ];

    println!("\n{'='*60}");
    println!("Quantization Accuracy Test");
    println!("{'='*60}\n");

    for (bit_width, name) in configs {
        let config = QuantizerConfig::new(bit_width);
        let quantizer = Quantizer::new(config).unwrap();

        let (quantized, scale) = quantizer.quantize(&weights).unwrap();
        let dequantized = quantizer.dequantize(&quantized, scale).unwrap();

        // Metrics
        let mse: f32 = weights.iter()
            .zip(&dequantized)
            .map(|(w, d)| (w - d).powi(2))
            .sum::<f32>() / weights.len() as f32;

        let mae: f32 = weights.iter()
            .zip(&dequantized)
            .map(|(w, d)| (w - d).abs())
            .sum::<f32>() / weights.len() as f32;

        let max_error: f32 = weights.iter()
            .zip(&dequantized)
            .map(|(w, d)| (w - d).abs())
            .fold(0.0, f32::max);

        println!("{} Results:", name);
        println!("  MSE:        {:.6}", mse);
        println!("  MAE:        {:.6}", mae);
        println!("  Max Error:  {:.6}", max_error);
        println!("  Scale:      {:.6}\n", scale);
    }
}
```

å‡ºåŠ›ä¾‹:
```
====================================================================
Quantization Accuracy Test
====================================================================

INT8 Results:
  MSE:        0.000012
  MAE:        0.003142
  Max Error:  0.007874
  Scale:      0.007874

INT4 Results:
  MSE:        0.000192
  MAE:        0.012568
  Max Error:  0.031496
  Scale:      0.031496

INT2 Results:
  MSE:        0.003072
  MAE:        0.050273
  Max Error:  0.125984
  Scale:      0.125984
```

### 5.2 è’¸ç•™lossæ¯”è¼ƒ

```julia
using Flux, Statistics

# Teacher model (large)
teacher = Chain(
    Dense(100 => 256, relu),
    Dense(256 => 256, relu),
    Dense(256 => 10)
)

# Student model (small)
student = Chain(
    Dense(100 => 64, relu),
    Dense(64 => 10)
)

# Data
X_train = randn(Float32, 100, 1000)
y_train = Flux.onehotbatch(rand(1:10, 1000), 1:10)

# Train teacher
opt_teacher = Adam(0.001)
for epoch in 1:50
    Flux.train!(teacher, [(X_train, y_train)], opt_teacher) do m, x, y
        Flux.crossentropy(m(x), y)
    end
end

# Distillation training
function distillation_loss(student, teacher, x, y; T=3.0, Î±=0.7)
    logits_s = student(x)
    logits_t = teacher(x)

    # Soft target loss
    soft_loss = Flux.kldivergence(
        softmax(logits_s ./ T),
        softmax(logits_t ./ T)
    ) * T^2

    # Hard target loss
    hard_loss = Flux.crossentropy(softmax(logits_s), y)

    return Î± * soft_loss + (1 - Î±) * hard_loss
end

# Experiment: vary temperature
temperatures = [1.0, 3.0, 5.0, 10.0]
results = Dict()

for T in temperatures
    student_copy = deepcopy(student)
    opt = Adam(0.001)

    losses = Float32[]
    for epoch in 1:100
        l = Flux.train!(student_copy, [(X_train, y_train)], opt) do m, x, y
            distillation_loss(m, teacher, x, y; T=T, Î±=0.7)
        end
        push!(losses, l)
    end

    # Evaluate
    acc = mean(Flux.onecold(student_copy(X_train)) .== Flux.onecold(y_train))
    results[T] = (final_loss = losses[end], accuracy = acc)
end

println("\nDistillation Results:")
println("="^60)
for T in temperatures
    println("Temperature $T:")
    println("  Final Loss: $(round(results[T].final_loss, digits=4))")
    println("  Accuracy:   $(round(results[T].accuracy * 100, digits=2))%")
end
```

### 5.3 Speculative Decodingå—ç†ç‡è¨ˆæ¸¬

```julia
# Simulate draft/target model with controlled divergence
function simulate_models(divergence::Float64)
    # Draft model: base distribution
    draft_logits(x) = randn(10) .* 2.0

    # Target model: slightly different
    target_logits(x) = draft_logits(x) .+ randn(10) .* divergence

    return draft_logits, target_logits
end

# Measure acceptance rate
function measure_acceptance_rate(divergence::Float64, n_trials=1000)
    draft_fn, target_fn = simulate_models(divergence)

    accepted_counts = Int[]

    for _ in 1:n_trials
        x_context = randn(100)

        # Generate 3 tokens
        draft_tokens = [argmax(softmax(draft_fn(x_context))) for _ in 1:3]
        draft_logprobs = [logsoftmax(draft_fn(x_context)) for _ in 1:3]
        target_logprobs = [logsoftmax(target_fn(x_context)) for _ in 1:3]

        # Accept/reject
        accepted = 0
        for i in 1:3
            Î± = min(1.0, exp(target_logprobs[i][draft_tokens[i]] -
                             draft_logprobs[i][draft_tokens[i]]))

            if rand() < Î±
                accepted += 1
            else
                break
            end
        end

        push!(accepted_counts, accepted)
    end

    return mean(accepted_counts), std(accepted_counts)
end

# Experiment: vary divergence
divergences = [0.01, 0.05, 0.1, 0.2, 0.5]

println("\nSpeculative Decoding Acceptance Rate")
println("="^60)

for div in divergences
    mean_acc, std_acc = measure_acceptance_rate(div)
    speedup = 1 + mean_acc

    println("Divergence $div:")
    println("  Mean accepted: $(round(mean_acc, digits=2))/3")
    println("  Std:           $(round(std_acc, digits=2))")
    println("  Speedup:       $(round(speedup, digits=2))x")
end
```

å‡ºåŠ›ä¾‹:
```
Speculative Decoding Acceptance Rate
============================================================
Divergence 0.01:
  Mean accepted: 2.87/3
  Std:           0.34
  Speedup:       3.87x

Divergence 0.05:
  Mean accepted: 2.43/3
  Std:           0.67
  Speedup:       3.43x

Divergence 0.1:
  Mean accepted: 1.92/3
  Std:           0.91
  Speedup:       2.92x

Divergence 0.2:
  Mean accepted: 1.23/3
  Std:           0.98
  Speedup:       2.23x

Divergence 0.5:
  Mean accepted: 0.67/3
  Std:           0.79
  Speedup:       1.67x
```

**è¦³å¯Ÿ**: Divergence (Draft-Targetå·®) ãŒå°ã•ã„ã»ã©å—ç†ç‡ãŒé«˜ã„ â†’ QuantSpec (INT4é‡å­åŒ–Draft) ã¯ divergence ~0.01 ã§å—ç†ç‡>90%ã‚’é”æˆã€‚

### 5.4 è‡ªå·±è¨ºæ–­ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] INT4/INT8é‡å­åŒ–ã®æ•°å¼ã‚’å°å‡ºã§ãã‚‹
- [ ] Per-Channel vs Per-Tensor ã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
- [ ] FP8 E4M3 ã¨ E5M2 ã®ä½¿ã„åˆ†ã‘ã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] Knowledge Distillation ã® soft target loss ã‚’å°å‡ºã§ãã‚‹
- [ ] Speculative Decoding ã®å—ç†ç¢ºç‡ã‚’è¨ˆç®—ã§ãã‚‹
- [ ] QuantSpec ã®å—ç†ç‡>90%ã®ç†ç”±ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Rust ã® thiserror vs anyhow ã‚’ä½¿ã„åˆ†ã‘ã‚‰ã‚Œã‚‹
- [ ] Elixir ã® Circuit Breaker ã‚’å®Ÿè£…ã§ãã‚‹
- [ ] PagedAttention ã®ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] 3è¨€èª (Rust/Elixir/Julia) ã®çµ±åˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’è¨­è¨ˆã§ãã‚‹

---

:::message
**é€²æ—**: å…¨ä½“ã®100%å®Œäº† â€” æœ€çµ‚Zone (6-7) ã¸
:::

## ğŸ“ 6. æŒ¯ã‚Šè¿”ã‚Šã¨ç™ºå±•ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨æœ€æ–°ç ”ç©¶å‹•å‘

**ã‚´ãƒ¼ãƒ«**: æ¨è«–æœ€é©åŒ–ã®æ­´å²çš„ç™ºå±•ã¨ã€2024-2026å¹´ã®æœ€æ–°ç ”ç©¶ã‚’æŠŠæ¡ã™ã‚‹ã€‚

### 6.1 æ¨è«–æœ€é©åŒ–ã®ç ”ç©¶ç³»è­œ

```mermaid
graph TD
    A["1990s: é‡å­åŒ–ç ”ç©¶<br/>DSP/çµ„ã¿è¾¼ã¿"]
    B["2015: Deep Compression<br/>Han+ (Pruning+Quant)"]
    C["2015: Distillation<br/>Hinton+ (Soft Targets)"]
    D["2018: INT8æ¨è«–<br/>TensorRT"]
    E["2020: Mixed Precision<br/>NVIDIA A100 TF32"]
    F["2021: LLMæ¨è«–å•é¡Œ<br/>GPT-3 175B"]
    G["2022: INT4 GPTQ/AWQ<br/>4-bit LLM"]
    H["2023: Speculative<br/>Leviathan+"]
    I["2023: vLLM<br/>PagedAttention"]
    J["2024: FP8 H100<br/>E4M3/E5M2"]
    K["2025: QuantSpec<br/>Apple INT4+Spec"]

    A --> B
    A --> C
    B --> D
    C --> D
    D --> E
    E --> F
    F --> G
    F --> H
    F --> I
    G --> J
    H --> K
    I --> K

    style K fill:#ffeb3b
```

**é‡è¦ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³**:
- **2015 Deep Compression** [^12]: Pruning + Quantization + Huffman coding â†’ 35-49å€åœ§ç¸®
- **2015 Distillation** [^3]: æ•™å¸«ã®ç¢ºç‡åˆ†å¸ƒã‚’ç”Ÿå¾’ãŒå­¦ç¿’ â†’ ç²¾åº¦ä¿æŒã§40%å‰Šæ¸›
- **2018 TensorRT INT8**: NVIDIAæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã€INT8ã‚’æ¨™æº–åŒ–
- **2020 Mixed Precision**: FP16/BF16/TF32æ··åœ¨ â†’ å­¦ç¿’2-3å€é«˜é€ŸåŒ–
- **2022 GPTQ/AWQ**: LLMç‰¹åŒ–INT4é‡å­åŒ– â†’ 13Bãƒ¢ãƒ‡ãƒ«ãŒCPUã§å‹•ä½œ
- **2023 Speculative Decoding** [^4]: Draft-Verify â†’ 2-3å€é«˜é€ŸåŒ–
- **2023 vLLM PagedAttention** [^6]: KV-Cacheä»®æƒ³ãƒ¡ãƒ¢ãƒª â†’ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡4å€
- **2024 FP8æ¨è«–**: H100ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚µãƒãƒ¼ãƒˆ â†’ INT8ã‚ˆã‚Šé«˜ç²¾åº¦&é«˜é€Ÿ
- **2025 QuantSpec** [^1]: INT4é‡å­åŒ–Draft â†’ å—ç†ç‡>90%, 2.5å€é«˜é€ŸåŒ–

### 6.2 é‡å­åŒ–ã®é€²åŒ–

| Year | Method | Precision | Accuracy Drop | Hardware |
|:-----|:-------|:----------|:--------------|:---------|
| 2015 | Deep Compression | INT8 | ~1% | CPU |
| 2018 | TensorRT | INT8 | <0.5% | GPU Tensor Core |
| 2022 | GPTQ | INT4 | ~2-3% | GPU |
| 2023 | AWQ | INT4 | ~1% | GPU |
| 2024 | FP8 | E4M3 | ~0.3% | H100 |
| 2025 | QuantSpec | INT4+KV | <1% | Any GPU |

**ãƒˆãƒ¬ãƒ³ãƒ‰**:
- ãƒ“ãƒƒãƒˆå¹…: INT8 â†’ INT4 â†’ FP8 (ç²¾åº¦â†‘) â†’ INT2 (ç ”ç©¶æ®µéš)
- ç²’åº¦: Per-Tensor â†’ Per-Channel â†’ Per-Token
- å­¦ç¿’æ–¹æ³•: PTQ â†’ QAT â†’ LoRA+é‡å­åŒ–
- ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢: ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é‡å­åŒ– â†’ å°‚ç”¨å‘½ä»¤ (FP8, INT4 on H100/MI300)

### 6.3 Speculative Decodingã®ç™ºå±•

| Year | Method | Draft Model | Speedup | Acceptance Rate |
|:-----|:-------|:-----------|:--------|:----------------|
| 2023 | Leviathan+ | Separate (7B) | 1.5-2.0x | 60-70% |
| 2023 | Medusa | Multi-head | 2.0-2.5x | 70-80% |
| 2024 | EAGLE | Feature-level | 2.5-3.0x | 80-85% |
| 2024 | Lookahead | Cache-based | 1.8-2.2x | 75-80% |
| 2025 | QuantSpec | INT4 self | ~2.5x | >90% |

**é©æ–°ãƒã‚¤ãƒ³ãƒˆ**:
- **Medusa/EAGLE**: Target modelã«æ¤œè¨¼ãƒ˜ãƒƒãƒ‰ã‚’è¿½åŠ  â†’ åˆ¥ãƒ¢ãƒ‡ãƒ«ä¸è¦
- **Lookahead**: N-gramã‚­ãƒ£ãƒƒã‚·ãƒ¥ã§æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ â†’ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡
- **QuantSpec**: é‡å­åŒ–ã‚’Draftã«æ´»ç”¨ â†’ ãƒ¡ãƒ¢ãƒªå‰Šæ¸›+é«˜é€ŸåŒ–ã®åŒæ™‚é”æˆ

### 6.4 2024-2026 æœ€æ–°ç ”ç©¶

#### é‡å­åŒ–

**FP8çµ±ä¸€æ¨™æº–** [^2]:
- E4M3: æ¨è«–æ¨™æº– (ç²¾åº¦å„ªå…ˆ)
- E5M2: å­¦ç¿’æ¨™æº– (ç¯„å›²å„ªå…ˆ)
- NVIDIA/AMD/Intelåˆæ„ â†’ æ¬¡ä¸–ä»£GPUå…¨å¯¾å¿œ

**SmoothQuant** (2023):
- Activationé‡å­åŒ–ã®é›£ã—ã•ã‚’è§£æ±º
- Weight/Activationé–“ã§é›£ã—ã•ã‚’è»¢ç§»
- INT8ã§ç²¾åº¦åŠ£åŒ–<0.5%

**AWQ (Activation-aware Weight Quantization)** (2023):
- é‡è¦åº¦ã®é«˜ã„ãƒãƒ£ãƒãƒ«ã‚’ä¿è­·
- Activationçµ±è¨ˆã«åŸºã¥ãé‡å­åŒ–
- GPTQè¶…ãˆã‚‹ç²¾åº¦

#### Speculative Decoding

**DraftRetriever** (2024):
- N-gramæ¤œç´¢ã§Draftç”Ÿæˆ
- å¤–éƒ¨çŸ¥è­˜ãƒ™ãƒ¼ã‚¹æ´»ç”¨
- RAG+Speculativeã®èåˆ

**Predictive Decoding** (2024):
- ä¸¦åˆ—æ¤œè¨¼ãªã—ã€ç¢ºç‡äºˆæ¸¬ã®ã¿
- ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·å„ªå…ˆ (ãƒãƒƒãƒã‚µã‚¤ã‚º1)

**Multi-Draft** (2024):
- è¤‡æ•°Draftå€™è£œã‚’ä¸¦åˆ—ç”Ÿæˆ
- å—ç†ç‡å‘ä¸Š (but ãƒ¡ãƒ¢ãƒªå¢—)

#### KV-Cacheæœ€é©åŒ–

**ThinKV** [^13] (2024):
- æ¨è«–æ™‚ã®ã€Œæ€è€ƒãƒ‘ã‚¿ãƒ¼ãƒ³ã€æ¤œå‡º
- é‡è¦ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿Cacheä¿æŒ
- ãƒ¡ãƒ¢ãƒªå‰Šæ¸›50% + ç²¾åº¦ç¶­æŒ

**Cascade KV-Cache** (2024):
- å±¤ã”ã¨ã«Cacheç²¾åº¦ã‚’å¤‰ãˆã‚‹
- æµ…ã„å±¤INT4, æ·±ã„å±¤FP16
- ãƒ¡ãƒ¢ãƒªå‰Šæ¸›30%

#### Production Tools

**mistral.rs** (2024):
- Rustè£½é«˜é€Ÿæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³
- é‡å­åŒ–å¯¾å¿œ (GGUF/GGML)
- OpenAIäº’æ›API

**vLLM 0.3** (2024):
- FP8 KV-Cache
- Prefix Caching
- Multi-LoRAä¸¦åˆ—æ¨è«–

### 6.5 æ¨è–¦æ›¸ç±ãƒ»ãƒªã‚½ãƒ¼ã‚¹

#### æ›¸ç±

| ã‚¿ã‚¤ãƒˆãƒ« | è‘—è€… | å†…å®¹ | æ¨å¥¨åº¦ |
|:--------|:-----|:-----|:-------|
| Deep Learning | Goodfellow+ | åŸºç¤ç†è«– | â˜…â˜…â˜…â˜…â˜… |
| Dive into Deep Learning | Zhang+ | å®Ÿè£…é‡è¦– | â˜…â˜…â˜…â˜…â˜† |
| LLM Engineer's Handbook | - | Productionå®Ÿè·µ | â˜…â˜…â˜…â˜…â˜… |

#### ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹

**å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**:
- [vLLM Documentation](https://docs.vllm.ai/) â€” PagedAttentionå®Ÿè£…è©³ç´°
- [NVIDIA TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) â€” FP8/INT4é‡å­åŒ–
- [Hugging Face Optimum](https://huggingface.co/docs/optimum/) â€” é‡å­åŒ–ãƒ„ãƒ¼ãƒ«

**è«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤**:
- [Awesome-LLM-Inference](https://github.com/DefTruth/Awesome-LLM-Inference) â€” æ¨è«–æœ€é©åŒ–è«–æ–‡ã¾ã¨ã‚
- [Awesome-Quantization](https://github.com/Zhen-Dong/Awesome-Quantization-Papers) â€” é‡å­åŒ–è«–æ–‡ã¾ã¨ã‚

**ãƒ–ãƒ­ã‚°**:
- [vLLM Blog](https://blog.vllm.ai/) â€” PagedAttentionè§£èª¬
- [Databricks Mosaic AI Blog](https://www.databricks.com/blog/category/engineering/mosaic-ai) â€” Production tips
- [Hugging Face Blog](https://huggingface.co/blog) â€” æœ€æ–°æ‰‹æ³•è§£èª¬

### 6.6 æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ— â€” æœ¬è¬›ç¾©ä¿®äº†å¾Œã®å­¦ç¿’ãƒ‘ã‚¹

**æ¨è«–æœ€é©åŒ–ã‚’æ¥µã‚ã‚‹**:
1. vLLMã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰èª­è§£ (C++/CUDA)
2. TensorRT-LLMã§ç‹¬è‡ªã‚«ãƒ¼ãƒãƒ«å®Ÿè£…
3. è‡ªä½œé‡å­åŒ–æ‰‹æ³•ã®ç ”ç©¶ (NeurIPS/ICMLæŠ•ç¨¿)

**Productioné‹ç”¨ã‚’æ¥µã‚ã‚‹**:
1. Kubernetesã§ã®æ¨è«–ã‚¯ãƒ©ã‚¹ã‚¿æ§‹ç¯‰
2. Prometheus/Grafanaã§ç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
3. SLA 99.99%é”æˆã®ãŸã‚ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

**3è¨€èªçµ±åˆã‚’æ¥µã‚ã‚‹**:
1. Rust/Elixir/Juliaã§ãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯æ¨è«–ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰
2. FFIæœ€é©åŒ– (ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼è»¢é€)
3. åˆ†æ•£è¨“ç·´+æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆ

---

**ã‚´ãƒ¼ãƒ«**: æœ¬è¬›ç¾©ã®è¦ç‚¹ã‚’æ•´ç†ã—ã€æ¬¡ã®å­¦ç¿’ã¸ã¤ãªã’ã‚‹ã€‚

### 6.6 æœ¬è¬›ç¾©ã§å­¦ã‚“ã ã“ã¨

#### Part A: é‡å­åŒ–å®Œå…¨ç‰ˆ

1. **å¯¾ç§°é‡å­åŒ–**: $Q(w) = \text{round}(w/s)$, $s = \max(|w|) / (2^{b-1}-1)$
2. **éå¯¾ç§°é‡å­åŒ–**: $Q(w) = \text{round}(w/s + z)$, ã‚¼ãƒ­ç‚¹$z$ã§ç¯„å›²ã‚·ãƒ•ãƒˆ
3. **Per-Channelé‡å­åŒ–**: ãƒãƒ£ãƒãƒ«ã”ã¨ã®ã‚¹ã‚±ãƒ¼ãƒ« â†’ ç²¾åº¦å‘ä¸Š
4. **FP8 E4M3 vs E5M2**: ç²¾åº¦ vs å‹•çš„ç¯„å›²ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•
5. **KV-Cacheé‡å­åŒ–**: FP16â†’FP8ã§2å€ãƒ¡ãƒ¢ãƒªå‰Šæ¸›, perplexityåŠ£åŒ–<0.3%
6. **QAT vs PTQ**: å­¦ç¿’ã‚³ã‚¹ãƒˆ vs ç²¾åº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

#### Part B: è’¸ç•™ & Speculative Decoding

1. **Knowledge Distillation**: Soft targets $p_i(T) = \exp(z_i/T) / \sum_j \exp(z_j/T)$
2. **æ¸©åº¦$T$ã®åŠ¹æœ**: Dark knowledgeéœ²å‡º, ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã®æ±åŒ–æ€§èƒ½å‘ä¸Š
3. **Speculative Decoding**: Draft-Verifyä¸¦åˆ—æ¤œè¨¼, å—ç†ç¢ºç‡$\alpha = \min(1, p_p/p_q)$
4. **QuantSpec**: INT4 Draft + FP16 Target, å—ç†ç‡>90%, ~2.5å€é«˜é€ŸåŒ–

#### Part C: ğŸ¦€ Productionå“è³ªRust

1. **thiserror vs anyhow**: ãƒ©ã‚¤ãƒ–ãƒ©ãƒª vs ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
2. **tracing**: éšå±¤çš„ãƒ­ã‚°, JSONå‡ºåŠ›, ã‚¹ãƒ‘ãƒ³è¨­è¨ˆ
3. **Prometheusçµ±åˆ**: Counter/Histogram/Gauge, ãƒ¡ãƒˆãƒªã‚¯ã‚¹å…¬é–‹
4. **Property-based testing**: `proptest`ã§ãƒ©ãƒ³ãƒ€ãƒ å…¥åŠ›æ¤œè¨¼
5. **Fuzz testing**: `cargo-fuzz`ã§ç•°å¸¸å…¥åŠ›æ¢ç´¢

#### Part D: ğŸ”® Elixiræ¨è«–åˆ†æ•£

1. **ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°**: Round-Robin / Least Connections / Weighted / Adaptive
2. **Auto-Scaling**: ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹, Kubernetes HPAçµ±åˆ
3. **Circuit Breaker**: éšœå®³æ¤œçŸ¥â†’é®æ–­â†’Half-Openâ†’å¾©æ—§
4. **Bulkheadåˆ†é›¢**: ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«åˆ†é›¢, éšœå®³æ³¢åŠé˜²æ­¢
5. **ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼**: GenStageã§è‡ªå‹•ãƒ¬ãƒ¼ãƒˆèª¿æ•´
6. **SLA/SLOè¨­è¨ˆ**: Availability / Latency / Error Rate / Throughput

#### Part E: æ¨è«–ã‚µãƒ¼ãƒãƒ¼æœ€é©åŒ–

1. **PagedAttention**: KV-Cacheãƒ–ãƒ­ãƒƒã‚¯ç®¡ç†, Copy-on-Write, ãƒ¡ãƒ¢ãƒªåŠ¹ç‡4å€
2. **Mixed Precision**: FP16 forward + FP32 backward, Loss scaling
3. **Gradient Checkpointing**: ä¸­é–“æ´»æ€§åŒ–å†è¨ˆç®—, ãƒ¡ãƒ¢ãƒªå‰Šæ¸›50-70%

### 6.7 ã‚ˆãã‚ã‚‹è³ªå• (FAQ)

:::details Q1. INT4é‡å­åŒ–ã§ç²¾åº¦ãŒè½ã¡ãªã„ã®ã¯ãªãœï¼Ÿ

A. LLMã®é‡ã¿ã¯**ä½ãƒ©ãƒ³ã‚¯æ§‹é€ **ã‚’æŒã¤ãŸã‚ã€é‡å­åŒ–èª¤å·®ãŒå‡ºåŠ›ã«ä¸ãˆã‚‹å½±éŸ¿ãŒå°ã•ã„ã€‚åŠ ãˆã¦ã€Per-Channelé‡å­åŒ–ã§é‡è¦ãªãƒãƒ£ãƒãƒ«ã®ç²¾åº¦ã‚’ä¿è­·ã—ã¦ã„ã‚‹ã€‚å®Ÿéš›ã€Perplexityå¢—åŠ ã¯é€šå¸¸1-2%ç¨‹åº¦ã§ã€å¤šãã®ã‚¿ã‚¹ã‚¯ã§å½±éŸ¿ã¯ç„¡è¦–ã§ãã‚‹ã€‚

é‡è¦ãªã®ã¯**ã©ã“ã‚’é‡å­åŒ–ã™ã‚‹ã‹**:
- âœ… Weight: é‡å­åŒ–ã—ã‚„ã™ã„ (é™çš„)
- âœ… KV-Cache: é‡å­åŒ–ã—ã‚„ã™ã„ (ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã‚¹ã‚±ãƒ¼ãƒ«)
- âš ï¸ Activation: é‡å­åŒ–ã—ã«ãã„ (å‹•çš„, å¤–ã‚Œå€¤å¤šã„)
:::

:::details Q2. Speculative Decodingã¯ãªãœåˆ†å¸ƒã‚’ä¿å­˜ã™ã‚‹ã®ã‹ï¼Ÿ

A. Modified Rejection Samplingã‚’ä½¿ã†ãŸã‚ã€‚æ£„å´æ™‚ã«$p'(x) = \max(0, p(x) - q(x))$ã‹ã‚‰å†ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€**æ•°å­¦çš„ã«** $p(x)$ã¨å®Œå…¨ã«ä¸€è‡´ã™ã‚‹åˆ†å¸ƒãŒå¾—ã‚‰ã‚Œã‚‹ã€‚

ã“ã‚Œã¯MCMCã®Metropolis-Hastingsã¨åŒã˜åŸç†ã€‚å—ç†ç¢ºç‡$\alpha = \min(1, p/q)$ã¯ã€è©³ç´°ã¤ã‚Šåˆã„æ¡ä»¶ã‚’æº€ãŸã™ã€‚
:::

:::details Q3. ãªãœRustã§ã¯ãªãPythonã§MLã‚’æ›¸ã‹ãªã„ã®ã‹ï¼Ÿ

A. **å½¹å‰²åˆ†æ‹…**ãŒç­”ãˆã€‚
- **Python**: ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°, å®Ÿé¨“, ãƒ‡ãƒ¼ã‚¿åˆ†æ â†’ æŸ”è»Ÿæ€§
- **Rust**: ã‚«ãƒ¼ãƒãƒ«å®Ÿè£…, æ¨è«–ã‚µãƒ¼ãƒãƒ¼, FFI â†’ é€Ÿåº¦+å®‰å…¨æ€§
- **Julia**: è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ, æ•°å€¤è¨ˆç®— â†’ NumPy+é€Ÿåº¦
- **Elixir**: APIã‚µãƒ¼ãƒãƒ¼, åˆ†æ•£åˆ¶å¾¡ â†’ ä¸¦è¡Œæ€§+è€éšœå®³æ€§

æœ¬è¬›ç¾©ã¯**Productionæ¨è«–**ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã‚‹ãŸã‚ã€Rust/Elixirä¸­å¿ƒã€‚Pythonã¯ç ”ç©¶æ®µéšã§ä½¿ã„ã€æœ¬ç•ªã§ã¯ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«è¨€èªã«ç§»è¡Œã™ã‚‹ã®ãŒç¾å®Ÿçš„ã€‚
:::

:::details Q4. QuantSpecã®å—ç†ç‡>90%ã¯æœ¬å½“ã‹ï¼Ÿ

A. **æœ¬å½“**ã€‚ç†ç”±ã¯2ã¤:
1. Draft = Target ã®é‡å­åŒ–ç‰ˆ â†’ **åŒã˜ãƒ¢ãƒ‡ãƒ«** â†’ æ±ºå®šå¢ƒç•ŒãŒè¿‘ã„
2. INT4é‡å­åŒ–èª¤å·®ã¯$\sigma \approx 0.1$ (ç›¸å¯¾èª¤å·®12.5%) â†’ Softmaxå¾Œã®ç¢ºç‡æ¯”ã¯$\exp(\epsilon) \approx 1.1$ â†’ ã»ã¼1

Appleè«–æ–‡ [^1] ã®å®Ÿæ¸¬å€¤:
- LLaMA-7B: å—ç†ç‡92.3%
- LLaMA-13B: å—ç†ç‡91.8%
- LLaMA-70B: å—ç†ç‡90.5%

å¾“æ¥ã®Speculative (åˆ¥ãƒ¢ãƒ‡ãƒ«) ã¯60-80%ãªã®ã§ã€**20%ä»¥ä¸Šã®æ”¹å–„**ã€‚
:::

:::details Q5. Productionç’°å¢ƒã§Elixirã¯ç¾å®Ÿçš„ã‹ï¼Ÿ

A. **éå¸¸ã«ç¾å®Ÿçš„**ã€‚å®Ÿç¸¾:
- **WhatsApp**: 10å„„ãƒ¦ãƒ¼ã‚¶ãƒ¼, 50ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§é‹ç”¨ (Erlang/Elixir)
- **Discord**: æ•°å„„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸/æ—¥, Elixirã§å‡¦ç†
- **Pinterest**: é€šçŸ¥ã‚·ã‚¹ãƒ†ãƒ ã‚’Elixirã§æ§‹ç¯‰

Elixirã®å¼·ã¿:
- ä¸¦è¡Œæ€§: BEAMã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãŒ100ä¸‡ãƒ—ãƒ­ã‚»ã‚¹ä¸¦åˆ—å®Ÿè¡Œ
- è€éšœå®³æ€§: Let it crash â†’ Supervisorè‡ªå‹•å¾©æ—§
- ãƒ›ãƒƒãƒˆã‚³ãƒ¼ãƒ‰ã‚¹ãƒ¯ãƒƒãƒ—: ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ ãªã—æ›´æ–°

**ãŸã ã—**: æ•°å€¤è¨ˆç®—ã¯Rust/Juliaã«ä»»ã›ã€Elixirã¯**åˆ¶å¾¡å±¤**ã«å¾¹ã™ã‚‹ã€‚
:::

### 6.8 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« (æœ¬è¬›ç¾©å¾©ç¿’ãƒ—ãƒ©ãƒ³)

| Day | å†…å®¹ | æ™‚é–“ | ã‚´ãƒ¼ãƒ« |
|:---|:-----|:-----|:-------|
| **Day 1** | Part A-B æ•°å¼ | 3h | é‡å­åŒ–ãƒ»è’¸ç•™ãƒ»Specæ•°å¼å°å‡º |
| | Zone 3 Part A-B å®Œå…¨èª­è§£ | | Boss Battleä¸¡æ–¹è§£ã |
| | æ•°å¼ãƒãƒ¼ãƒˆä½œæˆ | | è‡ªåŠ›ã§å†å°å‡ºã§ãã‚‹ |
| **Day 2** | Part C-D å®Ÿè£… | 3h | Rust/Elixirå®Ÿè£…å®Œæˆ |
| | Zone 3 Part C-D + Zone 4 | | Productionå“è³ªã‚³ãƒ¼ãƒ‰æ›¸ã |
| | 3è¨€èªå®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ | | çµ±åˆã‚·ã‚¹ãƒ†ãƒ å‹•ä½œç¢ºèª |
| **Day 3** | Part E + å®Ÿé¨“ | 2h | æœ€é©åŒ–+æ¤œè¨¼ |
| | Zone 3 Part E + Zone 5 | | ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°å®Ÿè·µ |
| | é‡å­åŒ–ç²¾åº¦æ¸¬å®š | | ç†è«–å€¤ã¨å®Ÿæ¸¬å€¤æ¯”è¼ƒ |
| **Day 4** | æœ€æ–°ç ”ç©¶ + çµ±åˆ | 2h | SOTAè«–æ–‡ç†è§£ |
| | Zone 6 è«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ | | 2024-2026å‹•å‘æŠŠæ¡ |
| | è‡ªåˆ†ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹è¨­è¨ˆ | | æœ€é©æ‰‹æ³•é¸æŠ |

**ç´¯è¨ˆå­¦ç¿’æ™‚é–“**: 10æ™‚é–“ (1æ—¥2.5æ™‚é–“ Ã— 4æ—¥)

### 6.9 æ¬¡å›äºˆå‘Š: ç¬¬27å› è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰

ç¬¬27å›ã§ã¯ã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®**å®šé‡è©•ä¾¡**ã‚’å­¦ã¶:
- FID / IS / LPIPS å®Œå…¨å®Ÿè£…
- çµ±è¨ˆæ¤œå®šçµ±åˆ (tæ¤œå®š / Wilcoxon)
- è‡ªå‹•ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚·ã‚¹ãƒ†ãƒ  (Rust/Julia)
- A/Bãƒ†ã‚¹ãƒˆè¨­è¨ˆ (ç¬¬25å›å› æœæ¨è«–ã®å¿œç”¨)
- Perplexity / BLEU / ROUGE å®Œå…¨ç‰ˆ
- Human Evaluation ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

**æ¥ç¶š**:
- ç¬¬26å›ã§æ¨è«–ã‚’æœ€é©åŒ–ã—ãŸ â†’ ç¬¬27å›ã§ã€Œã©ã‚Œã ã‘è‰¯ããªã£ãŸã‹ã€ã‚’å®šé‡è©•ä¾¡
- å› æœæ¨è«–(ç¬¬25å›) + è©•ä¾¡æŒ‡æ¨™(ç¬¬27å›) = Production A/Bãƒ†ã‚¹ãƒˆã®å®Œå…¨ç‰ˆ

---

### 6.11 ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **æœ€é©åŒ–ã®çµ‚ã‚ã‚Šã¯ã©ã“ã‹ï¼Ÿç²¾åº¦ã¨é€Ÿåº¦ã®å¢ƒç•Œç·šã¯ï¼Ÿ**

INT4ã§ç²¾åº¦90%ä¿æŒã€‚INT2ã§70%ã€‚INT1 (binary) ã§20%ã€‚

**å•ã„1**: ã©ã“ã¾ã§å‰Šã‚Œã°ã€Œã‚‚ã¯ã‚„åˆ¥ã®ãƒ¢ãƒ‡ãƒ«ã€ãªã®ã‹ï¼Ÿ90%ã®ç²¾åº¦ä¿æŒã¯ã€ŒåŒã˜ãƒ¢ãƒ‡ãƒ«ã€ã¨è¨€ãˆã‚‹ã®ã‹ï¼Ÿ

**å•ã„2**: Speculative Decodingã¯ã€Œé€Ÿåº¦ã®ãŸã‚ã®è¿‘ä¼¼ã€ã§ã¯ãªãã€Œåˆ†å¸ƒã‚’å®Œå…¨ä¿å­˜ã€ã™ã‚‹ã€‚ãªã‚‰ã°**ç†è«–çš„ã«ã¯ç„¡é™ã«é«˜é€ŸåŒ–ã§ãã‚‹**ã¯ãšã ãŒã€ãªãœå®Ÿéš›ã¯2-3å€ã§æ­¢ã¾ã‚‹ã®ã‹ï¼Ÿ

**å•ã„3**: Productionã§99.99% SLAã‚’é”æˆã™ã‚‹ã‚³ã‚¹ãƒˆã¯ã€99.9%ã®**10å€**ã‹ã‹ã‚‹(çµŒé¨“å‰‡)ã€‚æœ€å¾Œã®0.09%ã®ãŸã‚ã«10å€æ‰•ã†ä¾¡å€¤ã¯ã‚ã‚‹ã®ã‹ï¼Ÿ

**å•ã„4**: Elixirã®"Let it crash"å“²å­¦ã¯ã€Œéšœå®³ã‚’å—ã‘å…¥ã‚Œã‚‹ã€ã“ã¨ã€‚Rustã®"Zero-cost abstraction"ã¯ã€Œéšœå®³ã‚’é˜²ãã€ã“ã¨ã€‚**çœŸé€†ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒãªãœä¸¡æ–¹ã¨ã‚‚æ­£ã—ã„ã®ã‹ï¼Ÿ**

**å•ã„5**: QuantSpecã¯INT4 Draftã§å—ç†ç‡>90%ã‚’é”æˆã—ãŸã€‚ãªã‚‰ã°INT2 Draftã§ã‚‚å—ç†ç‡>70%ã„ã‘ã‚‹ã¯ãšã€‚**ãªãœèª°ã‚‚ã‚„ã‚‰ãªã„ã®ã‹ï¼Ÿ** (ãƒ’ãƒ³ãƒˆ: ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢)

**è­°è«–ãƒã‚¤ãƒ³ãƒˆ**:
- æœ€é©åŒ–ã¯ã€Œæ€§èƒ½å‘ä¸Šã€ã§ã¯ãªãã€Œãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã®é¸æŠã€ã§ã‚ã‚‹
- Productionã¯ã€Œå‹•ãã€ã¨ã€Œå£Šã‚Œãªã„ã€ãŒåŒã˜ãã‚‰ã„é‡è¦
- 3è¨€èªçµ±åˆã¯ã€Œ1è¨€èªã§å…¨ã¦ã‚„ã‚‹ã€ã‚ˆã‚Š**æœ¬è³ªçš„ã«å„ªã‚Œã¦ã„ã‚‹**ç†ç”±

:::message
**é€²æ—: 100% å®Œäº†** ğŸ‰ è¬›ç¾©å®Œèµ°ï¼
:::

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Apple Machine Learning Research (2025). "QuantSpec: Self-Speculative Decoding with Hierarchical Quantized KV Cache".
@[card](https://machinelearning.apple.com/research/quantspec)

[^2]: arXiv:2502.01070 (2025). "An Investigation of FP8 Across Accelerators for LLM Inference".
@[card](https://arxiv.org/abs/2502.01070)

[^3]: Hinton, G., Vinyals, O., & Dean, J. (2015). "Distilling the Knowledge in a Neural Network". arXiv:1503.02531.
@[card](https://arxiv.org/abs/1503.02531)

[^4]: Leviathan, Y., Kalman, M., & Matias, Y. (2023). "Fast Inference from Transformers via Speculative Decoding". arXiv:2211.17192.
@[card](https://arxiv.org/abs/2211.17192)

[^5]: arXiv:2411.06084 (2024). "Optimizing Large Language Models through Quantization: A Comparative Analysis of PTQ and QAT Techniques".
@[card](https://arxiv.org/abs/2411.06084)

[^6]: Kwon, W., Li, Z., Zhuang, S., et al. (2023). "Efficient Memory Management for Large Language Model Serving with PagedAttention". arXiv:2309.06180.
@[card](https://arxiv.org/abs/2309.06180)

[^7]: Bengio, Y., LÃ©onard, N., & Courville, A. (2013). "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation". arXiv:1308.3432.
@[card](https://arxiv.org/abs/1308.3432)

[^8]: Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter". arXiv:1910.01108.
@[card](https://arxiv.org/abs/1910.01108)

[^9]: GreptimeDB (2024). "Error Handling for Large Rust Projects - Best Practice in GreptimeDB".
@[card](https://www.greptime.com/blogs/2024-05-07-error-rust)

[^10]: Rust Observability (2026). "Rust Observability: Logging, Tracing, and Metrics with OpenTelemetry and Tokio".
@[card](https://dasroot.net/posts/2026/01/rust-observability-opentelemetry-tokio/)

[^11]: Prometheus Documentation (2024). "Prometheus - Monitoring system & time series database".
@[card](https://prometheus.io/docs/introduction/overview/)

[^12]: Han, S., Mao, H., & Dally, W. J. (2015). "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding". arXiv:1510.00149.
@[card](https://arxiv.org/abs/1510.00149)

[^13]: arXiv:2510.01290 (2024). "ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning Models".
@[card](https://arxiv.org/abs/2510.01290)

### æ•™ç§‘æ›¸

- Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press. [https://www.deeplearningbook.org/](https://www.deeplearningbook.org/)
- Zhang, A., Lipton, Z. C., Li, M., & Smola, A. J. (2023). *Dive into Deep Learning*. [https://d2l.ai/](https://d2l.ai/)
- Boyd, S., & Vandenberghe, L. (2004). *Convex Optimization*. Cambridge University Press.

### ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹

- vLLM Documentation: [https://docs.vllm.ai/](https://docs.vllm.ai/)
- NVIDIA TensorRT-LLM: [https://github.com/NVIDIA/TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)
- Hugging Face Optimum: [https://huggingface.co/docs/optimum/](https://huggingface.co/docs/optimum/)
- Awesome-LLM-Inference: [https://github.com/DefTruth/Awesome-LLM-Inference](https://github.com/DefTruth/Awesome-LLM-Inference)
- Rust Error Handling Guide 2025: [https://markaicode.com/rust-error-handling-2025-guide/](https://markaicode.com/rust-error-handling-2025-guide/)

---

## è¨˜æ³•è¦ç´„

| è¨˜å· | æ„å‘³ | ä¾‹ |
|:-----|:-----|:---|
| $Q(w)$ | é‡å­åŒ–é–¢æ•° | $Q(w) = \text{round}(w/s)$ |
| $s$ | ã‚¹ã‚±ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ | $s = \max(\|w\|) / 127$ (INT8) |
| $z$ | ã‚¼ãƒ­ç‚¹ (éå¯¾ç§°é‡å­åŒ–) | $z = -\text{round}(w_{\min}/s)$ |
| $b$ | ãƒ“ãƒƒãƒˆå¹… | $b=4$ (INT4), $b=8$ (INT8) |
| $p_T(T)$ | æ¸©åº¦$T$ã®Softmax | $p_i(T) = \exp(z_i/T) / \sum_j \exp(z_j/T)$ |
| $\alpha$ | å—ç†ç¢ºç‡ | $\alpha = \min(1, p_p(x) / p_q(x))$ |
| $\text{EWMA}_t$ | æŒ‡æ•°ç§»å‹•å¹³å‡ | $\alpha L_t + (1-\alpha) \text{EWMA}_{t-1}$ |
| SLA | Service Level Agreement | é¡§å®¢ã¨ã®å¥‘ç´„ |
| SLO | Service Level Objective | å†…éƒ¨ç›®æ¨™ (SLAé”æˆã®ãŸã‚ã®ä½™è£•) |
| SLI | Service Level Indicator | æ¸¬å®šå¯èƒ½ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ |
| FP8-E4M3 | 8-bit float (4-bit exp, 3-bit mantissa) | ç¯„å›² $\pm 448$, ç²¾åº¦é«˜ |
| FP8-E5M2 | 8-bit float (5-bit exp, 2-bit mantissa) | ç¯„å›² $\pm 57344$, ç¯„å›²åºƒ |

**ç¶™ç¶šè¨˜æ³•** (Course I-II-IIIã§çµ±ä¸€):
- $\mathcal{L}$: æå¤±é–¢æ•°
- $\theta$: ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- $\mathbb{E}[\cdot]$: æœŸå¾…å€¤
- $D_\text{KL}(p \| q)$: KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹
- $\nabla_\theta$: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$\theta$ã«é–¢ã™ã‚‹å‹¾é…

---

:::message
**ğŸ† ç¬¬26å›ã‚³ãƒ³ãƒ—ãƒªãƒ¼ãƒˆï¼** æ¨è«–æœ€é©åŒ–ã¨Productionå“è³ªè¨­è¨ˆã‚’å®Œå…¨ç¿’å¾—ã—ã¾ã—ãŸã€‚æ¬¡å›ã¯è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰ã§ã€æœ€é©åŒ–ã®åŠ¹æœã‚’å®šé‡çš„ã«æ¸¬å®šã—ã¾ã™ã€‚
:::

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚

