---
title: "ç¬¬25å›: å› æœæ¨è«–: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸ”—"
type: "tech"
topics: ["machinelearning", "causalinference", "julia", "statistics", "experiment"]
published: true
---

# ç¬¬25å›: å› æœæ¨è«– â€” ç›¸é–¢ã¯å› æœã§ã¯ãªã„ã€æ­£ã—ã„åŠ¹æœæ¸¬å®šã®æŠ€æ³•

> **ç›¸é–¢é–¢ä¿‚ãŒã‚ã£ã¦ã‚‚å› æœé–¢ä¿‚ã¨ã¯é™ã‚‰ãªã„ã€‚æ­£ã—ã„å› æœåŠ¹æœã‚’æ¸¬å®šã—ã€æ„æ€æ±ºå®šã‚’èª¤ã‚‰ãªã„ãŸã‚ã®å³å¯†ãªç†è«–ã¨å®Ÿè£…ã‚’ç¿’å¾—ã™ã‚‹ã€‚**

ç¬¬24å›ã§çµ±è¨ˆã®åŸºç¤ãŒå›ºã¾ã£ãŸã€‚ã ãŒç›¸é–¢ã¯å› æœã§ã¯ãªã„ã€‚ã‚¢ã‚¤ã‚¹ã‚¯ãƒªãƒ¼ãƒ å£²ä¸Šã¨æººæ­»è€…æ•°ã«ç›¸é–¢ãŒã‚ã£ã¦ã‚‚ã€ã‚¢ã‚¤ã‚¹ã‚¯ãƒªãƒ¼ãƒ ãŒæººæ­»ã‚’å¼•ãèµ·ã“ã™ã‚ã‘ã§ã¯ãªã„ã€‚çœŸã®å› æœåŠ¹æœã‚’æ¸¬å®šã™ã‚‹ã«ã¯ã€**äº¤çµ¡**ã‚’åˆ¶å¾¡ã—ã€**é¸æŠãƒã‚¤ã‚¢ã‚¹**ã‚’æ’é™¤ã—ã€**åå®Ÿä»®æƒ³**ã‚’æ­£ã—ãæ¨å®šã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚

æœ¬è¬›ç¾©ã§ã¯ã€Rubinå› æœãƒ¢ãƒ‡ãƒ«ï¼ˆæ½œåœ¨çš„çµæœãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼‰ã¨Pearlå› æœç†è«–ï¼ˆæ§‹é€ å› æœãƒ¢ãƒ‡ãƒ«ãƒ»do-æ¼”ç®—ï¼‰ã®2å¤§ç†è«–ã‚’å®Œå…¨ç¿’å¾—ã—ã€å‚¾å‘ã‚¹ã‚³ã‚¢ãƒ»æ“ä½œå¤‰æ•°æ³•ãƒ»RDDãƒ»DiDã¨ã„ã£ãŸå®Ÿè·µæ‰‹æ³•ã‚’ã€æ•°å¼ã‹ã‚‰Juliaå®Ÿè£…ã¾ã§ä¸€è²«ã—ã¦å­¦ã¶ã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2024-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚
:::

```mermaid
graph TD
    A["ğŸ“Š è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿"] --> B["ğŸ¯ å› æœåŠ¹æœ?"]
    B --> C["âŒ å˜ç´”æ¯”è¼ƒ<br/>äº¤çµ¡ãƒã‚¤ã‚¢ã‚¹"]
    B --> D["âœ… å› æœæ¨è«–<br/>ãƒã‚¤ã‚¢ã‚¹é™¤å»"]
    D --> E["ğŸ§® Rubin/Pearlç†è«–"]
    D --> F["ğŸ”§ å‚¾å‘ã‚¹ã‚³ã‚¢/IV/RDD/DiD"]
    E & F --> G["âœ¨ æ­£ã—ã„åŠ¹æœæ¸¬å®š"]
    style C fill:#ffebee
    style G fill:#c8e6c9
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | ç™ºå±•ã‚¾ãƒ¼ãƒ³ | 20åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 7 | æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” å‚¾å‘ã‚¹ã‚³ã‚¢ã§äº¤çµ¡é™¤å»

**ã‚´ãƒ¼ãƒ«**: å› æœæ¨è«–ã®å¨åŠ›ã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ã€‚

è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å› æœåŠ¹æœã‚’æ¨å®šã™ã‚‹æœ€ã‚‚ãƒãƒ”ãƒ¥ãƒ©ãƒ¼ãªæ‰‹æ³•ã®1ã¤ã€å‚¾å‘ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°ã‚’3è¡Œã§å‹•ã‹ã™ã€‚

```julia
using Statistics, LinearAlgebra

# Simulated observational data
# Treatment D: 1=treated, 0=control
# Confounders X: [age, income]
# Outcome Y: health improvement score
function generate_observational_data(n::Int=1000)
    X = randn(n, 2)  # confounders: age, income (standardized)
    # Treatment assignment depends on confounders (selection bias)
    propensity = 1 ./ (1 .+ exp.(-X[:, 1] - 0.5 * X[:, 2]))
    D = rand(n) .< propensity  # biased treatment assignment

    # True causal effect: treatment adds +2 to outcome
    # Outcome also depends on confounders (confounding)
    Y = 2 * D .+ X[:, 1] + 0.5 * X[:, 2] + randn(n) * 0.5

    return D, X, Y, propensity
end

# Naive comparison (WRONG - confounded)
D, X, Y, true_e = generate_observational_data(1000)
naive_ate = mean(Y[D]) - mean(Y[.!D])
println("Naive ATE (confounded): $(round(naive_ate, digits=3))")

# Propensity score matching (CORRECT)
function propensity_score_matching(D, X, Y)
    # Estimate propensity scores e(X) = P(D=1|X)
    e_hat = 1 ./ (1 .+ exp.(-X[:, 1] - 0.5 * X[:, 2]))  # simplified: use logistic regression

    # Inverse Probability Weighting (IPW) estimator
    # ATE = E[Y(1) - Y(0)] = E[D*Y/e(X)] - E[(1-D)*Y/(1-e(X))]
    weights_treated = D ./ e_hat
    weights_control = (1 .- D) ./ (1 .- e_hat)

    ate_ipw = mean(weights_treated .* Y) - mean(weights_control .* Y)
    return ate_ipw
end

ate_corrected = propensity_score_matching(D, X, Y)
println("IPW ATE (debiased): $(round(ate_corrected, digits=3))")
println("True ATE: 2.0")
```

å‡ºåŠ›:
```
Naive ATE (confounded): 2.847
IPW ATE (debiased): 2.012
True ATE: 2.0
```

**3è¡Œã§è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å› æœåŠ¹æœã‚’æ­£ã—ãæ¨å®šã—ãŸã€‚**

- **Naiveæ¯”è¼ƒ**: å‡¦ç½®ç¾¤ã¨å¯¾ç…§ç¾¤ã‚’å˜ç´”ã«æ¯”è¼ƒ â†’ 2.847ï¼ˆ**ãƒã‚¤ã‚¢ã‚¹ã‚ã‚Š**ï¼‰
- **IPWæ¨å®š**: å‚¾å‘ã‚¹ã‚³ã‚¢ã§é‡ã¿ä»˜ã‘ â†’ 2.012ï¼ˆ**çœŸå€¤2.0ã«è¿‘ã„**ï¼‰

ã“ã®èƒŒå¾Œã«ã‚ã‚‹ç†è«–:

$$
\begin{aligned}
\text{Naive ATE} &= \mathbb{E}[Y \mid D=1] - \mathbb{E}[Y \mid D=0] \quad \text{(confounded)} \\
\text{True ATE} &= \mathbb{E}[Y^1 - Y^0] \quad \text{(potential outcomes)} \\
\text{IPW ATE} &= \mathbb{E}\left[\frac{D \cdot Y}{e(X)}\right] - \mathbb{E}\left[\frac{(1-D) \cdot Y}{1-e(X)}\right] \quad \text{(debiased)}
\end{aligned}
$$

ã“ã“ã§ $e(X) = P(D=1 \mid X)$ ã¯**å‚¾å‘ã‚¹ã‚³ã‚¢**ï¼ˆpropensity scoreï¼‰ã€$Y^1, Y^0$ ã¯**æ½œåœ¨çš„çµæœ**ï¼ˆpotential outcomesï¼‰ã ã€‚ã“ã®å¼ã‚’Rubinã¨Pearlã®ç†è«–ã‹ã‚‰å®Œå…¨å°å‡ºã—ã¦ã„ãã€‚

:::message
**é€²æ—: 3% å®Œäº†** å› æœæ¨è«–ã®å¨åŠ›ã‚’ä½“æ„Ÿã—ãŸã€‚ã“ã“ã‹ã‚‰ç›¸é–¢vså› æœã®åŸºç¤â†’Rubin/Pearlç†è«–â†’å®Ÿè·µæ‰‹æ³•ã‚’ç¿’å¾—ã™ã‚‹ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” å› æœæ¨è«–ã®4ã¤ã®é¡”

### 1.1 ç›¸é–¢ vs å› æœ â€” ãªãœå˜ç´”æ¯”è¼ƒã§ã¯å¤±æ•—ã™ã‚‹ã®ã‹

#### 1.1.1 ã‚¢ã‚¤ã‚¹ã‚¯ãƒªãƒ¼ãƒ ã¨æººæ­» â€” å…¸å‹çš„ãªäº¤çµ¡ã®ä¾‹

```julia
# å­£ç¯€ã‚’äº¤çµ¡å› å­ã¨ã™ã‚‹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
function icecream_drowning_simulation()
    months = 1:12
    temperature = 15 .+ 10 * sin.(2Ï€ * (months .- 3) / 12)  # seasonal temperature

    # Ice cream sales driven by temperature
    icecream_sales = 100 .+ 50 * (temperature .- 15) / 10 + randn(12) * 5

    # Drowning incidents driven by temperature (more swimming)
    drownings = 10 .+ 8 * (temperature .- 15) / 10 + randn(12) * 2

    # Correlation
    corr_value = cor(icecream_sales, drownings)
    println("Correlation(Icecream, Drowning): $(round(corr_value, digits=3))")

    # But causal effect is ZERO (temperature is the confounder)
    # If we control for temperature:
    residual_icecream = icecream_sales - 50 * (temperature .- 15) / 10
    residual_drowning = drownings - 8 * (temperature .- 15) / 10
    partial_corr = cor(residual_icecream, residual_drowning)
    println("Partial correlation (control temp): $(round(partial_corr, digits=3))")

    return temperature, icecream_sales, drownings
end

temp, ice, drown = icecream_drowning_simulation()
```

å‡ºåŠ›:
```
Correlation(Icecream, Drowning): 0.923
Partial correlation (control temp): -0.089
```

**å¼·ã„ç›¸é–¢(0.923)ãŒã‚ã£ã¦ã‚‚ã€æ¸©åº¦ã‚’åˆ¶å¾¡ã™ã‚‹ã¨ç›¸é–¢ã¯æ¶ˆãˆã‚‹ã€‚** ã“ã‚ŒãŒäº¤çµ¡ã®å…¸å‹ä¾‹ã ã€‚

```mermaid
graph LR
    T["ğŸŒ¡ï¸ æ¸©åº¦<br/>(äº¤çµ¡å› å­)"] --> I["ğŸ¦ ã‚¢ã‚¤ã‚¹å£²ä¸Š"]
    T --> D["ğŸ’€ æººæ­»è€…æ•°"]
    I -.ç›¸é–¢ 0.92.-> D
    style T fill:#fff3e0
    style I fill:#e3f2fd
    style D fill:#ffebee
```

#### 1.1.2 Simpson's Paradox â€” é›†è¨ˆã™ã‚‹ã¨é€†è»¢ã™ã‚‹

Simpson's Paradox [^8] ã¯ã€å…¨ä½“ã§ã®å‚¾å‘ã¨éƒ¨åˆ†é›†å›£ã§ã®å‚¾å‘ãŒé€†è»¢ã™ã‚‹ç¾è±¡ã ã€‚

| ç—…é™¢ | å‡¦ç½®ç¾¤ | å¯¾ç…§ç¾¤ | å‡¦ç½®åŠ¹æœ |
|:-----|:-------|:-------|:---------|
| **ç—…é™¢A** | ç”Ÿå­˜ç‡ 50/100 = 50% | ç”Ÿå­˜ç‡ 40/100 = 40% | **+10%** (å‡¦ç½®ãŒæœ‰åŠ¹) |
| **ç—…é™¢B** | ç”Ÿå­˜ç‡ 90/100 = 90% | ç”Ÿå­˜ç‡ 85/100 = 85% | **+5%** (å‡¦ç½®ãŒæœ‰åŠ¹) |
| **å…¨ä½“** | ç”Ÿå­˜ç‡ 140/200 = 70% | ç”Ÿå­˜ç‡ 125/200 = 62.5% | **+7.5%** (å‡¦ç½®ãŒæœ‰åŠ¹) |

ä¸€è¦‹æ­£ã—ãã†ã ãŒã€**é‡ç—‡æ‚£è€…ãŒç—…é™¢Bã«é›†ä¸­**ã—ã¦ã„ãŸã‚‰ï¼Ÿ

| ç—…é™¢ | å‡¦ç½®ç¾¤ï¼ˆé‡ç—‡ç‡ï¼‰ | å¯¾ç…§ç¾¤ï¼ˆé‡ç—‡ç‡ï¼‰ |
|:-----|:----------------|:----------------|
| **ç—…é™¢A** | 50/100 (è»½ç—‡90%) | 40/100 (è»½ç—‡80%) |
| **ç—…é™¢B** | 90/100 (é‡ç—‡80%) | 85/100 (é‡ç—‡70%) |

é‡ç—‡åº¦ã‚’**äº¤çµ¡å› å­**ã¨ã—ã¦åˆ¶å¾¡ã™ã‚‹ã¨ã€å‡¦ç½®åŠ¹æœãŒé€†è»¢ã™ã‚‹å¯èƒ½æ€§ã™ã‚‰ã‚ã‚‹ã€‚Pearl [^8] ã¯ã“ã‚Œã‚’**do-æ¼”ç®—**ã§è§£æ±ºã™ã‚‹:

$$
P(\text{survival} \mid do(\text{treatment})) \neq P(\text{survival} \mid \text{treatment})
$$

å·¦è¾ºã¯**ä»‹å…¥**ï¼ˆå¼·åˆ¶çš„ã«å‡¦ç½®ã‚’ä¸ãˆã‚‹ï¼‰ã€å³è¾ºã¯**è¦³æ¸¬**ï¼ˆå‡¦ç½®ã‚’å—ã‘ãŸäººã‚’è¦‹ã‚‹ï¼‰ã€‚ã“ã®é•ã„ãŒå› æœæ¨è«–ã®æ ¸å¿ƒã ã€‚

#### 1.1.3 é¸æŠãƒã‚¤ã‚¢ã‚¹ â€” èª°ãŒå‡¦ç½®ã‚’å—ã‘ã‚‹ã‹

```julia
# Selection bias simulation
function selection_bias_simulation()
    n = 1000
    # True ability (unobserved confounder)
    ability = randn(n)

    # High-ability people more likely to get treatment
    treatment_prob = 1 ./ (1 .+ exp.(-ability))
    D = rand(n) .< treatment_prob

    # Outcome depends on BOTH ability and treatment
    # True treatment effect = +1.0
    Y = 1.0 * D .+ 2.0 * ability + randn(n) * 0.5

    # Naive comparison
    naive = mean(Y[D]) - mean(Y[.!D])

    # Selection bias = difference in ability
    ability_diff = mean(ability[D]) - mean(ability[.!D])

    println("Naive treatment effect: $(round(naive, digits=3))")
    println("True treatment effect: 1.0")
    println("Selection bias (ability diff): $(round(2.0 * ability_diff, digits=3))")

    return D, Y, ability
end

D, Y, ability = selection_bias_simulation()
```

å‡ºåŠ›:
```
Naive treatment effect: 2.987
True treatment effect: 1.0
Selection bias (ability diff): 1.994
```

**å‡¦ç½®ã‚’å—ã‘ãŸäººãŒå…ƒã€…å„ªç§€ã ã£ãŸã‚‰ã€åŠ¹æœãŒéå¤§è©•ä¾¡ã•ã‚Œã‚‹ã€‚** ã“ã‚ŒãŒé¸æŠãƒã‚¤ã‚¢ã‚¹ã ã€‚

### 1.2 å› æœæ¨è«–ã®4ã¤ã®ä¸»è¦ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

| ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ | æå”±è€… | ã‚³ã‚¢æ¦‚å¿µ | é©ç”¨å ´é¢ |
|:----------|:------|:---------|:---------|
| **æ½œåœ¨çš„çµæœ** | Rubin (1974) [^2] | $Y^1, Y^0$, SUTVA, ATE | RCT, å‚¾å‘ã‚¹ã‚³ã‚¢, ãƒãƒƒãƒãƒ³ã‚° |
| **æ§‹é€ å› æœãƒ¢ãƒ‡ãƒ«** | Pearl (2009) [^1] | DAG, do-æ¼”ç®—, ãƒãƒƒã‚¯ãƒ‰ã‚¢åŸºæº– | è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿, è¤‡é›‘ãªå› æœæ§‹é€  |
| **æ“ä½œå¤‰æ•°æ³•** | Wright (1928) | IV, 2SLS, LATE | å†…ç”Ÿæ€§, ãƒ©ãƒ³ãƒ€ãƒ åŒ–ã§ããªã„å ´åˆ |
| **å›å¸°ä¸é€£ç¶š** | Thistlethwaite (1960) | ã‚«ãƒƒãƒˆã‚ªãƒ•, å±€æ‰€ãƒ©ãƒ³ãƒ€ãƒ åŒ– | æ”¿ç­–è©•ä¾¡, é–¾å€¤ãƒ™ãƒ¼ã‚¹ã®å‡¦ç½® |

ã“ã®4ã¤ã‚’å®Œå…¨ç¿’å¾—ã™ã‚Œã°ã€**ã‚ã‚‰ã‚†ã‚‹å› æœæ¨è«–è«–æ–‡ãŒèª­ã‚ã‚‹**ã€‚

### 1.3 å› æœæ¨è«–ã®æ­´å² â€” Fisher ã‹ã‚‰ Pearl/Rubin ã¸

```mermaid
timeline
    title å› æœæ¨è«–ã®é€²åŒ–
    1920s : Fisher RCT<br/>ãƒ©ãƒ³ãƒ€ãƒ åŒ–å®Ÿé¨“
    1960s : Campbell æº–å®Ÿé¨“<br/>RDDç™ºæ˜
    1970s : Rubin æ½œåœ¨çš„çµæœ<br/>å‚¾å‘ã‚¹ã‚³ã‚¢æå”±
    1980s : Heckman é¸æŠãƒ¢ãƒ‡ãƒ«<br/>ãƒãƒ¼ãƒ™ãƒ«è³ 2000
    1990s-2000s : Pearl DAG+do-æ¼”ç®—<br/>Turingè³ 2011
    2010s : Athey/Imbens MLÃ—å› æœ<br/>Causal Forest/DML
    2020s : æ‹¡æ•£Ã—çµ±åˆ<br/>Staggered DiD/Sensitivity
```

:::message
**é€²æ—: 10% å®Œäº†** ç›¸é–¢vså› æœã®ç½ ã‚’ä½“æ„Ÿã—ãŸã€‚ã“ã“ã‹ã‚‰Rubin/Pearlç†è«–ã®å®Œå…¨å°å‡ºã«å…¥ã‚‹ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœå› æœæ¨è«–ãŒå¿…é ˆãªã®ã‹

### 2.1 æœ¬ã‚·ãƒªãƒ¼ã‚ºã«ãŠã‘ã‚‹ä½ç½®ã¥ã‘

```mermaid
graph TD
    C1["Course I<br/>æ•°å­¦åŸºç¤"] --> C2["Course II<br/>ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«–"]
    C2 --> C3["Course III<br/>ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç¤¾ä¼šå®Ÿè£…"]
    C3 --> L23["ç¬¬23å›<br/>Fine-tuning"]
    C3 --> L24["ç¬¬24å›<br/>çµ±è¨ˆå­¦"]
    L24 --> L25["ç¬¬25å›<br/>ğŸ”—å› æœæ¨è«–<br/>(ä»Šå›)"]
    L25 --> L26["ç¬¬26å›<br/>æ¨è«–æœ€é©åŒ–"]
    L25 --> L27["ç¬¬27å›<br/>è©•ä¾¡"]
    style L25 fill:#c8e6c9
```

**Course IIIã®ç†è«–ç·¨æœ€çµ‚ç« ã€‚** çµ±è¨ˆå­¦(ç¬¬24å›)ã§ä»®èª¬æ¤œå®šãƒ»ãƒ™ã‚¤ã‚ºçµ±è¨ˆã‚’å­¦ã³ã€æœ¬è¬›ç¾©ã§å› æœåŠ¹æœæ¸¬å®šã‚’å®Œæˆã•ã›ã‚‹ã€‚æ¬¡å›ã‹ã‚‰ã¯æ¨è«–æœ€é©åŒ–ãƒ»è©•ä¾¡ãƒ»RAGãƒ»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨å®Ÿè·µãƒ•ã‚§ãƒ¼ã‚ºã«å…¥ã‚‹ã€‚

### 2.2 å› æœæ¨è«–ãŒå¿…é ˆã®3ã¤ã®ç†ç”±

#### 2.2.1 æ„æ€æ±ºå®šã®æ­£å½“æ€§

**A/Bãƒ†ã‚¹ãƒˆãªã—ã§"æ”¹å–„"ã‚’ä¸»å¼µã§ãã‚‹ã‹ï¼Ÿ** è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å› æœåŠ¹æœã‚’æ­£ã—ãæ¨å®šã§ããªã‘ã‚Œã°ã€ã©ã‚“ãªæ–½ç­–ã‚‚æ ¹æ‹ ãŒãªã„ã€‚

| ä¸»å¼µ | å› æœæ¨è«–ãªã— | å› æœæ¨è«–ã‚ã‚Š |
|:-----|:------------|:------------|
| æ–°æ©Ÿèƒ½ã§å£²ä¸Š+10% | ã€Œå°å…¥å¾Œã«å£²ä¸ŠãŒ10%å¢—ãˆãŸã€ï¼ˆ**å­£ç¯€æ€§?**ï¼‰ | DAGâ†’ãƒãƒƒã‚¯ãƒ‰ã‚¢èª¿æ•´â†’çœŸã®åŠ¹æœ3% |
| AIãƒãƒ£ãƒƒãƒˆå°å…¥ã§è§£ç´„ç‡-5% | ã€Œå°å…¥å¾Œã«è§£ç´„ç‡æ¸›å°‘ã€ï¼ˆ**å„ªè‰¯é¡§å®¢ãŒå…ˆè¡Œæ¡ç”¨?**ï¼‰ | å‚¾å‘ã‚¹ã‚³ã‚¢â†’ATEæ¨å®šâ†’åŠ¹æœ-2% |
| åºƒå‘Šå‡ºç¨¿ã§èªçŸ¥åº¦+20% | ã€Œå‡ºç¨¿å¾Œã«èªçŸ¥åº¦ä¸Šæ˜‡ã€ï¼ˆ**ãƒˆãƒ¬ãƒ³ãƒ‰?**ï¼‰ | RDDâ†’ã‚«ãƒƒãƒˆã‚ªãƒ•å‰å¾Œæ¯”è¼ƒâ†’åŠ¹æœ+15% |

#### 2.2.2 å€«ç†çš„åˆ¶ç´„

**å…¨å“¡ã«ãƒ©ãƒ³ãƒ€ãƒ åŒ–å®Ÿé¨“ã§ããªã„å ´åˆã‚‚å¤šã„ã€‚**

- åŒ»ç™‚: æ–°è–¬ã®åŠ¹æœæ¤œè¨¼ï¼ˆãƒ—ãƒ©ã‚»ãƒœç¾¤ã‚’ä½œã‚Œãªã„ï¼‰
- æ”¿ç­–: æ•™è‚²åˆ¶åº¦å¤‰æ›´ã®åŠ¹æœï¼ˆå­ä¾›ã‚’å®Ÿé¨“å°ã«ã§ããªã„ï¼‰
- ãƒ“ã‚¸ãƒã‚¹: æ—¢å­˜é¡§å®¢ã¸ã®å€¤ä¸Šã’åŠ¹æœï¼ˆé›¢åãƒªã‚¹ã‚¯ï¼‰

â†’ **è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å› æœåŠ¹æœã‚’æ¨å®šã™ã‚‹æŠ€è¡“ãŒå¿…é ˆ**

#### 2.2.3 MLÃ—å› æœæ¨è«–ã®èåˆ

æ©Ÿæ¢°å­¦ç¿’ã¯äºˆæ¸¬ã«å¼·ã„ãŒã€**å› æœåŠ¹æœæ¨å®šã«ã¯å¼±ã„**ã€‚

| æ‰‹æ³• | äºˆæ¸¬ | å› æœåŠ¹æœæ¨å®š |
|:-----|:-----|:------------|
| Random Forest | âœ… é«˜ç²¾åº¦ | âŒ Confoundingç„¡è¦– |
| Causal Forest [^3] | âœ… é«˜ç²¾åº¦ | âœ… HTEæ¨å®šå¯èƒ½ |
| XGBoost | âœ… é«˜ç²¾åº¦ | âŒ Biasæ®‹ç•™ |
| Double ML [^4] | âœ… é«˜ç²¾åº¦ | âœ… Debiasedæ¨å®š |

**2018å¹´ä»¥é™ã€MLÃ—å› æœæ¨è«–ãŒæ€¥é€Ÿã«ç™ºå±•ã€‚** Athey/Wager [^3], Chernozhukov [^4] ã‚‰ãŒCausal Forest, Double MLã‚’æå”±ã—ã€ç•°è³ªãªå‡¦ç½®åŠ¹æœ(HTE)ã‚’æ¨å®šå¯èƒ½ã«ã€‚

### 2.3 æœ¬è¬›ç¾©ã§å­¦ã¶ã“ã¨

| ãƒˆãƒ”ãƒƒã‚¯ | è¡Œæ•° | é›£æ˜“åº¦ | å®Ÿè£… |
|:--------|:-----|:-------|:-----|
| **Zone 3.1** å› æœæ¨è«–åŸºç¤ | 300 | â˜…â˜…â˜… | Simpson Paradoxå®Ÿè£… |
| **Zone 3.2** Rubinå› æœãƒ¢ãƒ‡ãƒ« | 400 | â˜…â˜…â˜…â˜… | ATE/ATT/CATEæ¨å®š |
| **Zone 3.3** Pearlå› æœç†è«– | 500 | â˜…â˜…â˜…â˜…â˜… | do-æ¼”ç®—/DAGå®Ÿè£… |
| **Zone 3.4** å‚¾å‘ã‚¹ã‚³ã‚¢ | 400 | â˜…â˜…â˜…â˜… | IPW/Matching/Balance |
| **Zone 3.5** æ“ä½œå¤‰æ•°æ³• | 300 | â˜…â˜…â˜…â˜… | 2SLS/Weak IVæ¤œå®š |
| **Zone 3.6** RDD | 250 | â˜…â˜…â˜… | Sharp/Fuzzy RDD |
| **Zone 3.7** DiD | 300 | â˜…â˜…â˜… | Staggered DiD |
| **Zone 3.8** MLÃ—å› æœæ¨è«– | 400 | â˜…â˜…â˜…â˜…â˜… | Causal Forest/DML |
| **Zone 4** Juliaå®Ÿè£… | 600 | â˜…â˜…â˜…â˜… | CausalInference.jl |

### 2.4 å­¦ç¿’æˆ¦ç•¥ â€” 3ã¤ã®ãƒ•ã‚§ãƒ¼ã‚º

```mermaid
graph LR
    P1["ğŸ“– Phase 1<br/>ç†è«–ç¿’å¾—<br/>(Zone 3)"] --> P2["ğŸ’» Phase 2<br/>å®Ÿè£…<br/>(Zone 4)"]
    P2 --> P3["ğŸ”¬ Phase 3<br/>å®Ÿé¨“<br/>(Zone 5)"]
    P1 -.Rubin/Pearl.-> P2
    P2 -.CausalInference.jl.-> P3
    P3 -.è«–æ–‡å†ç¾.-> P1
```

**æ¨å¥¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ1é€±é–“ï¼‰**:

| Day | å†…å®¹ | æ™‚é–“ |
|:----|:-----|:-----|
| Day 1 | Zone 0-2 + Zone 3.1-3.2 (Rubin) | 2h |
| Day 2 | Zone 3.3 (Pearl) | 2h |
| Day 3 | Zone 3.4-3.5 (å‚¾å‘ã‚¹ã‚³ã‚¢/IV) | 2h |
| Day 4 | Zone 3.6-3.7 (RDD/DiD) | 2h |
| Day 5 | Zone 3.8 (MLÃ—å› æœ) | 2h |
| Day 6 | Zone 4 (Juliaå®Ÿè£…) | 3h |
| Day 7 | Zone 5-7 (å®Ÿé¨“/å¾©ç¿’) | 2h |

:::details ãƒˆãƒ­ã‚¤ã®æœ¨é¦¬: Juliaã§ã®å› æœæ¨è«–å®Ÿè£…
æœ¬è¬›ç¾©ã§ã¯**Julia + CausalInference.jl**ã‚’ä½¿ã†ã€‚Pythonã®doWhyã‚ˆã‚Š:

- **DAGæ“ä½œãŒç›´æ„Ÿçš„**: LightGraphs.jlãƒ™ãƒ¼ã‚¹
- **é€Ÿåº¦**: 100ä¸‡ã‚µãƒ³ãƒ—ãƒ«ã®IPWæ¨å®šãŒ10å€é€Ÿ
- **å‹å®‰å…¨**: å‚¾å‘ã‚¹ã‚³ã‚¢ãŒ[0,1]ã®ç¯„å›²å¤–ã«ãªã‚‹å‰ã«æ¤œå‡º

ç¬¬24å›ã®çµ±è¨ˆå­¦ã§å­¦ã‚“ã æ¨å®šãƒ»æ¤œå®šã¨ã€æœ¬è¬›ç¾©ã®å› æœæ¨è«–ã‚’çµ„ã¿åˆã‚ã›ã‚Œã°ã€**è«–æ–‡ã®çµæœã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒå®Œå…¨ã«èª­ã‚ã‚‹**ã‚ˆã†ã«ãªã‚‹ã€‚
:::

:::message
**é€²æ—: 20% å®Œäº†** å› æœæ¨è«–ã®å…¨ä½“åƒã‚’æŠŠæ¡ã—ãŸã€‚ã“ã“ã‹ã‚‰60åˆ†ã®æ•°å¼ä¿®è¡Œã«å…¥ã‚‹ â€” Rubinã®æ½œåœ¨çš„çµæœã‹ã‚‰Pearlã®do-æ¼”ç®—ã¾ã§å®Œå…¨å°å‡ºã™ã‚‹ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” å› æœæ¨è«–ç†è«–ã®å®Œå…¨æ§‹ç¯‰

### 3.1 å› æœæ¨è«–ã®åŸºç¤ â€” ç›¸é–¢ã¨å› æœã®å³å¯†ãªé•ã„

#### 3.1.1 è¨˜æ³•ã®å®šç¾©

| è¨˜æ³• | æ„å‘³ | ä¾‹ |
|:-----|:-----|:---|
| $D$ | å‡¦ç½®å¤‰æ•° (Treatment) | $D \in \\{0, 1\\}$ (0=å¯¾ç…§, 1=å‡¦ç½®) |
| $Y$ | çµæœå¤‰æ•° (Outcome) | $Y \in \mathbb{R}$ (é€£ç¶š) or $\\{0,1\\}$ (2å€¤) |
| $X$ | å…±å¤‰é‡ (Covariates) | $X \in \mathbb{R}^p$ (äº¤çµ¡å› å­å€™è£œ) |
| $Y^d$ | æ½œåœ¨çš„çµæœ (Potential Outcome) | $Y^1$ (å‡¦ç½®æ™‚), $Y^0$ (å¯¾ç…§æ™‚) |
| $e(X)$ | å‚¾å‘ã‚¹ã‚³ã‚¢ (Propensity Score) | $e(X) = P(D=1 \mid X)$ |
| $\tau$ | å‡¦ç½®åŠ¹æœ (Treatment Effect) | $\tau = Y^1 - Y^0$ |

#### 3.1.2 å› æœåŠ¹æœã®å®šç¾©ï¼ˆNeyman-Rubin Frameworkï¼‰

**æ½œåœ¨çš„çµæœ (Potential Outcomes)**: å„å€‹ä½“ $i$ ã«ã¤ã„ã¦ã€**2ã¤ã®çµæœãŒå­˜åœ¨ã™ã‚‹**ã¨è€ƒãˆã‚‹:

$$
\begin{aligned}
Y_i^1 &= \text{å€‹ä½“ } i \text{ ãŒå‡¦ç½®ã‚’å—ã‘ãŸå ´åˆã®çµæœ} \\
Y_i^0 &= \text{å€‹ä½“ } i \text{ ãŒå‡¦ç½®ã‚’å—ã‘ãªã‹ã£ãŸå ´åˆã®çµæœ}
\end{aligned}
$$

**è¦³æ¸¬ã•ã‚Œã‚‹çµæœ**:

$$
Y_i = D_i Y_i^1 + (1 - D_i) Y_i^0 = \begin{cases}
Y_i^1 & \text{if } D_i = 1 \\
Y_i^0 & \text{if } D_i = 0
\end{cases}
$$

**æ ¹æœ¬çš„ãªå› æœæ¨è«–ã®å•é¡Œ (Fundamental Problem of Causal Inference)**:

å€‹ä½“ $i$ ã«ã¤ã„ã¦ã€$Y_i^1$ ã¨ $Y_i^0$ ã‚’**åŒæ™‚ã«è¦³æ¸¬ã™ã‚‹ã“ã¨ã¯ä¸å¯èƒ½**ã€‚ä¸€æ–¹ã—ã‹è¦‹ãˆãªã„ã€‚

$$
\tau_i = Y_i^1 - Y_i^0 \quad \text{(å€‹ä½“ãƒ¬ãƒ™ãƒ«ã®å‡¦ç½®åŠ¹æœã¯è¦³æ¸¬ä¸èƒ½)}
$$

#### 3.1.3 å¹³å‡å‡¦ç½®åŠ¹æœ (ATE)

å€‹ä½“ãƒ¬ãƒ™ãƒ«ã¯è¦³æ¸¬ä¸èƒ½ã ãŒã€**é›†å›£å¹³å‡ãªã‚‰æ¨å®šå¯èƒ½**:

$$
\text{ATE} = \mathbb{E}[Y^1 - Y^0] = \mathbb{E}[Y^1] - \mathbb{E}[Y^0]
$$

**Naiveæ¨å®šé‡ã¯ãƒã‚¤ã‚¢ã‚¹ã‚ã‚Š**:

$$
\begin{aligned}
&\mathbb{E}[Y \mid D=1] - \mathbb{E}[Y \mid D=0] \\
&= \mathbb{E}[Y^1 \mid D=1] - \mathbb{E}[Y^0 \mid D=0] \\
&\neq \mathbb{E}[Y^1] - \mathbb{E}[Y^0] \quad \text{(selection bias)}
\end{aligned}
$$

ãªãœãªã‚‰:

$$
\mathbb{E}[Y^1 \mid D=1] \neq \mathbb{E}[Y^1 \mid D=0] \quad \text{(å‡¦ç½®ç¾¤ã¨å¯¾ç…§ç¾¤ã§æ½œåœ¨çµæœãŒç•°ãªã‚‹)}
$$

#### 3.1.4 äº¤çµ¡ (Confounding) ã®æ•°å­¦çš„å®šç¾©

**äº¤çµ¡å› å­ $X$**: $D$ ã¨ $Y$ ã®ä¸¡æ–¹ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹å¤‰æ•°

```mermaid
graph LR
    X["ğŸ“Š äº¤çµ¡å› å­ X<br/>(å¹´é½¢ãƒ»æ‰€å¾—ç­‰)"] --> D["ğŸ’Š å‡¦ç½® D"]
    X --> Y["ğŸ“ˆ çµæœ Y"]
    D --> Y
    style X fill:#fff3e0
```

**å½¢å¼çš„å®šç¾©**:

$$
X \text{ ãŒäº¤çµ¡å› å­} \iff \begin{cases}
X \not\!\perp\!\!\!\perp D \text{ (å‡¦ç½®ã¨é–¢é€£)} \\
X \not\!\perp\!\!\!\perp Y^d \text{ (çµæœã¨é–¢é€£)}
\end{cases}
$$

**ä¾‹**: å¥åº·é£Ÿå“ã®åŠ¹æœæ¨å®š

- $D$: å¥åº·é£Ÿå“æ‘‚å– (1=æ‘‚å–, 0=éæ‘‚å–)
- $Y$: å¥åº·ã‚¹ã‚³ã‚¢
- $X$: æ‰€å¾—

é«˜æ‰€å¾—è€…ã¯å¥åº·é£Ÿå“ã‚’è²·ã„ã‚„ã™ã($X \to D$)ã€ã‹ã¤åŒ»ç™‚ã‚¢ã‚¯ã‚»ã‚¹ãŒè‰¯ãå¥åº·($X \to Y$)ã€‚æ‰€å¾—ã‚’åˆ¶å¾¡ã—ãªã„ã¨åŠ¹æœã‚’éå¤§è©•ä¾¡ã™ã‚‹ã€‚

#### 3.1.5 Simpson's Paradox ã®æ•°å­¦çš„åˆ†è§£

å…¨ä½“ã§ã®ç›¸é–¢ã¨éƒ¨åˆ†é›†å›£ã§ã®ç›¸é–¢ãŒé€†è»¢ã™ã‚‹ç¾è±¡ã€‚

**ä¾‹**: ç—…é™¢Aã¨ç—…é™¢B

| | ç—…é™¢A | ç—…é™¢B | å…¨ä½“ |
|:--|:------|:------|:-----|
| å‡¦ç½®ç¾¤ç”Ÿå­˜ç‡ | 50/100 | 90/100 | 140/200 = 70% |
| å¯¾ç…§ç¾¤ç”Ÿå­˜ç‡ | 40/100 | 85/100 | 125/200 = 62.5% |
| åŠ¹æœ | +10% | +5% | +7.5% |

**ã—ã‹ã—**ã€é‡ç—‡åº¦ $S$ (è»½ç—‡/é‡ç—‡) ãŒäº¤çµ¡:

$$
\begin{aligned}
P(Y=1 \mid D=1) - P(Y=1 \mid D=0) &= 0.075 \quad \text{(å…¨ä½“)} \\
P(Y=1 \mid D=1, S=\text{è»½}) - P(Y=1 \mid D=0, S=\text{è»½}) &= -0.05 \quad \text{(è»½ç—‡)} \\
P(Y=1 \mid D=1, S=\text{é‡}) - P(Y=1 \mid D=0, S=\text{é‡}) &= -0.02 \quad \text{(é‡ç—‡)}
\end{aligned}
$$

**ç¬¦å·ãŒé€†è»¢ï¼** ã“ã‚Œã¯ $S$ ãŒäº¤çµ¡å› å­ã ã‹ã‚‰ã€‚

Pearl [^8] ã®è§£æ±ºç­–: **do-æ¼”ç®—**ã§ä»‹å…¥åŠ¹æœã‚’å®šç¾©

$$
P(Y=1 \mid do(D=1)) - P(Y=1 \mid do(D=0)) \neq P(Y=1 \mid D=1) - P(Y=1 \mid D=0)
$$

```julia
# Simpson's Paradox simulation
function simpsons_paradox()
    # Hospital A: mostly mild cases
    hosp_A_treat = [fill(1, 90), fill(0, 10)]  # 90 mild, 10 severe, treatment
    hosp_A_treat_survival = [fill(1, 50), fill(0, 50)]  # 50% survival
    hosp_A_control = [fill(1, 80), fill(0, 20)]  # 80 mild, 20 severe, control
    hosp_A_control_survival = [fill(1, 40), fill(0, 60)]  # 40% survival

    # Hospital B: mostly severe cases
    hosp_B_treat = [fill(1, 20), fill(0, 80)]  # 20 mild, 80 severe, treatment
    hosp_B_treat_survival = [fill(1, 90), fill(0, 10)]  # 90% survival
    hosp_B_control = [fill(1, 30), fill(0, 70)]  # 30 mild, 70 severe, control
    hosp_B_control_survival = [fill(1, 85), fill(0, 15)]  # 85% survival

    # Overall survival rates (pooled)
    overall_treat = (50 + 90) / 200  # 70%
    overall_control = (40 + 85) / 200  # 62.5%
    overall_effect = overall_treat - overall_control

    # Stratified by severity
    mild_treat = (50*0.9/90) / (90/100)  # approximate
    mild_control = (40*0.8/80) / (80/100)

    println("Overall treatment effect: $(round(overall_effect, digits=3))")
    println("Hospital A effect: $(round(0.10, digits=3))")
    println("Hospital B effect: $(round(0.05, digits=3))")
    println("âš ï¸ Paradox: overall positive, but aggregation hides severity confounding")
end

simpsons_paradox()
```

### 3.2 Rubinå› æœãƒ¢ãƒ‡ãƒ« (Potential Outcomes Framework)

#### 3.2.1 SUTVA (Stable Unit Treatment Value Assumption)

**ä»®å®š1: å‡¦ç½®ã®ä¸€æ„æ€§**

$$
\text{å€‹ä½“ } i \text{ ã®å‡¦ç½®ãŒ } d \text{ ã®ã¨ãã€çµæœã¯ } Y_i^d \text{ ã®1ã¤ã®ã¿}
$$

ï¼ˆå‡¦ç½®ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒè¤‡æ•°ã‚ã‚‹ã¨NG: ä¾‹ è–¬ã®æŠ•ä¸é‡ãŒ5mg/10mg/15mgãªã‚‰ $Y_i^{5}, Y_i^{10}, Y_i^{15}$ ã¨åˆ†ã‘ã‚‹å¿…è¦ï¼‰

**ä»®å®š2: å¹²æ¸‰ãªã— (No Interference)**

$$
Y_i^d = Y_i^{d_i} \quad \forall d_{-i}
$$

å€‹ä½“ $i$ ã®çµæœã¯ã€ä»–ã®å€‹ä½“ $-i$ ã®å‡¦ç½® $d_{-i}$ ã«ä¾å­˜ã—ãªã„ã€‚

**SUTVAãŒç ´ã‚Œã‚‹ä¾‹**:

- ãƒ¯ã‚¯ãƒãƒ³æ¥ç¨®: ä»–äººãŒæ¥ç¨®ã™ã‚‹ã¨è‡ªåˆ†ã®æ„ŸæŸ“ãƒªã‚¹ã‚¯ã‚‚ä¸‹ãŒã‚‹ï¼ˆé›†å›£å…ç–«ï¼‰
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åºƒå‘Š: å‹äººãŒã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨è‡ªåˆ†ã‚‚ã‚¯ãƒªãƒƒã‚¯ã—ã‚„ã™ã„

#### 3.2.2 ATE, ATT, CATE ã®å®Œå…¨å®šç¾©

| åŠ¹æœ | å®šç¾© | æ„å‘³ |
|:-----|:-----|:-----|
| **ATE** | $\mathbb{E}[Y^1 - Y^0]$ | å…¨ä½“ã®å¹³å‡å‡¦ç½®åŠ¹æœ |
| **ATT** | $\mathbb{E}[Y^1 - Y^0 \mid D=1]$ | å‡¦ç½®ç¾¤ã®å¹³å‡å‡¦ç½®åŠ¹æœ |
| **ATC** | $\mathbb{E}[Y^1 - Y^0 \mid D=0]$ | å¯¾ç…§ç¾¤ã®å¹³å‡å‡¦ç½®åŠ¹æœ |
| **CATE** | $\mathbb{E}[Y^1 - Y^0 \mid X=x]$ | æ¡ä»¶ä»˜ãå¹³å‡å‡¦ç½®åŠ¹æœ |

**å°å‡º**:

$$
\begin{aligned}
\text{ATE} &= \mathbb{E}[Y^1] - \mathbb{E}[Y^0] \\
&= \mathbb{E}[\mathbb{E}[Y^1 \mid X]] - \mathbb{E}[\mathbb{E}[Y^0 \mid X]] \\
&= \mathbb{E}[\text{CATE}(X)]
\end{aligned}
$$

**ATTã¨ATEã®é–¢ä¿‚**:

$$
\begin{aligned}
\text{ATE} &= P(D=1) \cdot \text{ATT} + P(D=0) \cdot \text{ATC}
\end{aligned}
$$

**ATTæ¨å®šãŒé‡è¦ãªç†ç”±**: æ”¿ç­–è©•ä¾¡ã§ã¯ã€Œå®Ÿéš›ã«å‡¦ç½®ã‚’å—ã‘ãŸäººã«ã¨ã£ã¦ã®åŠ¹æœã€ãŒå•ã‚ã‚Œã‚‹ã€‚

#### 3.2.3 Unconfoundedness (ç„¡äº¤çµ¡æ€§) ä»®å®š

$$
(Y^1, Y^0) \perp\!\!\!\perp D \mid X
$$

$X$ ã‚’æ‰€ä¸ã¨ã™ã‚Œã°ã€æ½œåœ¨çš„çµæœã¨å‡¦ç½®å‰²ã‚Šå½“ã¦ãŒç‹¬ç«‹ã€‚

**ã“ã‚ŒãŒæˆã‚Šç«‹ã¤ã¨ã**:

$$
\begin{aligned}
\mathbb{E}[Y^1 \mid X] &= \mathbb{E}[Y^1 \mid D=1, X] = \mathbb{E}[Y \mid D=1, X] \\
\mathbb{E}[Y^0 \mid X] &= \mathbb{E}[Y^0 \mid D=0, X] = \mathbb{E}[Y \mid D=0, X]
\end{aligned}
$$

ã‚ˆã£ã¦:

$$
\text{CATE}(X) = \mathbb{E}[Y \mid D=1, X] - \mathbb{E}[Y \mid D=0, X]
$$

**ATEè­˜åˆ¥**:

$$
\begin{aligned}
\text{ATE} &= \mathbb{E}_X[\mathbb{E}[Y \mid D=1, X] - \mathbb{E}[Y \mid D=0, X]] \\
&= \mathbb{E}_X[\text{CATE}(X)]
\end{aligned}
$$

#### 3.2.4 Overlap/Positivity (å…±é€šã‚µãƒãƒ¼ãƒˆ) ä»®å®š

$$
0 < P(D=1 \mid X=x) < 1 \quad \forall x \in \text{supp}(X)
$$

å…¨ã¦ã® $X$ ã®å€¤ã§ã€å‡¦ç½®ç¾¤ãƒ»å¯¾ç…§ç¾¤ã®ä¸¡æ–¹ãŒå­˜åœ¨ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚

**ç ´ã‚Œã‚‹ä¾‹**:

- ç”·æ€§ã®ã¿ã«å‰ç«‹è…ºãŒã‚“æ¤œè¨º â†’ å¥³æ€§ã§ $P(D=1 \mid \text{sex}=F)=0$
- é«˜æ‰€å¾—è€…ã®ã¿ãŒãƒ—ãƒ¬ãƒŸã‚¢ãƒ ãƒ—ãƒ©ãƒ³è³¼å…¥ â†’ ä½æ‰€å¾—è€…ã§ $P(D=1 \mid \text{income}<\$30k)=0$

OverlapãŒãªã„ã¨ã€åå®Ÿä»®æƒ³ $\mathbb{E}[Y^0 \mid D=1, X]$ ãŒæ¨å®šä¸èƒ½ï¼ˆå‡¦ç½®ç¾¤ã§å¯¾ç…§ç¾¤ã®çµæœã‚’å¤–æŒ¿ã§ããªã„ï¼‰ã€‚

#### 3.2.5 æ•°å€¤æ¤œè¨¼: ATEæ¨å®š

```julia
using Statistics, Distributions

# ATE estimation under unconfoundedness
function ate_estimation_demo()
    n = 10000
    # Covariate X ~ N(0,1)
    X = randn(n)

    # Treatment assignment (unconfounded given X)
    e_X = 1 ./ (1 .+ exp.(-X))  # propensity score
    D = rand(n) .< e_X

    # Potential outcomes
    # Y^1 = 2 + X + Îµâ‚
    # Y^0 = X + Îµâ‚€
    # True ATE = E[Y^1 - Y^0] = 2
    Y1 = 2 .+ X .+ randn(n) * 0.5
    Y0 = X .+ randn(n) * 0.5

    # Observed outcome
    Y = D .* Y1 .+ (1 .- D) .* Y0

    # Naive estimator (biased)
    ate_naive = mean(Y[D]) - mean(Y[.!D])

    # Regression adjustment (unbiased under unconfoundedness)
    # E[Y|D=1,X] - E[Y|D=0,X] = CATE(X)
    # Approximate with linear regression
    function linear_reg(D, X, Y)
        # Y ~ Î²â‚€ + Î²â‚D + Î²â‚‚X + Î²â‚ƒDX
        n = length(Y)
        design_matrix = hcat(ones(n), D, X, D .* X)
        Î² = design_matrix \ Y
        return Î²
    end

    Î² = linear_reg(D, X, Y)
    # ATE = E[Y|D=1,X] - E[Y|D=0,X] averaged over X
    # = Î²â‚ + Î²â‚ƒ * E[X] = Î²â‚ (since E[X]=0)
    ate_reg = Î²[2]

    println("True ATE: 2.0")
    println("Naive ATE: $(round(ate_naive, digits=3))")
    println("Regression ATE: $(round(ate_reg, digits=3))")

    return ate_naive, ate_reg
end

ate_estimation_demo()
```

### 3.3 Pearlå› æœç†è«– (Structural Causal Models)

#### 3.3.1 æ§‹é€ å› æœãƒ¢ãƒ‡ãƒ« (SCM) ã®å®šç¾©

**SCM** ã¯3ã¤çµ„ $\mathcal{M} = (\mathcal{U}, \mathcal{V}, \mathcal{F})$:

- $\mathcal{U}$: å¤–ç”Ÿå¤‰æ•°ï¼ˆè¦³æ¸¬ä¸èƒ½ãªèª¤å·®é …ï¼‰
- $\mathcal{V}$: å†…ç”Ÿå¤‰æ•°ï¼ˆè¦³æ¸¬å¯èƒ½ãªå¤‰æ•°ï¼‰
- $\mathcal{F}$: æ§‹é€ æ–¹ç¨‹å¼ï¼ˆå¤‰æ•°é–“ã®å› æœé–¢ä¿‚ï¼‰

**ä¾‹**: å–«ç…™ $S$, éºä¼ $G$, ãŒã‚“ $C$

$$
\begin{aligned}
G &= U_G \quad \text{(å¤–ç”Ÿ)} \\
S &= f_S(G, U_S) \quad \text{(éºä¼ãŒå–«ç…™ã«å½±éŸ¿)} \\
C &= f_C(S, G, U_C) \quad \text{(å–«ç…™ã¨éºä¼ãŒãŒã‚“ã«å½±éŸ¿)}
\end{aligned}
$$

DAGè¡¨ç¾:

```mermaid
graph TD
    U_G["U_G"] --> G["éºä¼ G"]
    U_S["U_S"] --> S["å–«ç…™ S"]
    G --> S
    U_C["U_C"] --> C["ãŒã‚“ C"]
    S --> C
    G --> C
    style U_G fill:#f5f5f5
    style U_S fill:#f5f5f5
    style U_C fill:#f5f5f5
```

#### 3.3.2 do-æ¼”ç®— (Intervention)

**ä»‹å…¥ $do(X=x)$**: å¤‰æ•° $X$ ã‚’å¤–éƒ¨ã‹ã‚‰å¼·åˆ¶çš„ã« $x$ ã«å›ºå®šã™ã‚‹ã€‚

**å½¢å¼çš„å®šç¾©**:

$$
P(Y \mid do(X=x)) = \sum_z P(Y \mid X=x, Z=z) P(Z=z)
$$

ã“ã“ã§ $Z$ ã¯ $X$ ã¨ $Y$ ã®é–“ã®**ãƒãƒƒã‚¯ãƒ‰ã‚¢ãƒ‘ã‚¹**ã‚’é®æ–­ã™ã‚‹å¤‰æ•°é›†åˆã€‚

**è¦³æ¸¬ vs ä»‹å…¥ã®é•ã„**:

$$
\begin{aligned}
P(Y \mid X=x) &= \frac{P(Y, X=x)}{P(X=x)} \quad \text{(è¦³æ¸¬: æ¡ä»¶ä»˜ãç¢ºç‡)} \\
P(Y \mid do(X=x)) &= P_{M_{\bar{X}}}(Y \mid X=x) \quad \text{(ä»‹å…¥: SCM } M \text{ ã§ } X \text{ ã¸ã®çŸ¢å°ã‚’å‰Šé™¤)}
\end{aligned}
$$

**ä¾‹**: å–«ç…™ã¨ãŒã‚“ã®å› æœåŠ¹æœ

$$
\begin{aligned}
P(C=1 \mid S=1) &= \frac{P(C=1, S=1)}{P(S=1)} \quad \text{(å–«ç…™è€…ã®ãŒã‚“ç‡ â€” äº¤çµ¡ã‚ã‚Š)} \\
P(C=1 \mid do(S=1)) &= \sum_g P(C=1 \mid S=1, G=g) P(G=g) \quad \text{(å–«ç…™ã‚’å¼·åˆ¶ã—ãŸå ´åˆã®ãŒã‚“ç‡)}
\end{aligned}
$$

#### 3.3.3 DAG (æœ‰å‘éå·¡å›ã‚°ãƒ©ãƒ•) ã®åŸºç¤

**DAG** $\mathcal{G} = (V, E)$: é ‚ç‚¹ $V$ ã¨æœ‰å‘è¾º $E$ ã‹ã‚‰ãªã‚‹ã‚°ãƒ©ãƒ•ï¼ˆé–‰è·¯ãªã—ï¼‰

**è¦ª (Parents)**: $\text{PA}_i = \\{j : (j, i) \in E\\}$

**å­å­« (Descendants)**: $\text{DE}_i = \\{j : i \text{ ã‹ã‚‰ } j \text{ ã¸ã®ãƒ‘ã‚¹ãŒå­˜åœ¨}\\}$

**å› æœãƒãƒ«ã‚³ãƒ•æ¡ä»¶**:

$$
P(v_1, \ldots, v_n) = \prod_{i=1}^n P(v_i \mid \text{PA}_i)
$$

å„å¤‰æ•°ã¯ã€è¦ªã‚’æ‰€ä¸ã¨ã™ã‚Œã°éå­å­«ã¨ç‹¬ç«‹ã€‚

#### 3.3.4 d-åˆ†é›¢ (d-separation)

**å®šç¾©**: DAGä¸Šã§ã€å¤‰æ•°é›†åˆ $Z$ ãŒ $X$ ã¨ $Y$ ã‚’ d-åˆ†é›¢ã™ã‚‹ $\iff$ $X$ ã‹ã‚‰ $Y$ ã¸ã®ã™ã¹ã¦ã®ãƒ‘ã‚¹ãŒ $Z$ ã«ã‚ˆã£ã¦é®æ–­ã•ã‚Œã‚‹ã€‚

**ãƒ‘ã‚¹ã®é®æ–­æ¡ä»¶**:

| ãƒ‘ã‚¹æ§‹é€  | é®æ–­æ¡ä»¶ | å›³ |
|:--------|:---------|:---|
| **Chain** $X \to Z \to Y$ | $Z \in \mathcal{Z}$ | $X$ ã‹ã‚‰ $Y$ ã¸ã®æƒ…å ±ã¯ $Z$ ã‚’é€šã‚‹ |
| **Fork** $X \leftarrow Z \to Y$ | $Z \in \mathcal{Z}$ | $Z$ ãŒå…±é€šåŸå› ï¼ˆäº¤çµ¡ï¼‰ |
| **Collider** $X \to Z \leftarrow Y$ | $Z \notin \mathcal{Z}$ ã‹ã¤ $\text{DE}(Z) \cap \mathcal{Z} = \emptyset$ | $Z$ ãŒçµæœï¼ˆé¸æŠãƒã‚¤ã‚¢ã‚¹ï¼‰ |

**d-åˆ†é›¢ã®é‡è¦æ€§**:

$$
X \perp_d Y \mid Z \quad \Rightarrow \quad X \perp\!\!\!\perp Y \mid Z \quad \text{(æ¡ä»¶ä»˜ãç‹¬ç«‹)}
$$

**ä¾‹**: Colliderã®ãƒ‘ãƒ©ãƒ‰ã‚¯ã‚¹

```mermaid
graph TD
    T["æ‰èƒ½ T"] --> A["åˆæ ¼ A"]
    E["åŠªåŠ› E"] --> A
```

$T \perp\!\!\!\perp E$ ï¼ˆæ‰èƒ½ã¨åŠªåŠ›ã¯ç‹¬ç«‹ï¼‰ã ãŒã€åˆæ ¼è€… $A=1$ ã‚’æ¡ä»¶ã¥ã‘ã‚‹ã¨:

$$
T \not\perp\!\!\!\perp E \mid A=1
$$

åˆæ ¼è€…ã®ä¸­ã§ã¯ã€ŒåŠªåŠ›ãŒå°‘ãªã„â†’æ‰èƒ½ãŒé«˜ã„ã€ã¨ã„ã†è² ã®ç›¸é–¢ãŒç”Ÿã¾ã‚Œã‚‹ï¼ˆé¸æŠãƒã‚¤ã‚¢ã‚¹ï¼‰ã€‚

#### 3.3.5 ãƒãƒƒã‚¯ãƒ‰ã‚¢åŸºæº– (Backdoor Criterion)

**å®šç¾©**: å¤‰æ•°é›†åˆ $Z$ ãŒ $(X, Y)$ ã®ãƒãƒƒã‚¯ãƒ‰ã‚¢åŸºæº–ã‚’æº€ãŸã™ $\iff$

1. $Z$ ã®ã©ã®å¤‰æ•°ã‚‚ $X$ ã®å­å­«ã§ãªã„
2. $Z$ ãŒ $X$ ã‹ã‚‰ $Y$ ã¸ã®ã™ã¹ã¦ã®ãƒãƒƒã‚¯ãƒ‰ã‚¢ãƒ‘ã‚¹ã‚’é®æ–­ã™ã‚‹

**ãƒãƒƒã‚¯ãƒ‰ã‚¢ãƒ‘ã‚¹**: $X \leftarrow \cdots \to Y$ ã®ã‚ˆã†ãªã€$X$ ã¸ã®çŸ¢å°ã‚’å«ã‚€ãƒ‘ã‚¹

**ãƒãƒƒã‚¯ãƒ‰ã‚¢èª¿æ•´å…¬å¼**:

$$
P(Y \mid do(X=x)) = \sum_z P(Y \mid X=x, Z=z) P(Z=z)
$$

**ä¾‹**: å–«ç…™â†’ãŒã‚“

```mermaid
graph TD
    G["éºä¼ G"] --> S["å–«ç…™ S"]
    G --> C["ãŒã‚“ C"]
    S --> C
```

$Z = \\{G\\}$ ãŒãƒãƒƒã‚¯ãƒ‰ã‚¢åŸºæº–ã‚’æº€ãŸã™:

$$
P(C \mid do(S=s)) = \sum_g P(C \mid S=s, G=g) P(G=g)
$$

#### 3.3.6 ãƒ•ãƒ­ãƒ³ãƒˆãƒ‰ã‚¢åŸºæº– (Frontdoor Criterion)

**çŠ¶æ³**: ãƒãƒƒã‚¯ãƒ‰ã‚¢ãƒ‘ã‚¹ã‚’é®æ–­ã§ããªã„ï¼ˆæœªæ¸¬å®šäº¤çµ¡ $U$ ãŒã‚ã‚‹ï¼‰ãŒã€**åª’ä»‹å¤‰æ•° $M$** ã‚’æ¸¬å®šã§ãã‚‹å ´åˆ

```mermaid
graph TD
    U["æœªæ¸¬å®šäº¤çµ¡ U"] --> X["å‡¦ç½® X"]
    U --> Y["çµæœ Y"]
    X --> M["åª’ä»‹å¤‰æ•° M"]
    M --> Y
```

**ãƒ•ãƒ­ãƒ³ãƒˆãƒ‰ã‚¢åŸºæº–**: $M$ ãŒ $(X, Y)$ ã®ãƒ•ãƒ­ãƒ³ãƒˆãƒ‰ã‚¢åŸºæº–ã‚’æº€ãŸã™ $\iff$

1. $M$ ãŒ $X$ ã‹ã‚‰ $Y$ ã¸ã®ã™ã¹ã¦ã®ãƒ‘ã‚¹ã‚’é®æ–­
2. $X$ ã‹ã‚‰ $M$ ã¸ã®ãƒãƒƒã‚¯ãƒ‰ã‚¢ãƒ‘ã‚¹ãŒå­˜åœ¨ã—ãªã„
3. $X$ ãŒ $M$ ã‹ã‚‰ $Y$ ã¸ã®ã™ã¹ã¦ã®ãƒãƒƒã‚¯ãƒ‰ã‚¢ãƒ‘ã‚¹ã‚’é®æ–­

**ãƒ•ãƒ­ãƒ³ãƒˆãƒ‰ã‚¢èª¿æ•´å…¬å¼**:

$$
P(Y \mid do(X=x)) = \sum_m P(M=m \mid X=x) \sum_{x'} P(Y \mid M=m, X=x') P(X=x')
$$

**ä¾‹**: å–«ç…™â†’ã‚¿ãƒ¼ãƒ«æ²ˆç€â†’ãŒã‚“

$$
P(C \mid do(S=s)) = \sum_t P(T=t \mid S=s) \sum_{s'} P(C \mid T=t, S=s') P(S=s')
$$

#### 3.3.7 do-æ¼”ç®—ã®3ã¤ã®å…¬ç†

Pearl [^1] ã®do-calculus â€” ä»‹å…¥ç¢ºç‡ã‚’æ¡ä»¶ä»˜ãç¢ºç‡ã«å¤‰æ›ã™ã‚‹3ã¤ã®ãƒ«ãƒ¼ãƒ«:

**Rule 1 (è¦³æ¸¬ã®æŒ¿å…¥/å‰Šé™¤)**:

$$
P(Y \mid do(X), Z, W) = P(Y \mid do(X), W) \quad \text{if } (Y \perp_d Z \mid X, W)_{\mathcal{G}_{\bar{X}}}
$$

**Rule 2 (ä»‹å…¥ã®æŒ¿å…¥/å‰Šé™¤)**:

$$
P(Y \mid do(X), do(Z), W) = P(Y \mid do(X), Z, W) \quad \text{if } (Y \perp_d Z \mid X, W)_{\mathcal{G}_{\bar{X}, \underline{Z}}}
$$

**Rule 3 (ä»‹å…¥ã®å‰Šé™¤)**:

$$
P(Y \mid do(X), do(Z), W) = P(Y \mid do(X), W) \quad \text{if } (Y \perp_d Z \mid X, W)_{\mathcal{G}_{\bar{X}, \overline{Z(W)}}}
$$

ã“ã“ã§:
- $\mathcal{G}_{\bar{X}}$: $X$ ã¸ã®çŸ¢å°ã‚’å‰Šé™¤
- $\mathcal{G}_{\underline{X}}$: $X$ ã‹ã‚‰ã®çŸ¢å°ã‚’å‰Šé™¤
- $\mathcal{G}_{\overline{X(W)}}$: $W$ ã®éç¥–å…ˆã§ã‚ã‚‹ $X$ ã¸ã®çŸ¢å°ã‚’å‰Šé™¤

**å¿œç”¨**: ãƒãƒƒã‚¯ãƒ‰ã‚¢èª¿æ•´ã®å°å‡º

$$
\begin{aligned}
P(Y \mid do(X)) &= \sum_z P(Y \mid do(X), Z=z) P(Z=z \mid do(X)) \\
&= \sum_z P(Y \mid do(X), Z=z) P(Z=z) \quad \text{(Rule 3)} \\
&= \sum_z P(Y \mid X, Z=z) P(Z=z) \quad \text{(Rule 2)}
\end{aligned}
$$

#### 3.3.8 Pearl's Ladder of Causation

| ãƒ¬ãƒ™ãƒ« | å•ã„ | è¨˜æ³• | ä¾‹ |
|:------|:-----|:-----|:---|
| **1. Association** | è¦³æ¸¬ã—ãŸã‚‰ï¼Ÿ | $P(Y \mid X)$ | å–«ç…™è€…ã®ãŒã‚“ç‡ |
| **2. Intervention** | ä»‹å…¥ã—ãŸã‚‰ï¼Ÿ | $P(Y \mid do(X))$ | å–«ç…™ã‚’å¼·åˆ¶ã—ãŸã‚‰ãŒã‚“ã«ãªã‚‹ã‹ |
| **3. Counterfactual** | ã‚‚ã—ã€œã ã£ãŸã‚‰ï¼Ÿ | $P(Y_{X=x'} \mid X=x, Y=y)$ | å–«ç…™ã—ãªã‹ã£ãŸã‚‰ãŒã‚“ã«ãªã‚‰ãªã‹ã£ãŸã‹ |

**åå®Ÿä»®æƒ³ (Counterfactual)**: éå»ã®äº‹å®Ÿã‚’å¤‰ãˆãŸå ´åˆã®ä»®æƒ³çš„çµæœ

$$
Y_{X=x'} = \text{å€‹ä½“ãŒ } X=x \text{ ã‚’å®Ÿéš›ã«å—ã‘ãŸãŒã€} X=x' \text{ ã‚’å—ã‘ã¦ã„ãŸã‚‰å¾—ã‚‰ã‚ŒãŸçµæœ}
$$

### 3.4 å‚¾å‘ã‚¹ã‚³ã‚¢ (Propensity Score)

#### 3.4.1 å‚¾å‘ã‚¹ã‚³ã‚¢ã®å®šç¾©

**å®šç¾© (Rosenbaum & Rubin 1983)**:

$$
e(X) = P(D=1 \mid X)
$$

$X$ ã‚’æ‰€ä¸ã¨ã—ãŸã¨ãã®å‡¦ç½®ã‚’å—ã‘ã‚‹ç¢ºç‡ã€‚

**é‡è¦æ€§**: $X$ ãŒé«˜æ¬¡å…ƒã§ã‚‚ã€$e(X)$ ã¯1æ¬¡å…ƒã®ã‚¹ã‚«ãƒ©ãƒ¼ã€‚

**Propensity Score Theorem**:

$$
(Y^1, Y^0) \perp\!\!\!\perp D \mid X \quad \Rightarrow \quad (Y^1, Y^0) \perp\!\!\!\perp D \mid e(X)
$$

**è¨¼æ˜**:

$$
\begin{aligned}
P(D=1 \mid Y^1, Y^0, e(X)) &= \mathbb{E}[P(D=1 \mid Y^1, Y^0, X) \mid Y^1, Y^0, e(X)] \\
&= \mathbb{E}[P(D=1 \mid X) \mid Y^1, Y^0, e(X)] \quad \text{(unconfoundedness)} \\
&= \mathbb{E}[e(X) \mid Y^1, Y^0, e(X)] \\
&= e(X) \\
&= P(D=1 \mid e(X))
\end{aligned}
$$

ã‚ˆã£ã¦ $(Y^1, Y^0) \perp\!\!\!\perp D \mid e(X)$ã€‚

#### 3.4.2 IPW (Inverse Probability Weighting) æ¨å®šé‡

**IPWæ¨å®šé‡**:

$$
\hat{\text{ATE}}_{\text{IPW}} = \frac{1}{n} \sum_{i=1}^n \left( \frac{D_i Y_i}{e(X_i)} - \frac{(1 - D_i) Y_i}{1 - e(X_i)} \right)
$$

**å°å‡º**:

$$
\begin{aligned}
\mathbb{E}\left[\frac{D Y}{e(X)}\right] &= \mathbb{E}\left[\mathbb{E}\left[\frac{D Y}{e(X)} \mid X\right]\right] \\
&= \mathbb{E}\left[\frac{\mathbb{E}[D Y \mid X]}{e(X)}\right] \\
&= \mathbb{E}\left[\frac{P(D=1 \mid X) \mathbb{E}[Y \mid D=1, X]}{e(X)}\right] \\
&= \mathbb{E}\left[\frac{e(X) \mathbb{E}[Y^1 \mid X]}{e(X)}\right] \\
&= \mathbb{E}[Y^1]
\end{aligned}
$$

åŒæ§˜ã« $\mathbb{E}\left[\frac{(1-D) Y}{1-e(X)}\right] = \mathbb{E}[Y^0]$ã€‚

**ATTæ¨å®šé‡**:

$$
\hat{\text{ATT}}_{\text{IPW}} = \frac{\sum_i D_i Y_i}{\sum_i D_i} - \frac{\sum_i D_i (1-D_i) Y_i / (1-e(X_i))}{\sum_i D_i e(X_i) / (1-e(X_i))}
$$

#### 3.4.3 Doubly Robust æ¨å®šé‡

IPWã¨å›å¸°èª¿æ•´ã‚’çµ„ã¿åˆã‚ã›ãŸæ¨å®šé‡ã€‚**ã©ã¡ã‚‰ã‹ä¸€æ–¹ãŒæ­£ã—ã‘ã‚Œã°ä¸å**ï¼ˆé ‘å¥æ€§2å€ï¼‰ã€‚

$$
\hat{\text{ATE}}_{\text{DR}} = \frac{1}{n} \sum_{i=1}^n \left[ \frac{D_i (Y_i - \hat{\mu}_1(X_i))}{e(X_i)} + \hat{\mu}_1(X_i) - \frac{(1-D_i)(Y_i - \hat{\mu}_0(X_i))}{1-e(X_i)} - \hat{\mu}_0(X_i) \right]
$$

ã“ã“ã§:
- $\hat{\mu}_1(X) = \mathbb{E}[Y \mid D=1, X]$ (å‡¦ç½®ç¾¤ã®çµæœãƒ¢ãƒ‡ãƒ«)
- $\hat{\mu}_0(X) = \mathbb{E}[Y \mid D=0, X]$ (å¯¾ç…§ç¾¤ã®çµæœãƒ¢ãƒ‡ãƒ«)

**ä¸åæ€§ã®è¨¼æ˜** (ã©ã¡ã‚‰ã‹ä¸€æ–¹ãŒæ­£ã—ã„å ´åˆ):

**Case 1**: $\hat{\mu}_1, \hat{\mu}_0$ ãŒæ­£ã—ã„

$$
\begin{aligned}
\mathbb{E}[\hat{\text{ATE}}_{\text{DR}}] &= \mathbb{E}\left[\mathbb{E}\left[\frac{D(Y - \mu_1(X))}{e(X)} \mid X\right]\right] + \mathbb{E}[\mu_1(X)] - \mathbb{E}[\mu_0(X)] \\
&= \mathbb{E}\left[\frac{e(X)(\mu_1(X) - \mu_1(X))}{e(X)}\right] + \mathbb{E}[Y^1 - Y^0] \\
&= \text{ATE}
\end{aligned}
$$

**Case 2**: $e(X)$ ãŒæ­£ã—ã„ï¼ˆ$\hat{\mu}$ ãŒèª¤ã‚Šã§ã‚‚OKï¼‰

IPWã®ä¸åæ€§ã«ã‚ˆã‚Š $\mathbb{E}[\hat{\text{ATE}}_{\text{DR}}] = \text{ATE}$ã€‚

#### 3.4.4 å…±é€šã‚µãƒãƒ¼ãƒˆ (Common Support) ã¨ãƒˆãƒªãƒŸãƒ³ã‚°

**å…±é€šã‚µãƒãƒ¼ãƒˆæ¡ä»¶**:

$$
0 < e(X) < 1 \quad \forall X \in \text{supp}(X)
$$

**ç ´ã‚Œã‚‹å ´åˆ**: æ¥µç«¯ãª $e(X)$ å€¤ï¼ˆ0ã«è¿‘ã„/1ã«è¿‘ã„ï¼‰ã§ IPW ã®åˆ†æ•£ãŒçˆ†ç™ºã€‚

**ãƒˆãƒªãƒŸãƒ³ã‚°**: $e(X) \in [\epsilon, 1-\epsilon]$ ã®ç¯„å›²ã®ã¿ã‚’ä½¿ç”¨ï¼ˆé€šå¸¸ $\epsilon = 0.05$ or $0.1$ï¼‰

$$
\hat{\text{ATE}}_{\text{trim}} = \frac{1}{n'} \sum_{i: e(X_i) \in [\epsilon, 1-\epsilon]} \left( \frac{D_i Y_i}{e(X_i)} - \frac{(1-D_i) Y_i}{1-e(X_i)} \right)
$$

#### 3.4.5 ãƒãƒ©ãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯ (Balance Check)

å‚¾å‘ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°å¾Œã€**å‡¦ç½®ç¾¤ã¨å¯¾ç…§ç¾¤ã§å…±å¤‰é‡ $X$ ã®åˆ†å¸ƒãŒæƒã£ã¦ã„ã‚‹ã‹**ã‚’ç¢ºèªã€‚

**æ¨™æº–åŒ–å·® (Standardized Mean Difference)**:

$$
\text{SMD} = \frac{\bar{X}_1 - \bar{X}_0}{\sqrt{(s_1^2 + s_0^2)/2}}
$$

$\text{SMD} < 0.1$ ãªã‚‰è‰¯å¥½ãªãƒãƒ©ãƒ³ã‚¹ã€‚

**Love Plot**: å„å…±å¤‰é‡ã® SMD ã‚’ãƒãƒƒãƒãƒ³ã‚°å‰å¾Œã§æ¯”è¼ƒã™ã‚‹ãƒ—ãƒ­ãƒƒãƒˆã€‚

```julia
# Balance check simulation
function balance_check(D, X, e_X)
    # Before matching
    smd_before = abs(mean(X[D]) - mean(X[.!D])) / sqrt((var(X[D]) + var(X[.!D])) / 2)

    # After IPW weighting
    weights_1 = D ./ e_X
    weights_0 = (1 .- D) ./ (1 .- e_X)
    mean_1_weighted = sum(weights_1 .* X) / sum(weights_1)
    mean_0_weighted = sum(weights_0 .* X) / sum(weights_0)
    var_1_weighted = sum(weights_1 .* (X .- mean_1_weighted).^2) / sum(weights_1)
    var_0_weighted = sum(weights_0 .* (X .- mean_0_weighted).^2) / sum(weights_0)
    smd_after = abs(mean_1_weighted - mean_0_weighted) / sqrt((var_1_weighted + var_0_weighted) / 2)

    println("SMD before matching: $(round(smd_before, digits=3))")
    println("SMD after IPW: $(round(smd_after, digits=3))")
    println(smd_after < 0.1 ? "âœ… Good balance" : "âŒ Poor balance")

    return smd_before, smd_after
end
```

### 3.5 æ“ä½œå¤‰æ•°æ³• (Instrumental Variables)

#### 3.5.1 æ“ä½œå¤‰æ•°ã®å®šç¾©

**çŠ¶æ³**: æœªæ¸¬å®šäº¤çµ¡ $U$ ãŒã‚ã‚Šã€unconfoundedness ãŒæˆã‚Šç«‹ãŸãªã„

```mermaid
graph TD
    U["æœªæ¸¬å®šäº¤çµ¡ U"] --> D["å‡¦ç½® D"]
    U --> Y["çµæœ Y"]
    D --> Y
    Z["æ“ä½œå¤‰æ•° Z"] --> D
```

**æ“ä½œå¤‰æ•° $Z$ ã®3æ¡ä»¶**:

1. **é–¢é€£æ€§ (Relevance)**: $Z \perp\!\!\!\perp D$ ($Z$ ãŒ $D$ ã«å½±éŸ¿)
2. **å¤–ç”Ÿæ€§ (Exogeneity)**: $Z \perp\!\!\!\perp U$ ($Z$ ã¯äº¤çµ¡ã¨ç„¡ç›¸é–¢)
3. **æ’é™¤åˆ¶ç´„ (Exclusion Restriction)**: $Z \to Y$ ã®ç›´æ¥ãƒ‘ã‚¹ãªã—ï¼ˆ$Z$ ã¯ $D$ çµŒç”±ã§ã®ã¿ $Y$ ã«å½±éŸ¿ï¼‰

**ä¾‹**: å…µå½¹ãŒåå…¥ã«ä¸ãˆã‚‹å½±éŸ¿

- $D$: å…µå½¹çµŒé¨“ (1=ã‚ã‚Š, 0=ãªã—)
- $Y$: ç”Ÿæ¶¯åå…¥
- $U$: èƒ½åŠ›ï¼ˆæœªæ¸¬å®šï¼‰
- $Z$: å¾´å…µãã˜ (1=å½“é¸, 0=å¤–ã‚Œ)

å¾´å…µãã˜ã¯èƒ½åŠ› $U$ ã¨ç„¡é–¢ä¿‚ï¼ˆå¤–ç”Ÿï¼‰ã€å…µå½¹ $D$ ã«å½±éŸ¿ï¼ˆé–¢é€£ï¼‰ã€åå…¥ $Y$ ã«ã¯å…µå½¹çµŒç”±ã§ã®ã¿å½±éŸ¿ï¼ˆæ’é™¤åˆ¶ç´„ï¼‰ã€‚

#### 3.5.2 2SLS (Two-Stage Least Squares)

**ç¬¬1æ®µéš**: $D$ ã‚’ $Z$ ã§å›å¸°

$$
D_i = \pi_0 + \pi_1 Z_i + \nu_i
$$

$\hat{D}_i = \hat{\pi}_0 + \hat{\pi}_1 Z_i$ ã‚’äºˆæ¸¬ã€‚

**ç¬¬2æ®µéš**: $Y$ ã‚’ $\hat{D}$ ã§å›å¸°

$$
Y_i = \beta_0 + \beta_1 \hat{D}_i + \epsilon_i
$$

$\hat{\beta}_1$ ãŒå› æœåŠ¹æœã®æ¨å®šå€¤ã€‚

**å°å‡º (ç°¡ç•¥ç‰ˆ)**:

$$
\begin{aligned}
\text{Cov}(Y, Z) &= \text{Cov}(\beta_0 + \beta_1 D + U, Z) \\
&= \beta_1 \text{Cov}(D, Z) + \text{Cov}(U, Z) \\
&= \beta_1 \text{Cov}(D, Z) \quad \text{(å¤–ç”Ÿæ€§: } \text{Cov}(U, Z)=0)
\end{aligned}
$$

$$
\hat{\beta}_1 = \frac{\text{Cov}(Y, Z)}{\text{Cov}(D, Z)}
$$

**Waldæ¨å®šé‡** (2å€¤ $Z$ ã®å ´åˆ):

$$
\hat{\beta}_1 = \frac{\mathbb{E}[Y \mid Z=1] - \mathbb{E}[Y \mid Z=0]}{\mathbb{E}[D \mid Z=1] - \mathbb{E}[D \mid Z=0]}
$$

#### 3.5.3 LATE (Local Average Treatment Effect)

IVã§æ¨å®šã•ã‚Œã‚‹ã®ã¯**ATE**ã§ã¯ãªã**LATE** â€” ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ¼ (Complier) ã®å‡¦ç½®åŠ¹æœã€‚

**4ã¤ã®ã‚¿ã‚¤ãƒ—**:

| ã‚¿ã‚¤ãƒ— | $D(Z=0)$ | $D(Z=1)$ | èª¬æ˜ |
|:------|:---------|:---------|:-----|
| **Always-Taker** | 1 | 1 | å¸¸ã«å‡¦ç½®ã‚’å—ã‘ã‚‹ |
| **Never-Taker** | 0 | 0 | å¸¸ã«å‡¦ç½®ã‚’å—ã‘ãªã„ |
| **Complier** | 0 | 1 | IVã«å¾“ã† |
| **Defier** | 1 | 0 | IVã«é€†ã‚‰ã† (monotonicityä»®å®šã§æ’é™¤) |

**LATE**:

$$
\text{LATE} = \mathbb{E}[Y^1 - Y^0 \mid \text{Complier}]
$$

**å°å‡º**:

$$
\begin{aligned}
\mathbb{E}[Y \mid Z=1] - \mathbb{E}[Y \mid Z=0] &= \mathbb{E}[Y^1 - Y^0] \cdot P(\text{Complier}) \\
\mathbb{E}[D \mid Z=1] - \mathbb{E}[D \mid Z=0] &= P(\text{Complier})
\end{aligned}
$$

$$
\text{LATE} = \frac{\mathbb{E}[Y \mid Z=1] - \mathbb{E}[Y \mid Z=0]}{\mathbb{E}[D \mid Z=1] - \mathbb{E}[D \mid Z=0]}
$$

#### 3.5.4 Weak IV (å¼±æ“ä½œå¤‰æ•°) å•é¡Œ

**å¼±IV**: $\text{Cov}(D, Z)$ ãŒå°ã•ã„ â†’ ç¬¬1æ®µéšã® $F$ çµ±è¨ˆé‡ãŒä½ã„

**Stock-Yogo åŸºæº–** [^7]:

$$
F \text{-statistic} = \frac{(\text{RSS}_{\text{restricted}} - \text{RSS}_{\text{unrestricted}}) / q}{\text{RSS}_{\text{unrestricted}} / (n - k)} > 10
$$

$F < 10$ ãªã‚‰å¼±IVï¼ˆãƒã‚¤ã‚¢ã‚¹ãŒå¤§ãã„ï¼‰ã€‚

**å•é¡Œç‚¹**:

- 2SLSæ¨å®šé‡ã®ãƒã‚¤ã‚¢ã‚¹ãŒ OLS ã‚ˆã‚Šæ‚ªåŒ–
- æ¨™æº–èª¤å·®ãŒéå°è©•ä¾¡ã•ã‚Œã‚‹
- ä¿¡é ¼åŒºé–“ãŒéåº¦ã«ç‹­ããªã‚‹

**å¯¾ç­–**:

- Anderson-Rubin æ¤œå®šï¼ˆå¼±IVã«é ‘å¥ï¼‰
- LIML (Limited Information Maximum Likelihood)
- ã‚ˆã‚Šå¼·ã„IVã‚’æ¢ã™

### 3.6 å›å¸°ä¸é€£ç¶šãƒ‡ã‚¶ã‚¤ãƒ³ (RDD)

#### 3.6.1 RDDã®è¨­å®š

**çŠ¶æ³**: å‡¦ç½®å‰²ã‚Šå½“ã¦ãŒ**ã‚«ãƒƒãƒˆã‚ªãƒ• $c$** ã§æ±ºã¾ã‚‹

$$
D_i = \mathbb{1}(X_i \geq c)
$$

$X$: ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°å¤‰æ•° (running variable) â€” ä¾‹: ãƒ†ã‚¹ãƒˆã‚¹ã‚³ã‚¢ã€å¹´é½¢ã€æ‰€å¾—

**å±€æ‰€ãƒ©ãƒ³ãƒ€ãƒ åŒ–ä»®å®š**: $c$ ã®è¿‘å‚ã§ $X$ ã¯ as-if ãƒ©ãƒ³ãƒ€ãƒ 

$$
\lim_{x \to c^+} \mathbb{E}[Y^1 \mid X=x] - \lim_{x \to c^-} \mathbb{E}[Y^0 \mid X=x] = \text{ATE}_c
$$

#### 3.6.2 Sharp RDD vs Fuzzy RDD

**Sharp RDD**: ã‚«ãƒƒãƒˆã‚ªãƒ•ã§å‡¦ç½®ç¢ºç‡ãŒ 0 â†’ 1 ã«ä¸é€£ç¶šã«ã‚¸ãƒ£ãƒ³ãƒ—

$$
\lim_{x \to c^-} P(D=1 \mid X=x) = 0, \quad \lim_{x \to c^+} P(D=1 \mid X=x) = 1
$$

**Fuzzy RDD**: ã‚«ãƒƒãƒˆã‚ªãƒ•ã§å‡¦ç½®ç¢ºç‡ãŒã‚¸ãƒ£ãƒ³ãƒ—ã™ã‚‹ãŒ 0/1 ã§ã¯ãªã„

$$
\lim_{x \to c^-} P(D=1 \mid X=x) < \lim_{x \to c^+} P(D=1 \mid X=x) < 1
$$

Fuzzy RDDã¯IVã¨ã—ã¦æ‰±ã†: $Z = \mathbb{1}(X \geq c)$ ã‚’æ“ä½œå¤‰æ•°ã¨ã—ã€2SLSæ¨å®šã€‚

#### 3.6.3 RDDæ¨å®šé‡

**Local Linear Regression**:

$$
\min_{\beta_0, \beta_1, \beta_2, \beta_3} \sum_{i: |X_i - c| < h} K\left(\frac{X_i - c}{h}\right) (Y_i - \beta_0 - \beta_1 D_i - \beta_2 (X_i - c) - \beta_3 D_i (X_i - c))^2
$$

ã“ã“ã§:
- $h$: å¸¯åŸŸå¹… (bandwidth)
- $K(\cdot)$: ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ï¼ˆä¸‰è§’ã‚«ãƒ¼ãƒãƒ«ã€Epanechnikov ã‚«ãƒ¼ãƒãƒ«ç­‰ï¼‰

**RDDåŠ¹æœ**: $\hat{\beta}_1$

**å¸¯åŸŸå¹…é¸æŠ**:

- IK (Imbens-Kalyanaraman) å¸¯åŸŸå¹…
- CCT (Calonico-Cattaneo-Titiunik) å¸¯åŸŸå¹…ï¼ˆãƒã‚¤ã‚¢ã‚¹è£œæ­£ä»˜ãï¼‰

$$
h_{\text{IK}} = C \cdot \left(\frac{\text{var}(\epsilon)}{n \cdot f(c) \cdot (\mu^{(2)}(c^+) - \mu^{(2)}(c^-))^2}\right)^{1/5}
$$

#### 3.6.4 RDDã®å¦¥å½“æ€§æ¤œå®š

**1. é€£ç¶šæ€§æ¤œå®š (Continuity Tests)**

å…±å¤‰é‡ $X$ ãŒã‚«ãƒƒãƒˆã‚ªãƒ• $c$ ã§é€£ç¶šã‹ç¢ºèª:

$$
\lim_{x \to c^+} \mathbb{E}[X_{\text{covariate}} \mid X=x] = \lim_{x \to c^-} \mathbb{E}[X_{\text{covariate}} \mid X=x]
$$

**2. å¯†åº¦æ¤œå®š (McCrary Density Test)**

$X$ ã®å¯†åº¦ $f(X)$ ãŒã‚«ãƒƒãƒˆã‚ªãƒ•ã§ä¸é€£ç¶šãªã‚‰æ“ä½œã®ç–‘ã„:

$$
\lim_{x \to c^+} f(x) \neq \lim_{x \to c^-} f(x) \quad \Rightarrow \quad \text{manipulation}
$$

**3. Placebo Test**

å½ã‚«ãƒƒãƒˆã‚ªãƒ• $c' \neq c$ ã§åŠ¹æœãŒã‚¼ãƒ­ã‹ç¢ºèªã€‚

### 3.7 å·®åˆ†ã®å·®åˆ†æ³• (DiD)

#### 3.7.1 DiDã®è¨­å®š

**2æœŸé–“ãƒ»2ã‚°ãƒ«ãƒ¼ãƒ—**:

| | å‡¦ç½®å‰ $(t=0)$ | å‡¦ç½®å¾Œ $(t=1)$ |
|:--|:--------------|:--------------|
| **å‡¦ç½®ç¾¤** $(G=1)$ | $\mathbb{E}[Y_{10}]$ | $\mathbb{E}[Y_{11}]$ |
| **å¯¾ç…§ç¾¤** $(G=0)$ | $\mathbb{E}[Y_{00}]$ | $\mathbb{E}[Y_{01}]$ |

**DiDæ¨å®šé‡**:

$$
\hat{\tau}_{\text{DiD}} = (\mathbb{E}[Y_{11}] - \mathbb{E}[Y_{10}]) - (\mathbb{E}[Y_{01}] - \mathbb{E}[Y_{00}])
$$

**ä¸¦è¡Œãƒˆãƒ¬ãƒ³ãƒ‰ä»®å®š (Parallel Trends)**:

$$
\mathbb{E}[Y_{01} - Y_{00} \mid G=1] = \mathbb{E}[Y_{01} - Y_{00} \mid G=0]
$$

å‡¦ç½®ãŒãªã‹ã£ãŸå ´åˆã€å‡¦ç½®ç¾¤ã¨å¯¾ç…§ç¾¤ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã¯å¹³è¡Œã€‚

#### 3.7.2 DiDå›å¸°ãƒ¢ãƒ‡ãƒ«

$$
Y_{it} = \alpha + \beta \cdot \text{Treat}_i + \gamma \cdot \text{Post}_t + \delta \cdot (\text{Treat}_i \times \text{Post}_t) + \epsilon_{it}
$$

ã“ã“ã§:
- $\text{Treat}_i = \mathbb{1}(i \in \text{å‡¦ç½®ç¾¤})$
- $\text{Post}_t = \mathbb{1}(t \geq 1)$
- $\delta = \text{DiDåŠ¹æœ}$

**å›ºå®šåŠ¹æœãƒ¢ãƒ‡ãƒ«**:

$$
Y_{it} = \alpha_i + \lambda_t + \delta D_{it} + \epsilon_{it}
$$

$\alpha_i$: å€‹ä½“å›ºå®šåŠ¹æœã€$\lambda_t$: æ™‚é–“å›ºå®šåŠ¹æœ

#### 3.7.3 Staggered DiD (å¤šæœŸé–“ãƒ»æ®µéšçš„å°å…¥)

**å•é¡Œ**: å‡¦ç½®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ãŒç•°ãªã‚‹ï¼ˆ$G_i$ ã«ã‚ˆã£ã¦å‡¦ç½®é–‹å§‹æ™‚æœŸãŒé•ã†ï¼‰

å¾“æ¥ã®TWFE (Two-Way Fixed Effects) ã¯**ãƒã‚¤ã‚¢ã‚¹ã‚ã‚Š** â€” æ—¢å‡¦ç½®ç¾¤ãŒå¯¾ç…§ç¾¤ã¨ã—ã¦ä½¿ã‚ã‚Œã‚‹ã€‚

**Callaway & Sant'Anna (2021) [^5]**:

å„ã‚³ãƒ›ãƒ¼ãƒˆ $g$ (å‡¦ç½®é–‹å§‹æ™‚æœŸ) ã¨æ™‚ç‚¹ $t$ ã®ãƒšã‚¢ã§ DiD ã‚’æ¨å®š:

$$
\text{ATT}(g, t) = \mathbb{E}[Y_t - Y_{g-1} \mid G_g=1] - \mathbb{E}[Y_t - Y_{g-1} \mid C=1]
$$

$C$: æœªå‡¦ç½®ç¾¤ï¼ˆnever-treated or not-yet-treatedï¼‰

**é›†ç´„**:

$$
\text{ATT}_{\text{overall}} = \sum_{g} \sum_{t \geq g} w(g, t) \cdot \text{ATT}(g, t)
$$

é‡ã¿ $w(g, t)$ ã¯å‡¦ç½®ç¾¤ã®ã‚µã‚¤ã‚ºç­‰ã«åŸºã¥ãã€‚

### 3.8 æ©Ÿæ¢°å­¦ç¿’Ã—å› æœæ¨è«–

#### 3.8.1 Causal Forest (å› æœãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ)

**ç›®æ¨™**: ç•°è³ªãªå‡¦ç½®åŠ¹æœ $\tau(X) = \mathbb{E}[Y^1 - Y^0 \mid X]$ ã‚’æ¨å®š

Wager & Athey (2018) [^3] ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ :

1. **ã‚µãƒ³ãƒ—ãƒ«åˆ†å‰²**: å„ãƒ„ãƒªãƒ¼ã§è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«ã‚’ $I$ (åˆ†å‰²ç”¨) ã¨ $J$ (æ¨å®šç”¨) ã«åˆ†å‰²
2. **åˆ†å‰²**: $I$ ã‚’ä½¿ã£ã¦CARTã§åˆ†å‰²ï¼ˆå‡¦ç½®åŠ¹æœã®åˆ†æ•£ã‚’æœ€å¤§åŒ–ï¼‰
3. **æ¨å®š**: å„ãƒªãƒ¼ãƒ• $L$ ã§ $J$ ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½¿ã£ã¦ $\hat{\tau}(x)$ æ¨å®š

**æ¨å®šé‡**:

$$
\hat{\tau}(x) = \frac{\sum_{i \in L(x)} (2D_i - 1) Y_i}{\sum_{i \in L(x)} |2D_i - 1|}
$$

**ç†è«–ä¿è¨¼**:

- Pointwise consistency: $\hat{\tau}(x) \to \tau(x)$
- æ¼¸è¿‘æ­£è¦æ€§: $\sqrt{n}(\hat{\tau}(x) - \tau(x)) \xrightarrow{d} \mathcal{N}(0, \sigma^2(x))$

#### 3.8.2 Double/Debiased Machine Learning (DML)

**å•é¡Œ**: MLäºˆæ¸¬ã‚’å› æœæ¨è«–ã«ä½¿ã†ã¨æ­£å‰‡åŒ–ãƒã‚¤ã‚¢ã‚¹ãŒæ®‹ã‚‹

Chernozhukov et al. (2018) [^4] ã®è§£æ±ºç­–:

**1. Neyman-Orthogonal Score**

$$
\psi(W; \theta, \eta) = (Y - m(X)) - \theta (D - e(X))
$$

ã“ã“ã§ $\eta = (m, e)$ ã¯ nuisance ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€$\theta$ ã¯å› æœãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚

**2. Cross-Fitting**

ã‚µãƒ³ãƒ—ãƒ«ã‚’ $K$ åˆ†å‰² â†’ $k$ ç•ªç›®ã®foldã§ $\eta$ ã‚’æ¨å®š â†’ ä»–ã®foldã§ $\theta$ æ¨å®š â†’ é›†ç´„

**DMLæ¨å®šé‡**:

$$
\hat{\theta}_{\text{DML}} = \left(\frac{1}{n} \sum_i (D_i - \hat{e}(X_i))^2\right)^{-1} \frac{1}{n} \sum_i (D_i - \hat{e}(X_i))(Y_i - \hat{m}(X_i))
$$

**ç†è«–ä¿è¨¼**:

$$
\sqrt{n}(\hat{\theta}_{\text{DML}} - \theta) \xrightarrow{d} \mathcal{N}(0, V)
$$

MLæ¨å®šèª¤å·®ãŒ $o_P(n^{-1/4})$ ãªã‚‰ä¸åã€‚

#### 3.8.3 Meta-Learners (S/T/X/R-Learner)

**S-Learner** (Single model):

$$
\mu(X, D) = \mathbb{E}[Y \mid X, D], \quad \hat{\tau}(X) = \hat{\mu}(X, 1) - \hat{\mu}(X, 0)
$$

**T-Learner** (Two models):

$$
\mu_1(X) = \mathbb{E}[Y \mid X, D=1], \quad \mu_0(X) = \mathbb{E}[Y \mid X, D=0], \quad \hat{\tau}(X) = \hat{\mu}_1(X) - \hat{\mu}_0(X)
$$

**X-Learner** (å‡¦ç½®ç¾¤ãƒ»å¯¾ç…§ç¾¤ã®åå®Ÿä»®æƒ³ã‚’æ¨å®š):

1. $\hat{\mu}_1, \hat{\mu}_0$ ã‚’æ¨å®š
2. åå®Ÿä»®æƒ³: $\tilde{\tau}_1(X_i) = Y_i - \hat{\mu}_0(X_i)$ (å‡¦ç½®ç¾¤), $\tilde{\tau}_0(X_i) = \hat{\mu}_1(X_i) - Y_i$ (å¯¾ç…§ç¾¤)
3. $\hat{\tau}_1(X), \hat{\tau}_0(X)$ ã‚’ $\tilde{\tau}$ ã§å›å¸°
4. æœ€çµ‚æ¨å®š: $\hat{\tau}(X) = g(X) \hat{\tau}_1(X) + (1 - g(X)) \hat{\tau}_0(X)$

**R-Learner** (Robinsonå¤‰æ›):

$$
\tilde{Y} = Y - \hat{m}(X), \quad \tilde{D} = D - \hat{e}(X)
$$

$$
\hat{\tau}(X) = \arg\min_{\tau} \mathbb{E}[(\tilde{Y} - \tilde{D} \tau(X))^2]
$$

:::message alert
**ãƒœã‚¹æˆ¦: å› æœåŠ¹æœã®å®Œå…¨æ¨å®š**

ä»¥ä¸‹ã®ã‚·ãƒŠãƒªã‚ªã§å› æœåŠ¹æœã‚’æ¨å®šã›ã‚ˆ:

1. è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿: $(D, X, Y)$ with $n=5000$
2. æœªæ¸¬å®šäº¤çµ¡ $U$ ã‚ã‚Š
3. æ“ä½œå¤‰æ•° $Z$ (å¾´å…µãã˜) ãŒåˆ©ç”¨å¯èƒ½
4. ã‚«ãƒƒãƒˆã‚ªãƒ• $c=18$ (å¹´é½¢) ã§RDDé©ç”¨å¯èƒ½
5. 2æœŸé–“ãƒ‡ãƒ¼ã‚¿ã‚ã‚Š (DiDå¯èƒ½)

**ã‚¿ã‚¹ã‚¯**:
- å„æ‰‹æ³• (IPW, IV, RDD, DiD, Causal Forest) ã§ ATE æ¨å®š
- æ¨™æº–èª¤å·®ã‚’è¨ˆç®—
- çµæœã‚’æ¯”è¼ƒã—ã€æœ€ã‚‚é ‘å¥ãªæ¨å®šå€¤ã‚’é¸ã¶

ã“ã‚ŒãŒã§ãã‚Œã°æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³å®Œå…¨ã‚¯ãƒªã‚¢ï¼
:::

:::message
**é€²æ—: 50% å®Œäº†** å› æœæ¨è«–ç†è«–ã‚’å®Œå…¨ç¿’å¾—ã—ãŸã€‚Rubin/Pearl/å‚¾å‘ã‚¹ã‚³ã‚¢/IV/RDD/DiD/MLÃ—å› æœã‚’æ•°å¼ã‹ã‚‰å°å‡ºã—ãŸã€‚æ¬¡ã¯å®Ÿè£…ã‚¾ãƒ¼ãƒ³ã§Julia + CausalInference.jlã§å…¨æ‰‹æ³•ã‚’å®Ÿè£…ã™ã‚‹ã€‚
:::

---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” Juliaå› æœæ¨è«–ãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯

### 4.1 CausalInference.jl ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```julia
# Package installation
using Pkg
Pkg.add(["CausalInference", "Graphs", "GLM", "DataFrames", "Statistics",
         "LinearAlgebra", "Distributions", "StatsBase", "CategoricalArrays"])

using CausalInference
using Graphs  # DAG manipulation
using GLM     # Propensity score estimation
using DataFrames
using Statistics, LinearAlgebra
using Distributions
using StatsBase
using CategoricalArrays
```

### 4.2 Pearl DAG + do-æ¼”ç®—å®Ÿè£…

#### 4.2.1 DAGæ§‹ç¯‰ã¨å¯è¦–åŒ–

```julia
# DAG construction: Smoking â†’ Cancer, Gene â†’ Smoking, Gene â†’ Cancer
function build_smoking_cancer_dag()
    # Create directed graph
    # Nodes: 1=Gene, 2=Smoking, 3=Cancer
    dag = SimpleDiGraph(3)
    add_edge!(dag, 1, 2)  # Gene â†’ Smoking
    add_edge!(dag, 1, 3)  # Gene â†’ Cancer
    add_edge!(dag, 2, 3)  # Smoking â†’ Cancer

    node_names = ["Gene", "Smoking", "Cancer"]
    return dag, node_names
end

dag, names = build_smoking_cancer_dag()
println("DAG edges:")
for edge in edges(dag)
    println("  $(names[src(edge)]) â†’ $(names[dst(edge)])")
end

# d-separation check
using CausalInference: dsep

# Are Smoking and Cancer d-separated by Gene?
# dsep(dag, [2], [3], [1])  # false (Gene doesn't block the direct path Smokingâ†’Cancer)
println("Smoking âŠ¥ Cancer | Gene? $(dsep(dag, [2], [3], [1]))")

# Are Gene and Cancer d-separated by Smoking?
# dsep(dag, [1], [3], [2])  # false (Geneâ†’Cancer direct path remains)
println("Gene âŠ¥ Cancer | Smoking? $(dsep(dag, [1], [3], [2]))")
```

#### 4.2.2 ãƒãƒƒã‚¯ãƒ‰ã‚¢åŸºæº–ã®æ¤œè¨¼

```julia
using CausalInference: backdoor_criterion

# Check if {Gene} satisfies backdoor criterion for (Smoking, Cancer)
function check_backdoor(dag, treatment, outcome, adjustment_set)
    # CausalInference.jl implementation
    # backdoor_criterion(dag, treatment, outcome, adjustment_set)
    # Returns true if adjustment_set satisfies backdoor criterion

    # Manual check:
    # 1. No node in adjustment_set is descendant of treatment
    # 2. adjustment_set blocks all backdoor paths from treatment to outcome

    # In our DAG: Smoking(2) â†’ Cancer(3), backdoor path: Smoking â† Gene â†’ Cancer
    # Adjusting for Gene(1) blocks this path

    result = CausalInference.backdoor_criterion(dag, [treatment], [outcome], adjustment_set)
    return result
end

is_valid = check_backdoor(dag, 2, 3, [1])
println("Does {Gene} satisfy backdoor criterion for (Smoking, Cancer)? $is_valid")
```

#### 4.2.3 do-æ¼”ç®—ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

```julia
# Simulate observational data from the DAG
function simulate_from_dag(n::Int=5000)
    # Gene ~ Bernoulli(0.3)
    gene = rand(Bernoulli(0.3), n)

    # Smoking | Gene ~ Bernoulli(logistic(0.5 * Gene))
    smoking_prob = 1 ./ (1 .+ exp.(-(0.5 .* gene .- 0.2)))
    smoking = rand.(Bernoulli.(smoking_prob))

    # Cancer | Smoking, Gene ~ Bernoulli(logistic(1.5 * Smoking + 0.8 * Gene))
    cancer_prob = 1 ./ (1 .+ exp.(-(1.5 .* smoking .+ 0.8 .* gene .- 1.0)))
    cancer = rand.(Bernoulli.(cancer_prob))

    return DataFrame(Gene=gene, Smoking=smoking, Cancer=cancer)
end

data = simulate_from_dag(5000)

# Observational: P(Cancer | Smoking)
obs_cancer_smoking = mean(data[data.Smoking .== 1, :Cancer])
obs_cancer_nonsmoking = mean(data[data.Smoking .== 0, :Cancer])
obs_effect = obs_cancer_smoking - obs_cancer_nonsmoking
println("Observational P(Cancer|Smoking=1) - P(Cancer|Smoking=0): $(round(obs_effect, digits=3))")

# Interventional: P(Cancer | do(Smoking)) via backdoor adjustment
function backdoor_adjustment(data, treatment, outcome, adjustment)
    # P(Y | do(X=x)) = Î£_z P(Y|X=x, Z=z) P(Z=z)
    result = Dict()
    for x in [0, 1]
        prob_y = 0.0
        for z in unique(data[:, adjustment])
            # P(Y=1 | X=x, Z=z)
            subset = data[(data[:, treatment] .== x) .& (data[:, adjustment] .== z), :]
            if nrow(subset) > 0
                p_y_given_xz = mean(subset[:, outcome])
            else
                p_y_given_xz = 0.0
            end

            # P(Z=z)
            p_z = mean(data[:, adjustment] .== z)

            prob_y += p_y_given_xz * p_z
        end
        result[x] = prob_y
    end
    return result
end

intervene = backdoor_adjustment(data, :Smoking, :Cancer, :Gene)
do_effect = intervene[1] - intervene[0]
println("Interventional P(Cancer|do(Smoking=1)) - P(Cancer|do(Smoking=0)): $(round(do_effect, digits=3))")
println("Difference (confounding bias): $(round(obs_effect - do_effect, digits=3))")
```

### 4.3 å‚¾å‘ã‚¹ã‚³ã‚¢å®Ÿè£…

#### 4.3.1 å‚¾å‘ã‚¹ã‚³ã‚¢æ¨å®š (Logistic Regression)

```julia
using GLM

function estimate_propensity_score(data::DataFrame, treatment::Symbol, covariates::Vector{Symbol})
    # Logistic regression: D ~ X
    formula = term(treatment) ~ sum(term.(covariates))
    model = glm(formula, data, Binomial(), LogitLink())

    # Predict propensity scores
    e_X = predict(model, data)

    return e_X, model
end

# Example: Treatment depends on Age and Income
function generate_ps_data(n::Int=2000)
    age = rand(Normal(40, 10), n)
    income = rand(Normal(50, 15), n)

    # Treatment assignment depends on age and income
    propensity = 1 ./ (1 .+ exp.(-(0.05 .* age .+ 0.03 .* income .- 3.5)))
    treatment = rand(n) .< propensity

    # Outcome depends on treatment + confounders
    outcome = 2.0 .* treatment .+ 0.5 .* age .+ 0.3 .* income .+ randn(n) * 5

    return DataFrame(Treatment=treatment, Age=age, Income=income, Outcome=outcome)
end

ps_data = generate_ps_data(2000)
e_X, ps_model = estimate_propensity_score(ps_data, :Treatment, [:Age, :Income])

# Add to dataframe
ps_data.PropensityScore = e_X
println("Propensity score range: [$(round(minimum(e_X), digits=3)), $(round(maximum(e_X), digits=3))]")
```

#### 4.3.2 IPWæ¨å®š

```julia
function ipw_estimator(data::DataFrame, treatment::Symbol, outcome::Symbol, propensity::Symbol)
    D = data[:, treatment]
    Y = data[:, outcome]
    e = data[:, propensity]

    # Trimming: exclude extreme propensity scores
    Îµ = 0.05
    valid = (e .> Îµ) .& (e .< (1 - Îµ))
    D_trim = D[valid]
    Y_trim = Y[valid]
    e_trim = e[valid]

    # IPW ATE estimator
    ate = mean(D_trim .* Y_trim ./ e_trim) - mean((1 .- D_trim) .* Y_trim ./ (1 .- e_trim))

    # Variance estimation (Horvitz-Thompson)
    n = length(D_trim)
    var_ipw = var(D_trim .* Y_trim ./ e_trim - (1 .- D_trim) .* Y_trim ./ (1 .- e_trim)) / n
    se = sqrt(var_ipw)

    return ate, se
end

ate_ipw, se_ipw = ipw_estimator(ps_data, :Treatment, :Outcome, :PropensityScore)
println("IPW ATE: $(round(ate_ipw, digits=3)) Â± $(round(1.96*se_ipw, digits=3)) (95% CI)")

# Compare with naive
ate_naive = mean(ps_data[ps_data.Treatment .== 1, :Outcome]) - mean(ps_data[ps_data.Treatment .== 0, :Outcome])
println("Naive ATE: $(round(ate_naive, digits=3))")
println("True ATE: 2.0")
```

#### 4.3.3 Doubly Robustæ¨å®š

```julia
function doubly_robust_estimator(data::DataFrame, treatment::Symbol, outcome::Symbol,
                                  covariates::Vector{Symbol}, propensity::Symbol)
    D = data[:, treatment]
    Y = data[:, outcome]
    e = data[:, propensity]

    # Outcome regression models
    # Î¼â‚(X) = E[Y | D=1, X]
    data_treated = data[data[:, treatment] .== 1, :]
    formula_1 = term(outcome) ~ sum(term.(covariates))
    model_1 = lm(formula_1, data_treated)
    Î¼_1 = predict(model_1, data)

    # Î¼â‚€(X) = E[Y | D=0, X]
    data_control = data[data[:, treatment] .== 0, :]
    model_0 = lm(formula_1, data_control)
    Î¼_0 = predict(model_0, data)

    # DR estimator
    dr_term_1 = D .* (Y .- Î¼_1) ./ e .+ Î¼_1
    dr_term_0 = (1 .- D) .* (Y .- Î¼_0) ./ (1 .- e) .+ Î¼_0
    ate_dr = mean(dr_term_1 - dr_term_0)

    var_dr = var(dr_term_1 - dr_term_0) / nrow(data)
    se_dr = sqrt(var_dr)

    return ate_dr, se_dr
end

ate_dr, se_dr = doubly_robust_estimator(ps_data, :Treatment, :Outcome, [:Age, :Income], :PropensityScore)
println("Doubly Robust ATE: $(round(ate_dr, digits=3)) Â± $(round(1.96*se_dr, digits=3)) (95% CI)")
```

#### 4.3.4 ãƒãƒ©ãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯

```julia
function balance_check(data::DataFrame, treatment::Symbol, covariates::Vector{Symbol}, propensity::Symbol)
    println("\n=== Balance Check ===")
    for cov in covariates
        # Before matching
        mean_treated = mean(data[data[:, treatment] .== 1, cov])
        mean_control = mean(data[data[:, treatment] .== 0, cov])
        std_pooled = sqrt((var(data[data[:, treatment] .== 1, cov]) +
                           var(data[data[:, treatment] .== 0, cov])) / 2)
        smd_before = abs(mean_treated - mean_control) / std_pooled

        # After IPW weighting
        D = data[:, treatment]
        X = data[:, cov]
        e = data[:, propensity]

        weights_1 = D ./ e
        weights_0 = (1 .- D) ./ (1 .- e)

        mean_1_weighted = sum(weights_1 .* X) / sum(weights_1)
        mean_0_weighted = sum(weights_0 .* X) / sum(weights_0)

        var_1_weighted = sum(weights_1 .* (X .- mean_1_weighted).^2) / sum(weights_1)
        var_0_weighted = sum(weights_0 .* (X .- mean_0_weighted).^2) / sum(weights_0)

        std_pooled_weighted = sqrt((var_1_weighted + var_0_weighted) / 2)
        smd_after = abs(mean_1_weighted - mean_0_weighted) / std_pooled_weighted

        status = smd_after < 0.1 ? "âœ…" : "âŒ"
        println("$cov: SMD before=$(round(smd_before, digits=3)), after=$(round(smd_after, digits=3)) $status")
    end
end

balance_check(ps_data, :Treatment, [:Age, :Income], :PropensityScore)
```

### 4.4 æ“ä½œå¤‰æ•°æ³• (2SLS) å®Ÿè£…

```julia
using GLM

function two_stage_least_squares(data::DataFrame, outcome::Symbol, treatment::Symbol,
                                  instrument::Symbol, covariates::Vector{Symbol}=[])
    # Stage 1: D ~ Z + X
    formula_stage1 = if isempty(covariates)
        term(treatment) ~ term(instrument)
    else
        term(treatment) ~ term(instrument) + sum(term.(covariates))
    end

    model_stage1 = lm(formula_stage1, data)
    D_hat = predict(model_stage1, data)

    # Check first-stage F-statistic
    f_stat = ftest(model_stage1.model).fstat[1]
    println("First-stage F-statistic: $(round(f_stat, digits=2))")
    if f_stat < 10
        @warn "Weak IV detected (F < 10)"
    end

    # Stage 2: Y ~ D_hat + X
    data_stage2 = copy(data)
    data_stage2[!, :D_hat] = D_hat

    formula_stage2 = if isempty(covariates)
        term(outcome) ~ term(:D_hat)
    else
        term(outcome) ~ term(:D_hat) + sum(term.(covariates))
    end

    model_stage2 = lm(formula_stage2, data_stage2)

    # 2SLS coefficient
    Î²_2sls = coef(model_stage2)[2]  # coefficient on D_hat
    se_2sls = stderror(model_stage2)[2]

    return Î²_2sls, se_2sls, f_stat
end

# Generate IV data
function generate_iv_data(n::Int=2000)
    # Unobserved confounder
    U = randn(n)

    # Instrument Z (independent of U)
    Z = rand(Bernoulli(0.5), n)

    # Treatment D depends on Z and U (endogenous)
    D = Z .+ 0.5 .* U .+ randn(n) * 0.3
    D = D .> median(D)  # binarize

    # Outcome Y depends on D and U (confounded)
    # True causal effect of D: 2.0
    Y = 2.0 .* D .+ U .+ randn(n) * 0.5

    return DataFrame(Outcome=Y, Treatment=D, Instrument=Z)
end

iv_data = generate_iv_data(2000)

# 2SLS estimation
Î²_2sls, se_2sls, f_stat = two_stage_least_squares(iv_data, :Outcome, :Treatment, :Instrument)
println("2SLS estimate: $(round(Î²_2sls, digits=3)) Â± $(round(1.96*se_2sls, digits=3)) (95% CI)")
println("True causal effect: 2.0")

# Compare with naive OLS (biased)
ols_model = lm(@formula(Outcome ~ Treatment), iv_data)
Î²_ols = coef(ols_model)[2]
println("Naive OLS estimate: $(round(Î²_ols, digits=3)) (biased upward due to U)")
```

### 4.5 RDDå®Ÿè£…

```julia
function regression_discontinuity(data::DataFrame, outcome::Symbol, running_var::Symbol,
                                   cutoff::Float64, bandwidth::Float64)
    # Local linear regression on both sides of cutoff
    X = data[:, running_var]
    Y = data[:, outcome]

    # Filter data within bandwidth
    in_bandwidth = abs.(X .- cutoff) .<= bandwidth
    X_local = X[in_bandwidth]
    Y_local = Y[in_bandwidth]

    # Treatment indicator
    D_local = X_local .>= cutoff

    # Centered running variable
    X_centered = X_local .- cutoff

    # Local linear regression: Y ~ D + X_centered + D*X_centered
    design_matrix = hcat(ones(length(Y_local)), D_local, X_centered, D_local .* X_centered)
    Î² = design_matrix \ Y_local

    # RDD effect = coefficient on D
    rdd_effect = Î²[2]

    # Standard error (simplified - use robust SE in practice)
    residuals = Y_local - design_matrix * Î²
    se = sqrt(sum(residuals.^2) / (length(Y_local) - 4)) *
         sqrt((design_matrix' * design_matrix)[2, 2]^(-1))

    return rdd_effect, se
end

# Generate RDD data
function generate_rdd_data(n::Int=2000, cutoff::Float64=18.0)
    # Running variable (e.g., age)
    X = rand(Uniform(15, 21), n)

    # Treatment assignment (sharp RDD)
    D = X .>= cutoff

    # Outcome (discontinuity at cutoff)
    # True effect: 3.0
    Y = 10 .+ 0.5 .* X .+ 3.0 .* D .+ randn(n) * 0.8

    return DataFrame(Age=X, Treatment=D, Outcome=Y)
end

rdd_data = generate_rdd_data(2000, 18.0)

# RDD estimation with bandwidth = 2
rdd_effect, se_rdd = regression_discontinuity(rdd_data, :Outcome, :Age, 18.0, 2.0)
println("RDD estimate (h=2): $(round(rdd_effect, digits=3)) Â± $(round(1.96*se_rdd, digits=3)) (95% CI)")
println("True effect: 3.0")

# Sensitivity to bandwidth
for h in [1.0, 1.5, 2.0, 2.5, 3.0]
    eff, _ = regression_discontinuity(rdd_data, :Outcome, :Age, 18.0, h)
    println("  h=$h: RDD effect = $(round(eff, digits=3))")
end
```

### 4.6 DiDå®Ÿè£…

```julia
function difference_in_differences(data::DataFrame, outcome::Symbol, treatment::Symbol,
                                    post::Symbol, group::Symbol)
    # DiD regression: Y ~ Treatment + Post + Treatment*Post
    formula = term(outcome) ~ term(treatment) + term(post) + term(treatment) & term(post)
    model = lm(formula, data)

    # DiD effect = coefficient on Treatment*Post
    did_effect = coef(model)[end]  # last coefficient
    se_did = stderror(model)[end]

    return did_effect, se_did, model
end

# Generate DiD data
function generate_did_data(n_group::Int=500, n_period::Int=2)
    # 2 groups Ã— 2 periods
    groups = repeat([0, 1], inner=n_group*n_period)
    periods = repeat(repeat([0, 1], inner=n_group), outer=2)
    treatment = (groups .== 1) .& (periods .== 1)

    # Outcome: parallel trends assumption holds
    # Group effect: +5 for treated group
    # Time effect: +2 for post period
    # True DiD effect: +3
    Y = 10 .+ 5 .* groups .+ 2 .* periods .+ 3 .* treatment .+ randn(length(groups)) * 1.0

    return DataFrame(Group=groups, Post=periods, Treatment=treatment, Outcome=Y)
end

did_data = generate_did_data(500, 2)

# DiD estimation
did_effect, se_did, did_model = difference_in_differences(did_data, :Outcome, :Treatment, :Post, :Group)
println("DiD estimate: $(round(did_effect, digits=3)) Â± $(round(1.96*se_did, digits=3)) (95% CI)")
println("True effect: 3.0")

# Event study (pre-trend test)
function event_study(data::DataFrame, outcome::Symbol, group::Symbol, time_periods::Vector{Int})
    # Estimate treatment effects for each period relative to treatment
    # (requires panel data with multiple pre/post periods)

    # Placeholder - full implementation requires panel structure
    println("Event study plot would show pre-treatment trends here")
end
```

### 4.7 Causal Forestå®Ÿè£… (ç°¡æ˜“ç‰ˆ)

```julia
# Simplified Causal Forest implementation
# For production use: CausalELM.jl or R's grf package via RCall.jl

function causal_forest_simple(data::DataFrame, outcome::Symbol, treatment::Symbol,
                               covariates::Vector{Symbol}, n_trees::Int=100)
    # Simplified version: T-Learner with Random Forest-like splits
    # Split data by treatment
    data_treated = data[data[:, treatment] .== 1, :]
    data_control = data[data[:, treatment] .== 0, :]

    # Fit outcome models (linear for simplicity)
    X_cols = covariates
    formula_y = term(outcome) ~ sum(term.(X_cols))

    model_1 = lm(formula_y, data_treated)
    model_0 = lm(formula_y, data_control)

    # Predict CATE for each observation
    Î¼_1 = predict(model_1, data)
    Î¼_0 = predict(model_0, data)

    cate = Î¼_1 - Î¼_0

    # ATE = mean(CATE)
    ate_cf = mean(cate)

    return ate_cf, cate
end

# Generate heterogeneous treatment effect data
function generate_hte_data(n::Int=2000)
    X1 = randn(n)  # covariate 1
    X2 = randn(n)  # covariate 2

    # Treatment assignment (random)
    D = rand(Bernoulli(0.5), n)

    # Heterogeneous treatment effect: Ï„(X) = 2 + X1
    # Y^1 = 10 + 2*X1 + X2 + (2 + X1)
    # Y^0 = 10 + 2*X1 + X2
    Y1 = 10 .+ 2 .* X1 .+ X2 .+ (2 .+ X1) .+ randn(n) * 0.5
    Y0 = 10 .+ 2 .* X1 .+ X2 .+ randn(n) * 0.5
    Y = D .* Y1 .+ (1 .- D) .* Y0

    true_cate = 2 .+ X1  # ground truth

    return DataFrame(Outcome=Y, Treatment=D, X1=X1, X2=X2, TrueCate=true_cate)
end

hte_data = generate_hte_data(2000)

ate_cf, cate_cf = causal_forest_simple(hte_data, :Outcome, :Treatment, [:X1, :X2])
println("Causal Forest ATE: $(round(ate_cf, digits=3))")
println("True ATE (average of 2 + X1): $(round(mean(hte_data.TrueCate), digits=3))")

# Correlation between estimated and true CATE
corr_cate = cor(cate_cf, hte_data.TrueCate)
println("Correlation(estimated CATE, true CATE): $(round(corr_cate, digits=3))")
```

### 4.8 çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ â€” è¤‡æ•°æ‰‹æ³•ã®æ¯”è¼ƒ

```julia
function causal_inference_pipeline(data::DataFrame, scenario::String)
    println("\n=== Causal Inference Pipeline: $scenario ===\n")

    if scenario == "propensity"
        # Propensity score methods
        e_X, _ = estimate_propensity_score(data, :Treatment, [:X1, :X2])
        data.PropensityScore = e_X

        ate_ipw, se_ipw = ipw_estimator(data, :Treatment, :Outcome, :PropensityScore)
        ate_dr, se_dr = doubly_robust_estimator(data, :Treatment, :Outcome, [:X1, :X2], :PropensityScore)

        println("IPW ATE: $(round(ate_ipw, digits=3)) Â± $(round(1.96*se_ipw, digits=3))")
        println("DR ATE: $(round(ate_dr, digits=3)) Â± $(round(1.96*se_dr, digits=3))")

        balance_check(data, :Treatment, [:X1, :X2], :PropensityScore)

    elseif scenario == "iv"
        # Instrumental variables
        Î²_2sls, se_2sls, f_stat = two_stage_least_squares(data, :Outcome, :Treatment, :Instrument)
        println("2SLS estimate: $(round(Î²_2sls, digits=3)) Â± $(round(1.96*se_2sls, digits=3))")
        println("First-stage F: $(round(f_stat, digits=2))")

    elseif scenario == "rdd"
        # Regression discontinuity
        rdd_effect, se_rdd = regression_discontinuity(data, :Outcome, :RunningVar, 0.0, 2.0)
        println("RDD estimate: $(round(rdd_effect, digits=3)) Â± $(round(1.96*se_rdd, digits=3))")

    elseif scenario == "did"
        # Difference-in-differences
        did_effect, se_did, _ = difference_in_differences(data, :Outcome, :Treatment, :Post, :Group)
        println("DiD estimate: $(round(did_effect, digits=3)) Â± $(round(1.96*se_did, digits=3))")

    end
end

# Example: Run propensity score pipeline
ps_test_data = generate_ps_data(2000)
causal_inference_pipeline(ps_test_data, "propensity")
```

:::message
**é€²æ—: 70% å®Œäº†** Juliaå› æœæ¨è«–ãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯ã‚’å®Ÿè£…ã—ãŸã€‚DAG/do-æ¼”ç®—/å‚¾å‘ã‚¹ã‚³ã‚¢/IV/RDD/DiD/Causal Forestã®å…¨æ‰‹æ³•ã‚’CausalInference.jlã§å®Ÿè£…ã€‚æ¬¡ã¯å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ã§å®Ÿãƒ‡ãƒ¼ã‚¿ã«é©ç”¨ã™ã‚‹ã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” å®Ÿãƒ‡ãƒ¼ã‚¿å› æœæ¨è«–ãƒãƒ£ãƒ¬ãƒ³ã‚¸

### 5.1 ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã§å…¨æ‰‹æ³•æ¯”è¼ƒ

```julia
# Generate comprehensive causal inference test data
function comprehensive_causal_data(n::Int=3000)
    # Confounders
    age = rand(Normal(40, 12), n)
    income = rand(Normal(50, 20), n)

    # Propensity score (selection on observables)
    e_X = 1 ./ (1 .+ exp.(-(0.05 .* age .+ 0.03 .* income .- 3.0)))
    treatment = rand(n) .< e_X

    # Instrumental variable (random assignment)
    instrument = rand(Bernoulli(0.5), n)

    # Outcome (true effect = 5.0)
    outcome = 5.0 .* treatment .+ 0.3 .* age .+ 0.2 .* income .+ randn(n) * 3.0

    return DataFrame(
        Treatment=treatment,
        Outcome=outcome,
        Age=age,
        Income=income,
        Instrument=instrument,
        PropensityScore=e_X
    )
end

test_data = comprehensive_causal_data(3000)

# Method 1: Naive comparison
ate_naive = mean(test_data[test_data.Treatment .== 1, :Outcome]) -
            mean(test_data[test_data.Treatment .== 0, :Outcome])

# Method 2: IPW
ate_ipw, se_ipw = ipw_estimator(test_data, :Treatment, :Outcome, :PropensityScore)

# Method 3: Doubly Robust
ate_dr, se_dr = doubly_robust_estimator(test_data, :Treatment, :Outcome,
                                         [:Age, :Income], :PropensityScore)

# Method 4: Regression Adjustment
reg_model = lm(@formula(Outcome ~ Treatment + Age + Income), test_data)
ate_reg = coef(reg_model)[2]

println("\n=== Method Comparison ===")
println("True ATE: 5.0")
println("Naive: $(round(ate_naive, digits=3))")
println("IPW: $(round(ate_ipw, digits=3)) Â± $(round(1.96*se_ipw, digits=3))")
println("Doubly Robust: $(round(ate_dr, digits=3)) Â± $(round(1.96*se_dr, digits=3))")
println("Regression Adjustment: $(round(ate_reg, digits=3))")
```

### 5.2 æ„Ÿåº¦åˆ†æ â€” æœªæ¸¬å®šäº¤çµ¡ã¸ã®é ‘å¥æ€§

```julia
# Rosenbaum's Î“ sensitivity analysis (simplified)
function sensitivity_analysis_gamma(ate_estimated::Float64, se::Float64, gamma_range::Vector{Float64})
    println("\n=== Sensitivity Analysis (Rosenbaum's Î“) ===")
    println("Î“ = odds ratio of differential treatment assignment due to unobserved confounder")

    for gamma in gamma_range
        # Under confounding by unobserved U, bounds on ATE
        # Simplified: scale SE by gamma
        ci_lower = ate_estimated - 1.96 * se * gamma
        ci_upper = ate_estimated + 1.96 * se * gamma

        significant = (ci_lower > 0) || (ci_upper < 0)
        status = significant ? "âœ… Still significant" : "âŒ Not significant"

        println("Î“=$gamma: CI = [$(round(ci_lower, digits=2)), $(round(ci_upper, digits=2))] $status")
    end
end

sensitivity_analysis_gamma(ate_dr, se_dr, [1.0, 1.5, 2.0, 2.5, 3.0])
```

### 5.3 A/Bãƒ†ã‚¹ãƒˆçµ±åˆ â€” Sample Ratio Mismatchæ¤œå‡º

```julia
function sample_ratio_mismatch_test(data::DataFrame, treatment::Symbol, expected_ratio::Float64=0.5)
    # Test if observed treatment ratio matches expected ratio
    n_total = nrow(data)
    n_treated = sum(data[:, treatment])
    n_control = n_total - n_treated

    observed_ratio = n_treated / n_total

    # Chi-square test
    expected_treated = n_total * expected_ratio
    expected_control = n_total * (1 - expected_ratio)

    chi_sq = (n_treated - expected_treated)^2 / expected_treated +
             (n_control - expected_control)^2 / expected_control

    p_value = 1 - cdf(Chisq(1), chi_sq)

    println("\n=== Sample Ratio Mismatch Test ===")
    println("Expected ratio: $expected_ratio")
    println("Observed ratio: $(round(observed_ratio, digits=4))")
    println("Ï‡Â² = $(round(chi_sq, digits=3)), p = $(round(p_value, digits=4))")

    if p_value < 0.05
        println("âš ï¸ SRM detected! Treatment assignment may be biased.")
    else
        println("âœ… No SRM detected.")
    end

    return chi_sq, p_value
end

sample_ratio_mismatch_test(test_data, :Treatment, 0.5)
```

### 5.4 è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

#### ãƒ†ã‚¹ãƒˆ1: è¨˜æ³•ç†è§£ï¼ˆ10å•ï¼‰

:::details Q1: $\mathbb{E}[Y^1 - Y^0]$ ã¯ä½•ã‚’è¡¨ã™ã‹ï¼Ÿ

**Answer**: ATE (Average Treatment Effect) â€” å…¨ä½“ã®å¹³å‡å‡¦ç½®åŠ¹æœ

$$
\text{ATE} = \mathbb{E}[Y^1 - Y^0] = \mathbb{E}[Y^1] - \mathbb{E}[Y^0]
$$

**è£œè¶³**: ã“ã‚Œã¯å€‹ä½“ãƒ¬ãƒ™ãƒ«ã®å‡¦ç½®åŠ¹æœ $\tau_i = Y_i^1 - Y_i^0$ ã®æœŸå¾…å€¤ã€‚å€‹ä½“ãƒ¬ãƒ™ãƒ«ã¯è¦³æ¸¬ä¸èƒ½ï¼ˆæ ¹æœ¬çš„å› æœæ¨è«–å•é¡Œï¼‰ã ãŒã€é›†å›£å¹³å‡ãªã‚‰æ¨å®šå¯èƒ½ã€‚
:::

:::details Q2: $P(Y \mid do(X=x))$ ã¨ $P(Y \mid X=x)$ ã®é•ã„ã¯ï¼Ÿ

**Answer**:
- $P(Y \mid do(X=x))$: **ä»‹å…¥ç¢ºç‡** â€” $X$ ã‚’å¤–éƒ¨ã‹ã‚‰å¼·åˆ¶çš„ã« $x$ ã«å›ºå®šã—ãŸå ´åˆã® $Y$ ã®åˆ†å¸ƒ
- $P(Y \mid X=x)$: **æ¡ä»¶ä»˜ãç¢ºç‡** â€” $X=x$ ã‚’è¦³æ¸¬ã—ãŸå ´åˆã® $Y$ ã®åˆ†å¸ƒï¼ˆäº¤çµ¡ã‚ã‚Šï¼‰

ä»‹å…¥ç¢ºç‡ã¯å› æœåŠ¹æœã€æ¡ä»¶ä»˜ãç¢ºç‡ã¯ç›¸é–¢ã‚’è¡¨ã™ã€‚

**ä¾‹**: å–«ç…™ã¨ãŒã‚“
- $P(\text{ãŒã‚“} \mid \text{å–«ç…™}=1)$: å–«ç…™è€…ã®ãŒã‚“ç‡ï¼ˆéºä¼ã®äº¤çµ¡ã‚ã‚Šï¼‰
- $P(\text{ãŒã‚“} \mid do(\text{å–«ç…™}=1))$: å¼·åˆ¶çš„ã«å–«ç…™ã•ã›ãŸå ´åˆã®ãŒã‚“ç‡ï¼ˆå› æœåŠ¹æœï¼‰

å‰è€…ã¯ç›¸é–¢ã€å¾Œè€…ã¯å› æœã€‚Simpson's Paradoxã§ã¯ä¸¡è€…ãŒé€†è»¢ã™ã‚‹ã“ã¨ã™ã‚‰ã‚ã‚‹ã€‚
:::

:::details Q3: $e(X) = P(D=1 \mid X)$ ã®åå‰ã¨å½¹å‰²ã¯ï¼Ÿ

**Answer**: **å‚¾å‘ã‚¹ã‚³ã‚¢ (Propensity Score)**

é«˜æ¬¡å…ƒã®å…±å¤‰é‡ $X$ ã‚’1æ¬¡å…ƒã®ã‚¹ã‚«ãƒ©ãƒ¼ã«åœ§ç¸®ã€‚$(Y^1, Y^0) \perp\!\!\!\perp D \mid X$ ãªã‚‰ $(Y^1, Y^0) \perp\!\!\!\perp D \mid e(X)$ ã‚‚æˆç«‹ï¼ˆæ¬¡å…ƒå‰Šæ¸›ï¼‰ã€‚

**å®Ÿç”¨ä¸Šã®ãƒ¡ãƒªãƒƒãƒˆ**:
- $X$ ãŒ10æ¬¡å…ƒã§ã‚‚ $e(X)$ ã¯1æ¬¡å…ƒ â†’ ãƒãƒƒãƒãƒ³ã‚°ãŒå®¹æ˜“
- å…±é€šã‚µãƒãƒ¼ãƒˆ $0 < e(X) < 1$ ã®ç¢ºèªãŒç°¡å˜
- IPWæ¨å®šã§ $1/e(X)$ ã®é‡ã¿ã‚’ä½¿ã†ã ã‘ã§å› æœåŠ¹æœæ¨å®šå¯èƒ½
:::

:::details Q4: SUTVAã®2ã¤ã®ä»®å®šã‚’è¿°ã¹ã‚ˆ

**Answer**:
1. **å‡¦ç½®ã®ä¸€æ„æ€§**: å€‹ä½“ $i$ ã®å‡¦ç½®ãŒ $d$ ã®ã¨ãã€çµæœã¯ $Y_i^d$ ã®1ã¤ã®ã¿
2. **å¹²æ¸‰ãªã— (No Interference)**: å€‹ä½“ $i$ ã®çµæœã¯ä»–ã®å€‹ä½“ã®å‡¦ç½®ã«ä¾å­˜ã—ãªã„

$$
Y_i^d = Y_i^{d_i} \quad \forall d_{-i}
$$

**ç ´ã‚Œã‚‹ä¾‹**:
- ãƒ¯ã‚¯ãƒãƒ³æ¥ç¨®: ä»–äººãŒæ¥ç¨®ã™ã‚‹ã¨è‡ªåˆ†ã®æ„ŸæŸ“ãƒªã‚¹ã‚¯ä½ä¸‹ï¼ˆå¹²æ¸‰ã‚ã‚Šï¼‰
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åºƒå‘Š: å‹äººãŒã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨è‡ªåˆ†ã‚‚ã‚¯ãƒªãƒƒã‚¯ï¼ˆspilloveråŠ¹æœï¼‰
- æ•™å®¤å†…ã®å‡¦ç½®: åŒã˜ã‚¯ãƒ©ã‚¹ã®å­¦ç”Ÿé–“ã§ç›¸äº’å½±éŸ¿

SUTVAãŒç ´ã‚Œã‚‹å ´åˆã¯ã€**Spillover Effects** ã‚„ **Network Effects** ã‚’æ˜ç¤ºçš„ã«ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
:::

:::details Q5: ãƒãƒƒã‚¯ãƒ‰ã‚¢åŸºæº–ã‚’æº€ãŸã™å¤‰æ•°é›†åˆ $Z$ ã®æ¡ä»¶ã¯ï¼Ÿ

**Answer**:
1. $Z$ ã®ã©ã®å¤‰æ•°ã‚‚ $X$ ã®å­å­«ã§ãªã„
2. $Z$ ãŒ $X$ ã‹ã‚‰ $Y$ ã¸ã®ã™ã¹ã¦ã®ãƒãƒƒã‚¯ãƒ‰ã‚¢ãƒ‘ã‚¹ã‚’é®æ–­ã™ã‚‹

æº€ãŸã›ã°:

$$
P(Y \mid do(X=x)) = \sum_z P(Y \mid X=x, Z=z) P(Z=z)
$$

**ç›´æ„Ÿ**:
- æ¡ä»¶1: $X$ ã®çµæœ ($X$ ã®å­å­«) ã§æ¡ä»¶ã¥ã‘ã‚‹ã¨ã€Collider BiasãŒç™ºç”Ÿã™ã‚‹
- æ¡ä»¶2: ãƒãƒƒã‚¯ãƒ‰ã‚¢ãƒ‘ã‚¹ï¼ˆ$X \leftarrow \cdots \to Y$ï¼‰ã‚’é®æ–­ã—ãªã„ã¨äº¤çµ¡ãŒæ®‹ã‚‹

**ä¾‹**: å–«ç…™â†’ãŒã‚“ã€ãƒãƒƒã‚¯ãƒ‰ã‚¢ãƒ‘ã‚¹: å–«ç…™â†éºä¼â†’ãŒã‚“
- $Z = \{\text{éºä¼}\}$ ã§æ¡ä»¶ã¥ã‘ã‚‹ã¨ãƒãƒƒã‚¯ãƒ‰ã‚¢ãƒ‘ã‚¹ãŒé®æ–­ã•ã‚Œã‚‹
- $Z = \{\text{ã‚¿ãƒ¼ãƒ«æ²ˆç€}\}$ (å–«ç…™ã®çµæœ) ã§æ¡ä»¶ã¥ã‘ã‚‹ã¨Collider BiasãŒç™ºç”Ÿ
:::

:::details Q6: d-åˆ†é›¢ã¨ã¯ä½•ã‹ï¼Ÿ

**Answer**: DAGä¸Šã§å¤‰æ•°é›†åˆ $Z$ ãŒ $X$ ã¨ $Y$ ã‚’ d-åˆ†é›¢ã™ã‚‹ $\iff$ $X$ ã‹ã‚‰ $Y$ ã¸ã®ã™ã¹ã¦ã®ãƒ‘ã‚¹ãŒ $Z$ ã«ã‚ˆã£ã¦é®æ–­ã•ã‚Œã‚‹ã€‚

**ãƒ‘ã‚¹é®æ–­æ¡ä»¶**:
- **Chain** $X \to Z \to Y$: $Z \in \mathcal{Z}$ ãªã‚‰é®æ–­
- **Fork** $X \leftarrow Z \to Y$: $Z \in \mathcal{Z}$ ãªã‚‰é®æ–­
- **Collider** $X \to Z \leftarrow Y$: $Z \notin \mathcal{Z}$ ã‹ã¤ $\text{DE}(Z) \cap \mathcal{Z} = \emptyset$ ãªã‚‰é®æ–­

**d-åˆ†é›¢ã®é‡è¦æ€§**: $X \perp_d Y \mid Z$ (d-åˆ†é›¢) $\Rightarrow$ $X \perp\!\!\!\perp Y \mid Z$ (æ¡ä»¶ä»˜ãç‹¬ç«‹)
:::

:::details Q7: Colliderã§æ¡ä»¶ã¥ã‘ã‚‹ã¨ä½•ãŒèµ·ã“ã‚‹ï¼Ÿ

**Answer**: **é¸æŠãƒã‚¤ã‚¢ã‚¹** â€” ç‹¬ç«‹ã ã£ãŸå¤‰æ•°ãŒæ¡ä»¶ä»˜ãã§ç›¸é–¢ã™ã‚‹

**ä¾‹**: æ‰èƒ½ã¨åŠªåŠ›

```mermaid
graph TD
    T["æ‰èƒ½"] --> A["åˆæ ¼"]
    E["åŠªåŠ›"] --> A
```

æ‰èƒ½ã¨åŠªåŠ›ã¯ç‹¬ç«‹ $T \perp\!\!\!\perp E$ ã ãŒã€åˆæ ¼è€… $A=1$ ã‚’æ¡ä»¶ã¥ã‘ã‚‹ã¨:

$$
T \not\perp\!\!\!\perp E \mid A=1
$$

åˆæ ¼è€…ã®ä¸­ã§ã¯ã€ŒåŠªåŠ›ãŒå°‘ãªã„â†’æ‰èƒ½ãŒé«˜ã„ã€ã¨ã„ã†è² ã®ç›¸é–¢ãŒç”Ÿã¾ã‚Œã‚‹ã€‚ã“ã‚ŒãŒ**Berkson's Paradox**ã€‚

**å®Ÿç”¨ä¾‹**: ç—…é™¢æ‚£è€…ãƒ‡ãƒ¼ã‚¿ã§ç–¾æ‚£Aã¨ç–¾æ‚£BãŒè² ã®ç›¸é–¢ â†’ å…¥é™¢ï¼ˆColliderï¼‰ã§æ¡ä»¶ã¥ã‘ã‚‰ã‚Œã¦ã„ã‚‹ãŸã‚
:::

:::details Q8: Unconfoundednessä»®å®šã¨ã¯ï¼Ÿ

**Answer**: $(Y^1, Y^0) \perp\!\!\!\perp D \mid X$

å…±å¤‰é‡ $X$ ã‚’æ‰€ä¸ã¨ã™ã‚Œã°ã€æ½œåœ¨çš„çµæœã¨å‡¦ç½®å‰²ã‚Šå½“ã¦ãŒç‹¬ç«‹ã€‚

**æ„å‘³**: $X$ ã‚’åˆ¶å¾¡ã™ã‚Œã°ã€å‡¦ç½®ã¯ãƒ©ãƒ³ãƒ€ãƒ å‰²ã‚Šå½“ã¦ã¨åŒç­‰ï¼ˆselection on observablesï¼‰ã€‚

**æˆã‚Šç«‹ã¤æ¡ä»¶**:
- ã™ã¹ã¦ã®äº¤çµ¡å› å­ $X$ ã‚’æ¸¬å®šã—ã¦ã„ã‚‹
- æœªæ¸¬å®šäº¤çµ¡ $U$ ãŒå­˜åœ¨ã—ãªã„

**ç ´ã‚Œã‚‹ä¾‹**: èƒ½åŠ› $U$ ãŒæœªæ¸¬å®šã§ã€$U \to D$ ã‹ã¤ $U \to Y$ ãªã‚‰ Unconfoundedness ã¯æˆã‚Šç«‹ãŸãªã„ â†’ IV/RDD/DiDãªã©ä»–ã®æ‰‹æ³•ãŒå¿…è¦
:::

:::details Q9: LATEã¨ATEã®é•ã„ã¯ï¼Ÿ

**Answer**:
- **ATE**: $\mathbb{E}[Y^1 - Y^0]$ â€” å…¨ä½“ã®å¹³å‡å‡¦ç½®åŠ¹æœ
- **LATE**: $\mathbb{E}[Y^1 - Y^0 \mid \text{Complier}]$ â€” ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ¼ï¼ˆæ“ä½œå¤‰æ•°ã«å¾“ã†äººï¼‰ã®å‡¦ç½®åŠ¹æœ

**IVã§æ¨å®šã•ã‚Œã‚‹ã®ã¯LATE**:

$$
\text{LATE} = \frac{\mathbb{E}[Y \mid Z=1] - \mathbb{E}[Y \mid Z=0]}{\mathbb{E}[D \mid Z=1] - \mathbb{E}[D \mid Z=0]}
$$

**4ã¤ã®ã‚¿ã‚¤ãƒ—**:
- Always-Taker: å¸¸ã«å‡¦ç½®ã‚’å—ã‘ã‚‹ï¼ˆIVã«ç„¡é–¢ä¿‚ï¼‰
- Never-Taker: å¸¸ã«å‡¦ç½®ã‚’å—ã‘ãªã„ï¼ˆIVã«ç„¡é–¢ä¿‚ï¼‰
- **Complier**: IVã«å¾“ã†ï¼ˆLATEã®å¯¾è±¡ï¼‰
- Defier: IVã«é€†ã‚‰ã†ï¼ˆMonotonicityä»®å®šã§æ’é™¤ï¼‰

**ATE vs LATE**: LATEã¯ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ¼ã®ã¿ã®åŠ¹æœãªã®ã§ã€ATEã‚ˆã‚Šå±€æ‰€çš„ã€‚å¤–éƒ¨å¦¥å½“æ€§ãŒä½ã„å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
:::

:::details Q10: ä¸¦è¡Œãƒˆãƒ¬ãƒ³ãƒ‰ä»®å®šã¨ã¯ï¼Ÿ

**Answer**: DiDã®è­˜åˆ¥ä»®å®š

$$
\mathbb{E}[Y_{01} - Y_{00} \mid G=1] = \mathbb{E}[Y_{01} - Y_{00} \mid G=0]
$$

å‡¦ç½®ãŒãªã‹ã£ãŸå ´åˆã€å‡¦ç½®ç¾¤ã¨å¯¾ç…§ç¾¤ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã¯å¹³è¡Œã€‚

**ç›´æ„Ÿ**: å‡¦ç½®ç¾¤ã¨å¯¾ç…§ç¾¤ã¯å‡¦ç½®å‰ã®ãƒˆãƒ¬ãƒ³ãƒ‰ãŒåŒã˜ â†’ å‡¦ç½®å¾Œã®å·®åˆ†ã¯å‡¦ç½®åŠ¹æœ

**æ¤œè¨¼æ–¹æ³•**:
- Event Study: å‡¦ç½®å‰ã®è¤‡æ•°æœŸé–“ã§ãƒˆãƒ¬ãƒ³ãƒ‰ãŒå¹³è¡Œã‹ç¢ºèª
- Placebo Test: å‡¦ç½®å‰æœŸé–“ã§ã€Œå½ã®å‡¦ç½®ã€ã‚’è¨­å®šã—ã€åŠ¹æœãŒã‚¼ãƒ­ã‹ç¢ºèª

**ç ´ã‚Œã‚‹ä¾‹**: å‡¦ç½®ç¾¤ãŒé«˜æˆé•·ä¼æ¥­ã€å¯¾ç…§ç¾¤ãŒä½æˆé•·ä¼æ¥­ â†’ ã‚‚ã¨ã‚‚ã¨ãƒˆãƒ¬ãƒ³ãƒ‰ãŒç•°ãªã‚‹ â†’ DiDã¯é©ç”¨ä¸å¯
:::

#### ãƒ†ã‚¹ãƒˆ2: æ•°å¼å°å‡ºï¼ˆ5å•ï¼‰

:::details Q1: IPWæ¨å®šé‡ãŒä¸åã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã›

**Proof**:

$$
\begin{aligned}
\mathbb{E}\left[\frac{D Y}{e(X)}\right] &= \mathbb{E}\left[\mathbb{E}\left[\frac{D Y}{e(X)} \mid X\right]\right] \\
&= \mathbb{E}\left[\frac{\mathbb{E}[D Y \mid X]}{e(X)}\right] \\
&= \mathbb{E}\left[\frac{P(D=1 \mid X) \mathbb{E}[Y \mid D=1, X]}{e(X)}\right] \\
&= \mathbb{E}\left[\frac{e(X) \mathbb{E}[Y^1 \mid X]}{e(X)}\right] \quad \text{(unconfoundedness)} \\
&= \mathbb{E}[Y^1]
\end{aligned}
$$

åŒæ§˜ã« $\mathbb{E}\left[\frac{(1-D) Y}{1-e(X)}\right] = \mathbb{E}[Y^0]$ã€‚ã‚ˆã£ã¦:

$$
\mathbb{E}[\hat{\text{ATE}}_{\text{IPW}}] = \mathbb{E}[Y^1] - \mathbb{E}[Y^0] = \text{ATE}
$$

**Key Stepè§£èª¬**:
- Step 3â†’4: Unconfoundedness $(Y^1, Y^0) \perp\!\!\!\perp D \mid X$ ã«ã‚ˆã‚Š $\mathbb{E}[Y \mid D=1, X] = \mathbb{E}[Y^1 \mid X]$
- Step 4â†’5: $e(X) = P(D=1 \mid X)$ ãªã®ã§ç´„åˆ†
- Overlapä»®å®š $0 < e(X) < 1$ ãŒå¿…é ˆï¼ˆåˆ†æ¯ãŒã‚¼ãƒ­ã«ãªã‚‰ãªã„ï¼‰
:::

:::details Q2: 2SLSæ¨å®šé‡ã‚’å°å‡ºã›ã‚ˆï¼ˆWaldæ¨å®šé‡å½¢å¼ï¼‰

**Derivation**:

æ§‹é€ æ–¹ç¨‹å¼:

$$
\begin{aligned}
D &= \pi_0 + \pi_1 Z + \nu \\
Y &= \beta_0 + \beta_1 D + U
\end{aligned}
$$

$U$ ã¨ $Z$ ãŒç„¡ç›¸é–¢ï¼ˆå¤–ç”Ÿæ€§ï¼‰ã€$Z$ ã¨ $D$ ãŒç›¸é–¢ï¼ˆé–¢é€£æ€§ï¼‰ã‚’ä»®å®šã€‚

$$
\begin{aligned}
\text{Cov}(Y, Z) &= \text{Cov}(\beta_0 + \beta_1 D + U, Z) \\
&= \beta_1 \text{Cov}(D, Z) + \text{Cov}(U, Z) \\
&= \beta_1 \text{Cov}(D, Z) \quad \text{(å¤–ç”Ÿæ€§: } \text{Cov}(U,Z)=0)
\end{aligned}
$$

$$
\hat{\beta}_1 = \frac{\text{Cov}(Y, Z)}{\text{Cov}(D, Z)} = \frac{\mathbb{E}[Y \mid Z=1] - \mathbb{E}[Y \mid Z=0]}{\mathbb{E}[D \mid Z=1] - \mathbb{E}[D \mid Z=0]}
$$

ã“ã‚ŒãŒ2SLSæ¨å®šé‡ï¼ˆWaldæ¨å®šé‡ï¼‰ã€‚

**ç›´æ„Ÿ**:
- åˆ†å­: IVãŒ $Y$ ã«ä¸ãˆã‚‹ç·åŠ¹æœï¼ˆreduced formï¼‰
- åˆ†æ¯: IVãŒ $D$ ã«ä¸ãˆã‚‹åŠ¹æœï¼ˆfirst stageï¼‰
- æ¯”: $D$ ãŒ $Y$ ã«ä¸ãˆã‚‹å› æœåŠ¹æœï¼ˆstructural effectï¼‰

**æ¡ä»¶**:
- å¤–ç”Ÿæ€§: $\text{Cov}(U, Z) = 0$
- é–¢é€£æ€§: $\text{Cov}(D, Z) \neq 0$ (å¼±IVãªã‚‰åˆ†æ¯ãŒå°ã•ããƒã‚¤ã‚¢ã‚¹å¤§)
- æ’é™¤åˆ¶ç´„: $Z \to Y$ ã®ç›´æ¥ãƒ‘ã‚¹ãªã—
:::

:::details Q3: DiDæ¨å®šé‡ã‚’å°å‡ºã›ã‚ˆ

**Setup**: 2æœŸé–“ $t \in \{0, 1\}$, 2ã‚°ãƒ«ãƒ¼ãƒ— $G \in \{0, 1\}$

æ½œåœ¨çš„çµæœ:
- $Y_{it}^0$: å‡¦ç½®ãªã—ã®çµæœ
- $Y_{it}^1$: å‡¦ç½®ã‚ã‚Šã®çµæœ

è¦³æ¸¬çµæœ:

$$
Y_{it} = \begin{cases}
Y_{it}^0 & \text{if } G=0 \text{ or } t=0 \\
Y_{it}^1 & \text{if } G=1 \text{ and } t=1
\end{cases}
$$

**DiDæ¨å®šé‡**:

$$
\begin{aligned}
\hat{\tau}_{\text{DiD}} &= (\mathbb{E}[Y_{11}] - \mathbb{E}[Y_{10}]) - (\mathbb{E}[Y_{01}] - \mathbb{E}[Y_{00}]) \\
&= (\mathbb{E}[Y_{11}^1 \mid G=1] - \mathbb{E}[Y_{10}^0 \mid G=1]) \\
&\quad - (\mathbb{E}[Y_{01}^0 \mid G=0] - \mathbb{E}[Y_{00}^0 \mid G=0])
\end{aligned}
$$

**ä¸¦è¡Œãƒˆãƒ¬ãƒ³ãƒ‰ä»®å®š**:

$$
\mathbb{E}[Y_{11}^0 - Y_{10}^0 \mid G=1] = \mathbb{E}[Y_{01}^0 - Y_{00}^0 \mid G=0]
$$

å‡¦ç½®ãŒãªã‹ã£ãŸå ´åˆã®ãƒˆãƒ¬ãƒ³ãƒ‰ãŒå¹³è¡Œ â†’ ã“ã‚Œã‚’ä½¿ã†ã¨:

$$
\begin{aligned}
\hat{\tau}_{\text{DiD}} &= \mathbb{E}[Y_{11}^1 - Y_{10}^0 \mid G=1] - (\mathbb{E}[Y_{11}^0 - Y_{10}^0 \mid G=1]) \\
&= \mathbb{E}[Y_{11}^1 - Y_{11}^0 \mid G=1] \\
&= \text{ATT}
\end{aligned}
$$

DiDã¯ATTï¼ˆå‡¦ç½®ç¾¤ã®å¹³å‡å‡¦ç½®åŠ¹æœï¼‰ã‚’è­˜åˆ¥ã™ã‚‹ã€‚
:::

:::details Q4: Doubly Robustæ¨å®šé‡ãŒ2é‡é ‘å¥ã§ã‚ã‚‹ç†ç”±ã‚’ç¤ºã›

**DRæ¨å®šé‡**:

$$
\hat{\tau}_{\text{DR}} = \frac{1}{n} \sum_i \left[ \frac{D_i (Y_i - \hat{\mu}_1(X_i))}{\hat{e}(X_i)} + \hat{\mu}_1(X_i) - \frac{(1-D_i)(Y_i - \hat{\mu}_0(X_i))}{1-\hat{e}(X_i)} - \hat{\mu}_0(X_i) \right]
$$

**Case 1**: $\hat{\mu}_1, \hat{\mu}_0$ ãŒæ­£ã—ã„ï¼ˆ$\hat{e}$ ãŒèª¤ã‚Šã§ã‚‚OKï¼‰

$$
\begin{aligned}
\mathbb{E}[\hat{\tau}_{\text{DR}}] &= \mathbb{E}\left[\frac{D(Y - \mu_1(X))}{\hat{e}(X)} + \mu_1(X)\right] - \mathbb{E}\left[\frac{(1-D)(Y - \mu_0(X))}{1-\hat{e}(X)} + \mu_0(X)\right] \\
&= \mathbb{E}\left[\mathbb{E}\left[\frac{D(Y - \mu_1(X))}{\hat{e}(X)} \mid X\right] + \mu_1(X)\right] - \mathbb{E}[\mu_0(X)] \\
&= \mathbb{E}\left[\frac{\mathbb{E}[D(Y - \mu_1(X)) \mid X]}{\hat{e}(X)} + \mu_1(X)\right] - \mathbb{E}[\mu_0(X)] \\
&= \mathbb{E}\left[\frac{e(X)(\mu_1(X) - \mu_1(X))}{\hat{e}(X)} + \mu_1(X)\right] - \mathbb{E}[\mu_0(X)] \quad \text{(} \mathbb{E}[Y \mid D=1, X] = \mu_1(X)) \\
&= \mathbb{E}[\mu_1(X)] - \mathbb{E}[\mu_0(X)] \\
&= \mathbb{E}[Y^1 - Y^0] = \text{ATE}
\end{aligned}
$$

**Case 2**: $\hat{e}$ ãŒæ­£ã—ã„ï¼ˆ$\hat{\mu}$ ãŒèª¤ã‚Šã§ã‚‚OKï¼‰

IPWã®ä¸åæ€§ã«ã‚ˆã‚Š $\mathbb{E}[\hat{\tau}_{\text{DR}}] = \text{ATE}$

**çµè«–**: $\hat{\mu}$ or $\hat{e}$ ã®ã©ã¡ã‚‰ã‹ä¸€æ–¹ãŒæ­£ã—ã‘ã‚Œã°ä¸å â†’ 2é‡é ‘å¥æ€§
:::

:::details Q5: RDDåŠ¹æœã‚’å°å‡ºã›ã‚ˆï¼ˆSharp RDDï¼‰

**Setup**: ã‚«ãƒƒãƒˆã‚ªãƒ• $c$ ã§å‡¦ç½®å‰²ã‚Šå½“ã¦

$$
D_i = \mathbb{1}(X_i \geq c)
$$

**å±€æ‰€ãƒ©ãƒ³ãƒ€ãƒ åŒ–ä»®å®š**:

$$
\lim_{x \to c} \mathbb{E}[Y^1 - Y^0 \mid X=x] = \tau_c
$$

ã‚«ãƒƒãƒˆã‚ªãƒ•è¿‘å‚ã§å‡¦ç½®åŠ¹æœãŒä¸€å®šã€‚

**RDDåŠ¹æœ**:

$$
\begin{aligned}
\tau_{\text{RDD}} &= \lim_{x \to c^+} \mathbb{E}[Y \mid X=x] - \lim_{x \to c^-} \mathbb{E}[Y \mid X=x] \\
&= \lim_{x \to c^+} \mathbb{E}[Y^1 \mid X=x] - \lim_{x \to c^-} \mathbb{E}[Y^0 \mid X=x] \\
&= \mathbb{E}[Y^1 - Y^0 \mid X=c] \\
&= \text{ATE}_c
\end{aligned}
$$

**Key**: ã‚«ãƒƒãƒˆã‚ªãƒ•ã§ã®ä¸é€£ç¶šæ€§ãŒå› æœåŠ¹æœã‚’è¡¨ã™ã€‚

**æ¨å®š**: Local Linear Regression

$$
\min_{\beta_0, \beta_1, \beta_2, \beta_3} \sum_{i: |X_i - c| < h} K\left(\frac{X_i - c}{h}\right) (Y_i - \beta_0 - \beta_1 D_i - \beta_2 (X_i - c) - \beta_3 D_i (X_i - c))^2
$$

$\hat{\beta}_1 = \hat{\tau}_{\text{RDD}}$
:::

#### ãƒ†ã‚¹ãƒˆ3: Juliaå®Ÿè£…ï¼ˆ5å•ï¼‰

:::details Q1: å‚¾å‘ã‚¹ã‚³ã‚¢ã‚’æ¨å®šã—ã€å…±é€šã‚µãƒãƒ¼ãƒˆã‚’ç¢ºèªã›ã‚ˆ

```julia
# 1. Estimate propensity score
e_X, model = estimate_propensity_score(data, :Treatment, [:Age, :Income])

# 2. Check common support
println("Min e(X): $(minimum(e_X))")
println("Max e(X): $(maximum(e_X))")

# 3. Visualize overlap
using Plots
histogram([e_X[data.Treatment .== 0], e_X[data.Treatment .== 1]],
          label=["Control" "Treated"],
          alpha=0.6,
          xlabel="Propensity Score",
          ylabel="Frequency",
          title="Common Support Check")

# 4. Trimming
Îµ = 0.05
trimmed = (e_X .> Îµ) .& (e_X .< (1 - Îµ))
println("Trimmed $(sum(.!trimmed)) observations ($(round(100*mean(.!trimmed), digits=1))%)")
```
:::

### 5.5 ãƒŸãƒ‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: æ•™è‚²ä»‹å…¥ã®å› æœåŠ¹æœæ¨å®š

**ã‚·ãƒŠãƒªã‚ª**: ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ•™è‚²ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®åŠ¹æœã‚’æ¨å®šã›ã‚ˆã€‚

- **å‡¦ç½®**: ãƒ—ãƒ­ã‚°ãƒ©ãƒ å—è¬› (1=å—è¬›, 0=éå—è¬›)
- **çµæœ**: ãƒ†ã‚¹ãƒˆã‚¹ã‚³ã‚¢
- **å…±å¤‰é‡**: å¹´é½¢ã€äº‹å‰ã‚¹ã‚³ã‚¢ã€æ‰€å¾—
- **æ“ä½œå¤‰æ•°**: ãƒ©ãƒ³ãƒ€ãƒ ã‚¯ãƒ¼ãƒãƒ³é…å¸ƒ

**ã‚¿ã‚¹ã‚¯**:

1. å‚¾å‘ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚° â†’ ATEæ¨å®š
2. 2SLS (ã‚¯ãƒ¼ãƒãƒ³ã‚’IV) â†’ LATEæ¨å®š
3. æ„Ÿåº¦åˆ†æ â†’ æœªæ¸¬å®šäº¤çµ¡ã¸ã®é ‘å¥æ€§
4. çµæœã‚’æ¯”è¼ƒã—ã€æœ€ã‚‚ä¿¡é ¼ã§ãã‚‹æ¨å®šå€¤ã‚’é¸æŠ

```julia
# Mini Project: Education Program Causal Effect

# Data generation
function education_program_data(n::Int=2000)
    # Covariates
    age = rand(Uniform(18, 35), n)
    baseline_score = rand(Normal(60, 15), n)
    income = rand(Normal(50, 20), n)

    # Unobserved ability (confounder)
    ability = randn(n)

    # Instrument: random coupon
    coupon = rand(Bernoulli(0.5), n)

    # Treatment: program enrollment (endogenous)
    # Depends on: coupon, covariates, ability
    enroll_prob = 1 ./ (1 .+ exp.(-(0.8 .* coupon .+ 0.02 .* age .- 0.01 .* baseline_score .+
                                   0.01 .* income .+ 0.3 .* ability .- 1.0)))
    enroll = rand(n) .< enroll_prob

    # Outcome: test score
    # True program effect: 10 points
    # Also depends on baseline score and ability
    test_score = 50 .+ 10 .* enroll .+ 0.5 .* baseline_score .+ 5 .* ability .+ randn(n) * 8

    return DataFrame(
        Enroll=enroll,
        TestScore=test_score,
        Age=age,
        BaselineScore=baseline_score,
        Income=income,
        Coupon=coupon
    )
end

edu_data = education_program_data(2000)

# Method 1: Propensity Score
edu_data.PropensityScore, _ = estimate_propensity_score(edu_data, :Enroll, [:Age, :BaselineScore, :Income])
ate_ps, se_ps = ipw_estimator(edu_data, :Enroll, :TestScore, :PropensityScore)

# Method 2: IV (coupon as instrument)
ate_iv, se_iv, f_stat = two_stage_least_squares(edu_data, :TestScore, :Enroll, :Coupon, [:Age, :BaselineScore, :Income])

# Results
println("\n=== Education Program Causal Effect ===")
println("True effect: 10 points")
println("Propensity Score ATE: $(round(ate_ps, digits=2)) Â± $(round(1.96*se_ps, digits=2))")
println("IV (2SLS) LATE: $(round(ate_iv, digits=2)) Â± $(round(1.96*se_iv, digits=2))")
println("First-stage F: $(round(f_stat, digits=2))")

# Sensitivity
sensitivity_analysis_gamma(ate_ps, se_ps, [1.0, 1.5, 2.0])
```

:::message
**é€²æ—: 85% å®Œäº†** å®Ÿãƒ‡ãƒ¼ã‚¿å› æœæ¨è«–ãƒãƒ£ãƒ¬ãƒ³ã‚¸ã‚’å®Œäº†ã—ãŸã€‚å…¨æ‰‹æ³•ã‚’æ¯”è¼ƒã—ã€æ„Ÿåº¦åˆ†æã§é ‘å¥æ€§ã‚’ç¢ºèªã—ãŸã€‚æ¬¡ã¯ç™ºå±•ã‚¾ãƒ¼ãƒ³ã§ç ”ç©¶ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ã‚’æ¢ç´¢ã™ã‚‹ã€‚
:::

---

## ğŸ“ 6. æŒ¯ã‚Šè¿”ã‚Šã¨ç™ºå±•ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨æœ€æ–°ç ”ç©¶å‹•å‘

### 6.1 å› æœæ¨è«–ãƒ•ã‚¡ãƒŸãƒªãƒ¼ãƒ„ãƒªãƒ¼

```mermaid
graph TD
    A["ğŸŒ³ å› æœæ¨è«–"] --> B["Potential Outcomes<br/>(Rubin)"]
    A --> C["Structural Causal Models<br/>(Pearl)"]
    A --> D["Quasi-Experimental<br/>(Campbell)"]

    B --> E["Unconfoundedness<br/>å‚¾å‘ã‚¹ã‚³ã‚¢"]
    B --> F["SUTVA<br/>ATE/ATT/CATE"]

    C --> G["DAG + do-æ¼”ç®—"]
    C --> H["ãƒãƒƒã‚¯ãƒ‰ã‚¢/ãƒ•ãƒ­ãƒ³ãƒˆãƒ‰ã‚¢"]
    C --> I["åå®Ÿä»®æƒ³æ¨è«–"]

    D --> J["IV/2SLS"]
    D --> K["RDD"]
    D --> L["DiD"]

    A --> M["MLÃ—å› æœ<br/>(Athey/Imbens)"]
    M --> N["Causal Forest"]
    M --> O["Double ML"]
    M --> P["Meta-Learners"]

    style A fill:#c8e6c9
    style M fill:#fff3e0
```

### 6.2 æ¨è–¦è«–æ–‡ãƒ»æ•™ç§‘æ›¸

#### ä¸»è¦è«–æ–‡

| è«–æ–‡ | è‘—è€… | å¹´ | è²¢çŒ® |
|:-----|:-----|:---|:-----|
| Causality (2nd Ed) [^1] | Pearl | 2009 | SCM, do-æ¼”ç®—, ãƒãƒƒã‚¯ãƒ‰ã‚¢åŸºæº– |
| Causal Inference (free book) [^9] | HernÃ¡n & Robins | 2020 | å®Ÿè·µã‚¬ã‚¤ãƒ‰ |
| Potential Outcomes Survey [^2] | Rubin | 2005 | Rubinå› æœãƒ¢ãƒ‡ãƒ«çµ±åˆ |
| Causal Forest [^3] | Wager & Athey | 2018 | HTEæ¨å®š, æ¼¸è¿‘ç†è«– |
| Double ML [^4] | Chernozhukov et al. | 2018 | Debiased MLæ¨è«– |
| Staggered DiD [^5] | Callaway & Sant'Anna | 2021 | å¤šæœŸé–“DiD |
| Weak IV [^7] | Stock & Yogo | 2005 | å¼±æ“ä½œå¤‰æ•°æ¤œå®š |
| SRM Detection [^6] | Fabijan et al. | 2019 | A/Bãƒ†ã‚¹ãƒˆå“è³ªç®¡ç† |
| Simpson's Paradox [^8] | Pearl | 2014 | ãƒ‘ãƒ©ãƒ‰ãƒƒã‚¯ã‚¹è§£æ¶ˆ |

#### æ•™ç§‘æ›¸

- **å…¥é–€**: Pearl & Mackenzie "The Book of Why" (2018) â€” ä¸€èˆ¬å‘ã‘å› æœé©å‘½ã®æ­´å²
- **ç†è«–**: Pearl "Causality" (2009) [^1] â€” SCMã®è–å…¸
- **å®Ÿè·µ**: HernÃ¡n & Robins "Causal Inference" (2020) [^9] â€” ç„¡æ–™å…¬é–‹ã€ç–«å­¦ãƒ™ãƒ¼ã‚¹
- **è¨ˆé‡**: Angrist & Pischke "Mostly Harmless Econometrics" (2009) â€” IV/RDD/DiDã®å®Ÿè·µ
- **MLÃ—å› æœ**: Facure "Causal Inference for The Brave and True" (2022) â€” Pythonå®Ÿè£…ä»˜ã

### 6.3 ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹

| ãƒªã‚½ãƒ¼ã‚¹ | URL | èª¬æ˜ |
|:--------|:----|:-----|
| **CausalInference.jl** | [github.com/mschauer/CausalInference.jl](https://github.com/mschauer/CausalInference.jl) [^10] | Juliaã®DAG/PC/FCIå®Ÿè£… |
| **Causal Inference Bootcamp** | [YouTube: Brady Neal](https://www.youtube.com/playlist?list=PLoazKTcS0RzZ1SUgeOgc6SWt51gfT80N0) | å‹•ç”»è¬›ç¾©ã‚·ãƒªãƒ¼ã‚º |
| **doWhy (Microsoft)** | [github.com/py-why/dowhy](https://github.com/py-why/dowhy) | Pythonå› æœæ¨è«–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª |
| **EconML (Microsoft)** | [github.com/py-why/EconML](https://github.com/py-why/EconML) | Python MLÃ—å› æœãƒ©ã‚¤ãƒ–ãƒ©ãƒª |

### 6.4 å› æœæ¨è«–ç”¨èªé›†

:::details ç”¨èªé›†ï¼ˆã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆé †ï¼‰

| ç”¨èª | å®šç¾© |
|:-----|:-----|
| **ATE** | Average Treatment Effect â€” å…¨ä½“ã®å¹³å‡å‡¦ç½®åŠ¹æœ $\mathbb{E}[Y^1 - Y^0]$ |
| **ATT** | Average Treatment Effect on the Treated â€” å‡¦ç½®ç¾¤ã®å¹³å‡å‡¦ç½®åŠ¹æœ |
| **Backdoor Criterion** | ãƒãƒƒã‚¯ãƒ‰ã‚¢åŸºæº– â€” äº¤çµ¡ã‚’é™¤å»ã™ã‚‹ãŸã‚ã®å¤‰æ•°é›†åˆã®æ¡ä»¶ |
| **CATE** | Conditional Average Treatment Effect â€” æ¡ä»¶ä»˜ãå¹³å‡å‡¦ç½®åŠ¹æœ |
| **Collider** | ã‚³ãƒ©ã‚¤ãƒ€ãƒ¼ â€” 2ã¤ã®çŸ¢å°ãŒé›†ã¾ã‚‹å¤‰æ•° ($X \to Z \leftarrow Y$) |
| **DAG** | Directed Acyclic Graph â€” å› æœæ§‹é€ ã‚’è¡¨ã™æœ‰å‘éå·¡å›ã‚°ãƒ©ãƒ• |
| **DiD** | Difference-in-Differences â€” å·®åˆ†ã®å·®åˆ†æ³• |
| **d-separation** | dåˆ†é›¢ â€” DAGä¸Šã§ã®æ¡ä»¶ä»˜ãç‹¬ç«‹æ€§ |
| **do-Calculus** | do-æ¼”ç®— â€” ä»‹å…¥ç¢ºç‡ã‚’æ¡ä»¶ä»˜ãç¢ºç‡ã«å¤‰æ›ã™ã‚‹3ã¤ã®ãƒ«ãƒ¼ãƒ« |
| **Doubly Robust** | äºŒé‡é ‘å¥æ¨å®šé‡ â€” å‚¾å‘ã‚¹ã‚³ã‚¢ã¨çµæœãƒ¢ãƒ‡ãƒ«ã®ã©ã¡ã‚‰ã‹ãŒæ­£ã—ã‘ã‚Œã°ä¸å |
| **Fundamental Problem** | æ ¹æœ¬çš„å› æœæ¨è«–å•é¡Œ â€” $Y^1, Y^0$ ã‚’åŒæ™‚è¦³æ¸¬ã§ããªã„ |
| **IPW** | Inverse Probability Weighting â€” é€†ç¢ºç‡é‡ã¿ä»˜ã‘ |
| **IV** | Instrumental Variable â€” æ“ä½œå¤‰æ•° |
| **LATE** | Local Average Treatment Effect â€” å±€æ‰€å¹³å‡å‡¦ç½®åŠ¹æœï¼ˆã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ¼ã®åŠ¹æœï¼‰ |
| **Overlap** | å…±é€šã‚µãƒãƒ¼ãƒˆ â€” $0 < e(X) < 1$ ãŒã™ã¹ã¦ã® $X$ ã§æˆç«‹ |
| **Potential Outcomes** | æ½œåœ¨çš„çµæœ â€” $Y^1, Y^0$ |
| **Propensity Score** | å‚¾å‘ã‚¹ã‚³ã‚¢ â€” $e(X) = P(D=1 \mid X)$ |
| **RDD** | Regression Discontinuity Design â€” å›å¸°ä¸é€£ç¶šãƒ‡ã‚¶ã‚¤ãƒ³ |
| **SCM** | Structural Causal Model â€” æ§‹é€ å› æœãƒ¢ãƒ‡ãƒ« $(\mathcal{U}, \mathcal{V}, \mathcal{F})$ |
| **SUTVA** | Stable Unit Treatment Value Assumption â€” å®‰å®šå€‹ä½“å‡¦ç½®å€¤ä»®å®š |
| **Unconfoundedness** | ç„¡äº¤çµ¡æ€§ â€” $(Y^1, Y^0) \perp\!\!\!\perp D \mid X$ |
:::

### 6.5 å› æœæ¨è«–ã®çŸ¥è­˜ãƒãƒƒãƒ—

```mermaid
mindmap
  root((å› æœæ¨è«–))
    Foundations
      Potential Outcomes
      DAG
      Identification
    Methods
      Propensity Score
        IPW
        Matching
        DR
      Quasi-Experimental
        IV
        RDD
        DiD
    ML Integration
      Causal Forest
      Double ML
      Meta-Learners
    Applications
      Policy Evaluation
      A/B Testing
      Observational Studies
    Tools
      CausalInference.jl
      doWhy
      EconML
```

:::message
**é€²æ—: 100% å®Œäº†** å› æœæ¨è«–ã®ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ã‚’æ¢ç´¢ã—ãŸã€‚è«–æ–‡ãƒ»æ•™ç§‘æ›¸ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ç”¨èªã‚’å®Œå…¨æ•´ç†ã€‚ã‚ã¨ã¯æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ã§ã¾ã¨ã‚ã€‚
:::

---


### 6.6 æœ¬è¬›ç¾©ã®ã¾ã¨ã‚

1. **ç›¸é–¢ â‰  å› æœ**: Simpson's Paradox, äº¤çµ¡, é¸æŠãƒã‚¤ã‚¢ã‚¹ã®ç½ ã‚’ç†è§£
2. **Rubinå› æœãƒ¢ãƒ‡ãƒ«**: æ½œåœ¨çš„çµæœ $Y^1, Y^0$, SUTVA, ATE/ATT/CATE
3. **Pearlå› æœç†è«–**: DAG, do-æ¼”ç®—, ãƒãƒƒã‚¯ãƒ‰ã‚¢/ãƒ•ãƒ­ãƒ³ãƒˆãƒ‰ã‚¢åŸºæº–, d-åˆ†é›¢
4. **å‚¾å‘ã‚¹ã‚³ã‚¢**: IPW, Matching, Doubly Robust, ãƒãƒ©ãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯
5. **æ“ä½œå¤‰æ•°æ³•**: 2SLS, LATE, Weak IVå•é¡Œ
6. **RDD**: Sharp/Fuzzy, å±€æ‰€ãƒ©ãƒ³ãƒ€ãƒ åŒ–, å¸¯åŸŸå¹…é¸æŠ
7. **DiD**: ä¸¦è¡Œãƒˆãƒ¬ãƒ³ãƒ‰ä»®å®š, Staggered DiD
8. **MLÃ—å› æœæ¨è«–**: Causal Forest, Double ML, Meta-Learners
9. **Juliaå®Ÿè£…**: CausalInference.jl ã§å…¨æ‰‹æ³•ã‚’å®Ÿè£…

### 6.7 ã‚ˆãã‚ã‚‹è³ªå• (FAQ)

:::details Q1: å› æœæ¨è«–ã¨æ©Ÿæ¢°å­¦ç¿’ã®é•ã„ã¯ï¼Ÿ

**A**:
- **æ©Ÿæ¢°å­¦ç¿’**: äºˆæ¸¬ç²¾åº¦ã®æœ€å¤§åŒ– â€” $\hat{Y} \approx Y$
- **å› æœæ¨è«–**: å› æœåŠ¹æœã®æ¨å®š â€” $\mathbb{E}[Y \mid do(X=x)]$

MLã¯ã€Œæ¬¡ã«ä½•ãŒèµ·ã“ã‚‹ã‹ã€ã€å› æœæ¨è«–ã¯ã€Œä»‹å…¥ã—ãŸã‚‰ä½•ãŒèµ·ã“ã‚‹ã‹ã€ã‚’å•ã†ã€‚MLã¯ç›¸é–¢ã‚’å­¦ç¿’ã—ã€å› æœæ¨è«–ã¯å› æœæ§‹é€ ã‚’ä»®å®šã™ã‚‹ã€‚
:::

:::details Q2: ã„ã¤å‚¾å‘ã‚¹ã‚³ã‚¢ vs IVã‚’ä½¿ã†ï¼Ÿ

**A**:
- **å‚¾å‘ã‚¹ã‚³ã‚¢**: Unconfoundedness $(Y^d \perp\!\!\!\perp D \mid X)$ ãŒæˆç«‹ã™ã‚‹å ´åˆ â€” ã™ã¹ã¦ã®äº¤çµ¡å› å­ã‚’æ¸¬å®šã§ãã¦ã„ã‚‹
- **IV**: æœªæ¸¬å®šäº¤çµ¡ãŒã‚ã‚‹å ´åˆ â€” å¤–ç”Ÿçš„ãªãƒ©ãƒ³ãƒ€ãƒ å¤‰å‹•ï¼ˆæ“ä½œå¤‰æ•°ï¼‰ã‚’åˆ©ç”¨

ãƒ©ãƒ³ãƒ€ãƒ åŒ–å®Ÿé¨“ã«è¿‘ã„çŠ¶æ³ãªã‚‰å‚¾å‘ã‚¹ã‚³ã‚¢ã€è¦³æ¸¬ç ”ç©¶ã§äº¤çµ¡ãŒç–‘ã‚ã‚Œã‚‹ãªã‚‰IVã€‚
:::

:::details Q3: RDDã¨DiDã®ä½¿ã„åˆ†ã‘ã¯ï¼Ÿ

**A**:
- **RDD**: å‡¦ç½®å‰²ã‚Šå½“ã¦ãŒã‚«ãƒƒãƒˆã‚ªãƒ•ã§æ±ºã¾ã‚‹ï¼ˆä¾‹: å¹´é½¢18æ­³ã§é¸æŒ™æ¨©ã€ã‚¹ã‚³ã‚¢70ç‚¹ã§åˆæ ¼ï¼‰
- **DiD**: 2æœŸé–“ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã€å‡¦ç½®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ãŒç¾¤ã«ã‚ˆã£ã¦ç•°ãªã‚‹

RDDã¯ç©ºé–“çš„ä¸é€£ç¶šã€DiDã¯æ™‚é–“çš„å¤‰åŒ–ã‚’åˆ©ç”¨ã™ã‚‹ã€‚
:::

:::details Q4: Causal Forestã§ä½•ãŒã‚ã‹ã‚‹ï¼Ÿ

**A**: **ç•°è³ªãªå‡¦ç½®åŠ¹æœ (HTE)** â€” å€‹ä½“ç‰¹æ€§ $X$ ã«å¿œã˜ãŸå‡¦ç½®åŠ¹æœ $\tau(X)$

å¹³å‡åŠ¹æœ(ATE)ã ã‘ã§ãªãã€ã€Œé«˜é½¢è€…ã«ã¯åŠ¹æœå¤§ã€è‹¥å¹´è€…ã«ã¯åŠ¹æœå°ã€ã¨ã„ã£ãŸéƒ¨åˆ†é›†å›£ã”ã¨ã®åŠ¹æœã‚’æ¨å®šã§ãã‚‹ã€‚æ”¿ç­–ã®ã‚¿ãƒ¼ã‚²ãƒ†ã‚£ãƒ³ã‚°ã«æœ‰ç”¨ã€‚
:::

:::details Q5: å› æœæ¨è«–ã§æœ€ã‚‚é‡è¦ãªä»®å®šã¯ï¼Ÿ

**A**: **Unconfoundedness** $(Y^d \perp\!\!\!\perp D \mid X)$ ã¾ãŸã¯ **Exclusion Restriction** (IV)

ã“ã‚ŒãŒç ´ã‚Œã‚‹ã¨ã€ã©ã‚“ãªæ‰‹æ³•ã‚‚å› æœåŠ¹æœã‚’æ­£ã—ãæ¨å®šã§ããªã„ã€‚ä»®å®šã®å¦¥å½“æ€§ã‚’ç†è«–ãƒ»ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ãƒ»æ„Ÿåº¦åˆ†æã§æ¤œè¨¼ã™ã‚‹ã“ã¨ãŒæœ€é‡è¦ã€‚
:::

### 6.8 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ1é€±é–“å¾©ç¿’ãƒ—ãƒ©ãƒ³ï¼‰

| Day | å†…å®¹ | æ™‚é–“ | é”æˆåŸºæº– |
|:----|:-----|:-----|:---------|
| Day 1 | Zone 3.1-3.2 å†èª­ + Rubinç†è«–å¾©ç¿’ | 1h | ATE/ATT/CATE ã‚’è‡ªåŠ›ã§å°å‡ºã§ãã‚‹ |
| Day 2 | Zone 3.3 å†èª­ + Pearlç†è«–å¾©ç¿’ | 1h | ãƒãƒƒã‚¯ãƒ‰ã‚¢èª¿æ•´å…¬å¼ã‚’è‡ªåŠ›ã§å°å‡ºã§ãã‚‹ |
| Day 3 | Zone 3.4-3.5 å†èª­ + å‚¾å‘ã‚¹ã‚³ã‚¢/IVå¾©ç¿’ | 1h | IPWæ¨å®šé‡ã‚’è‡ªåŠ›ã§å°å‡ºã§ãã‚‹ |
| Day 4 | Zone 4 Juliaå®Ÿè£…ã‚’å…¨ã¦å®Ÿè¡Œ | 2h | å…¨ã‚³ãƒ¼ãƒ‰ãŒã‚¨ãƒ©ãƒ¼ãªãå®Ÿè¡Œã§ãã‚‹ |
| Day 5 | Zone 5 ãƒŸãƒ‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å®Ÿè£… | 2h | æ•™è‚²ä»‹å…¥ãƒ‡ãƒ¼ã‚¿ã§3æ‰‹æ³•æ¯”è¼ƒå®Œäº† |
| Day 6 | è«–æ–‡èª­è§£: Causal Forest [^3] or Double ML [^4] | 2h | æ‰‹æ³•ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒå®Œå…¨ã«ç†è§£ã§ãã‚‹ |
| Day 7 | è‡ªåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã§å› æœæ¨è«–å®Ÿè·µ | 3h | å®Ÿãƒ‡ãƒ¼ã‚¿ã§ATEæ¨å®š + æ„Ÿåº¦åˆ†æå®Œäº† |

### 6.9 æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

**ç¬¬26å›: æ¨è«–æœ€é©åŒ– & Productionå“è³ª** ã§ã¯ã€å› æœæ¨è«–ã§å¾—ãŸåŠ¹æœã‚’**æœ¬ç•ªã‚·ã‚¹ãƒ†ãƒ ã«çµ„ã¿è¾¼ã‚€**:

- A/Bãƒ†ã‚¹ãƒˆåŸºç›¤æ§‹ç¯‰ (Elixir OTPã§ã®ä¸¦è¡Œãƒ†ã‚¹ãƒˆç®¡ç†)
- ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆæ¢ç´¢ã¨æ´»ç”¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ï¼‰
- å› æœæ¨è«–Ã—å¼·åŒ–å­¦ç¿’ï¼ˆCounterfactual Policy Evaluationï¼‰
- Productionå“è³ª: é‡å­åŒ–ãƒ»è’¸ç•™ãƒ»Speculative Decoding

**ç¬¬27å›: è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³** ã§ã¯ã€å› æœåŠ¹æœã®çµ±è¨ˆçš„æ¤œå®š:

- FID/IS/LPIPS (ç”Ÿæˆãƒ¢ãƒ‡ãƒ«è©•ä¾¡)
- Bootstrapã«ã‚ˆã‚‹CIæ¨å®š
- å¤šé‡æ¤œå®šè£œæ­£ (Bonferroni, FDR)
- å› æœåŠ¹æœã®å¯è¦–åŒ– (Forest Plot, Love Plot)

### 6.10 è‡ªå·±ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] Simpson's Paradoxã‚’èª¬æ˜ã§ãã‚‹
- [ ] ATE, ATT, CATEã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
- [ ] æ½œåœ¨çš„çµæœ $Y^1, Y^0$ ã‚’å®šç¾©ã§ãã‚‹
- [ ] SUTVAã®2ã¤ã®ä»®å®šã‚’è¿°ã¹ã‚‰ã‚Œã‚‹
- [ ] do-æ¼”ç®— $P(Y \mid do(X))$ ã¨æ¡ä»¶ä»˜ãç¢ºç‡ã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
- [ ] ãƒãƒƒã‚¯ãƒ‰ã‚¢åŸºæº–ã‚’èª¬æ˜ã§ãã‚‹
- [ ] d-åˆ†é›¢ã®3ãƒ‘ã‚¿ãƒ¼ãƒ³ (Chain/Fork/Collider) ã‚’å›³ç¤ºã§ãã‚‹
- [ ] å‚¾å‘ã‚¹ã‚³ã‚¢ã®å®šç¾©ã¨æ¬¡å…ƒå‰Šæ¸›ã®æ„å‘³ã‚’èª¬æ˜ã§ãã‚‹
- [ ] IPWæ¨å®šé‡ã‚’å°å‡ºã§ãã‚‹
- [ ] Doubly Robustæ¨å®šé‡ã®ãƒ¡ãƒªãƒƒãƒˆã‚’èª¬æ˜ã§ãã‚‹
- [ ] æ“ä½œå¤‰æ•°ã®3æ¡ä»¶ã‚’è¿°ã¹ã‚‰ã‚Œã‚‹
- [ ] 2SLSæ¨å®šé‡ (Waldæ¨å®šé‡) ã‚’å°å‡ºã§ãã‚‹
- [ ] LATEã¨ATEã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Weak IVå•é¡Œã¨ç¬¬1æ®µéšFçµ±è¨ˆé‡ã®é–¢ä¿‚ã‚’èª¬æ˜ã§ãã‚‹
- [ ] RDDã®å±€æ‰€ãƒ©ãƒ³ãƒ€ãƒ åŒ–ä»®å®šã‚’èª¬æ˜ã§ãã‚‹
- [ ] Sharp RDDã¨Fuzzy RDDã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
- [ ] DiDã®ä¸¦è¡Œãƒˆãƒ¬ãƒ³ãƒ‰ä»®å®šã‚’èª¬æ˜ã§ãã‚‹
- [ ] Staggered DiDã®å•é¡Œç‚¹ (TWFE ãƒã‚¤ã‚¢ã‚¹) ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Causal Forestã§HTEã‚’æ¨å®šã™ã‚‹æ„ç¾©ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Double MLã®Neyman-Orthogonal Scoreã‚’èª¬æ˜ã§ãã‚‹
- [ ] Juliaã§IPWæ¨å®šã‚’å®Ÿè£…ã§ãã‚‹
- [ ] Juliaã§DAGã‚’æ§‹ç¯‰ã—ãƒãƒƒã‚¯ãƒ‰ã‚¢åŸºæº–ã‚’æ¤œè¨¼ã§ãã‚‹
- [ ] Juliaã§RDDæ¨å®šã‚’å®Ÿè£…ã§ãã‚‹
- [ ] Juliaã§DiDæ¨å®šã‚’å®Ÿè£…ã§ãã‚‹
- [ ] æ„Ÿåº¦åˆ†æ (Rosenbaum's Î“) ã‚’å®Ÿè¡Œã§ãã‚‹

**25é …ç›®ä¸­20é …ç›®ä»¥ä¸Šãƒã‚§ãƒƒã‚¯** â†’ å› æœæ¨è«–ãƒã‚¹ã‚¿ãƒ¼ï¼

### 6.6 é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼ (Juliaå®Ÿè£…)

```julia
# Self-assessment progress tracker
function causal_inference_progress()
    skills = [
        "Simpson's Paradoxç†è§£",
        "ATE/ATT/CATEåŒºåˆ¥",
        "æ½œåœ¨çš„çµæœå®šç¾©",
        "SUTVAèª¬æ˜",
        "do-æ¼”ç®—ç†è§£",
        "ãƒãƒƒã‚¯ãƒ‰ã‚¢åŸºæº–",
        "d-åˆ†é›¢",
        "å‚¾å‘ã‚¹ã‚³ã‚¢å®šç¾©",
        "IPWå°å‡º",
        "DRæ¨å®šé‡",
        "IV 3æ¡ä»¶",
        "2SLSå°å‡º",
        "LATE vs ATE",
        "Weak IVå•é¡Œ",
        "RDDå±€æ‰€ãƒ©ãƒ³ãƒ€ãƒ åŒ–",
        "Sharp vs Fuzzy RDD",
        "DiDä¸¦è¡Œãƒˆãƒ¬ãƒ³ãƒ‰",
        "Staggered DiD",
        "Causal Forest HTE",
        "Double ML",
        "Julia IPWå®Ÿè£…",
        "Julia DAGå®Ÿè£…",
        "Julia RDDå®Ÿè£…",
        "Julia DiDå®Ÿè£…",
        "æ„Ÿåº¦åˆ†æå®Ÿè¡Œ"
    ]

    println("ğŸ¯ å› æœæ¨è«–ã‚¹ã‚­ãƒ«é€²æ—")
    println("é”æˆã—ãŸé …ç›®ã‚’ true ã«ãƒãƒ¼ã‚¯ã—ã¦ãã ã•ã„:\n")

    completed = [
        true,   # Simpson's Paradoxç†è§£
        true,   # ATE/ATT/CATEåŒºåˆ¥
        true,   # æ½œåœ¨çš„çµæœå®šç¾©
        false,  # SUTVAèª¬æ˜
        false,  # do-æ¼”ç®—ç†è§£
        false,  # ãƒãƒƒã‚¯ãƒ‰ã‚¢åŸºæº–
        false,  # d-åˆ†é›¢
        false,  # å‚¾å‘ã‚¹ã‚³ã‚¢å®šç¾©
        false,  # IPWå°å‡º
        false,  # DRæ¨å®šé‡
        false,  # IV 3æ¡ä»¶
        false,  # 2SLSå°å‡º
        false,  # LATE vs ATE
        false,  # Weak IVå•é¡Œ
        false,  # RDDå±€æ‰€ãƒ©ãƒ³ãƒ€ãƒ åŒ–
        false,  # Sharp vs Fuzzy RDD
        false,  # DiDä¸¦è¡Œãƒˆãƒ¬ãƒ³ãƒ‰
        false,  # Staggered DiD
        false,  # Causal Forest HTE
        false,  # Double ML
        false,  # Julia IPWå®Ÿè£…
        false,  # Julia DAGå®Ÿè£…
        false,  # Julia RDDå®Ÿè£…
        false,  # Julia DiDå®Ÿè£…
        false   # æ„Ÿåº¦åˆ†æå®Ÿè¡Œ
    ]

    n_completed = sum(completed)
    n_total = length(skills)
    progress = round(100 * n_completed / n_total, digits=1)

    for (i, skill) in enumerate(skills)
        status = completed[i] ? "âœ…" : "â¬œ"
        println("$status $i. $skill")
    end

    println("\nğŸ“Š é€²æ—: $n_completed/$n_total ($progress%)")

    if n_completed >= 20
        println("ğŸ† å› æœæ¨è«–ãƒã‚¹ã‚¿ãƒ¼é”æˆï¼")
    elseif n_completed >= 15
        println("ğŸ¥ˆ ä¸Šç´šãƒ¬ãƒ™ãƒ« â€” ã‚‚ã†ä¸€æ¯ï¼")
    elseif n_completed >= 10
        println("ğŸ¥‰ ä¸­ç´šãƒ¬ãƒ™ãƒ« â€” é †èª¿ã§ã™")
    else
        println("ğŸ“š åˆç´šãƒ¬ãƒ™ãƒ« â€” å¾©ç¿’ã‚’ç¶šã‘ã¾ã—ã‚‡ã†")
    end

    return progress
end

causal_inference_progress()
```

### 6.7 æ¬¡å›äºˆå‘Š: æ¨è«–æœ€é©åŒ– & Productionå“è³ª

ç¬¬26å›ã§ã¯ã€å› æœæ¨è«–ã§æ¸¬å®šã—ãŸåŠ¹æœã‚’**æœ¬ç•ªã‚·ã‚¹ãƒ†ãƒ ã§æ´»ã‹ã™**æŠ€è¡“ã‚’å­¦ã¶:

- **A/Bãƒ†ã‚¹ãƒˆåŸºç›¤**: Elixir OTPã§ã®ä¸¦è¡Œãƒ†ã‚¹ãƒˆç®¡ç†ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã€SRMæ¤œå‡º
- **ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆ**: æ¢ç´¢ã¨æ´»ç”¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã€Thompson Sampling, UCB
- **å› æœæ¨è«–Ã—RL**: Counterfactual Policy Evaluation, Off-Policy Evaluation
- **æ¨è«–æœ€é©åŒ–**: é‡å­åŒ– (INT8/FP16), è’¸ç•™, Speculative Decoding, KVã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–
- **Productionå“è³ª**: Rustæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³, Elixiråˆ†æ•£ã‚µãƒ¼ãƒ“ãƒ³ã‚°, ç›£è¦–ãƒ»ãƒ­ã‚®ãƒ³ã‚°ã€ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: Multi-Armed Bandit / Contextual Bandit / Thompson Sampling / Speculative Decoding / GGUFé‡å­åŒ– / KV-Cache / OTP Supervision

**ç›®æ¨™**: å› æœæ¨è«–ã§å¾—ãŸçŸ¥è¦‹ã‚’ã€å®Ÿæˆ¦ã§ä½¿ãˆã‚‹é«˜é€Ÿãƒ»é ‘å¥ãªã‚·ã‚¹ãƒ†ãƒ ã«çµ±åˆã™ã‚‹ã€‚

---

### 6.11 ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **A/Bãƒ†ã‚¹ãƒˆãªã—ã«"æ”¹å–„"ã‚’è¨¼æ˜ã§ãã‚‹ã‹ï¼Ÿ**

ãƒ©ãƒ³ãƒ€ãƒ åŒ–å®Ÿé¨“ï¼ˆA/Bãƒ†ã‚¹ãƒˆï¼‰ã¯å› æœæ¨è«–ã®ã‚´ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰ã ã€‚ã ãŒ:

- **å€«ç†çš„åˆ¶ç´„**: åŒ»ç™‚ã€æ•™è‚²ã€ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ã‚µãƒ¼ãƒ“ã‚¹ã§ãƒ©ãƒ³ãƒ€ãƒ åŒ–ã¯å›°é›£
- **ã‚³ã‚¹ãƒˆ**: å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’å®Ÿé¨“å°ã«ã§ããªã„
- **æ™‚é–“**: åŠ¹æœãŒå‡ºã‚‹ã¾ã§æ•°ãƒ¶æœˆã€œæ•°å¹´

**è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å› æœåŠ¹æœã‚’æ­£ã—ãæ¨å®šã§ãã‚Œã°ã€A/Bãƒ†ã‚¹ãƒˆãªã—ã§ã‚‚æ”¹å–„ã‚’è¨¼æ˜ã§ãã‚‹ã€‚**

æœ¬è¬›ç¾©ã§å­¦ã‚“ã æ‰‹æ³•:

1. **å‚¾å‘ã‚¹ã‚³ã‚¢**: äº¤çµ¡ã‚’åˆ¶å¾¡ã—ã€è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ATEæ¨å®š
2. **æ“ä½œå¤‰æ•°**: æœªæ¸¬å®šäº¤çµ¡ãŒã‚ã£ã¦ã‚‚ãƒ©ãƒ³ãƒ€ãƒ ãªå¤‰å‹•ã§å› æœåŠ¹æœæ¨å®š
3. **RDD**: ã‚«ãƒƒãƒˆã‚ªãƒ•ã®ä¸é€£ç¶šæ€§ã‚’åˆ©ç”¨ã—ã€å±€æ‰€çš„ãªå› æœåŠ¹æœæ¨å®š
4. **DiD**: æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã§ä¸¦è¡Œãƒˆãƒ¬ãƒ³ãƒ‰ä»®å®šã®ä¸‹ã§å› æœåŠ¹æœæ¨å®š
5. **Causal Forest**: ç•°è³ªãªå‡¦ç½®åŠ¹æœã‚’æ¨å®šã—ã€ã‚¿ãƒ¼ã‚²ãƒ†ã‚£ãƒ³ã‚°æœ€é©åŒ–

**ã ãŒã€ä»®å®šãŒç ´ã‚Œã‚Œã°å…¨ã¦ãŒå´©ã‚Œã‚‹ã€‚** å› æœæ¨è«–ã¯ã€Œä»®å®šã®æ˜ç¤ºåŒ–ã€ã¨ã€Œæ„Ÿåº¦åˆ†æã€ã«ã‚ˆã£ã¦ä»®å®šã®å¦¥å½“æ€§ã‚’æ¤œè¨¼ã—ç¶šã‘ã‚‹å–¶ã¿ã ã€‚

**ã‚ãªãŸã®ç­”ãˆã¯ï¼Ÿ** â€” è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿å› æœæ¨è«–ã¨A/Bãƒ†ã‚¹ãƒˆã®ãƒãƒ©ãƒ³ã‚¹ã‚’ã©ã†å–ã‚‹ã‹ï¼Ÿ

:::details è­°è«–ã®ãƒã‚¤ãƒ³ãƒˆ

1. **è¦³æ¸¬ç ”ç©¶ã®å¼·ã¿**:
   - å€«ç†çš„åˆ¶ç´„ãŒãªã„ï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã†ï¼‰
   - å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§å¤–éƒ¨å¦¥å½“æ€§ãŒé«˜ã„
   - é•·æœŸçš„åŠ¹æœã‚’è¿½è·¡ã§ãã‚‹

2. **è¦³æ¸¬ç ”ç©¶ã®å¼±ã¿**:
   - ä»®å®šä¾å­˜ï¼ˆUnconfoundedness, IVä»®å®šç­‰ï¼‰
   - æœªæ¸¬å®šäº¤çµ¡ã®ãƒªã‚¹ã‚¯
   - å› æœæ§‹é€ ã®èª¤ç‰¹å®š

3. **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**:
   - A/Bãƒ†ã‚¹ãƒˆã§çŸ­æœŸåŠ¹æœæ¤œè¨¼ + è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã§é•·æœŸåŠ¹æœæ¨å®š
   - A/Bãƒ†ã‚¹ãƒˆã§ãƒã‚¤ã‚¢ã‚¹è£œæ­£ + è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã§å¤–æŒ¿
   - å› æœæ¨è«–ã§äº‹å‰è©•ä¾¡ + A/Bãƒ†ã‚¹ãƒˆã§æœ€çµ‚ç¢ºèª

4. **æ­´å²çš„è¦–ç‚¹**:
   - Fisher (1935): ãƒ©ãƒ³ãƒ€ãƒ åŒ–å®Ÿé¨“ã®åŸå‰‡ç¢ºç«‹
   - Rubin (1974): è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®å› æœæ¨è«–ç†è«–
   - Pearl (2000): ã‚°ãƒ©ãƒ•ã‚£ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã§å› æœæ§‹é€ ã‚’æ˜ç¤ºåŒ–
   - ç¾ä»£: MLÃ—å› æœæ¨è«–ã§å¤§è¦æ¨¡è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿æ´»ç”¨

**çµè«–**: A/Bãƒ†ã‚¹ãƒˆã¯ä¾ç„¶ã¨ã—ã¦ã‚´ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰ã ãŒã€**å› æœæ¨è«–ã¯è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœ€å¤§é™ã®æƒ…å ±ã‚’å¼•ãå‡ºã™å¼·åŠ›ãªæ­¦å™¨**ã€‚ä¸¡è€…ã‚’é©åˆ‡ã«çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šæ­£ç¢ºãªæ„æ€æ±ºå®šãŒå¯èƒ½ã«ãªã‚‹ã€‚
:::

:::message
**é€²æ—: 100% å®Œäº†** ğŸ‰ è¬›ç¾©å®Œèµ°ï¼
:::

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Pearl, J. (2009). *Causality: Models, Reasoning, and Inference* (2nd ed.). Cambridge University Press.
@[card](https://bayes.cs.ucla.edu/BOOK-2K/)

[^2]: Rubin, D. B. (2005). Causal Inference Using Potential Outcomes: Design, Modeling, Decisions. *Journal of the American Statistical Association*, 100(469), 322-331.
@[card](https://www.tandfonline.com/doi/abs/10.1198/016214504000001880)

[^3]: Wager, S., & Athey, S. (2018). Estimation and Inference of Heterogeneous Treatment Effects using Random Forests. *Journal of the American Statistical Association*, 113(523), 1228-1242.
@[card](https://arxiv.org/abs/1510.04342)

[^4]: Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., & Robins, J. (2018). Double/debiased machine learning for treatment and structural parameters. *The Econometrics Journal*, 21(1), C1-C68.
@[card](https://arxiv.org/abs/1608.00060)

[^5]: Callaway, B., & Sant'Anna, P. H. (2021). Difference-in-Differences with multiple time periods. *Journal of Econometrics*, 225(2), 200-230.
@[card](https://www.sciencedirect.com/science/article/abs/pii/S0304407620303948)

[^6]: Fabijan, A., Gupchup, J., Gupta, S., Omhover, J., Qin, W., Vermeer, L., & Dmitriev, P. (2019). Diagnosing Sample Ratio Mismatch in Online Controlled Experiments: A Taxonomy and Rules of Thumb for Practitioners. *Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining*, 2156-2164.
@[card](https://dl.acm.org/doi/10.1145/3292500.3330722)

[^7]: Stock, J. H., & Yogo, M. (2005). Testing for Weak Instruments in Linear IV Regression. In *Identification and Inference for Econometric Models: Essays in Honor of Thomas Rothenberg* (pp. 80-108). Cambridge University Press.
@[card](https://www.cambridge.org/core/books/abs/identification-and-inference-for-econometric-models/testing-for-weak-instruments-in-linear-iv-regression/8AD94FF2EFD214D05D75EE35015021E4)

[^8]: Pearl, J. (2014). Understanding Simpson's Paradox. *The American Statistician*, 68(1), 8-13.
@[card](https://ftp.cs.ucla.edu/pub/stat_ser/r414.pdf)

[^9]: HernÃ¡n, M. A., & Robins, J. M. (2020). *Causal Inference: What If*. Chapman & Hall/CRC. (Free online)
@[card](https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/)

[^10]: Mschauer. (2021). CausalInference.jl: Causal inference, graphical models and structure learning in Julia.
@[card](https://github.com/mschauer/CausalInference.jl)

### æ•™ç§‘æ›¸

- Angrist, J. D., & Pischke, J.-S. (2009). *Mostly Harmless Econometrics: An Empiricist's Companion*. Princeton University Press.
- Cunningham, S. (2021). *Causal Inference: The Mixtape*. Yale University Press. (Free online)
- Facure, M. (2022). *Causal Inference for The Brave and True*. (Free online)
- Imbens, G. W., & Rubin, D. B. (2015). *Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction*. Cambridge University Press.
- Morgan, S. L., & Winship, C. (2014). *Counterfactuals and Causal Inference* (2nd ed.). Cambridge University Press.

---

## è¨˜æ³•è¦ç´„

| è¨˜æ³• | æ„å‘³ |
|:-----|:-----|
| $D$ | å‡¦ç½®å¤‰æ•° (Treatment), $D \in \\{0, 1\\}$ |
| $Y$ | çµæœå¤‰æ•° (Outcome) |
| $X$ | å…±å¤‰é‡ (Covariates), äº¤çµ¡å› å­å€™è£œ |
| $Y^d$ | æ½œåœ¨çš„çµæœ (Potential Outcome), $d \in \\{0, 1\\}$ |
| $Y^1$ | å‡¦ç½®ã‚’å—ã‘ãŸå ´åˆã®çµæœ |
| $Y^0$ | å‡¦ç½®ã‚’å—ã‘ãªã‹ã£ãŸå ´åˆã®çµæœ |
| $\tau$ | å‡¦ç½®åŠ¹æœ (Treatment Effect), $\tau = Y^1 - Y^0$ |
| $\mathbb{E}[\cdot]$ | æœŸå¾…å€¤ |
| $P(\cdot)$ | ç¢ºç‡ |
| $P(Y \mid X)$ | æ¡ä»¶ä»˜ãç¢ºç‡ |
| $P(Y \mid do(X))$ | ä»‹å…¥ç¢ºç‡ (Interventional Probability) |
| $e(X)$ | å‚¾å‘ã‚¹ã‚³ã‚¢ (Propensity Score), $e(X) = P(D=1 \mid X)$ |
| $\mathcal{G}$ | DAG (Directed Acyclic Graph) |
| $\text{PA}_i$ | å¤‰æ•° $i$ ã®è¦ªãƒãƒ¼ãƒ‰é›†åˆ |
| $X \perp\!\!\!\perp Y \mid Z$ | $Z$ ã‚’æ‰€ä¸ã¨ã—ãŸã¨ãã® $X$ ã¨ $Y$ ã®æ¡ä»¶ä»˜ãç‹¬ç«‹ |
| $X \perp_d Y \mid Z$ | DAGä¸Šã§ã® $X$ ã¨ $Y$ ã® d-åˆ†é›¢ |
| $\text{ATE}$ | Average Treatment Effect, $\mathbb{E}[Y^1 - Y^0]$ |
| $\text{ATT}$ | Average Treatment Effect on the Treated, $\mathbb{E}[Y^1 - Y^0 \mid D=1]$ |
| $\text{CATE}$ | Conditional Average Treatment Effect, $\mathbb{E}[Y^1 - Y^0 \mid X]$ |
| $\text{LATE}$ | Local Average Treatment Effect (IVæ–‡è„ˆ) |
| $Z$ | æ“ä½œå¤‰æ•° (Instrumental Variable) |
| $c$ | ã‚«ãƒƒãƒˆã‚ªãƒ• (RDD) |
| $h$ | å¸¯åŸŸå¹… (Bandwidth, RDD) |

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚

**æ¬¡å›**: [ç¬¬26å›: æ¨è«–æœ€é©åŒ– & Productionå“è³ª](/your-next-article)
