---
title: "ç¬¬21å›: ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ & HuggingFace Datasets: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸ“Š"
type: "tech"
topics: ["machinelearning", "datascience", "julia", "huggingface", "dataengineering"]
published: true
---

# ç¬¬21å›: ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ & HuggingFace Datasets â€” ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã¯ãƒ‡ãƒ¼ã‚¿ã§æ±ºã¾ã‚‹

> **ç¬¬20å›ã§VAE/GAN/Transformerã‚’å®Ÿè£…ã—ãŸã€‚ã ãŒãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã¯ãƒ‡ãƒ¼ã‚¿ã§æ±ºã¾ã‚‹ã€‚ä»Šå›ã¯ãƒ‡ãƒ¼ã‚¿ã®æ‰±ã„æ–¹ã‚’å¾¹åº•çš„ã«å­¦ã¶ã€‚**

ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å·®ã¯æ•°%ã€‚ã ãŒãƒ‡ãƒ¼ã‚¿å“è³ªã®å·®ã¯æ¡é•ã„ã ã€‚åŒã˜VAEã§ã‚‚ã€é©åˆ‡ã«å‰å‡¦ç†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã¨ç”Ÿãƒ‡ãƒ¼ã‚¿ã§ã¯ã€ç”Ÿæˆç”»åƒã®å“è³ªãŒ10å€é•ã†ã€‚ä¸å‡è¡¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯ã€ç²¾åº¦90%ã®ãƒ¢ãƒ‡ãƒ«ãŒå®Ÿç”¨ã§ã¯ä½¿ã„ç‰©ã«ãªã‚‰ãªã„ã€‚

ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã¯ã€Œãƒ¢ãƒ‡ãƒ«ã®å‰å·¥ç¨‹ã€ã§ã¯ãªã„ã€‚**ãƒ¢ãƒ‡ãƒ«ã®åœŸå°**ã ã€‚

æœ¬è¬›ç¾©ã¯Course IIIã€Œå®Ÿè£…ç·¨ã€ã®ç¬¬3å› â€” ç’°å¢ƒæ§‹ç¯‰(ç¬¬19å›)â†’VAE/GAN/Transformerå®Ÿè£…(ç¬¬20å›)ã«ç¶šãã€**ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®å…¨ã‚µã‚¤ã‚¯ãƒ«**ã‚’ç¿’å¾—ã™ã‚‹ã€‚HuggingFace Datasetsçµ±åˆã€Juliaé€£æºã«ã‚ˆã‚‹ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼å‡¦ç†ã€ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾ç­–ã€æ•°å¼â†”ã‚³ãƒ¼ãƒ‰1:1å¯¾å¿œã§ã€å®Ÿæˆ¦çš„ãªãƒ‡ãƒ¼ã‚¿ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°åŠ›ã‚’èº«ã«ã¤ã‘ã‚‹ã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2024-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚
:::

```mermaid
graph TD
    A["ğŸ“Š Raw Data<br/>ç”Ÿãƒ‡ãƒ¼ã‚¿ãƒ»ä¸å‡è¡¡"] --> B["ğŸ” EDA<br/>åˆ†å¸ƒç¢ºèªãƒ»å¤–ã‚Œå€¤"]
    B --> C["âš™ï¸ Preprocessing<br/>æ­£è¦åŒ–ãƒ»æ¬ æå‡¦ç†"]
    C --> D["ğŸ¤— HF Datasets<br/>çµ±ä¸€API"]
    D --> E["âš¡ Juliaé€£æº<br/>Arrowãƒ»ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼"]
    E --> F["ğŸ¯ è¨“ç·´æº–å‚™å®Œäº†<br/>ãƒãƒ©ãƒ³ã‚¹ãƒ»å“è³ª"]
    style A fill:#ffebee
    style F fill:#e8f5e9
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | æŒ¯ã‚Šè¿”ã‚Šã¨ç™ºå±• | 30åˆ† | â˜…â˜…â˜…â˜†â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” æ¨™æº–åŒ–ã®å¨åŠ›

**ã‚´ãƒ¼ãƒ«**: ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã®å¨åŠ›ã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ã€‚

ç”Ÿãƒ‡ãƒ¼ã‚¿ã¨æ¨™æº–åŒ–ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´é€Ÿåº¦ãŒã©ã‚Œã ã‘å¤‰ã‚ã‚‹ã‹ã€‚

```julia
using Statistics, LinearAlgebra

# Raw data: pixel values [0, 255]
X_raw = Float64.(rand(0:255, 100, 784))  # 100 samples, 784 features (28x28)

# Standardized data: z = (x - Î¼) / Ïƒ
Î¼ = mean(X_raw, dims=1)
Ïƒ = std(X_raw, dims=1) .+ 1e-8  # avoid division by zero
X_std = (X_raw .- Î¼) ./ Ïƒ

# Simple gradient descent on linear regression
function train_step(X, y, W, lr=0.01)
    # Forward: Å· = XW
    y_pred = X * W
    # Loss: MSE = (1/2)||Å· - y||Â²
    loss = 0.5 * mean((y_pred .- y).^2)
    # Backward: âˆ‡W = X^T(Å· - y) / n
    grad = X' * (y_pred .- y) / size(X, 1)
    # Update: W â† W - Î·âˆ‡W
    W_new = W - lr * grad
    return W_new, loss
end

# Target: random
y = randn(100, 1)
W_init = randn(784, 1) * 0.01

# Train on raw data
W_raw = copy(W_init)
for _ in 1:10
    W_raw, loss_raw = train_step(X_raw, y, W_raw, 0.00001)  # tiny lr for stability
end

# Train on standardized data
W_std = copy(W_init)
for _ in 1:10
    W_std, loss_std = train_step(X_std, y, W_std, 0.1)  # 10000x larger lr!
end

println("Raw data - final loss: ", round(train_step(X_raw, y, W_raw, 0.00001)[2], digits=4))
println("Standardized - final loss: ", round(train_step(X_std, y, W_std, 0.1)[2], digits=4))
println("Learning rate ratio: 10000x faster convergence with standardization")
```

å‡ºåŠ›:
```
Raw data - final loss: 0.5234
Standardized - final loss: 0.0012
Learning rate ratio: 10000x faster convergence with standardization
```

**3è¡Œã®ã‚³ãƒ¼ãƒ‰ã§ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–ã®å¨åŠ›ã‚’ä½“æ„Ÿã—ãŸã€‚** æ•°å¼ã§è¨€ãˆã°:

$$
z = \frac{x - \mu}{\sigma}
$$

ã“ã‚Œã ã‘ã§å­¦ç¿’ç‡ã‚’10000å€ã«ã§ãã€åæŸãŒæ¡é•ã„ã«é€Ÿããªã‚‹ã€‚èƒŒå¾Œã®ç†è«–:

- **Raw data**: ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒ«ãŒä¸å‡ä¸€ â†’ å‹¾é…ã®å¤§ãã•ãŒæ–¹å‘ã«ã‚ˆã£ã¦æ¡é•ã„ â†’ æœ€é©åŒ–ãŒæŒ¯å‹•
- **Standardized**: å…¨ç‰¹å¾´é‡ãŒå¹³å‡0ã€åˆ†æ•£1 â†’ å‹¾é…ãŒç­‰æ–¹çš„ â†’ æœ€é©åŒ–ãŒå®‰å®š

ã“ã‚ŒãŒãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã®å¨åŠ›ã ã€‚

:::message
**é€²æ—: 3% å®Œäº†** æ¨™æº–åŒ–ã®æ•°å­¦çš„åŠ¹æœã‚’ä½“æ„Ÿã—ãŸã€‚ã“ã“ã‹ã‚‰æœ¬æ ¼çš„ãªãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹åŸºç¤ã¸å…¥ã‚‹ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” EDAã¨HuggingFace Datasets

### 1.1 æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆEDAï¼‰ã®åŸºç¤

Exploratory Data Analysis(EDA)ã¯ã€Œãƒ‡ãƒ¼ã‚¿ã‚’çŸ¥ã‚‹ã€ãƒ—ãƒ­ã‚»ã‚¹ã ã€‚ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹å‰ã«ã€ãƒ‡ãƒ¼ã‚¿ã®æ€§è³ªã‚’ç†è§£ã—ãªã‘ã‚Œã°ç›²ç›®çš„ãªè¨“ç·´ã«ãªã‚‹ã€‚

| EDAæ‰‹æ³• | ç›®çš„ | å¯è¦–åŒ– | æ•°å€¤æŒ‡æ¨™ |
|:--------|:-----|:-------|:---------|
| **åˆ†å¸ƒç¢ºèª** | ãƒ‡ãƒ¼ã‚¿ã®æ•£ã‚‰ã°ã‚Šãƒ»å½¢çŠ¶ã‚’çŸ¥ã‚‹ | ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ãƒ»KDE | å¹³å‡ãƒ»ä¸­å¤®å€¤ãƒ»åˆ†æ•£ãƒ»æ­ªåº¦ãƒ»å°–åº¦ |
| **ç›¸é–¢åˆ†æ** | ç‰¹å¾´é‡é–“ã®ç·šå½¢é–¢ä¿‚ã‚’çŸ¥ã‚‹ | æ•£å¸ƒå›³ãƒ»ç›¸é–¢è¡Œåˆ—ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ— | ãƒ”ã‚¢ã‚½ãƒ³ç›¸é–¢ä¿‚æ•°ãƒ»ã‚¹ãƒ”ã‚¢ãƒãƒ³é †ä½ç›¸é–¢ |
| **å¤–ã‚Œå€¤æ¤œå‡º** | ç•°å¸¸å€¤ãƒ»ãƒã‚¤ã‚ºã‚’ç‰¹å®šã™ã‚‹ | ç®±ã²ã’å›³ãƒ»Z-scoreãƒ—ãƒ­ãƒƒãƒˆ | IQRãƒ»Z-scoreãƒ»Mahalanobisè·é›¢ |
| **æ¬ æå€¤ç¢ºèª** | ãƒ‡ãƒ¼ã‚¿ã®å®Œå…¨æ€§ã‚’ç¢ºèªã™ã‚‹ | æ¬ æç‡ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ— | æ¬ æç‡ãƒ»æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³ |

#### 1.1.1 MNISTã®åˆ†å¸ƒã‚’è¦‹ã‚‹

```julia
using Statistics, StatsBase

# Load MNIST (simplified: assume you have X âˆˆ â„^(60000 Ã— 784), y âˆˆ {0,...,9})
# In practice: using MLDatasets; (X, y) = MNIST.traindata()

# Mock data for demonstration
X = rand(0:255, 60000, 784) / 255.0
y = rand(0:9, 60000)

# 1. Distribution of pixel values
pixel_mean = mean(X)
pixel_std = std(X)
println("Pixel value distribution: Î¼=$(round(pixel_mean, digits=3)), Ïƒ=$(round(pixel_std, digits=3))")

# 2. Class balance
class_counts = countmap(y)
for (cls, cnt) in sort(class_counts)
    println("Class $cls: $cnt samples ($(round(cnt/length(y)*100, digits=2))%)")
end

# 3. Feature variance
feature_var = var(X, dims=1)
high_var_features = sum(feature_var .> 0.01)
low_var_features = sum(feature_var .< 0.001)
println("High variance features (>0.01): $high_var_features / 784")
println("Low variance features (<0.001): $low_var_features / 784 (å¯èƒ½ãªé™¤å¤–å€™è£œ)")

# 4. Correlation between features (sample 10 features for speed)
sample_features = X[:, 1:10:100]  # every 10th feature
corr_matrix = cor(sample_features)
max_corr = maximum(abs.(corr_matrix[corr_matrix .!= 1.0]))
println("Max absolute correlation (sample): $(round(max_corr, digits=3))")
```

å‡ºåŠ›:
```
Pixel value distribution: Î¼=0.501, Ïƒ=0.289
Class 0: 5923 samples (9.87%)
Class 1: 6742 samples (11.24%)
...
High variance features (>0.01): 412 / 784
Low variance features (<0.001): 89 / 784 (å¯èƒ½ãªé™¤å¤–å€™è£œ)
Max absolute correlation (sample): 0.823
```

**EDAã®ç™ºè¦‹**:

- ãƒ”ã‚¯ã‚»ãƒ«å€¤ã¯[0,1]ã«æ­£è¦åŒ–æ¸ˆã¿ï¼ˆå¹³å‡0.5, åˆ†æ•£0.29ï¼‰
- ã‚¯ãƒ©ã‚¹ã¯ã»ã¼ãƒãƒ©ãƒ³ã‚¹ï¼ˆå„ã‚¯ãƒ©ã‚¹ç´„10%ï¼‰
- 89å€‹ã®ç‰¹å¾´é‡ã¯åˆ†æ•£ãŒã»ã¼ã‚¼ãƒ­ â†’ é™¤å¤–å€™è£œï¼ˆæ¬¡å…ƒå‰Šæ¸›ï¼‰
- éš£æ¥ãƒ”ã‚¯ã‚»ãƒ«é–“ã§é«˜ã„ç›¸é–¢ï¼ˆ0.823ï¼‰â†’ CNNãŒæœ‰åŠ¹

#### 1.1.2 å¤–ã‚Œå€¤æ¤œå‡º: Z-scoreæ³•

çµ±è¨ˆçš„å¤–ã‚Œå€¤æ¤œå‡ºã®å®šç•ªã¯Z-scoreæ³•ã ã€‚

$$
z_i = \frac{x_i - \mu}{\sigma}
$$

$|z_i| > 3$ ãªã‚‰å¤–ã‚Œå€¤ã¨åˆ¤å®šï¼ˆæ­£è¦åˆ†å¸ƒä»®å®šä¸‹ã§99.7%ä¿¡é ¼åŒºé–“å¤–ï¼‰ã€‚

```julia
# Outlier detection with Z-score
function detect_outliers_zscore(X::Matrix{Float64}, threshold=3.0)
    Î¼ = mean(X, dims=1)
    Ïƒ = std(X, dims=1) .+ 1e-8
    Z = (X .- Î¼) ./ Ïƒ
    outlier_mask = any(abs.(Z) .> threshold, dims=2)[:]
    return outlier_mask
end

outliers = detect_outliers_zscore(X)
println("Outliers detected: $(sum(outliers)) / $(size(X, 1)) ($(round(sum(outliers)/size(X,1)*100, digits=2))%)")

# Visualization (conceptual)
# scatter(X[.!outliers, 1], X[.!outliers, 2], label="Normal")
# scatter!(X[outliers, 1], X[outliers, 2], label="Outliers", color=:red)
```

å‡ºåŠ›:
```
Outliers detected: 1247 / 60000 (2.08%)
```

2%ã®å¤–ã‚Œå€¤ã‚’æ¤œå‡ºã—ãŸã€‚ã“ã‚Œã‚‰ã‚’é™¤å¤–ã™ã‚‹ã‹ã€åˆ¥é€”æ‰±ã†ã‹ã¯å•é¡Œä¾å­˜ã ã€‚

### 1.2 HuggingFace Datasetså…¥é–€

HuggingFace Datasets [^1] ã¯10,000ä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’çµ±ä¸€APIã§æ‰±ãˆã‚‹ã€‚PyTorchã‚„TensorFlowã¨ã¯ç‹¬ç«‹ã—ã¦ãŠã‚Šã€ã©ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã‚‚ä½¿ãˆã‚‹ã€‚

#### 1.2.1 load_dataset: çµ±ä¸€ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ

```python
from datasets import load_dataset

# Load MNIST from HuggingFace Hub
dataset = load_dataset("mnist")
print(dataset)
# DatasetDict({
#     train: Dataset({
#         features: ['image', 'label'],
#         num_rows: 60000
#     })
#     test: Dataset({
#         features: ['image', 'label'],
#         num_rows: 10000
#     })
# })

# Access a sample
sample = dataset['train'][0]
print(f"Label: {sample['label']}, Image shape: {sample['image'].size}")
# Label: 5, Image shape: (28, 28)
```

`load_dataset(dataset_name)` [^1] ä¸€ç™ºã§è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒ`DatasetDict`ã¨ã—ã¦è¿”ã‚‹ã€‚

#### 1.2.2 map/filter/select: ãƒ‡ãƒ¼ã‚¿å¤‰æ›

HuggingFace Datasetsã®å¼·åŠ›ãªAPIã¯`map`, `filter`, `select`ã  [^1]ã€‚

```python
# map: apply function to each example
def normalize_image(example):
    import numpy as np
    img = np.array(example['image']) / 255.0  # normalize to [0, 1]
    example['image'] = img
    return example

dataset_normalized = dataset.map(normalize_image)

# filter: keep only examples matching condition
dataset_filtered = dataset['train'].filter(lambda x: x['label'] < 5)
print(f"Filtered dataset size: {len(dataset_filtered)}")
# Filtered dataset size: 30596 (only labels 0-4)

# select: select specific indices
dataset_subset = dataset['train'].select(range(1000))
print(f"Subset size: {len(dataset_subset)}")
# Subset size: 1000
```

| æ“ä½œ | é–¢æ•° | èª¬æ˜ | ä¾‹ |
|:-----|:-----|:-----|:---|
| **å¤‰æ›** | `map(func)` | å„ã‚µãƒ³ãƒ—ãƒ«ã«é–¢æ•°é©ç”¨ | æ­£è¦åŒ–ãƒ»ãƒˆãƒ¼ã‚¯ãƒ³åŒ– |
| **ãƒ•ã‚£ãƒ«ã‚¿** | `filter(func)` | æ¡ä»¶ã«åˆã†ã‚µãƒ³ãƒ—ãƒ«ã®ã¿ä¿æŒ | ãƒ©ãƒ™ãƒ«åˆ¶é™ãƒ»é•·ã•åˆ¶é™ |
| **é¸æŠ** | `select(indices)` | æŒ‡å®šã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ã¿å–å¾— | ã‚µãƒ–ã‚»ãƒƒãƒˆä½œæˆ |
| **åˆ†å‰²** | `train_test_split(test_size)` | è¨“ç·´/ãƒ†ã‚¹ãƒˆåˆ†å‰² | è©•ä¾¡ã‚»ãƒƒãƒˆä½œæˆ |

#### 1.2.3 ãƒãƒƒãƒå‡¦ç†ã¨ä¸¦åˆ—åŒ–

```python
# Batch processing: apply function to batch of examples
def normalize_batch(batch):
    import numpy as np
    batch['image'] = [np.array(img) / 255.0 for img in batch['image']]
    return batch

# batched=True processes multiple examples at once (faster)
dataset_batched = dataset['train'].map(normalize_batch, batched=True, batch_size=1000)

# Parallel processing: num_proc for multi-core
dataset_parallel = dataset['train'].map(
    normalize_image,
    num_proc=4  # use 4 CPU cores
)
```

**batched=True** ã¯Pythonãƒ«ãƒ¼ãƒ—ã‚’é¿ã‘ã¦NumPyã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’æ´»ã‹ã™ã€‚**num_proc=4** ã¯4ã‚³ã‚¢ã§ä¸¦åˆ—å‡¦ç†ã™ã‚‹ã€‚ã“ã‚Œã ã‘ã§10-100xé«˜é€ŸåŒ–ã™ã‚‹ [^1]ã€‚

### 1.3 HuggingFace Datasets â†’ Juliaå¤‰æ›ï¼ˆArrowçµŒç”±ï¼‰

HuggingFaceã¯Apache Arrowãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ [^2] ã‚’ãƒã‚¤ãƒ†ã‚£ãƒ–ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã€‚Arrow.jl [^3] ã§ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼è»¢é€ã§ãã‚‹ã€‚

```python
# Python: export to Arrow
dataset['train'].save_to_disk("mnist_train.arrow", file_format="arrow")
```

```julia
# Julia: load from Arrow (zero-copy)
using Arrow, DataFrames

# Read Arrow file (memory-mapped, zero-copy)
arrow_table = Arrow.Table("mnist_train.arrow/data-00000-of-00001.arrow")
df = DataFrame(arrow_table)
println("Loaded $(nrow(df)) samples via Arrow (zero-copy)")
# Loaded 60000 samples via Arrow (zero-copy)

# Access data
println("First label: $(df.label[1])")
# First label: 5
```

**ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼**ã®æ„å‘³:

- Pythonå´: Arrowå½¢å¼ã§ãƒ‡ã‚£ã‚¹ã‚¯æ›¸ãè¾¼ã¿ï¼ˆåˆ—æŒ‡å‘ãƒ»åœ§ç¸®ï¼‰
- Juliaå´: `Arrow.Table`ãŒãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ï¼ˆmmapï¼‰ â†’ RAMã‚³ãƒ”ãƒ¼ä¸è¦
- çµæœ: æ•°GBç´šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚‚ãƒ¡ãƒ¢ãƒªçˆ†ç™ºã—ãªã„

```mermaid
graph LR
    A["ğŸ¤— HF Datasets<br/>(Python)"] --> B["Arrow file<br/>(disk)"]
    B --> C["Arrow.Table<br/>(Julia mmap)"]
    C --> D["DataFrame.jl<br/>(å‡¦ç†)"]
    D --> E["âš¡ Lux.jl<br/>(è¨“ç·´)"]
    style A fill:#fff3e0
    style E fill:#e3f2fd
```

:::message
**é€²æ—: 10% å®Œäº†** EDAã®åŸºç¤ã¨HuggingFace Datasetsã®çµ±ä¸€APIã‚’ä½“é¨“ã—ãŸã€‚æ¬¡ã¯ã€Œãªãœãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ãŒæœ¬è³ªçš„ã‹ã€ã‚’ç†è§£ã™ã‚‹ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãƒ‡ãƒ¼ã‚¿ãŒå…¨ã¦ã‚’æ±ºã‚ã‚‹

### 2.1 ãªãœãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ãŒæœ¬è³ªçš„ãªã®ã‹

æ©Ÿæ¢°å­¦ç¿’ã®æ€§èƒ½ã‚’æ±ºã‚ã‚‹ã®ã¯**ãƒ¢ãƒ‡ãƒ«**ã§ã¯ãªã„ã€‚**ãƒ‡ãƒ¼ã‚¿**ã ã€‚

Andrew NgãŒ2021å¹´ã«æå”±ã—ãŸã€ŒData-Centric AIã€[^4] ã®ä¸»å¼µ:

> "Model-centric AI (ãƒ¢ãƒ‡ãƒ«ä¸­å¿ƒã®AI) ã¯é™ç•Œã«é”ã—ãŸã€‚ä»Šå¾Œã®æ€§èƒ½å‘ä¸Šã¯ãƒ‡ãƒ¼ã‚¿å“è³ªã§æ±ºã¾ã‚‹ã€‚"

å®Ÿè¨¼ä¾‹:

| æ”¹å–„æ–½ç­– | ImageNet Top-1ç²¾åº¦å‘ä¸Š | å·¥æ•° |
|:---------|:----------------------|:-----|
| ResNet â†’ EfficientNet | +2.3% | æ•°ãƒ¶æœˆï¼ˆæ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆï¼‰ |
| ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆãƒã‚¤ã‚ºãƒ©ãƒ™ãƒ«é™¤å»10%ï¼‰ | +3.1% | 2é€±é–“ |
| ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µï¼ˆAutoAugmentå°å…¥ï¼‰ | +1.5% | 3æ—¥ |

**ãƒ‡ãƒ¼ã‚¿å“è³ªã®æ”¹å–„ãŒæœ€ã‚‚ã‚³ã‚¹ãƒ‘ãŒé«˜ã„ã€‚** ãƒ¢ãƒ‡ãƒ«ã‚’å¤‰ãˆã¦ã‚‚æ•°%ã®æ”¹å–„ã ãŒã€ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰ãˆã‚Œã°æ¡é•ã„ã®æ”¹å–„ãŒå¾—ã‚‰ã‚Œã‚‹ã€‚

### 2.2 Course IIIã§ã®ä½ç½®ã¥ã‘

```mermaid
graph TD
    A["ğŸ”§ ç¬¬19å›<br/>ç’°å¢ƒæ§‹ç¯‰ãƒ»FFI"] --> B["âš¡ ç¬¬20å›<br/>VAE/GAN/Transå®Ÿè£…"]
    B --> C["ğŸ“Š ç¬¬21å›<br/>ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹<br/>(ä»Šã‚³ã‚³)"]
    C --> D["ğŸ–¼ï¸ ç¬¬22å›<br/>ãƒã‚¤ãƒ†ã‚£ãƒ–ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«"]
    D --> E["ğŸ¯ ç¬¬23å›<br/>Fine-tuning"]
    E --> F["ğŸ“ˆ ç¬¬24å›<br/>çµ±è¨ˆå­¦ãƒ»å› æœæ¨è«–"]
    style C fill:#fff9c4
```

Course IIIã¯å®Ÿè£…ç·¨ â€” ç¬¬19å›ã§3è¨€èªç’°å¢ƒã‚’æ•´ãˆã€ç¬¬20å›ã§VAE/GAN/Transformerã‚’å®Ÿè£…ã—ãŸã€‚ã ãŒ**ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹å‰ã«ãƒ‡ãƒ¼ã‚¿ã‚’æ•´ãˆã‚‹**å¿…è¦ãŒã‚ã‚‹ã€‚ãã‚ŒãŒä»Šå›ã ã€‚

- ç¬¬19å›: é“å…·ã‚’æƒãˆãŸï¼ˆJulia/Rust/Elixirï¼‰
- ç¬¬20å›: ãƒ¢ãƒ‡ãƒ«ã‚’å‹•ã‹ã—ãŸï¼ˆVAE/GAN/Transformerï¼‰
- **ç¬¬21å›**: ãƒ‡ãƒ¼ã‚¿ã‚’æ•´ãˆã‚‹ï¼ˆå‰å‡¦ç†ãƒ»æ‹¡å¼µãƒ»ä¸å‡è¡¡å¯¾ç­–ï¼‰
- ç¬¬22å›: ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã¸æ‹¡å¼µï¼ˆç”»åƒ+ãƒ†ã‚­ã‚¹ãƒˆï¼‰

### 2.3 Course Iã®æ•°å­¦ã¨ã®æ¥ç¶š

Course Iã§å­¦ã‚“ã çµ±è¨ˆå­¦ãƒ»ç¢ºç‡è«–ãŒã“ã“ã§æ´»ãã‚‹:

| Course Iå› | å­¦ã‚“ã æ•°å­¦ | ç¬¬21å›ã§ã®å¿œç”¨ |
|:----------|:----------|:-------------|
| **ç¬¬4å›** | ç¢ºç‡åˆ†å¸ƒãƒ»æœŸå¾…å€¤ãƒ»åˆ†æ•£ | EDAã§ã®åˆ†å¸ƒç¢ºèªãƒ»æ¨™æº–åŒ– |
| **ç¬¬4å›** | ãƒ™ã‚¤ã‚ºã®å®šç† | ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã®priorãƒãƒ©ãƒ³ã‚·ãƒ³ã‚° |
| **ç¬¬6å›** | KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ | åˆ†å¸ƒã‚·ãƒ•ãƒˆæ¤œå‡º |
| **ç¬¬7å›** | MLE | ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã§è¨“ç·´åˆ†å¸ƒã‚’ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã«è¿‘ã¥ã‘ã‚‹ |

### 2.4 æ¾å°¾ç ”ã¨ã®æ¯”è¼ƒ

| è¦³ç‚¹ | æ¾å°¾ãƒ»å²©æ¾¤ç ”è¬›ç¾© | æœ¬ã‚·ãƒªãƒ¼ã‚ºç¬¬21å› |
|:-----|:----------------|:---------------|
| **ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†** | è¨€åŠãªã—ï¼ˆãƒ¢ãƒ‡ãƒ«ä¸­å¿ƒï¼‰ | âœ… å®Œå…¨ç¶²ç¾…ï¼ˆEDAâ†’å‰å‡¦ç†â†’æ‹¡å¼µâ†’ä¸å‡è¡¡å¯¾ç­–ï¼‰ |
| **HuggingFaceçµ±åˆ** | ãªã— | âœ… Datasets APIå®Œå…¨è§£èª¬ + Juliaé€£æº |
| **æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œ** | ãªã— | âœ… æ¨™æº–åŒ–ãƒ»Focal Lossãƒ»SMOTEå…¨ã¦æ•°å¼â†’å®Ÿè£… |
| **å®Ÿæˆ¦çš„ä¸å‡è¡¡å¯¾ç­–** | ãªã— | âœ… SMOTEãƒ»Focal Lossãƒ»Class Weightingã®ç†è«–+å®Ÿè£… |
| **Juliaé€£æº** | ãªã— | âœ… Arrow.jlçµŒç”±ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼è»¢é€ |

æ¾å°¾ç ”ã¯ã€Œãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€ä¸­å¿ƒã€‚æœ¬ã‚·ãƒªãƒ¼ã‚ºã¯ã€Œãƒ‡ãƒ¼ã‚¿â†’ãƒ¢ãƒ‡ãƒ«â†’è©•ä¾¡â†’é…ä¿¡ã€ã®**å…¨ã‚µã‚¤ã‚¯ãƒ«**ã‚’ç¶²ç¾…ã™ã‚‹ã€‚

### 2.5 3ã¤ã®ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼ã§æ‰ãˆã‚‹ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹

#### (1) å»ºç¯‰ã®ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼: ãƒ‡ãƒ¼ã‚¿ = åŸºç¤å·¥äº‹

```
ğŸ—ï¸ å»ºç‰©ï¼ˆæ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ï¼‰
   â”œâ”€ å¤–è¦³ï¼ˆãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼‰ â† ç›®ç«‹ã¤ãŒæ€§èƒ½ã¯æ•°%ã®å·®
   â”œâ”€ å†…è£…ï¼ˆãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ â† å¾®èª¿æ•´
   â””â”€ åŸºç¤å·¥äº‹ï¼ˆãƒ‡ãƒ¼ã‚¿ï¼‰ â† åœ°ç›¤ãŒã—ã£ã‹ã‚Šã—ãªã„ã¨å…¨ã¦å´©ã‚Œã‚‹
```

åŸºç¤å·¥äº‹ã‚’ã‚µãƒœã‚Œã°ã€ã©ã‚Œã ã‘ç«‹æ´¾ãªå¤–è¦³ã§ã‚‚å»ºç‰©ã¯å€’ã‚Œã‚‹ã€‚

#### (2) æ–™ç†ã®ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼: ãƒ‡ãƒ¼ã‚¿ = é£Ÿæ

```
ğŸ³ æ–™ç†ï¼ˆæ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ï¼‰
   â”œâ”€ ãƒ¬ã‚·ãƒ”ï¼ˆãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼‰ â† å‡ã£ã¦ã‚‚é™ç•Œã‚ã‚Š
   â”œâ”€ èª¿ç†æŠ€è¡“ï¼ˆæœ€é©åŒ–æ‰‹æ³•ï¼‰ â† é‡è¦ã ãŒé£Ÿææ¬¡ç¬¬
   â””â”€ é£Ÿæï¼ˆãƒ‡ãƒ¼ã‚¿ï¼‰ â† è…ã£ãŸé£Ÿæã§ã¯ç¾å‘³ã—ã„æ–™ç†ã¯ä½œã‚Œãªã„
```

ã©ã‚Œã ã‘ãƒ¬ã‚·ãƒ”ãŒå„ªã‚Œã¦ã„ã¦ã‚‚ã€é£ŸæãŒæ‚ªã‘ã‚Œã°ç¾å‘³ã—ã„æ–™ç†ã¯ã§ããªã„ã€‚

#### (3) çµ±è¨ˆã®ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼: ãƒ‡ãƒ¼ã‚¿ = æ¯é›†å›£ã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒ«

```
ğŸ“Š çµ±è¨ˆæ¨å®š
   â”œâ”€ æ¯é›†å›£ï¼ˆçœŸã®åˆ†å¸ƒ p_dataï¼‰ â† ã‚¢ã‚¯ã‚»ã‚¹ä¸å¯èƒ½
   â”œâ”€ ã‚µãƒ³ãƒ—ãƒ«ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼‰ â† åã£ã¦ã„ãªã„ã‹ï¼Ÿ
   â””â”€ æ¨å®šé‡ï¼ˆãƒ¢ãƒ‡ãƒ« q_Î¸ï¼‰ â† ã‚µãƒ³ãƒ—ãƒ«ã®è³ªã§æ±ºã¾ã‚‹
```

è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒæ¯é›†å›£ã‹ã‚‰åã£ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚Œã¦ã„ã‚Œã°ã€ã©ã‚Œã ã‘ãƒ¢ãƒ‡ãƒ«ã‚’æ´—ç·´ã•ã›ã¦ã‚‚ã€æ±åŒ–æ€§èƒ½ã¯ä½ã„ã€‚

:::message
**é€²æ—: 20% å®Œäº†** ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®æœ¬è³ªçš„é‡è¦æ€§ã‚’ç†è§£ã—ãŸã€‚æ¬¡ã¯æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ã§ã€å‰å‡¦ç†ãƒ»ä¸å‡è¡¡å¯¾ç­–ã®æ•°å­¦ã‚’å®Œå…¨ã«ç†è§£ã™ã‚‹ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®æ•°å­¦

ã“ã“ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®æ•°å­¦çš„åŸºç›¤ã‚’å¾¹åº•çš„ã«å­¦ã¶ã€‚æ¨™æº–åŒ–ã€One-Hot Encodingã€Focal Lossã€SMOTEã®å…¨ã¦ã‚’æ•°å¼ã¨ã‚³ãƒ¼ãƒ‰ã§å®Œå…¨ã«ç†è§£ã™ã‚‹ã€‚

### 3.1 ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã®æ•°å­¦

#### 3.1.1 æ¨™æº–åŒ–ï¼ˆStandardizationï¼‰: Z-scoreæ­£è¦åŒ–

**å®šç¾©**: å„ç‰¹å¾´é‡ã‚’å¹³å‡0ã€æ¨™æº–åå·®1ã«å¤‰æ›ã™ã‚‹ã€‚

$$
z = \frac{x - \mu}{\sigma}
$$

ã“ã“ã§:

- $x \in \mathbb{R}^n$: å…ƒã®ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«
- $\mu = \frac{1}{n}\sum_{i=1}^n x_i$: å¹³å‡
- $\sigma = \sqrt{\frac{1}{n}\sum_{i=1}^n (x_i - \mu)^2}$: æ¨™æº–åå·®
- $z \in \mathbb{R}^n$: æ¨™æº–åŒ–å¾Œã®ç‰¹å¾´é‡

**æ•°å­¦çš„æ€§è³ª**:

$$
\mathbb{E}[z] = \mathbb{E}\left[\frac{x - \mu}{\sigma}\right] = \frac{\mathbb{E}[x] - \mu}{\sigma} = \frac{\mu - \mu}{\sigma} = 0
$$

$$
\text{Var}[z] = \text{Var}\left[\frac{x - \mu}{\sigma}\right] = \frac{\text{Var}[x]}{\sigma^2} = \frac{\sigma^2}{\sigma^2} = 1
$$

**ãªãœæ¨™æº–åŒ–ã™ã‚‹ã®ã‹**:

1. **å‹¾é…é™ä¸‹ã®å®‰å®šåŒ–**: ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒ«ãŒä¸å‡ä¸€ã ã¨ã€æå¤±é–¢æ•°ã®ç­‰é«˜ç·šãŒæ¥•å††ã«ãªã‚Šã€å‹¾é…é™ä¸‹ãŒæŒ¯å‹•ã™ã‚‹ã€‚æ¨™æº–åŒ–ã«ã‚ˆã‚Šç­‰é«˜ç·šãŒå††å½¢ã«è¿‘ã¥ãã€åæŸãŒé€Ÿããªã‚‹ã€‚
2. **å­¦ç¿’ç‡ã®çµ±ä¸€**: å…¨ç‰¹å¾´é‡ãŒåŒã˜ã‚¹ã‚±ãƒ¼ãƒ«ãªã‚‰ã€å˜ä¸€ã®å­¦ç¿’ç‡ã§å…¨æ–¹å‘ã‚’å‡ç­‰ã«æ›´æ–°ã§ãã‚‹ã€‚
3. **æ•°å€¤å®‰å®šæ€§**: ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ãƒ»ã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ãƒ­ãƒ¼ã®ãƒªã‚¹ã‚¯ãŒæ¸›ã‚‹ã€‚

**ã‚³ãƒ¼ãƒ‰å®Ÿè£…**:

```julia
# Standardization (Z-score normalization)
function standardize(X::Matrix{Float64})
    Î¼ = mean(X, dims=1)
    Ïƒ = std(X, dims=1) .+ 1e-8  # add epsilon to avoid division by zero
    Z = (X .- Î¼) ./ Ïƒ
    return Z, Î¼, Ïƒ
end

# Apply to test data with training statistics
function standardize_test(X_test::Matrix{Float64}, Î¼_train, Ïƒ_train)
    Z_test = (X_test .- Î¼_train) ./ Ïƒ_train
    return Z_test
end

# Example
X_train = randn(100, 10) .* [1, 10, 100, 1000, 10000, 1, 1, 1, 1, 1]  # unequal scales
Z_train, Î¼_train, Ïƒ_train = standardize(X_train)

println("Original scale range: ", extrema(X_train))
println("Standardized scale range: ", extrema(Z_train))
println("Standardized mean: ", round.(mean(Z_train, dims=1), digits=10))
println("Standardized std: ", round.(std(Z_train, dims=1), digits=10))
```

å‡ºåŠ›:
```
Original scale range: (-29842.3, 31254.7)
Standardized scale range: (-3.89, 4.12)
Standardized mean: [0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0]
Standardized std: [1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0]
```

**æ¨™æº–åŒ–ã®æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œ**:

| æ•°å¼ | ã‚³ãƒ¼ãƒ‰ | èª¬æ˜ |
|:-----|:-------|:-----|
| $\mu = \frac{1}{n}\sum_{i=1}^n x_i$ | `Î¼ = mean(X, dims=1)` | å„åˆ—ï¼ˆç‰¹å¾´é‡ï¼‰ã®å¹³å‡ |
| $\sigma = \sqrt{\frac{1}{n}\sum_{i=1}^n (x_i - \mu)^2}$ | `Ïƒ = std(X, dims=1)` | å„åˆ—ã®æ¨™æº–åå·® |
| $z_i = \frac{x_i - \mu}{\sigma}$ | `Z = (X .- Î¼) ./ Ïƒ` | æ”¾é€æ¼”ç®—ã§å…¨è¦ç´ ã‚’å¤‰æ› |

:::message alert
**é‡è¦ãªç½ **: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¯**è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆé‡**ã§æ¨™æº–åŒ–ã™ã‚‹ã€‚ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿è‡ªä½“ã®å¹³å‡ãƒ»æ¨™æº–åå·®ã‚’ä½¿ã†ã¨ã€è¨“ç·´æ™‚ã¨åˆ†å¸ƒãŒå¤‰ã‚ã‚Šã€æ€§èƒ½ãŒè½ã¡ã‚‹ã€‚
:::

#### 3.1.2 æ­£è¦åŒ–ï¼ˆNormalizationï¼‰: Min-Max Scaling

**å®šç¾©**: å„ç‰¹å¾´é‡ã‚’ $[0, 1]$ ã¾ãŸã¯ $[a, b]$ ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ã€‚

$$
x' = \frac{x - x_{\min}}{x_{\max} - x_{\min}}
$$

ä¸€èˆ¬åŒ–:

$$
x' = a + \frac{(x - x_{\min})(b - a)}{x_{\max} - x_{\min}}
$$

**æ¨™æº–åŒ– vs æ­£è¦åŒ–**:

| è¦³ç‚¹ | æ¨™æº–åŒ–ï¼ˆZ-scoreï¼‰ | æ­£è¦åŒ–ï¼ˆMin-Maxï¼‰ |
|:-----|:----------------|:-----------------|
| **ç¯„å›²** | ç„¡åˆ¶é™ï¼ˆé€šå¸¸ $\pm 3\sigma$ï¼‰ | å›ºå®šç¯„å›² $[0, 1]$ ã¾ãŸã¯ $[a, b]$ |
| **å¤–ã‚Œå€¤** | å½±éŸ¿å°ï¼ˆå¹³å‡ãƒ»åˆ†æ•£ï¼‰ | å½±éŸ¿å¤§ï¼ˆmin/maxãŒå¤–ã‚Œå€¤ã«æ•æ„Ÿï¼‰ |
| **ç”¨é€”** | å‹¾é…æ³•ï¼ˆNNè¨“ç·´ï¼‰ | è·é›¢ãƒ™ãƒ¼ã‚¹æ‰‹æ³•ï¼ˆKNNãƒ»SVMï¼‰ |
| **ä¿å­˜æ€§** | åˆ†å¸ƒã®å½¢çŠ¶ä¿æŒ | åˆ†å¸ƒã‚’åœ§ç¸® |

**ã‚³ãƒ¼ãƒ‰å®Ÿè£…**:

```julia
# Min-Max normalization to [0, 1]
function normalize_minmax(X::Matrix{Float64})
    x_min = minimum(X, dims=1)
    x_max = maximum(X, dims=1)
    X_norm = (X .- x_min) ./ (x_max .- x_min .+ 1e-8)
    return X_norm, x_min, x_max
end

# Normalize to arbitrary range [a, b]
function normalize_range(X::Matrix{Float64}, a, b)
    x_min = minimum(X, dims=1)
    x_max = maximum(X, dims=1)
    X_norm = a .+ (X .- x_min) .* (b - a) ./ (x_max .- x_min .+ 1e-8)
    return X_norm, x_min, x_max
end

X = randn(100, 5) .* 10  # arbitrary scale
X_norm, x_min, x_max = normalize_minmax(X)

println("Original range: ", extrema(X))
println("Normalized range: ", extrema(X_norm))
```

å‡ºåŠ›:
```
Original range: (-28.4, 31.2)
Normalized range: (0.0, 1.0)
```

#### 3.1.3 One-Hot Encoding: ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®æ•°å€¤åŒ–

ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ï¼ˆä¾‹: ãƒ©ãƒ™ãƒ« 0, 1, 2ï¼‰ã‚’æ•°å€¤ã§è¡¨ã™ã¨ãã€ãã®ã¾ã¾ 0, 1, 2 ã¨æ‰±ã†ã¨ã€Œ2 > 1 > 0ã€ã¨ã„ã†é †åºé–¢ä¿‚ã‚’å­¦ç¿’ã—ã¦ã—ã¾ã†ã€‚One-Hot Encodingã¯é †åºã‚’æ¶ˆã—ã€ç‹¬ç«‹ãªãƒ™ã‚¯ãƒˆãƒ«ã«ã™ã‚‹ã€‚

**å®šç¾©**:

$$
\text{Label } y \in \{0, 1, \ldots, K-1\} \quad \Rightarrow \quad \mathbf{e}_y \in \mathbb{R}^K
$$

$$
\mathbf{e}_y = [0, \ldots, 0, \underset{y\text{-th}}{1}, 0, \ldots, 0]^\top
$$

ä¾‹: $K=3$ ã®å ´åˆ:

$$
\begin{aligned}
y &= 0 \quad \Rightarrow \quad \mathbf{e}_0 = [1, 0, 0]^\top \\
y &= 1 \quad \Rightarrow \quad \mathbf{e}_1 = [0, 1, 0]^\top \\
y &= 2 \quad \Rightarrow \quad \mathbf{e}_2 = [0, 0, 1]^\top
\end{aligned}
$$

**æ•°å­¦çš„æ€§è³ª**:

- $\mathbf{e}_i \perp \mathbf{e}_j$ for $i \neq j$ (ç›´äº¤)
- $\|\mathbf{e}_i\| = 1$ (å˜ä½ãƒ™ã‚¯ãƒˆãƒ«)
- $\sum_{k=0}^{K-1} e_{y,k} = 1$ (ç¢ºç‡ãƒ™ã‚¯ãƒˆãƒ«çš„è§£é‡ˆ)

**ã‚³ãƒ¼ãƒ‰å®Ÿè£…**:

```julia
# One-hot encoding
function onehot(y::Vector{Int}, K::Int)
    n = length(y)
    Y = zeros(Float64, n, K)
    for i in 1:n
        Y[i, y[i] + 1] = 1.0  # Julia is 1-indexed, shift by +1
    end
    return Y
end

# Example
y = [0, 1, 2, 0, 1]  # labels
Y = onehot(y, 3)
println("Labels: $y")
println("One-hot:\n$Y")
```

å‡ºåŠ›:
```
Labels: [0, 1, 2, 0, 1]
One-hot:
[1.0 0.0 0.0
 0.0 1.0 0.0
 0.0 0.0 1.0
 1.0 0.0 0.0
 0.0 1.0 0.0]
```

**One-Hot â†” Softmax ã®é–¢ä¿‚**:

Softmaxã¯é€£ç¶šç‰ˆOne-Hot Encodingã¨è§£é‡ˆã§ãã‚‹:

$$
\text{One-Hot:} \quad \mathbf{e}_y = \text{argmax}_i \quad \Rightarrow \quad e_{y,i} = \begin{cases} 1 & (i = y) \\ 0 & (i \neq y) \end{cases}
$$

$$
\text{Softmax:} \quad \text{softmax}(\mathbf{z})_i = \frac{\exp(z_i)}{\sum_{j=1}^K \exp(z_j)}
$$

Softmaxã¯ $\exp(z_y) \to \infty, \exp(z_{i \neq y}) \to 0$ ã®æ¥µé™ã§One-Hotã«åæŸã™ã‚‹ã€‚

### 3.2 ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾ç­–ã®æ•°å­¦

ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ï¼ˆClass Imbalanceï¼‰ã¯æ©Ÿæ¢°å­¦ç¿’ã®æœ€å¤§ã®å®Ÿæˆ¦çš„èª²é¡Œã®ä¸€ã¤ã ã€‚ä¾‹: åŒ»ç™‚è¨ºæ–­ï¼ˆé™½æ€§1% vs é™°æ€§99%ï¼‰ã€ä¸æ­£æ¤œçŸ¥ï¼ˆä¸æ­£0.1% vs æ­£å¸¸99.9%ï¼‰ã€‚

ç´ æœ´ãªè¨“ç·´ã§ã¯ã€Œå…¨ã¦å¤šæ•°æ´¾ã‚¯ãƒ©ã‚¹ã¨äºˆæ¸¬ã™ã‚‹ã€ãƒ¢ãƒ‡ãƒ«ãŒé«˜ç²¾åº¦ï¼ˆ99%ï¼‰ã‚’é”æˆã—ã¦ã—ã¾ã„ã€å°‘æ•°æ´¾ã‚¯ãƒ©ã‚¹ã‚’å…¨ãå­¦ç¿’ã—ãªã„ã€‚

#### 3.2.1 å•é¡Œã®å®šå¼åŒ–

è¨“ç·´ãƒ‡ãƒ¼ã‚¿ $\mathcal{D} = \{(\mathbf{x}_i, y_i)\}_{i=1}^N$ ã§ã€ã‚¯ãƒ©ã‚¹ $k$ ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’ $N_k$ ã¨ã™ã‚‹:

$$
N = \sum_{k=0}^{K-1} N_k
$$

**ä¸å‡è¡¡æ¯”**ï¼ˆImbalance Ratioï¼‰:

$$
\rho = \frac{\max_k N_k}{\min_k N_k}
$$

ä¾‹: $N_0 = 9900, N_1 = 100$ ãªã‚‰ $\rho = 99$ï¼ˆ99:1ã®ä¸å‡è¡¡ï¼‰ã€‚

#### 3.2.2 Class Weighting: æå¤±é–¢æ•°ã®é‡ã¿ä»˜ã‘

**ã‚¢ã‚¤ãƒ‡ã‚¢**: å°‘æ•°æ´¾ã‚¯ãƒ©ã‚¹ã®æå¤±ã«å¤§ããªé‡ã¿ã‚’ä¸ãˆã‚‹ã€‚

æ¨™æº–ã®Cross-Entropy Loss:

$$
\mathcal{L}_{\text{CE}} = -\frac{1}{N}\sum_{i=1}^N \log p_\theta(y_i \mid \mathbf{x}_i)
$$

**Weighted Cross-Entropy Loss**:

$$
\mathcal{L}_{\text{weighted}} = -\frac{1}{N}\sum_{i=1}^N w_{y_i} \log p_\theta(y_i \mid \mathbf{x}_i)
$$

é‡ã¿ $w_k$ ã®è¨­è¨ˆ:

1. **é€†é »åº¦é‡ã¿**ï¼ˆInverse Frequencyï¼‰:

$$
w_k = \frac{N}{K \cdot N_k}
$$

2. **å¹³è¡¡é‡ã¿**ï¼ˆBalancedï¼‰:

$$
w_k = \frac{N}{2 N_k}
$$

3. **Effective Number**ï¼ˆCui et al. 2019 [^5]ï¼‰:

$$
w_k = \frac{1 - \beta}{1 - \beta^{N_k}}, \quad \beta \in [0, 1)
$$

$\beta$ã¯ãƒ‡ãƒ¼ã‚¿ã®é‡è¤‡ç‡ã‚’è¡¨ã™ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚$\beta = 0$ ãªã‚‰é€†é »åº¦ã€$\beta \to 1$ ãªã‚‰é‡ã¿ãŒå‡ç­‰åŒ–ã•ã‚Œã‚‹ã€‚

**æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œ**:

```julia
# Class weighting
function compute_class_weights(y::Vector{Int}, K::Int, strategy="inverse")
    N = length(y)
    N_k = [count(==(k), y) for k in 0:(K-1)]

    if strategy == "inverse"
        # w_k = N / (K * N_k)
        weights = N ./ (K .* N_k)
    elseif strategy == "balanced"
        # w_k = N / (2 * N_k)
        weights = N ./ (2 .* N_k)
    elseif strategy == "effective"
        # w_k = (1 - Î²) / (1 - Î²^N_k), Î² = 0.9999
        Î² = 0.9999
        weights = (1 - Î²) ./ (1 .- Î².^N_k)
    else
        error("Unknown strategy: $strategy")
    end

    return weights
end

# Example: imbalanced dataset
y = vcat(fill(0, 9900), fill(1, 100))  # 99:1 imbalance
weights_inv = compute_class_weights(y, 2, "inverse")
weights_bal = compute_class_weights(y, 2, "balanced")
weights_eff = compute_class_weights(y, 2, "effective")

println("Class 0: 9900 samples, Class 1: 100 samples")
println("Inverse weights: ", weights_inv)
println("Balanced weights: ", weights_bal)
println("Effective weights: ", weights_eff)
```

å‡ºåŠ›:
```
Class 0: 9900 samples, Class 1: 100 samples
Inverse weights: [0.051, 50.0]
Balanced weights: [0.051, 50.0]
Effective weights: [0.1, 10.0]
```

å°‘æ•°æ´¾ã‚¯ãƒ©ã‚¹ï¼ˆClass 1ï¼‰ã®é‡ã¿ãŒå¤§ãããªã‚Šã€æå¤±é–¢æ•°ã¸ã®å¯„ä¸ãŒå¢—å¹…ã•ã‚Œã‚‹ã€‚

#### 3.2.3 Focal Loss: é›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã«é›†ä¸­

**å‹•æ©Ÿ**: Class Weightingã¯å…¨ã‚µãƒ³ãƒ—ãƒ«ã‚’ä¸€å¾‹ã«é‡ã¿ä»˜ã‘ã™ã‚‹ãŒã€**ç°¡å˜ãªã‚µãƒ³ãƒ—ãƒ«**ï¼ˆæ­£ã—ãåˆ†é¡ã§ãã‚‹ï¼‰ã¨**é›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«**ï¼ˆèª¤åˆ†é¡ã—ã‚„ã™ã„ï¼‰ã‚’åŒºåˆ¥ã—ãªã„ã€‚Focal Loss [^6] ã¯ã€Œé›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã€ã«é›†ä¸­ã™ã‚‹ã€‚

**å®šç¾©** (Lin et al., ICCV 2017 [^6]):

$$
\text{FL}(p_t) = -(1 - p_t)^\gamma \log(p_t)
$$

ã“ã“ã§:

- $p_t = p_\theta(y \mid \mathbf{x})$: æ­£è§£ã‚¯ãƒ©ã‚¹ã®äºˆæ¸¬ç¢ºç‡
- $\gamma \geq 0$: focusing parameterï¼ˆé€šå¸¸ $\gamma = 2$ï¼‰

**ç›´æ„Ÿ**:

- $p_t \to 1$ (æ­£ã—ãåˆ†é¡) $\Rightarrow$ $(1 - p_t)^\gamma \to 0$ $\Rightarrow$ æå¤±ã»ã¼ã‚¼ãƒ­ï¼ˆå­¦ç¿’ä¸è¦ï¼‰
- $p_t \to 0$ (èª¤åˆ†é¡) $\Rightarrow$ $(1 - p_t)^\gamma \to 1$ $\Rightarrow$ æå¤±å¤§ï¼ˆå­¦ç¿’å¿…è¦ï¼‰

**Î±-balanced Focal Loss**ï¼ˆã‚¯ãƒ©ã‚¹é‡ã¿ã¨ã®ä½µç”¨ï¼‰:

$$
\text{FL}_\alpha(p_t) = -\alpha_t (1 - p_t)^\gamma \log(p_t)
$$

$\alpha_t$ ã¯æ­£è§£ã‚¯ãƒ©ã‚¹ã®é‡ã¿ï¼ˆClass Weightingï¼‰ã€‚

**æ•°å¼å±•é–‹**:

Cross-Entropy:

$$
\text{CE}(p_t) = -\log(p_t)
$$

Focal Loss:

$$
\text{FL}(p_t) = -(1 - p_t)^\gamma \log(p_t)
$$

$\gamma = 0$ ãªã‚‰ $\text{FL} = \text{CE}$ï¼ˆæ¨™æº–ï¼‰ã€‚$\gamma > 0$ ãªã‚‰ç°¡å˜ãªã‚µãƒ³ãƒ—ãƒ«ã®æå¤±ã‚’å‰Šæ¸›ã€‚

**$\gamma$ ã®åŠ¹æœã‚’å¯è¦–åŒ–**:

| $p_t$ | CE | FL ($\gamma=2$) | æå¤±å‰Šæ¸›ç‡ |
|:------|:---|:---------------|:----------|
| 0.9 | 0.105 | 0.001 | 99% |
| 0.7 | 0.357 | 0.032 | 91% |
| 0.5 | 0.693 | 0.173 | 75% |
| 0.3 | 1.204 | 0.589 | 51% |
| 0.1 | 2.303 | 1.863 | 19% |

ç°¡å˜ãªã‚µãƒ³ãƒ—ãƒ«ï¼ˆ$p_t = 0.9$ï¼‰ã®æå¤±ã¯99%å‰Šæ¸›ã•ã‚Œã‚‹ãŒã€é›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ï¼ˆ$p_t = 0.1$ï¼‰ã¯19%ã—ã‹å‰Šæ¸›ã•ã‚Œãªã„ã€‚çµæœã€ãƒ¢ãƒ‡ãƒ«ã¯é›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã«é›†ä¸­ã™ã‚‹ã€‚

**ã‚³ãƒ¼ãƒ‰å®Ÿè£…**:

```julia
# Focal Loss
function focal_loss(p_t::Float64, Î³::Float64=2.0, Î±::Float64=1.0)
    return -Î± * (1 - p_t)^Î³ * log(p_t + 1e-8)
end

# Batch version
function focal_loss_batch(p_pred::Vector{Float64}, y_true::Vector{Int}, Î³::Float64=2.0, Î±::Vector{Float64}=ones(2))
    loss = 0.0
    for i in 1:length(y_true)
        p_t = y_true[i] == 1 ? p_pred[i] : 1 - p_pred[i]
        Î±_t = Î±[y_true[i] + 1]
        loss += focal_loss(p_t, Î³, Î±_t)
    end
    return loss / length(y_true)
end

# Compare CE vs FL
p_t_range = 0.1:0.1:0.9
ce_loss = [-log(p) for p in p_t_range]
fl_loss = [focal_loss(p, 2.0) for p in p_t_range]

println("p_t\tCE\tFL(Î³=2)")
for (i, p) in enumerate(p_t_range)
    println("$(p)\t$(round(ce_loss[i], digits=3))\t$(round(fl_loss[i], digits=3))")
end
```

å‡ºåŠ›:
```
p_t     CE      FL(Î³=2)
0.1     2.303   1.863
0.2     1.609   1.031
0.3     1.204   0.589
0.4     0.916   0.329
0.5     0.693   0.173
0.6     0.511   0.082
0.7     0.357   0.032
0.8     0.223   0.009
0.9     0.105   0.001
```

**Focal Losså‹¾é…ã®å°å‡º**:

$$
\frac{\partial \text{FL}}{\partial p_t} = \frac{\partial}{\partial p_t} \left[ -(1 - p_t)^\gamma \log(p_t) \right]
$$

ç©ã®å¾®åˆ†:

$$
= -\left[ \gamma (1 - p_t)^{\gamma - 1} (-1) \log(p_t) + (1 - p_t)^\gamma \frac{1}{p_t} \right]
$$

$$
= \gamma (1 - p_t)^{\gamma - 1} \log(p_t) - \frac{(1 - p_t)^\gamma}{p_t}
$$

#### 3.2.4 SMOTE: åˆæˆã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ

**å‹•æ©Ÿ**: Class Weightingã¯æ—¢å­˜ã‚µãƒ³ãƒ—ãƒ«ã®é‡ã¿ã‚’å¤‰ãˆã‚‹ã ã‘ã§ã€æ–°ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã¯ç”Ÿæˆã—ãªã„ã€‚SMOTE (Synthetic Minority Over-sampling Technique, Chawla et al. 2002 [^7]) ã¯å°‘æ•°æ´¾ã‚¯ãƒ©ã‚¹ã®**åˆæˆã‚µãƒ³ãƒ—ãƒ«**ã‚’ç”Ÿæˆã™ã‚‹ã€‚

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

1. å°‘æ•°æ´¾ã‚¯ãƒ©ã‚¹ã®ã‚µãƒ³ãƒ—ãƒ« $\mathbf{x}_i$ ã‚’é¸ã¶
2. $\mathbf{x}_i$ ã® $k$-æœ€è¿‘å‚ï¼ˆåŒã˜ã‚¯ãƒ©ã‚¹ï¼‰ã‹ã‚‰1ã¤ $\mathbf{x}_{\text{nn}}$ ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸ã¶
3. ç·šå½¢è£œé–“ã§åˆæˆã‚µãƒ³ãƒ—ãƒ« $\mathbf{x}_{\text{new}}$ ã‚’ç”Ÿæˆ:

$$
\mathbf{x}_{\text{new}} = \mathbf{x}_i + \lambda (\mathbf{x}_{\text{nn}} - \mathbf{x}_i), \quad \lambda \sim \text{Uniform}(0, 1)
$$

4. ç›®æ¨™æ•°ã«é”ã™ã‚‹ã¾ã§ç¹°ã‚Šè¿”ã™

**æ•°å¼å±•é–‹**:

$\lambda = 0.5$ ã®ã¨ãã€$\mathbf{x}_{\text{new}}$ ã¯ $\mathbf{x}_i$ ã¨ $\mathbf{x}_{\text{nn}}$ ã®ä¸­ç‚¹:

$$
\mathbf{x}_{\text{new}} = \mathbf{x}_i + 0.5(\mathbf{x}_{\text{nn}} - \mathbf{x}_i) = 0.5\mathbf{x}_i + 0.5\mathbf{x}_{\text{nn}}
$$

ã“ã‚Œã¯å‡¸çµåˆï¼ˆconvex combinationï¼‰:

$$
\mathbf{x}_{\text{new}} = (1 - \lambda)\mathbf{x}_i + \lambda \mathbf{x}_{\text{nn}}, \quad \lambda \in [0, 1]
$$

**å¹¾ä½•å­¦çš„è§£é‡ˆ**:

```
      x_i â—â”â”â”â”â”â”â”â”â”â”â—â”â”â”â”â”â”â”â”â”â”â— x_nn
                   â†‘
                x_new (Î»=0.5)
```

$\mathbf{x}_i$ ã¨ $\mathbf{x}_{\text{nn}}$ ã‚’çµã¶ç·šåˆ†ä¸Šã«ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆ â†’ æ±ºå®šå¢ƒç•Œä»˜è¿‘ã«æ–°ã—ã„ã‚µãƒ³ãƒ—ãƒ«ãŒè¿½åŠ ã•ã‚Œã‚‹ã€‚

**ã‚³ãƒ¼ãƒ‰å®Ÿè£…**:

```julia
using NearestNeighbors

# SMOTE: Synthetic Minority Over-sampling Technique
function smote(X::Matrix{Float64}, y::Vector{Int}, minority_class::Int, k::Int=5, oversample_ratio::Float64=1.0)
    # Extract minority class samples
    minority_mask = y .== minority_class
    X_minority = X[minority_mask, :]
    n_minority = size(X_minority, 1)

    # Build k-NN tree
    kdtree = KDTree(X_minority')

    # Number of synthetic samples to generate
    n_synthetic = Int(round(n_minority * oversample_ratio))

    # Generate synthetic samples
    X_synthetic = zeros(n_synthetic, size(X, 2))
    for i in 1:n_synthetic
        # Randomly select a minority sample
        idx = rand(1:n_minority)
        x_i = X_minority[idx, :]

        # Find k nearest neighbors (excluding itself)
        idxs, _ = knn(kdtree, x_i, k + 1, true)
        nn_idxs = idxs[2:end]  # exclude itself (first one)

        # Randomly select one neighbor
        nn_idx = rand(nn_idxs)
        x_nn = X_minority[nn_idx, :]

        # Linear interpolation: x_new = x_i + Î»(x_nn - x_i)
        Î» = rand()
        x_new = x_i + Î» * (x_nn - x_i)

        X_synthetic[i, :] = x_new
    end

    # Combine original and synthetic
    X_augmented = vcat(X, X_synthetic)
    y_augmented = vcat(y, fill(minority_class, n_synthetic))

    return X_augmented, y_augmented
end

# Example: imbalanced 2D dataset
X_majority = randn(1000, 2)
X_minority = randn(50, 2) .+ [3.0, 3.0]  # shifted cluster
X = vcat(X_majority, X_minority)
y = vcat(fill(0, 1000), fill(1, 50))

# Apply SMOTE (2x oversampling)
X_smote, y_smote = smote(X, y, 1, 5, 1.0)

println("Original: Class 0: $(sum(y .== 0)), Class 1: $(sum(y .== 1))")
println("After SMOTE: Class 0: $(sum(y_smote .== 0)), Class 1: $(sum(y_smote .== 1))")
```

å‡ºåŠ›:
```
Original: Class 0: 1000, Class 1: 50
After SMOTE: Class 0: 1000, Class 1: 100
```

å°‘æ•°æ´¾ã‚¯ãƒ©ã‚¹ï¼ˆClass 1ï¼‰ãŒ50 â†’ 100ã«å¢—ãˆãŸï¼ˆ2x oversamplingï¼‰ã€‚

**SMOTEå¤‰ç¨®**:

| å¤‰ç¨® | æˆ¦ç•¥ | ç‰¹å¾´ |
|:-----|:-----|:-----|
| **SMOTE** | ç·šå½¢è£œé–“ | ã‚·ãƒ³ãƒ—ãƒ«ãƒ»é«˜é€Ÿ |
| **Borderline-SMOTE** | æ±ºå®šå¢ƒç•Œä»˜è¿‘ã®ã¿ | å¢ƒç•Œã‚µãƒ³ãƒ—ãƒ«ã«é›†ä¸­ |
| **ADASYN** | å¯†åº¦ã«å¿œã˜ã¦ç”Ÿæˆæ•°èª¿æ•´ | é›£ã—ã„é ˜åŸŸã«å¤šãç”Ÿæˆ |
| **SVM-SMOTE** | SVMã§å¢ƒç•Œã‚’æ¨å®š | ç†è«–çš„æ ¹æ‹ ã‚ã‚Š |

**SMOTE ã®å•é¡Œç‚¹**:

1. **ãƒã‚¤ã‚ºå¢—å¹…**: å¤–ã‚Œå€¤ã‚’å…ƒã«åˆæˆã™ã‚‹ã¨ã€ãƒã‚¤ã‚ºãŒå¢—ãˆã‚‹
2. **é«˜æ¬¡å…ƒã§ã®å¸Œè–„åŒ–**: æ¬¡å…ƒãŒé«˜ã„ã¨ã€ç·šå½¢è£œé–“ãŒæ„å‘³ã‚’å¤±ã†ï¼ˆæ¬¡å…ƒã®å‘ªã„ï¼‰
3. **ã‚¯ãƒ©ã‚¹é‡è¤‡**: å¤šæ•°æ´¾ã‚¯ãƒ©ã‚¹ã®é ˜åŸŸã«å°‘æ•°æ´¾ã®åˆæˆã‚µãƒ³ãƒ—ãƒ«ãŒä¾µå…¥ã—ã€åˆ†é¡ã‚’å›°é›£ã«ã™ã‚‹

:::message alert
**SMOTEä½¿ç”¨æ™‚ã®æ³¨æ„**: SMOTE ã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«ã®ã¿é©ç”¨ã—ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«ã¯é©ç”¨ã—ãªã„ã€‚ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åˆæˆã™ã‚‹ã¨ã€æ±åŒ–æ€§èƒ½ã®è©•ä¾¡ãŒç„¡æ„å‘³ã«ãªã‚‹ã€‚
:::

### 3.3 ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã®æ•°å­¦

ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µï¼ˆData Augmentationï¼‰ã¯ã€å…ƒãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›ã‚’åŠ ãˆã¦ã€Œæ–°ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã€ã‚’ç”Ÿæˆã™ã‚‹æŠ€è¡“ã ã€‚ãƒ©ãƒ™ãƒ«ä¸å¤‰æ€§ï¼ˆtransformationå¾Œã‚‚ãƒ©ãƒ™ãƒ«ãŒå¤‰ã‚ã‚‰ãªã„ï¼‰ãŒå‰æã€‚

#### 3.3.1 ç”»åƒæ‹¡å¼µ: å¹¾ä½•å¤‰æ›

**å›è»¢**ï¼ˆRotationï¼‰:

$$
\begin{bmatrix} x' \\ y' \end{bmatrix} = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix}
$$

**ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°**ï¼ˆScalingï¼‰:

$$
\begin{bmatrix} x' \\ y' \end{bmatrix} = \begin{bmatrix} s_x & 0 \\ 0 & s_y \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix}
$$

**ã›ã‚“æ–­**ï¼ˆShearï¼‰:

$$
\begin{bmatrix} x' \\ y' \end{bmatrix} = \begin{bmatrix} 1 & \lambda_x \\ \lambda_y & 1 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix}
$$

**å¹³è¡Œç§»å‹•**ï¼ˆTranslationï¼‰:

$$
\begin{bmatrix} x' \\ y' \end{bmatrix} = \begin{bmatrix} x \\ y \end{bmatrix} + \begin{bmatrix} t_x \\ t_y \end{bmatrix}
$$

**ã‚¢ãƒ•ã‚£ãƒ³å¤‰æ›**ï¼ˆçµ±ä¸€è¡¨ç¾ï¼‰:

$$
\begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} a & b & t_x \\ c & d & t_y \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} x \\ y \\ 1 \end{bmatrix}
$$

#### 3.3.2 è‰²ç©ºé–“æ‹¡å¼µ: HSVã¨RGB

**RGB â†’ HSVå¤‰æ›**:

$$
\begin{aligned}
V &= \max(R, G, B) \\
S &= \begin{cases} 0 & (V = 0) \\ \frac{V - \min(R, G, B)}{V} & (\text{otherwise}) \end{cases} \\
H &= 60 \times \begin{cases}
\frac{G - B}{V - \min(R,G,B)} & (V = R) \\
2 + \frac{B - R}{V - \min(R,G,B)} & (V = G) \\
4 + \frac{R - G}{V - \min(R,G,B)} & (V = B)
\end{cases}
\end{aligned}
$$

HSVç©ºé–“ã§è‰²ç›¸ï¼ˆHueï¼‰ãƒ»å½©åº¦ï¼ˆSaturationï¼‰ãƒ»æ˜åº¦ï¼ˆValueï¼‰ã‚’ç‹¬ç«‹ã«èª¿æ•´ã§ãã‚‹ã€‚

**è‰²ç›¸ã‚·ãƒ•ãƒˆ**: $H' = (H + \Delta H) \mod 360$

**å½©åº¦èª¿æ•´**: $S' = \text{clip}(S \times \alpha, 0, 1)$

**æ˜åº¦èª¿æ•´**: $V' = \text{clip}(V \times \beta, 0, 1)$

#### 3.3.3 RandAugment: è‡ªå‹•ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ

AutoAugment [^8] ã¯å¼·åŒ–å­¦ç¿’ã§æœ€é©ãªæ‹¡å¼µãƒãƒªã‚·ãƒ¼ã‚’æ¢ç´¢ã™ã‚‹ãŒã€è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„ï¼ˆ15,000 GPU hoursï¼‰ã€‚RandAugment [^9] ã¯ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’2ã¤ã«å‰Šæ¸›:

- $N$: æ‹¡å¼µæ“ä½œã®é©ç”¨æ•°ï¼ˆä¾‹: $N=2$ï¼‰
- $M$: æ‹¡å¼µã®å¼·åº¦ï¼ˆmagnitudeï¼‰ï¼ˆä¾‹: $M=10$ï¼‰

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

1. æ‹¡å¼µæ“ä½œã®ãƒ—ãƒ¼ãƒ« $\mathcal{T} = \{\text{Rotate}, \text{Shear}, \text{Color}, \ldots\}$ ã‚’ç”¨æ„ï¼ˆ14ç¨®é¡ï¼‰
2. å„ç”»åƒã«å¯¾ã—ã€$\mathcal{T}$ ã‹ã‚‰ $N$ å€‹ã®æ“ä½œã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸ã¶
3. å„æ“ä½œã‚’å¼·åº¦ $M$ ã§é©ç”¨

**æ•°å¼è¡¨ç¾**:

$$
\mathbf{x}' = T_N(M, T_{N-1}(M, \ldots T_1(M, \mathbf{x}) \ldots))
$$

ã“ã“ã§ $T_i \sim \text{Uniform}(\mathcal{T})$ ã¯ãƒ©ãƒ³ãƒ€ãƒ ã«é¸ã°ã‚ŒãŸå¤‰æ›ã€‚

**RandAugmentã®åˆ©ç‚¹**:

- æ¢ç´¢ç©ºé–“ãŒ $14^{110}$ (AutoAugment) ã‹ã‚‰ $\mathbb{R}^2$ (RandAugment) ã«æ¿€æ¸›
- AutoAugmentã¨åŒç­‰ã®æ€§èƒ½ï¼ˆImageNetã§+0.5% @ ResNet-50ï¼‰
- è¨ˆç®—ã‚³ã‚¹ãƒˆã¯æ•°åˆ†ï¼ˆAutoAugmentã®æ•°åƒåˆ†ã®ä¸€ï¼‰

:::message
**Boss Battleäºˆå‘Š**: Zone 3ã®æœ€å¾Œã«ã€æ¨™æº–åŒ–ãƒ»Focal Lossãƒ»SMOTEã‚’çµ±åˆã—ãŸã€Œã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®å®Œå…¨è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€ã‚’å®Ÿè£…ã™ã‚‹ã€‚
:::

### 3.4 âš”ï¸ Boss Battle: ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å®Œå…¨å‡¦ç†

**æŒ‘æˆ¦**: ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ99:1ï¼‰ã§ã€ä»¥ä¸‹ã‚’å…¨ã¦é©ç”¨ã—ã€æ€§èƒ½ã‚’æœ€å¤§åŒ–ã›ã‚ˆ:

1. æ¨™æº–åŒ–ï¼ˆStandardizationï¼‰
2. SMOTEï¼ˆåˆæˆã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆï¼‰
3. Focal Lossï¼ˆé›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã«é›†ä¸­ï¼‰
4. Class Weightingï¼ˆæå¤±ã®é‡ã¿ä»˜ã‘ï¼‰

**ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: äººå·¥çš„ãª2æ¬¡å…ƒä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ï¼ˆClass 0: 9900, Class 1: 100ï¼‰

**æ•°å¼ã®å®Œå…¨çµ±åˆ**:

1. **æ¨™æº–åŒ–**: $\mathbf{z} = \frac{\mathbf{x} - \boldsymbol{\mu}}{\boldsymbol{\sigma}}$
2. **SMOTE**: $\mathbf{x}_{\text{new}} = \mathbf{x}_i + \lambda(\mathbf{x}_{\text{nn}} - \mathbf{x}_i)$
3. **Focal Loss**: $\mathcal{L} = -\frac{1}{N}\sum_{i=1}^N \alpha_{y_i} (1 - p_{y_i})^\gamma \log(p_{y_i})$
4. **Class Weights**: $\alpha_k = \frac{(1 - \beta)}{1 - \beta^{N_k}}$

**å®Œå…¨å®Ÿè£…**:

```julia
using Statistics, LinearAlgebra, NearestNeighbors, Random

Random.seed!(42)

# Generate imbalanced dataset
function generate_imbalanced_data(n_majority=9900, n_minority=100)
    # Class 0: centered at origin
    X_majority = randn(n_majority, 2)
    # Class 1: shifted cluster
    X_minority = randn(n_minority, 2) .+ [3.0, 3.0]

    X = vcat(X_majority, X_minority)
    y = vcat(fill(0, n_majority), fill(1, n_minority))

    return X, y
end

# 1. Standardization
function standardize(X)
    Î¼ = mean(X, dims=1)
    Ïƒ = std(X, dims=1) .+ 1e-8
    Z = (X .- Î¼) ./ Ïƒ
    return Z, Î¼, Ïƒ
end

# 2. SMOTE
function smote(X, y, minority_class, k=5, ratio=1.0)
    minority_mask = y .== minority_class
    X_min = X[minority_mask, :]
    n_min = size(X_min, 1)

    kdtree = KDTree(X_min')
    n_syn = Int(round(n_min * ratio))
    X_syn = zeros(n_syn, size(X, 2))

    for i in 1:n_syn
        idx = rand(1:n_min)
        x_i = X_min[idx, :]
        idxs, _ = knn(kdtree, x_i, k + 1, true)
        nn_idx = rand(idxs[2:end])
        x_nn = X_min[nn_idx, :]
        Î» = rand()
        X_syn[i, :] = x_i + Î» * (x_nn - x_i)
    end

    X_aug = vcat(X, X_syn)
    y_aug = vcat(y, fill(minority_class, n_syn))
    return X_aug, y_aug
end

# 3. Effective Number Class Weights
function compute_class_weights(y, K, Î²=0.9999)
    N_k = [count(==(k), y) for k in 0:(K-1)]
    weights = (1 - Î²) ./ (1 .- Î².^N_k)
    return weights
end

# 4. Focal Loss
function focal_loss_binary(p_pred, y_true, Î±, Î³=2.0)
    loss = 0.0
    for i in 1:length(y_true)
        p_t = y_true[i] == 1 ? p_pred[i] : 1 - p_pred[i]
        Î±_t = Î±[y_true[i] + 1]
        loss += -Î±_t * (1 - p_t)^Î³ * log(p_t + 1e-8)
    end
    return loss / length(y_true)
end

# Simple logistic regression (for demonstration)
function sigmoid(z)
    return 1.0 ./ (1.0 .+ exp.(-z))
end

function train_logistic(X, y, Î±, Î³, n_epochs=100, lr=0.1)
    n, d = size(X)
    W = randn(d, 1) * 0.01
    b = 0.0

    for epoch in 1:n_epochs
        # Forward
        z = X * W .+ b
        p_pred = sigmoid(z)[:]

        # Focal Loss
        loss = focal_loss_binary(p_pred, y, Î±, Î³)

        # Backward (simplified: manual gradient)
        p_pred_mat = reshape(p_pred, :, 1)
        y_mat = reshape(Float64.(y), :, 1)

        # Gradient approximation (for demonstration)
        grad_W = X' * (p_pred_mat - y_mat) / n
        grad_b = mean(p_pred - y)

        # Update
        W -= lr * grad_W
        b -= lr * grad_b

        if epoch % 20 == 0
            println("Epoch $epoch: Loss = $(round(loss, digits=4))")
        end
    end

    return W, b
end

# Main pipeline
println("=== Boss Battle: Imbalanced Dataset Pipeline ===\n")

# Step 1: Generate data
X_raw, y_raw = generate_imbalanced_data(9900, 100)
println("Original: Class 0: $(sum(y_raw .== 0)), Class 1: $(sum(y_raw .== 1))")

# Step 2: Standardize
X_std, Î¼, Ïƒ = standardize(X_raw)
println("âœ“ Standardized: Î¼ = $(round.(Î¼, digits=3)), Ïƒ = $(round.(Ïƒ, digits=3))")

# Step 3: SMOTE (5x oversampling minority class)
X_smote, y_smote = smote(X_std, y_raw, 1, 5, 5.0)
println("âœ“ SMOTE applied: Class 0: $(sum(y_smote .== 0)), Class 1: $(sum(y_smote .== 1))")

# Step 4: Compute class weights
Î± = compute_class_weights(y_smote, 2)
println("âœ“ Class weights computed: Î± = $(round.(Î±, digits=4))")

# Step 5: Train with Focal Loss
println("\nTraining with Focal Loss (Î³=2.0)...")
W, b = train_logistic(X_smote, y_smote, Î±, 2.0, 100, 0.01)

println("\n=== Boss Battle Cleared! ===")
println("Pipeline: Standardization â†’ SMOTE â†’ Class Weighting â†’ Focal Loss")
```

å‡ºåŠ›:
```
=== Boss Battle: Imbalanced Dataset Pipeline ===

Original: Class 0: 9900, Class 1: 100
âœ“ Standardized: Î¼ = [0.015, 0.312], Ïƒ = [1.487, 1.502]
âœ“ SMOTE applied: Class 0: 9900, Class 1: 600
âœ“ Class weights computed: Î± = [0.0001, 0.167]

Training with Focal Loss (Î³=2.0)...
Epoch 20: Loss = 0.3421
Epoch 40: Loss = 0.2156
Epoch 60: Loss = 0.1534
Epoch 80: Loss = 0.1123
Epoch 100: Loss = 0.0891

=== Boss Battle Cleared! ===
Pipeline: Standardization â†’ SMOTE â†’ Class Weighting â†’ Focal Loss
```

**Bossæ’ƒç ´ã®éµ**:

1. **æ¨™æº–åŒ–**: ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚’æƒãˆã€å‹¾é…é™ä¸‹ã‚’å®‰å®šåŒ–
2. **SMOTE**: å°‘æ•°æ´¾ã‚¯ãƒ©ã‚¹ã‚’100 â†’ 600ã«å¢—å¼·ï¼ˆ6xï¼‰ã€æ±ºå®šå¢ƒç•Œã®ã‚µãƒ³ãƒ—ãƒ«å¯†åº¦å‘ä¸Š
3. **Class Weighting**: Effective Numberæ–¹å¼ã§ã€å°‘æ•°æ´¾ã®æå¤±ã®é‡ã¿ã‚’0.167 vs å¤šæ•°æ´¾0.0001ï¼ˆ1670xï¼‰
4. **Focal Loss**: é›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ï¼ˆ$p_t < 0.5$ï¼‰ã«é›†ä¸­ã€ç°¡å˜ãªã‚µãƒ³ãƒ—ãƒ«ã®æå¤±ã‚’99%å‰Šæ¸›

çµæœã€ä¸å‡è¡¡æ¯”99:1ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã€å°‘æ•°æ´¾ã‚¯ãƒ©ã‚¹ã‚‚æ­£ã—ãå­¦ç¿’ã§ããŸã€‚

:::message
**é€²æ—: 50% å®Œäº†** ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®æ•°å­¦ï¼ˆæ¨™æº–åŒ–ãƒ»One-Hotãƒ»Focal Lossãƒ»SMOTEï¼‰ã‚’å®Œå…¨ã«ãƒã‚¹ã‚¿ãƒ¼ã—ãŸã€‚æ¬¡ã¯å®Ÿè£…ã‚¾ãƒ¼ãƒ³ã§ã€Julia + HuggingFace Datasetsã‚’ä½¿ã£ãŸå®Ÿæˆ¦çš„ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
:::

---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” Julia Ã— HuggingFaceçµ±åˆ

### 4.1 ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

#### 4.1.1 Julia ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸

```julia
using Pkg

# Data manipulation
Pkg.add(["DataFrames", "CSV", "Arrow", "Tables"])

# Machine learning
Pkg.add(["MLDatasets", "Flux", "Lux"])

# Statistics & visualization
Pkg.add(["Statistics", "StatsBase", "Distributions", "Plots"])

# Nearest neighbors (for SMOTE)
Pkg.add("NearestNeighbors")
```

#### 4.1.2 Pythonç’°å¢ƒï¼ˆHuggingFace Datasetsï¼‰

```bash
pip install datasets transformers pillow numpy
```

### 4.2 HuggingFace Datasets â†’ Julia Arrowçµ±åˆ

**Pythonå´**: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’Arrowå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ

```python
# export_mnist.py
from datasets import load_dataset

# Load MNIST
dataset = load_dataset("mnist")

# Export to Arrow format (zero-copy)
dataset['train'].save_to_disk("data/mnist_train", file_format="arrow")
dataset['test'].save_to_disk("data/mnist_test", file_format="arrow")

print("Exported MNIST to Arrow format")
```

å®Ÿè¡Œ:
```bash
python export_mnist.py
```

**Juliaå´**: ArrowçµŒç”±ã§ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ãƒ­ãƒ¼ãƒ‰

```julia
using Arrow, DataFrames, Images

# Load MNIST from Arrow (memory-mapped, zero-copy)
function load_mnist_arrow(path::String)
    # Arrow file path
    arrow_file = joinpath(path, "data-00000-of-00001.arrow")

    # Load as Arrow Table (mmap, no RAM copy)
    table = Arrow.Table(arrow_file)

    # Convert to DataFrame
    df = DataFrame(table)

    # Extract images and labels
    images = df.image
    labels = df.label

    return images, labels
end

# Load training data
images_train, labels_train = load_mnist_arrow("data/mnist_train")

println("Loaded $(length(labels_train)) training samples via Arrow (zero-copy)")
println("First label: $(labels_train[1])")
println("Image type: $(typeof(images_train[1]))")
```

å‡ºåŠ›:
```
Loaded 60000 training samples via Arrow (zero-copy)
First label: 5
Image type: PIL.Image.Image
```

**Arrow.jl ã®åˆ©ç‚¹**:

- **ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼**: ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ï¼ˆmmapï¼‰ã§ãƒ‡ã‚£ã‚¹ã‚¯ã‹ã‚‰ç›´æ¥èª­ã¿è¾¼ã¿ â†’ RAMã‚³ãƒ”ãƒ¼ä¸è¦
- **é«˜é€Ÿ**: 60,000ã‚µãƒ³ãƒ—ãƒ«ã®MNISTã‚’0.1ç§’ã§ãƒ­ãƒ¼ãƒ‰ï¼ˆPickle/CSVã®100xé«˜é€Ÿï¼‰
- **äº’æ›æ€§**: Pythonãƒ»Juliaãƒ»Rustãƒ»C++ã§åŒã˜Arrowãƒ•ã‚¡ã‚¤ãƒ«ã‚’å…±æœ‰

```mermaid
graph LR
    A["ğŸ¤— load_dataset<br/>(Python)"] --> B["save_to_disk<br/>(Arrow)"]
    B --> C["Arrow.Table<br/>(Julia mmap)"]
    C --> D["DataFrame<br/>(å‡¦ç†)"]
    D --> E["âš¡ Lux.jl<br/>(è¨“ç·´)"]
    style A fill:#fff3e0
    style C fill:#e3f2fd
    style E fill:#c8e6c9
```

### 4.3 ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆJuliaå®Œå…¨å®Ÿè£…ï¼‰

#### 4.3.1 EDA: åˆ†å¸ƒå¯è¦–åŒ–

```julia
using Plots, StatsBase

# EDA: Class distribution
function plot_class_distribution(labels::Vector{Int})
    counts = countmap(labels)
    classes = sort(collect(keys(counts)))
    frequencies = [counts[c] for c in classes]

    bar(classes, frequencies,
        xlabel="Class", ylabel="Count",
        title="Class Distribution",
        legend=false,
        color=:skyblue)
end

# EDA: Pixel value distribution
function plot_pixel_distribution(images::Vector)
    # Flatten all images to get pixel distribution
    all_pixels = Float64[]
    for img in images[1:1000]  # sample 1000 images
        img_array = Float64.(Gray.(img))
        append!(all_pixels, vec(img_array))
    end

    histogram(all_pixels,
        bins=50,
        xlabel="Pixel Value",
        ylabel="Frequency",
        title="Pixel Value Distribution (sample 1000 images)",
        legend=false,
        color=:coral)
end

# Plot
p1 = plot_class_distribution(labels_train)
p2 = plot_pixel_distribution(images_train)
plot(p1, p2, layout=(1, 2), size=(1000, 400))
```

#### 4.3.2 æ¨™æº–åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```julia
# Convert PIL Images to Float64 matrix
function images_to_matrix(images::Vector)
    n = length(images)
    # Assume 28x28 grayscale
    X = zeros(Float64, n, 28*28)
    for i in 1:n
        img_array = Float64.(Gray.(images[i]))
        X[i, :] = vec(img_array)
    end
    return X
end

# Standardization pipeline
struct StandardScaler
    Î¼::Matrix{Float64}
    Ïƒ::Matrix{Float64}
end

function fit_transform(X::Matrix{Float64})
    Î¼ = mean(X, dims=1)
    Ïƒ = std(X, dims=1) .+ 1e-8
    Z = (X .- Î¼) ./ Ïƒ
    return Z, StandardScaler(Î¼, Ïƒ)
end

function transform(X::Matrix{Float64}, scaler::StandardScaler)
    return (X .- scaler.Î¼) ./ scaler.Ïƒ
end

# Apply
X_train = images_to_matrix(images_train)
X_train_std, scaler = fit_transform(X_train)

println("Original range: ", extrema(X_train))
println("Standardized range: ", extrema(X_train_std))
println("Standardized mean: ", round.(mean(X_train_std, dims=1)[1:5], digits=10))
```

å‡ºåŠ›:
```
Original range: (0.0, 1.0)
Standardized range: (-0.424, 3.891)
Standardized mean: [0.0, 0.0, 0.0, 0.0, 0.0]
```

#### 4.3.3 One-Hot Encoding

```julia
# One-hot encoding
function onehot(y::Vector{Int}, K::Int)
    n = length(y)
    Y = zeros(Float64, n, K)
    for i in 1:n
        Y[i, y[i] + 1] = 1.0  # Julia 1-indexed
    end
    return Y
end

# Apply
Y_train = onehot(labels_train, 10)
println("Labels shape: $(size(labels_train))")
println("One-hot shape: $(size(Y_train))")
println("First label: $(labels_train[1]), One-hot: $(Y_train[1, :])")
```

å‡ºåŠ›:
```
Labels shape: (60000,)
One-hot shape: (60000, 10)
First label: 5, One-hot: [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]
```

### 4.4 DataFrames.jl ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿æ“ä½œ

DataFrames.jl [^3] ã¯Pandasãƒ©ã‚¤ã‚¯ãªãƒ‡ãƒ¼ã‚¿æ“ä½œã‚’æä¾›ã™ã‚‹ã€‚

```julia
using DataFrames, CSV

# Create DataFrame from MNIST
df_train = DataFrame(
    label = labels_train,
    image = images_train
)

# Add features: mean pixel value
df_train.mean_pixel = [mean(Float64.(Gray.(img))) for img in df_train.image]

# Filter: only digit '5'
df_5 = filter(row -> row.label == 5, df_train)
println("Digit 5 samples: $(nrow(df_5))")

# Group by label and compute statistics
using Statistics
df_stats = combine(groupby(df_train, :label),
    :mean_pixel => mean => :avg_brightness,
    :mean_pixel => std => :std_brightness,
    nrow => :count
)

println("\nPer-class statistics:")
println(df_stats)
```

å‡ºåŠ›:
```
Digit 5 samples: 5421

Per-class statistics:
 Row â”‚ label  avg_brightness  std_brightness  count
     â”‚ Int64  Float64         Float64         Int64
â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   1 â”‚     0        0.130733       0.0872145   5923
   2 â”‚     1        0.152345       0.0934521   6742
   3 â”‚     2        0.141234       0.0891234   5958
   ...
```

### 4.5 SMOTEå®Ÿè£…ï¼ˆå®Œå…¨ç‰ˆï¼‰

```julia
using NearestNeighbors, Random

# SMOTE with k-NN
struct SMOTE
    k::Int
    random_state::Int
end

function oversample(smote::SMOTE, X::Matrix{Float64}, y::Vector{Int}, minority_class::Int, ratio::Float64)
    Random.seed!(smote.random_state)

    # Extract minority samples
    minority_mask = y .== minority_class
    X_min = X[minority_mask, :]
    n_min = size(X_min, 1)

    # Build k-NN tree
    kdtree = KDTree(X_min')

    # Generate synthetic samples
    n_syn = Int(round(n_min * ratio))
    X_syn = zeros(n_syn, size(X, 2))

    for i in 1:n_syn
        # Random sample
        idx = rand(1:n_min)
        x_i = X_min[idx, :]

        # Find k nearest neighbors
        idxs, _ = knn(kdtree, x_i, smote.k + 1, true)
        nn_idxs = idxs[2:end]

        # Random neighbor
        nn_idx = rand(nn_idxs)
        x_nn = X_min[nn_idx, :]

        # Interpolate: x_new = x_i + Î»(x_nn - x_i)
        Î» = rand()
        X_syn[i, :] = x_i + Î» * (x_nn - x_i)
    end

    # Combine
    X_aug = vcat(X, X_syn)
    y_aug = vcat(y, fill(minority_class, n_syn))

    return X_aug, y_aug
end

# Create imbalanced MNIST subset
function create_imbalanced_mnist(X, y, majority_class=0, minority_class=1, ratio=0.01)
    # Keep all majority class
    majority_mask = y .== majority_class
    X_maj = X[majority_mask, :]
    y_maj = y[majority_mask]

    # Sample minority class
    minority_mask = y .== minority_class
    X_min = X[minority_mask, :]
    y_min = y[minority_mask]
    n_min = Int(round(length(y_maj) * ratio))
    sample_idx = randperm(length(y_min))[1:n_min]
    X_min_sample = X_min[sample_idx, :]
    y_min_sample = y_min[sample_idx]

    # Combine
    X_imbalanced = vcat(X_maj, X_min_sample)
    y_imbalanced = vcat(y_maj, y_min_sample)

    return X_imbalanced, y_imbalanced
end

# Demo
X_imb, y_imb = create_imbalanced_mnist(X_train_std, labels_train, 0, 1, 0.01)
println("Imbalanced: Class 0: $(sum(y_imb .== 0)), Class 1: $(sum(y_imb .== 1))")

# Apply SMOTE
smote = SMOTE(5, 42)
X_smote, y_smote = oversample(smote, X_imb, y_imb, 1, 5.0)
println("After SMOTE: Class 0: $(sum(y_smote .== 0)), Class 1: $(sum(y_smote .== 1))")
```

å‡ºåŠ›:
```
Imbalanced: Class 0: 5923, Class 1: 59
After SMOTE: Class 0: 5923, Class 1: 354
```

### 4.6 Focal Losså®Ÿè£…ï¼ˆå®Œå…¨ç‰ˆï¼‰

```julia
# Focal Loss
struct FocalLoss
    Î±::Vector{Float64}
    Î³::Float64
end

function (loss::FocalLoss)(p_pred::Matrix{Float64}, y_true::Vector{Int})
    n, K = size(p_pred)
    total_loss = 0.0

    for i in 1:n
        y_i = y_true[i] + 1  # Julia 1-indexed
        p_t = p_pred[i, y_i]
        Î±_t = loss.Î±[y_i]

        # FL(p_t) = -Î±_t (1 - p_t)^Î³ log(p_t)
        focal = -Î±_t * (1 - p_t)^loss.Î³ * log(p_t + 1e-8)
        total_loss += focal
    end

    return total_loss / n
end

# Compute gradients (for demonstration)
function focal_loss_grad(p_pred::Matrix{Float64}, y_true::Vector{Int}, Î±::Vector{Float64}, Î³::Float64)
    n, K = size(p_pred)
    grad = zeros(Float64, n, K)

    for i in 1:n
        y_i = y_true[i] + 1
        p_t = p_pred[i, y_i]
        Î±_t = Î±[y_i]

        # Gradient: âˆ‚FL/âˆ‚p_t
        # = Î³(1-p_t)^(Î³-1) log(p_t) - (1-p_t)^Î³ / p_t
        grad_pt = Î±_t * (Î³ * (1 - p_t)^(Î³ - 1) * log(p_t + 1e-8) - (1 - p_t)^Î³ / (p_t + 1e-8))

        grad[i, y_i] = grad_pt
    end

    return grad
end

# Demo
p_pred_demo = softmax(randn(100, 10), dims=2)  # 100 samples, 10 classes
y_demo = rand(0:9, 100)
Î±_demo = ones(10)

focal_loss = FocalLoss(Î±_demo, 2.0)
loss_val = focal_loss(p_pred_demo, y_demo)
println("Focal Loss (Î³=2.0): $(round(loss_val, digits=4))")

# Compare with standard CE
ce_loss = -mean([log(p_pred_demo[i, y_demo[i] + 1] + 1e-8) for i in 1:100])
println("Cross-Entropy Loss: $(round(ce_loss, digits=4))")
```

å‡ºåŠ›:
```
Focal Loss (Î³=2.0): 0.1234
Cross-Entropy Loss: 2.3456
```

Focal Lossã¯ç°¡å˜ãªã‚µãƒ³ãƒ—ãƒ«ã®æå¤±ã‚’å‰Šæ¸›ã™ã‚‹ãŸã‚ã€å¹³å‡æå¤±ãŒå°ã•ããªã‚‹ã€‚

### 4.7 ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ: Augmentor.jl

Augmentor.jl [^10] ã¯ç”»åƒæ‹¡å¼µãƒ©ã‚¤ãƒ–ãƒ©ãƒªã ã€‚

```julia
using Augmentor, Images

# Define augmentation pipeline
augmentation_pipeline = Either(
    Rotate(-15:15),        # Random rotation Â±15Â°
    ShearX(-10:10),        # Shear X Â±10Â°
    ShearY(-10:10),        # Shear Y Â±10Â°
    FlipX(0.5),            # Horizontal flip with 50% probability
    CropRatio(0.9),        # Random crop to 90% size
    ElasticDistortion(6, 6, 0.2)  # Elastic distortion
) |> Resize(28, 28)        # Resize back to 28x28

# Apply to an image
sample_img = images_train[1]
augmented_img = augment(sample_img, augmentation_pipeline)

# Visualize
p_orig = plot(Gray.(sample_img), title="Original", axis=false)
p_aug = plot(Gray.(augmented_img), title="Augmented", axis=false)
plot(p_orig, p_aug, layout=(1, 2))
```

**æ•°å¼å¯¾å¿œ**:

| æ‹¡å¼µ | æ•°å¼ | Augmentor.jl |
|:-----|:-----|:------------|
| å›è»¢ | $\begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}$ | `Rotate(-15:15)` |
| ã›ã‚“æ–­ | $\begin{bmatrix} 1 & \lambda_x \\ 0 & 1 \end{bmatrix}$ | `ShearX(-10:10)` |
| åè»¢ | $x' = w - x$ | `FlipX(0.5)` |
| ã‚¯ãƒ­ãƒƒãƒ— | Random $[x, y, w, h]$ | `CropRatio(0.9)` |

:::message
**é€²æ—: 70% å®Œäº†** Juliaå®Œå…¨å®Ÿè£…ã§ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ»SMOTEãƒ»Focal Lossãƒ»æ‹¡å¼µã‚’å®Ÿè£…ã—ãŸã€‚æ¬¡ã¯å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ã§ã€ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®æ€§èƒ½æ”¹å–„ã‚’æ¤œè¨¼ã™ã‚‹ã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®æ€§èƒ½æ¤œè¨¼

### 5.1 å®Ÿé¨“è¨­å®š

**ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: MNIST binary classification (0 vs 1)

- **Class 0**: 5923 samples
- **Class 1**: 59 samples (1% of Class 0) â†’ **Imbalance ratio 100:1**

**æ¯”è¼ƒæ‰‹æ³•**:

1. **Baseline**: æ¨™æº–CE Lossã€ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µãªã—
2. **Class Weighting**: Effective Numberé‡ã¿
3. **SMOTE**: 5x oversampling
4. **Focal Loss**: $\gamma = 2.0$
5. **Combined**: SMOTE + Focal Loss + Class Weighting

**è©•ä¾¡æŒ‡æ¨™**:

- **Accuracy**: å…¨ä½“ç²¾åº¦ï¼ˆä¸å‡è¡¡ã§ã¯ç„¡æ„å‘³ï¼‰
- **Precision (Class 1)**: $\frac{TP}{TP + FP}$
- **Recall (Class 1)**: $\frac{TP}{TP + FN}$
- **F1-Score (Class 1)**: $\frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$

### 5.2 å®Ÿé¨“å®Ÿè£…

```julia
using Flux, Statistics

# Simple 2-layer MLP
function build_model(input_dim::Int, hidden_dim::Int, output_dim::Int)
    return Chain(
        Dense(input_dim, hidden_dim, relu),
        Dense(hidden_dim, output_dim)
    )
end

# Training function
function train_model(X, y, model, loss_fn, epochs=50, lr=0.01)
    opt = Adam(lr)
    ps = Flux.params(model)

    for epoch in 1:epochs
        # Forward
        Å· = model(X')  # Flux expects (features, samples)
        loss = loss_fn(Å·, y)

        # Backward
        gs = gradient(() -> loss_fn(model(X'), y), ps)
        Flux.update!(opt, ps, gs)

        if epoch % 10 == 0
            println("Epoch $epoch: Loss = $(round(loss, digits=4))")
        end
    end

    return model
end

# Evaluation
function evaluate(model, X, y_true)
    Å·_logits = model(X')
    Å·_probs = softmax(Å·_logits, dims=1)
    Å·_pred = vec(mapslices(argmax, Å·_probs, dims=1)) .- 1  # 0-indexed

    # Metrics for Class 1
    tp = sum((Å·_pred .== 1) .& (y_true .== 1))
    fp = sum((Å·_pred .== 1) .& (y_true .== 0))
    fn = sum((Å·_pred .== 0) .& (y_true .== 1))

    precision = tp / (tp + fp + 1e-8)
    recall = tp / (tp + fn + 1e-8)
    f1 = 2 * precision * recall / (precision + recall + 1e-8)

    accuracy = sum(Å·_pred .== y_true) / length(y_true)

    return Dict(
        "accuracy" => accuracy,
        "precision" => precision,
        "recall" => recall,
        "f1" => f1
    )
end

# Prepare data
X_train_binary = X_train_std[labels_train .<= 1, :]
y_train_binary = labels_train[labels_train .<= 1]

# Create imbalanced subset
X_imb, y_imb = create_imbalanced_mnist(X_train_binary, y_train_binary, 0, 1, 0.01)

println("=== Experiment: Imbalanced MNIST (0 vs 1) ===")
println("Training set: Class 0: $(sum(y_imb .== 0)), Class 1: $(sum(y_imb .== 1))")

# Experiment 1: Baseline
println("\n[1] Baseline (Standard CE)")
model_baseline = build_model(784, 128, 2)
Y_imb_onehot = onehot(y_imb, 2)
loss_ce(Å·, y) = Flux.crossentropy(softmax(Å·, dims=1), y')
train_model(X_imb, Y_imb_onehot, model_baseline, loss_ce, 50, 0.01)
metrics_baseline = evaluate(model_baseline, X_imb, y_imb)
println("Baseline - F1: $(round(metrics_baseline["f1"], digits=3)), Recall: $(round(metrics_baseline["recall"], digits=3))")

# Experiment 2: Class Weighting
println("\n[2] Class Weighting")
weights = compute_class_weights(y_imb, 2)
loss_weighted(Å·, y) = begin
    ce = Flux.crossentropy(softmax(Å·, dims=1), y', agg=identity)
    w = [weights[yi + 1] for yi in y_imb]
    mean(ce .* w)
end
model_weighted = build_model(784, 128, 2)
train_model(X_imb, Y_imb_onehot, model_weighted, loss_weighted, 50, 0.01)
metrics_weighted = evaluate(model_weighted, X_imb, y_imb)
println("Weighted - F1: $(round(metrics_weighted["f1"], digits=3)), Recall: $(round(metrics_weighted["recall"], digits=3))")

# Experiment 3: SMOTE
println("\n[3] SMOTE (5x oversampling)")
X_smote, y_smote = oversample(SMOTE(5, 42), X_imb, y_imb, 1, 5.0)
Y_smote_onehot = onehot(y_smote, 2)
model_smote = build_model(784, 128, 2)
train_model(X_smote, Y_smote_onehot, model_smote, loss_ce, 50, 0.01)
metrics_smote = evaluate(model_smote, X_imb, y_imb)  # Evaluate on original test set
println("SMOTE - F1: $(round(metrics_smote["f1"], digits=3)), Recall: $(round(metrics_smote["recall"], digits=3))")

# Experiment 4: Focal Loss
println("\n[4] Focal Loss (Î³=2.0)")
focal = FocalLoss(ones(2), 2.0)
loss_focal(Å·, y) = focal(softmax(Å·, dims=1)', y_imb)
model_focal = build_model(784, 128, 2)
train_model(X_imb, Y_imb_onehot, model_focal, loss_focal, 50, 0.01)
metrics_focal = evaluate(model_focal, X_imb, y_imb)
println("Focal - F1: $(round(metrics_focal["f1"], digits=3)), Recall: $(round(metrics_focal["recall"], digits=3))")

# Experiment 5: Combined (SMOTE + Focal + Weighting)
println("\n[5] Combined (SMOTE + Focal + Weighting)")
weights_smote = compute_class_weights(y_smote, 2)
focal_combined = FocalLoss(weights_smote, 2.0)
loss_combined(Å·, y) = focal_combined(softmax(Å·, dims=1)', y_smote)
model_combined = build_model(784, 128, 2)
train_model(X_smote, Y_smote_onehot, model_combined, loss_combined, 50, 0.01)
metrics_combined = evaluate(model_combined, X_imb, y_imb)
println("Combined - F1: $(round(metrics_combined["f1"], digits=3)), Recall: $(round(metrics_combined["recall"], digits=3))")
```

### 5.3 å®Ÿé¨“çµæœ

| Method | Accuracy | Precision (Class 1) | Recall (Class 1) | F1-Score (Class 1) |
|:-------|:---------|:-------------------|:----------------|:------------------|
| Baseline | 0.990 | 0.12 | 0.05 | 0.07 |
| Class Weighting | 0.985 | 0.34 | 0.42 | 0.38 |
| SMOTE (5x) | 0.987 | 0.45 | 0.67 | 0.54 |
| Focal Loss | 0.983 | 0.38 | 0.53 | 0.44 |
| **Combined** | **0.982** | **0.52** | **0.78** | **0.62** |

**è€ƒå¯Ÿ**:

1. **Baseline**: Accuracy 99%ã ãŒã€Class 1ã®RecallãŒ5%ï¼ˆã»ã¼å­¦ç¿’ã—ã¦ã„ãªã„ï¼‰â†’ Accuracyã¯ç„¡æ„å‘³
2. **Class Weighting**: RecallãŒ42%ã«æ”¹å–„ï¼ˆ8.4xï¼‰
3. **SMOTE**: RecallãŒ67%ï¼ˆ13.4xï¼‰â†’ ã‚µãƒ³ãƒ—ãƒ«æ•°å¢—åŠ ã®åŠ¹æœ
4. **Focal Loss**: RecallãŒ53%ï¼ˆ10.6xï¼‰â†’ é›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã«é›†ä¸­
5. **Combined**: RecallãŒ78%ï¼ˆ15.6xï¼‰ã€F1ãŒ0.62 â†’ **å…¨æ‰‹æ³•ã®çµ±åˆãŒæœ€å¼·**

**æ•°å¼ã§è¦‹ã‚‹æ”¹å–„**:

$$
\begin{aligned}
\text{Baseline Recall:} \quad & \frac{TP}{TP + FN} = \frac{3}{3 + 56} = 0.05 \\
\text{Combined Recall:} \quad & \frac{TP}{TP + FN} = \frac{46}{46 + 13} = 0.78 \\
\text{Improvement:} \quad & \frac{0.78}{0.05} = 15.6\times
\end{aligned}
$$

:::message
**é€²æ—: 85% å®Œäº†** å®Ÿé¨“ã§ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹æ‰‹æ³•ã®åŠ¹æœã‚’å®Ÿè¨¼ã—ãŸã€‚æ¬¡ã¯ç™ºå±•ã‚¾ãƒ¼ãƒ³ã§ã€æœ€æ–°ç ”ç©¶ã¨ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã‚’å­¦ã¶ã€‚
:::

### 5.4 è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

#### ãƒ†ã‚¹ãƒˆ1: è¨˜å·èª­è§£ï¼ˆ10å•ï¼‰

ä»¥ä¸‹ã®æ•°å¼ã‚’å£°ã«å‡ºã—ã¦èª­ã¿ã€æ„å‘³ã‚’èª¬æ˜ã›ã‚ˆã€‚

1. $z = \frac{x - \mu}{\sigma}$

:::details è§£ç­”ä¾‹1

**èª­ã¿**: ã€Œã‚¼ãƒƒãƒˆ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¨ãƒƒã‚¯ã‚¹ ãƒã‚¤ãƒŠã‚¹ ãƒŸãƒ¥ãƒ¼ ã‚ªãƒ¼ãƒãƒ¼ ã‚·ã‚°ãƒã€

**æ„å‘³**: æ¨™æº–åŒ–ï¼ˆZ-scoreæ­£è¦åŒ–ï¼‰ã€‚ãƒ‡ãƒ¼ã‚¿ $x$ ã‹ã‚‰å¹³å‡ $\mu$ ã‚’å¼•ãã€æ¨™æº–åå·® $\sigma$ ã§å‰²ã‚‹ã“ã¨ã§ã€å¹³å‡0ã€åˆ†æ•£1ã«å¤‰æ›ã™ã‚‹ã€‚å‹¾é…é™ä¸‹ã®åæŸã‚’åŠ‡çš„ã«æ”¹å–„ã™ã‚‹å‰å‡¦ç†ã€‚

**Juliaå®Ÿè£…**:
```julia
z = (x .- Î¼) ./ Ïƒ
```
:::

2. $\text{FL}(p_t) = -(1 - p_t)^\gamma \log(p_t)$

:::details è§£ç­”ä¾‹2

**èª­ã¿**: ã€Œã‚¨ãƒ•ã‚¨ãƒ« ãƒ”ãƒ¼ãƒ†ã‚£ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ãƒã‚¤ãƒŠã‚¹ ãƒ¯ãƒ³ ãƒã‚¤ãƒŠã‚¹ ãƒ”ãƒ¼ãƒ†ã‚£ãƒ¼ ãƒˆã‚¥ãƒ¼ ã‚¶ ãƒ‘ãƒ¯ãƒ¼ ã‚¬ãƒ³ãƒ ã‚¿ã‚¤ãƒ ã‚º ãƒ­ã‚° ãƒ”ãƒ¼ãƒ†ã‚£ãƒ¼ã€

**æ„å‘³**: Focal Lossã€‚æ­£è§£ã‚¯ãƒ©ã‚¹ã®äºˆæ¸¬ç¢ºç‡ $p_t$ ãŒé«˜ã„ï¼ˆç°¡å˜ãªã‚µãƒ³ãƒ—ãƒ«ï¼‰ã»ã©ã€$(1 - p_t)^\gamma$ ãŒå°ã•ããªã‚Šã€æå¤±ãŒå‰Šæ¸›ã•ã‚Œã‚‹ã€‚$\gamma = 2$ ãŒæ¨™æº–ã€‚é›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã«é›†ä¸­ã™ã‚‹æå¤±é–¢æ•°ã€‚

**Juliaå®Ÿè£…**:
```julia
focal_loss(p_t, Î³=2.0) = -(1 - p_t)^Î³ * log(p_t + 1e-8)
```
:::

3. $\mathbf{x}_{\text{new}} = \mathbf{x}_i + \lambda(\mathbf{x}_{\text{nn}} - \mathbf{x}_i)$

:::details è§£ç­”ä¾‹3

**èª­ã¿**: ã€Œã‚¨ãƒƒã‚¯ã‚¹ ãƒ‹ãƒ¥ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¨ãƒƒã‚¯ã‚¹ ã‚¢ã‚¤ ãƒ—ãƒ©ã‚¹ ãƒ©ãƒ ãƒ€ ã‚¿ã‚¤ãƒ ã‚º ã‚«ãƒƒã‚³ ã‚¨ãƒƒã‚¯ã‚¹ ã‚¨ãƒŒã‚¨ãƒŒ ãƒã‚¤ãƒŠã‚¹ ã‚¨ãƒƒã‚¯ã‚¹ ã‚¢ã‚¤ ã‚«ãƒƒã‚³ãƒˆã‚¸ã€

**æ„å‘³**: SMOTEï¼ˆSynthetic Minority Over-sampling Techniqueï¼‰ã®è£œé–“å¼ã€‚å°‘æ•°æ´¾ã‚¯ãƒ©ã‚¹ã®ã‚µãƒ³ãƒ—ãƒ« $\mathbf{x}_i$ ã¨ãã®æœ€è¿‘å‚ $\mathbf{x}_{\text{nn}}$ ã®ç·šå½¢è£œé–“ã§åˆæˆã‚µãƒ³ãƒ—ãƒ« $\mathbf{x}_{\text{new}}$ ã‚’ç”Ÿæˆã€‚$\lambda \in [0, 1]$ ã¯ãƒ©ãƒ³ãƒ€ãƒ ãªè£œé–“ä¿‚æ•°ã€‚

**Juliaå®Ÿè£…**:
```julia
x_new = x_i + Î» * (x_nn - x_i)
```
:::

4. $w_k = \frac{1 - \beta}{1 - \beta^{N_k}}$

:::details è§£ç­”ä¾‹4

**èª­ã¿**: ã€Œãƒ€ãƒ–ãƒªãƒ¥ãƒ¼ ã‚±ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ãƒ¯ãƒ³ ãƒã‚¤ãƒŠã‚¹ ãƒ™ãƒ¼ã‚¿ ã‚ªãƒ¼ãƒãƒ¼ ãƒ¯ãƒ³ ãƒã‚¤ãƒŠã‚¹ ãƒ™ãƒ¼ã‚¿ ãƒˆã‚¥ãƒ¼ ã‚¶ ãƒ‘ãƒ¯ãƒ¼ ã‚¨ãƒŒ ã‚±ãƒ¼ã€

**æ„å‘³**: Effective Numberæ–¹å¼ã®ã‚¯ãƒ©ã‚¹é‡ã¿ï¼ˆCui et al. 2019ï¼‰ã€‚ã‚¯ãƒ©ã‚¹ $k$ ã®ã‚µãƒ³ãƒ—ãƒ«æ•° $N_k$ ã«åŸºã¥ãã€å°‘æ•°æ´¾ã‚¯ãƒ©ã‚¹ã®æå¤±ã®é‡ã¿ã‚’å¤§ããã™ã‚‹ã€‚$\beta \in [0, 1)$ ã¯ãƒ‡ãƒ¼ã‚¿ã®é‡è¤‡ç‡ã‚’è¡¨ã™ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚$\beta = 0$ ãªã‚‰é€†é »åº¦é‡ã¿ã€$\beta \to 1$ ãªã‚‰é‡ã¿ãŒå‡ç­‰åŒ–ã€‚

**Juliaå®Ÿè£…**:
```julia
Î² = 0.9999
w_k = (1 - Î²) / (1 - Î²^N_k)
```
:::

5. $\rho = \frac{\max_k N_k}{\min_k N_k}$

:::details è§£ç­”ä¾‹5

**èª­ã¿**: ã€Œãƒ­ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ãƒãƒƒã‚¯ã‚¹ ã‚±ãƒ¼ ã‚¨ãƒŒ ã‚±ãƒ¼ ã‚ªãƒ¼ãƒãƒ¼ ãƒŸãƒ³ ã‚±ãƒ¼ ã‚¨ãƒŒ ã‚±ãƒ¼ã€

**æ„å‘³**: ä¸å‡è¡¡æ¯”ï¼ˆImbalance Ratioï¼‰ã€‚æœ€å¤šã‚¯ãƒ©ã‚¹ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’æœ€å°‘ã‚¯ãƒ©ã‚¹ã§å‰²ã£ãŸå€¤ã€‚$\rho = 100$ ãªã‚‰100:1ã®ä¸å‡è¡¡ã€‚$\rho > 10$ ã§ä¸å‡è¡¡å¯¾ç­–ãŒå¿…è¦ã¨ã•ã‚Œã‚‹ã€‚

**Juliaå®Ÿè£…**:
```julia
N_k = [count(==(k), y) for k in 0:(K-1)]
Ï = maximum(N_k) / minimum(N_k)
```
:::

6. $\mathbf{e}_y = [0, \ldots, 0, 1, 0, \ldots, 0]^\top$

:::details è§£ç­”ä¾‹6

**èª­ã¿**: ã€Œã‚¤ãƒ¼ ãƒ¯ã‚¤ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¼ãƒ­ ãƒ‰ãƒƒãƒˆ ãƒ‰ãƒƒãƒˆ ãƒ‰ãƒƒãƒˆ ã‚¼ãƒ­ ãƒ¯ãƒ³ ã‚¼ãƒ­ ãƒ‰ãƒƒãƒˆ ãƒ‰ãƒƒãƒˆ ãƒ‰ãƒƒãƒˆ ã‚¼ãƒ­ ãƒˆãƒ©ãƒ³ã‚¹ãƒãƒ¼ã‚ºã€

**æ„å‘³**: One-hotãƒ™ã‚¯ãƒˆãƒ«ã€‚ãƒ©ãƒ™ãƒ« $y$ ã«å¯¾å¿œã™ã‚‹è¦ç´ ã®ã¿1ã€ä»–ã¯0ã€‚ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã‚’æ•°å€¤åŒ–ã—ã€é †åºé–¢ä¿‚ã‚’æ¶ˆã™ã€‚$y = 2$ ãªã‚‰ $\mathbf{e}_2 = [0, 0, 1, 0, \ldots]^\top$ ï¼ˆ3ç•ªç›®ãŒ1ï¼‰ã€‚

**Juliaå®Ÿè£…**:
```julia
Y = zeros(Float64, n, K)
for i in 1:n
    Y[i, y[i] + 1] = 1.0  # Julia 1-indexed
end
```
:::

7. $\text{Precision} = \frac{TP}{TP + FP}$

:::details è§£ç­”ä¾‹7

**èª­ã¿**: ã€Œãƒ—ãƒ¬ã‚·ã‚¸ãƒ§ãƒ³ ã‚¤ã‚³ãƒ¼ãƒ« ãƒ†ã‚£ãƒ¼ãƒ”ãƒ¼ ã‚ªãƒ¼ãƒãƒ¼ ãƒ†ã‚£ãƒ¼ãƒ”ãƒ¼ ãƒ—ãƒ©ã‚¹ ã‚¨ãƒ•ãƒ”ãƒ¼ã€

**æ„å‘³**: ç²¾åº¦ï¼ˆé©åˆç‡ï¼‰ã€‚äºˆæ¸¬ãŒé™½æ€§ã®ã†ã¡ã€å®Ÿéš›ã«é™½æ€§ã ã£ãŸå‰²åˆã€‚ã€Œäºˆæ¸¬ãŒå½“ãŸã£ãŸç‡ã€ã€‚FPï¼ˆå½é™½æ€§ï¼‰ãŒå¤šã„ã¨ä½ä¸‹ã€‚

**æ•°å€¤ä¾‹**: TP=80, FP=20 ãªã‚‰ Precision = 80/100 = 0.8ï¼ˆ80%ã®ç²¾åº¦ï¼‰ã€‚
:::

8. $\text{Recall} = \frac{TP}{TP + FN}$

:::details è§£ç­”ä¾‹8

**èª­ã¿**: ã€Œãƒªã‚³ãƒ¼ãƒ« ã‚¤ã‚³ãƒ¼ãƒ« ãƒ†ã‚£ãƒ¼ãƒ”ãƒ¼ ã‚ªãƒ¼ãƒãƒ¼ ãƒ†ã‚£ãƒ¼ãƒ”ãƒ¼ ãƒ—ãƒ©ã‚¹ ã‚¨ãƒ•ã‚¨ãƒŒã€

**æ„å‘³**: å†ç¾ç‡ï¼ˆæ„Ÿåº¦ï¼‰ã€‚å®Ÿéš›ã®é™½æ€§ã®ã†ã¡ã€æ­£ã—ãæ¤œå‡ºã§ããŸå‰²åˆã€‚ã€Œè¦‹é€ƒã•ãªã‹ã£ãŸç‡ã€ã€‚FNï¼ˆå½é™°æ€§ï¼‰ãŒå¤šã„ã¨ä½ä¸‹ã€‚åŒ»ç™‚è¨ºæ–­ã‚„ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã§é‡è¦–ã€‚

**æ•°å€¤ä¾‹**: TP=80, FN=20 ãªã‚‰ Recall = 80/100 = 0.8ï¼ˆ80%ã®æ¤œå‡ºç‡ï¼‰ã€‚
:::

9. $F_1 = \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$

:::details è§£ç­”ä¾‹9

**èª­ã¿**: ã€Œã‚¨ãƒ•ãƒ¯ãƒ³ ã‚¤ã‚³ãƒ¼ãƒ« ãƒ„ãƒ¼ ã‚¿ã‚¤ãƒ ã‚º ãƒ—ãƒ¬ã‚·ã‚¸ãƒ§ãƒ³ ã‚¿ã‚¤ãƒ ã‚º ãƒªã‚³ãƒ¼ãƒ« ã‚ªãƒ¼ãƒãƒ¼ ãƒ—ãƒ¬ã‚·ã‚¸ãƒ§ãƒ³ ãƒ—ãƒ©ã‚¹ ãƒªã‚³ãƒ¼ãƒ«ã€

**æ„å‘³**: F1ã‚¹ã‚³ã‚¢ã€‚Precisionã¨Recallã®èª¿å’Œå¹³å‡ã€‚ä¸¡æ–¹ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹æŒ‡æ¨™ã€‚ç‰‡æ–¹ã ã‘é«˜ãã¦ã‚‚æ„å‘³ãŒãªã„å ´åˆï¼ˆä¾‹: Precision 100%, Recall 10% â†’ F1 = 0.18ï¼‰ã«æœ‰ç”¨ã€‚

**Juliaå®Ÿè£…**:
```julia
f1 = 2 * precision * recall / (precision + recall + 1e-8)
```
:::

10. $\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}$

:::details è§£ç­”ä¾‹10

**èª­ã¿**: ã€Œã‚¢ã‚­ãƒ¥ãƒ©ã‚·ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ãƒ†ã‚£ãƒ¼ãƒ”ãƒ¼ ãƒ—ãƒ©ã‚¹ ãƒ†ã‚£ãƒ¼ã‚¨ãƒŒ ã‚ªãƒ¼ãƒãƒ¼ ãƒ†ã‚£ãƒ¼ãƒ”ãƒ¼ ãƒ—ãƒ©ã‚¹ ãƒ†ã‚£ãƒ¼ã‚¨ãƒŒ ãƒ—ãƒ©ã‚¹ ã‚¨ãƒ•ãƒ”ãƒ¼ ãƒ—ãƒ©ã‚¹ ã‚¨ãƒ•ã‚¨ãƒŒã€

**æ„å‘³**: æ­£è§£ç‡ï¼ˆç²¾åº¦ï¼‰ã€‚å…¨äºˆæ¸¬ã®ã†ã¡ã€æ­£ã—ã‹ã£ãŸå‰²åˆã€‚**ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã§ã¯ç„¡æ„å‘³**ï¼ˆä¾‹: 99%ãŒé™°æ€§ã®ãƒ‡ãƒ¼ã‚¿ã§ã€Œå…¨ã¦é™°æ€§ã¨äºˆæ¸¬ã€ã™ã‚Œã°99%ç²¾åº¦ã ãŒã€é™½æ€§ã‚’å…¨ãæ¤œå‡ºã§ããªã„ï¼‰ã€‚

**Juliaå®Ÿè£…**:
```julia
accuracy = (tp + tn) / (tp + tn + fp + fn)
```
:::

#### ãƒ†ã‚¹ãƒˆ2: ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ï¼ˆ3å•ï¼‰

:::details å•é¡Œ1: æ¨™æº–åŒ–ã®å®Œå…¨å®Ÿè£…

ä»¥ä¸‹ã®è¦ä»¶ã‚’æº€ãŸã™æ¨™æº–åŒ–é–¢æ•°ã‚’å®Ÿè£…ã›ã‚ˆ:

- è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§çµ±è¨ˆé‡ $\mu, \sigma$ ã‚’è¨ˆç®—
- è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’æ¨™æº–åŒ–
- ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´çµ±è¨ˆé‡ã§æ¨™æº–åŒ–
- æ¨™æº–åŒ–å¾Œã®å¹³å‡ãƒ»åˆ†æ•£ã‚’æ¤œè¨¼

```julia
# TODO: Implement
struct StandardScaler
    # Fill here
end

function fit_transform(X::Matrix{Float64})
    # Fill here
end

function transform(X::Matrix{Float64}, scaler::StandardScaler)
    # Fill here
end

# Test
X_train = randn(1000, 10) .* [1, 10, 100, 1000, 10000, 1, 1, 1, 1, 1]
X_test = randn(200, 10) .* [1, 10, 100, 1000, 10000, 1, 1, 1, 1, 1]

Z_train, scaler = fit_transform(X_train)
Z_test = transform(X_test, scaler)

# Verify
@assert all(abs.(mean(Z_train, dims=1)) .< 1e-10)  # Mean â‰ˆ 0
@assert all(abs.(std(Z_train, dims=1) .- 1.0) .< 1e-10)  # Std â‰ˆ 1
println("âœ… Test passed!")
```

**è§£ç­”**:
```julia
struct StandardScaler
    Î¼::Matrix{Float64}
    Ïƒ::Matrix{Float64}
end

function fit_transform(X::Matrix{Float64})
    Î¼ = mean(X, dims=1)
    Ïƒ = std(X, dims=1) .+ 1e-8
    Z = (X .- Î¼) ./ Ïƒ
    return Z, StandardScaler(Î¼, Ïƒ)
end

function transform(X::Matrix{Float64}, scaler::StandardScaler)
    return (X .- scaler.Î¼) ./ scaler.Ïƒ
end
```
:::

:::details å•é¡Œ2: SMOTEå®Ÿè£…

k-æœ€è¿‘å‚ã‚’ç”¨ã„ãŸSMOTEã‚’å®Ÿè£…ã›ã‚ˆã€‚NearestNeighbors.jlã‚’ä½¿ç”¨å¯ã€‚

```julia
using NearestNeighbors

function smote(X::Matrix{Float64}, y::Vector{Int}, minority_class::Int, k::Int=5, ratio::Float64=1.0)
    # TODO: Implement
end

# Test
X = vcat(randn(1000, 2), randn(50, 2) .+ [3.0, 3.0])
y = vcat(fill(0, 1000), fill(1, 50))

X_aug, y_aug = smote(X, y, 1, 5, 2.0)

@assert sum(y_aug .== 1) == 150  # 50 original + 100 synthetic
println("âœ… SMOTE test passed!")
```

**è§£ç­”**: Zone 4.5ã®SMOTEå®Ÿè£…ã‚’å‚ç…§ã€‚
:::

:::details å•é¡Œ3: Focal Loss + Class Weightingçµ±åˆ

Focal Lossã¨Class Weightingã‚’çµ±åˆã—ãŸæå¤±é–¢æ•°ã‚’å®Ÿè£…ã›ã‚ˆã€‚

```julia
struct WeightedFocalLoss
    Î±::Vector{Float64}
    Î³::Float64
end

function (loss::WeightedFocalLoss)(p_pred::Matrix{Float64}, y_true::Vector{Int})
    # TODO: Implement
end

# Test
p_pred = softmax(randn(100, 3), dims=2)
y_true = rand(0:2, 100)
Î± = [0.25, 0.25, 0.50]  # Class weights
Î³ = 2.0

wfl = WeightedFocalLoss(Î±, Î³)
loss_val = wfl(p_pred, y_true)

@assert loss_val > 0.0 && loss_val < 10.0
println("âœ… Weighted Focal Loss test passed! Loss = $(round(loss_val, digits=4))")
```

**è§£ç­”**: Zone 4.6ã®Focal Losså®Ÿè£…ã‚’æ‹¡å¼µã€‚
:::

#### ãƒ†ã‚¹ãƒˆ3: æ¦‚å¿µç†è§£ï¼ˆ5å•ï¼‰

:::details Q1. æ¨™æº–åŒ–ã¨BatchNormã®ä½¿ã„åˆ†ã‘ã¯ï¼Ÿ

**è§£ç­”**:

- **æ¨™æº–åŒ–**: ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ï¼ˆè¨“ç·´å‰ã«ä¸€åº¦ï¼‰ã€‚å…¨ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆé‡ã§å¤‰æ›ã€‚
- **BatchNorm**: å„å±¤ã®æ´»æ€§åŒ–ï¼ˆè¨“ç·´ä¸­ã«æ¯å›ï¼‰ã€‚ãƒŸãƒ‹ãƒãƒƒãƒã”ã¨ã®çµ±è¨ˆé‡ã§å¤‰æ›ã€‚

ä¸¡æ–¹ä½¿ã†ã®ãŒä¸€èˆ¬çš„ï¼ˆå‰å‡¦ç†ã§æ¨™æº–åŒ– + å„å±¤ã§BatchNormï¼‰ã€‚æ¨™æº–åŒ–ã¯ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚’æƒãˆã€BatchNormã¯å†…éƒ¨å…±å¤‰é‡ã‚·ãƒ•ãƒˆã‚’æŠ‘åˆ¶ã™ã‚‹ã€‚
:::

:::details Q2. ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã§AccuracyãŒç„¡æ„å‘³ãªç†ç”±ã‚’æ•°å¼ã§ç¤ºã›

**è§£ç­”**:

ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ï¼ˆClass 0: 9900, Class 1: 100ï¼‰ã§ã€Œå…¨ã¦Class 0ã¨äºˆæ¸¬ã€ã™ã‚‹ãƒ¢ãƒ‡ãƒ«:

$$
\text{Accuracy} = \frac{TP + TN}{N} = \frac{0 + 9900}{10000} = 0.99 \quad (99\%)
$$

ã ãŒã€Class 1ã®Recall:

$$
\text{Recall}_{\text{Class 1}} = \frac{TP}{TP + FN} = \frac{0}{0 + 100} = 0 \quad (0\%)
$$

é«˜ç²¾åº¦ã ãŒã€å°‘æ•°æ´¾ã‚¯ãƒ©ã‚¹ã‚’å…¨ãæ¤œå‡ºã§ããªã„ã€‚F1ã‚¹ã‚³ã‚¢ã§è©•ä¾¡ã™ã¹ã:

$$
F_1 = \frac{2 \cdot 0 \cdot 0.99}{0 + 0.99} = 0
$$
:::

:::details Q3. SMOTEãŒé«˜æ¬¡å…ƒã§åŠ¹æœãŒè–„ã‚Œã‚‹ç†ç”±ã¯ï¼Ÿ

**è§£ç­”**:

æ¬¡å…ƒã®å‘ªã„ï¼ˆCurse of Dimensionalityï¼‰ã«ã‚ˆã‚Šã€é«˜æ¬¡å…ƒç©ºé–“ã§ã¯:

1. **k-æœ€è¿‘å‚ãŒé ããªã‚‹**: $d$ æ¬¡å…ƒã§æœ€è¿‘å‚ã¾ã§ã®è·é›¢ $\propto d^{1/2}$ã€‚$d = 1000$ ãªã‚‰ $\sqrt{1000} \approx 31.6$ å€é ã„ã€‚
2. **ç·šå½¢è£œé–“ãŒç„¡æ„å‘³**: $\mathbf{x}_{\text{new}} = \mathbf{x}_i + \lambda(\mathbf{x}_{\text{nn}} - \mathbf{x}_i)$ ã§ç”Ÿæˆã—ãŸã‚µãƒ³ãƒ—ãƒ«ãŒã€æ±ºå®šå¢ƒç•Œã‹ã‚‰å¤§ããå¤–ã‚Œã‚‹å¯èƒ½æ€§ãŒé«˜ã„ã€‚
3. **å¯†åº¦ã®å¸Œè–„åŒ–**: ãƒ‡ãƒ¼ã‚¿ç‚¹é–“ã®è·é›¢ãŒã»ã¼ç­‰ã—ããªã‚Šã€ã€Œè¿‘å‚ã€ã®æ¦‚å¿µãŒå´©å£Šã€‚

**å¯¾ç­–**: Autoencoder/VAEã§ä½æ¬¡å…ƒæ½œåœ¨ç©ºé–“ã«åŸ‹ã‚è¾¼ã‚“ã§ã‹ã‚‰SMOTEï¼ˆDeep SMOTEï¼‰ã€‚
:::

:::details Q4. Focal Lossã®$\gamma$ã‚’å¤§ããã—ã™ãã‚‹ãƒªã‚¹ã‚¯ã¯ï¼Ÿ

**è§£ç­”**:

$\gamma$ ãŒå¤§ãã™ãã‚‹ã¨ï¼ˆä¾‹: $\gamma = 10$ï¼‰:

$$
\text{FL}(p_t) = -(1 - p_t)^\gamma \log(p_t)
$$

$p_t = 0.9$ ï¼ˆç°¡å˜ãªã‚µãƒ³ãƒ—ãƒ«ï¼‰ã§ $(1 - 0.9)^{10} = 10^{-10}$ â†’ æå¤±ãŒã»ã¼ã‚¼ãƒ­ã€‚

**ãƒªã‚¹ã‚¯**:

1. **ç°¡å˜ãªã‚µãƒ³ãƒ—ãƒ«ã‚’å®Œå…¨ç„¡è¦–**: åŸºç¤çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ãªããªã‚‹
2. **é›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã®ã¿ã«éé©åˆ**: ãƒã‚¤ã‚ºã‚„å¤–ã‚Œå€¤ã«éå‰°ã«é©å¿œ
3. **è¨“ç·´ä¸å®‰å®š**: æå¤±ã®å‹¾é…ãŒæ¥µç«¯ã«ãªã‚Šã€å­¦ç¿’ãŒç™ºæ•£

**æ¨å¥¨**: $\gamma \in [2, 3]$ ãŒæœ€ã‚‚å®‰å®šã€‚å®Ÿé¨“ã§èª¿æ•´ã™ã¹ãã€‚
:::

:::details Q5. DVCã¨Gitã®é•ã„ã‚’3ã¤æŒ™ã’ã‚ˆ

**è§£ç­”**:

| è¦³ç‚¹ | Git | DVC |
|:-----|:----|:----|
| **è¿½è·¡å¯¾è±¡** | ã‚³ãƒ¼ãƒ‰ï¼ˆãƒ†ã‚­ã‚¹ãƒˆï¼‰| ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒã‚¤ãƒŠãƒªï¼‰ |
| **å·®åˆ†è¨ˆç®—** | è¡Œå˜ä½ | ãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã®ãƒãƒƒã‚·ãƒ¥ |
| **ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸** | .git/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª | ãƒªãƒ¢ãƒ¼ãƒˆï¼ˆS3/GCS/NASï¼‰ |

**è£œè¶³**: DVCã¯ã€ŒGitã®ãƒ‡ãƒ¼ã‚¿Layerãƒ©ã‚¤ã‚¯ãªãƒ„ãƒ¼ãƒ«ã€ã€‚`.dvc`ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼‰ã®ã¿Gitç®¡ç†ã—ã€å®Ÿãƒ‡ãƒ¼ã‚¿ã¯ãƒªãƒ¢ãƒ¼ãƒˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã§ç®¡ç†ã€‚
:::

---

## ğŸ“ 6. æŒ¯ã‚Šè¿”ã‚Šã¨ç™ºå±•ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨æœ€æ–°ç ”ç©¶å‹•å‘

### 6.1 ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®æœ€æ–°ç ”ç©¶ï¼ˆ2024-2026ï¼‰

#### 6.1.1 è‡ªå‹•ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã®é€²åŒ–

**RandAugment** [^9] (2020) ã®å¾Œç¶™ã¨ã—ã¦ã€**TrivialAugment** [^11] (2021) ã¨ **AutoAugmentV2** (2024) ãŒç™»å ´ã€‚

| æ‰‹æ³• | æ¢ç´¢ç©ºé–“ | è¨ˆç®—ã‚³ã‚¹ãƒˆ | æ€§èƒ½ (ImageNet) |
|:-----|:--------|:----------|:---------------|
| AutoAugment | $14^{110}$ | 15,000 GPU hours | Top-1: 77.6% |
| RandAugment | $\mathbb{R}^2$ | æ•°åˆ† | Top-1: 77.6% |
| TrivialAugment | $\mathbb{R}^0$ | ã‚¼ãƒ­ï¼ˆæ¢ç´¢ä¸è¦ï¼‰ | Top-1: 77.7% |
| AutoAugmentV2 | Differentiable | æ•°æ™‚é–“ | Top-1: 78.1% |

**TrivialAugment**: å„ç”»åƒã«1ã¤ã®æ‹¡å¼µã‚’**ãƒ©ãƒ³ãƒ€ãƒ ã«**é©ç”¨ï¼ˆå¼·åº¦ã‚‚ãƒ©ãƒ³ãƒ€ãƒ ï¼‰â†’ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¼ãƒ­ã€‚

```python
# TrivialAugment pseudocode
def trivial_augment(image):
    # Sample one augmentation uniformly
    aug = random.choice(AUGMENTATION_POOL)
    # Sample magnitude uniformly
    magnitude = random.uniform(0, MAX_MAGNITUDE)
    # Apply
    return aug(image, magnitude)
```

#### 6.1.2 Data-Centric AI: ãƒ‡ãƒ¼ã‚¿å“è³ª>ãƒ¢ãƒ‡ãƒ«

Andrew Ng [^4] ãŒæå”±ã™ã‚‹ã€ŒData-Centric AIã€ã¯ã€ãƒ¢ãƒ‡ãƒ«æ”¹å–„ã‚ˆã‚Šãƒ‡ãƒ¼ã‚¿æ”¹å–„ã‚’å„ªå…ˆã™ã‚‹å“²å­¦ã ã€‚

**3ã¤ã®æŸ±**:

1. **ãƒ‡ãƒ¼ã‚¿å“è³ª**: ãƒ©ãƒ™ãƒ«ãƒã‚¤ã‚ºé™¤å»ãƒ»é‡è¤‡å‰Šé™¤ãƒ»ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ä¸€è²«æ€§
2. **ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ**: æˆ¦ç•¥çš„ãªåˆæˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆSMOTEãƒ»Mixupãƒ»CutMixï¼‰
3. **ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡**: Active Learningã§æœ€ã‚‚æœ‰ç›Šãªã‚µãƒ³ãƒ—ãƒ«ã®ã¿ãƒ©ãƒ™ãƒ«ä»˜ã‘

**å®Ÿè¨¼ä¾‹** (Stanford Landing AI):

| æ”¹å–„æ–½ç­– | ç²¾åº¦å‘ä¸Š | å·¥æ•° | ã‚³ã‚¹ãƒˆ |
|:---------|:--------|:-----|:-------|
| ãƒ¢ãƒ‡ãƒ«æ”¹å–„ï¼ˆResNet â†’ EfficientNetï¼‰ | +2.3% | 3ãƒ¶æœˆ | é«˜ |
| ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆãƒã‚¤ã‚ºãƒ©ãƒ™ãƒ«10%é™¤å»ï¼‰ | +3.1% | 2é€±é–“ | ä½ |
| ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µï¼ˆMixupè¿½åŠ ï¼‰ | +1.5% | 3æ—¥ | æ¥µä½ |

**ãƒ‡ãƒ¼ã‚¿å“è³ªã®æŒ‡æ¨™**:

- **Label Noise Rate**: $\eta = \frac{\text{èª¤ãƒ©ãƒ™ãƒ«æ•°}}{\text{ç·ã‚µãƒ³ãƒ—ãƒ«æ•°}}$
- **Feature Completeness**: $\kappa = 1 - \frac{\text{æ¬ æå€¤æ•°}}{\text{ç·ç‰¹å¾´é‡æ•°}}$
- **Class Balance**: Imbalance Ratio $\rho = \frac{\max_k N_k}{\min_k N_k}$

#### 6.1.3 Automated Data Augmentation: AutoML for Data

**DADA** (Differentiable Automatic Data Augmentation, 2024) [^12]:

å¾“æ¥ã®AutoAugmentã¯é›¢æ•£æ¢ç´¢ï¼ˆRL/é€²åŒ–è¨ˆç®—ï¼‰ã ã£ãŸãŒã€DADAã¯æ‹¡å¼µãƒãƒªã‚·ãƒ¼ã‚’å¾®åˆ†å¯èƒ½ã«ã—ã€å‹¾é…æ³•ã§æœ€é©åŒ–ã™ã‚‹ã€‚

$$
\mathcal{L}_{\text{DADA}} = \mathbb{E}_{\text{aug} \sim \pi_\theta}[\mathcal{L}_{\text{task}}(\text{model}(\text{aug}(\mathbf{x})))] + \lambda \text{KL}[\pi_\theta \| \pi_0]
$$

ã“ã“ã§:

- $\pi_\theta$: æ‹¡å¼µãƒãƒªã‚·ãƒ¼ã®åˆ†å¸ƒï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã§åˆ¶å¾¡ï¼‰
- $\pi_0$: äº‹å‰åˆ†å¸ƒï¼ˆä¾‹: ä¸€æ§˜åˆ†å¸ƒï¼‰
- $\mathcal{L}_{\text{task}}$: ã‚¿ã‚¹ã‚¯ã®æå¤±ï¼ˆä¾‹: Cross-Entropyï¼‰
- $\lambda$: æ­£å‰‡åŒ–é …ã®é‡ã¿

**åˆ©ç‚¹**: æ¢ç´¢ãŒå‹¾é…æ³•ã§é«˜é€Ÿï¼ˆAutoAugmentã®100xé«˜é€Ÿï¼‰ã€‚

### 6.2 ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°: DVCå…¥é–€

**å•é¡Œ**: ãƒ¢ãƒ‡ãƒ«ã¯Gitã§ç®¡ç†ã§ãã‚‹ãŒã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆæ•°GBã€œTBï¼‰ã¯ï¼Ÿ

**è§£æ±º**: DVC (Data Version Control) [^13] â€” Gitãƒ©ã‚¤ã‚¯ãªãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã€‚

#### 6.2.1 DVCã®ä»•çµ„ã¿

```bash
# Initialize DVC
dvc init

# Add dataset to DVC tracking
dvc add data/mnist_train.arrow

# Git commit the .dvc file (metadata only)
git add data/mnist_train.arrow.dvc .gitignore
git commit -m "Add MNIST training data"

# Push data to remote storage (S3/GCS/Azure/NAS)
dvc remote add -d myremote s3://mybucket/dvc-storage
dvc push
```

**ä»•çµ„ã¿**:

1. `dvc add`ã§ãƒ‡ãƒ¼ã‚¿ã®MD5ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®— â†’ `.dvc`ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
2. å®Ÿãƒ‡ãƒ¼ã‚¿ã¯`.dvc/cache/`ã«ç§»å‹•ï¼ˆGitã¯è¿½è·¡ã—ãªã„ï¼‰
3. `.dvc`ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿Gitã«ã‚³ãƒŸãƒƒãƒˆï¼ˆæ•°KBï¼‰
4. `dvc push`ã§å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’ãƒªãƒ¢ãƒ¼ãƒˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
5. ä»–ã®äººã¯`dvc pull`ã§å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

**æ•°å¼è¡¨ç¾**:

$$
\text{DVC}(\mathcal{D}) = (\text{hash}(\mathcal{D}), \text{metadata})
$$

$$
\text{Git}(\text{DVC}(\mathcal{D})) \quad \text{(only metadata, not data)}
$$

#### 6.2.2 å®Ÿé¨“å†ç¾ã®ãŸã‚ã®DVCãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```yaml
# dvc.yaml
stages:
  preprocess:
    cmd: python preprocess.py
    deps:
      - data/raw/mnist.csv
    params:
      - preprocess.normalize
      - preprocess.augment
    outs:
      - data/processed/mnist_train.arrow

  train:
    cmd: python train.py
    deps:
      - data/processed/mnist_train.arrow
      - src/model.py
    params:
      - train.epochs
      - train.learning_rate
    metrics:
      - metrics/train.json:
          cache: false
    outs:
      - models/vae_mnist.pth
```

`dvc repro`ã§å…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å†å®Ÿè¡Œ â†’ ãƒ‡ãƒ¼ã‚¿ãƒ»ã‚³ãƒ¼ãƒ‰ãƒ»ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¤‰æ›´ã‚’è‡ªå‹•è¿½è·¡ã€‚

**å†ç¾æ€§ã®ä¿è¨¼**:

$$
\text{Reproducibility} = f(\text{Code}, \text{Data}, \text{Params})
$$

DVCã¯å…¨ã¦ã‚’ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç† â†’ éå»ã®ä»»æ„ã®æ™‚ç‚¹ã‚’å®Œå…¨å†ç¾ã€‚

### 6.3 ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®ç ”ç©¶ç³»è­œ

```mermaid
graph TD
    A["ğŸ“Š Classical Statistics<br/>Fisher 1922: MLE"] --> B["ğŸ”¬ Robust Statistics<br/>Huber 1964: M-estimator"]
    A --> C["ğŸ“ˆ EDA<br/>Tukey 1977: Boxplot"]
    B --> D["âš–ï¸ Imbalanced Learning<br/>Chawla 2002: SMOTE"]
    D --> E["ğŸ¯ Focal Loss<br/>Lin 2017: Hard Example Mining"]
    C --> F["ğŸ¤– AutoML<br/>Feurer 2015: Auto-sklearn"]
    F --> G["ğŸ”„ AutoAugment<br/>Cubuk 2019: RL-based"]
    G --> H["âš¡ RandAugment<br/>Cubuk 2020: Simplified"]
    H --> I["ğŸ² TrivialAugment<br/>MÃ¼ller 2021: Parameter-free"]
    E --> J["ğŸ“Š Data-Centric AI<br/>Ng 2021: Quality>Model"]
    J --> K["ğŸ”® 2024-2026<br/>Differentiable Aug/Active Learning"]
    style A fill:#e8f5e9
    style K fill:#fff9c4
```

**ä¸»è¦ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³**:

| å¹´ | è²¢çŒ® | è«–æ–‡/äººç‰© | å½±éŸ¿ |
|:---|:-----|:---------|:-----|
| 1922 | æœ€å°¤æ¨å®šï¼ˆMLEï¼‰ | Fisher | çµ±è¨ˆçš„æ¨è«–ã®åŸºç›¤ |
| 1977 | æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆEDAï¼‰ | Tukey | ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–ã®ä½“ç³»åŒ– |
| 2002 | SMOTE | Chawla et al. | ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾ç­–ã®å®šç•ª |
| 2017 | Focal Loss | Lin et al. (ICCV) | One-stageæ¤œå‡ºå™¨ã‚’å®Ÿç”¨åŒ– |
| 2019 | AutoAugment | Cubuk et al. (CVPR) | è‡ªå‹•ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã®é–‹å¹• |
| 2020 | RandAugment | Cubuk et al. (NeurIPS) | æ¢ç´¢ã‚³ã‚¹ãƒˆã‚’1/1000ã«å‰Šæ¸› |
| 2021 | Data-Centric AI | Andrew Ng | ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚·ãƒ•ãƒˆã‚’å®£è¨€ |

### 6.4 æ¬ æå€¤å‡¦ç†ã®ç†è«–ã¨å®Ÿè£…

ãƒ‡ãƒ¼ã‚¿ã®ä¸å®Œå…¨æ€§ã¯é¿ã‘ã‚‰ã‚Œãªã„ã€‚å®Ÿä¸–ç•Œã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç´„15-40%ã«ã¯æ¬ æå€¤ãŒå­˜åœ¨ã™ã‚‹ã€‚é©åˆ‡ãªæ¬ æå€¤å‡¦ç†ãŒæ€§èƒ½ã‚’å·¦å³ã™ã‚‹ã€‚

#### 6.4.1 æ¬ æãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®åˆ†é¡ï¼ˆRubin 1976ï¼‰

æ¬ æå€¤ã¯3ã¤ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã«åˆ†é¡ã•ã‚Œã‚‹ [^14]:

**1. MCAR (Missing Completely At Random)**

$$
P(R = 0 \mid X_{\text{obs}}, X_{\text{miss}}) = P(R = 0)
$$

ã“ã“ã§ $R$ ã¯æ¬ æã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ï¼ˆ0=æ¬ æã€1=è¦³æ¸¬ï¼‰ã€‚æ¬ æã¯å®Œå…¨ã«ãƒ©ãƒ³ãƒ€ãƒ  â†’ æ¬ æãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¦ã‚‚åã‚Šãªã—ã€‚

**ä¾‹**: ã‚»ãƒ³ã‚µãƒ¼æ•…éšœã§ä¸€éƒ¨ã®ãƒ‡ãƒ¼ã‚¿ãŒè¨˜éŒ²ã•ã‚Œãªã„ã€‚

**2. MAR (Missing At Random)**

$$
P(R = 0 \mid X_{\text{obs}}, X_{\text{miss}}) = P(R = 0 \mid X_{\text{obs}})
$$

æ¬ æãŒè¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã®ã¿ã«ä¾å­˜ï¼ˆæ¬ æå€¤è‡ªä½“ã«ã¯ä¾å­˜ã—ãªã„ï¼‰ã€‚

**ä¾‹**: é«˜é½¢è€…ã»ã©å¥åº·è¨ºæ–­ã®ã€Œä½“é‡ã€é …ç›®ã‚’è¨˜å…¥ã—ãªã„ â†’ å¹´é½¢ï¼ˆè¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ï¼‰ã‹ã‚‰æ¬ æã‚’äºˆæ¸¬å¯èƒ½ã€‚

**3. MNAR (Missing Not At Random)**

$$
P(R = 0 \mid X_{\text{obs}}, X_{\text{miss}}) \neq P(R = 0 \mid X_{\text{obs}})
$$

æ¬ æãŒæ¬ æå€¤è‡ªä½“ã«ä¾å­˜ â†’ è£œå®ŒãŒå›°é›£ã€‚

**ä¾‹**: é«˜æ‰€å¾—è€…ã»ã©ã€Œå¹´åã€ã‚’è¨˜å…¥ã—ãªã„ â†’ æ¬ æå€¤ï¼ˆé«˜æ‰€å¾—ï¼‰ãã®ã‚‚ã®ãŒæ¬ æã‚’å¼•ãèµ·ã“ã™ã€‚

#### 6.4.2 æ¬ æå€¤è£œå®Œæ‰‹æ³•

| æ‰‹æ³• | æˆ¦ç•¥ | ä»®å®š | é©ç”¨å ´é¢ |
|:-----|:-----|:-----|:--------|
| **Listwise Deletion** | æ¬ æã‚’å«ã‚€è¡Œã‚’å‰Šé™¤ | MCAR | ãƒ‡ãƒ¼ã‚¿é‡ãŒååˆ†ï¼ˆ>10% redundancyï¼‰ |
| **Mean Imputation** | å¹³å‡å€¤ã§è£œå®Œ | MCAR | æ¬ æç‡<5%ã€åˆ†æ•£ãŒé‡è¦ã§ãªã„ |
| **KNN Imputation** | k-æœ€è¿‘å‚ã®å¹³å‡ | MAR | ç‰¹å¾´é‡é–“ã«ç›¸é–¢ |
| **MICE** | å¤šé‡ä»£å…¥ | MAR | çµ±è¨ˆçš„æ¨è«–ï¼ˆä¿¡é ¼åŒºé–“æ¨å®šï¼‰ |
| **MissForest** | Random Forestè£œå®Œ | MAR | éç·šå½¢é–¢ä¿‚ã€é«˜æ¬¡å…ƒ |
| **Deep Learning** | Autoencoderè£œå®Œ | MAR/MNAR | å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã€è¤‡é›‘ãªä¾å­˜é–¢ä¿‚ |

#### 6.4.3 K-NN Imputationå®Ÿè£…

```julia
using NearestNeighbors, Statistics

function knn_impute(X::Matrix{Float64}, k::Int=5)
    n, d = size(X)
    X_imputed = copy(X)

    # Find missing entries
    missing_mask = isnan.(X)

    for j in 1:d  # for each feature
        if !any(missing_mask[:, j])
            continue  # no missing values in this feature
        end

        # Rows with observed values in feature j
        observed_idx = findall(.!missing_mask[:, j])
        X_obs = X[observed_idx, :]

        # Rows with missing values in feature j
        missing_idx = findall(missing_mask[:, j])

        # Build k-NN tree on observed data (excluding feature j)
        features_excl_j = setdiff(1:d, j)
        X_obs_excl_j = X_obs[:, features_excl_j]

        # Remove rows with NaN in other features (for tree building)
        valid_rows = findall(row -> !any(isnan.(row)), eachrow(X_obs_excl_j))
        X_tree = X_obs_excl_j[valid_rows, :]

        if isempty(X_tree)
            # Fallback: mean imputation
            X_imputed[missing_idx, j] .= mean(X[observed_idx, j])
            continue
        end

        kdtree = KDTree(X_tree')

        # Impute missing values
        for i in missing_idx
            query = X[i, features_excl_j]
            if any(isnan.(query))
                # If query has NaN in other features, use mean
                X_imputed[i, j] = mean(X[observed_idx, j])
                continue
            end

            # Find k nearest neighbors
            idxs, _ = knn(kdtree, query, min(k, size(X_tree, 1)), true)

            # Impute as mean of neighbors
            neighbor_values = X_obs[valid_rows[idxs], j]
            X_imputed[i, j] = mean(neighbor_values)
        end
    end

    return X_imputed
end

# Example
X = randn(100, 5)
# Introduce 10% missing values (MCAR)
missing_idx = rand(1:length(X), Int(round(0.1 * length(X))))
X[missing_idx] .= NaN

println("Missing values: $(sum(isnan.(X))) / $(length(X))")
X_imputed = knn_impute(X, 5)
println("After imputation: $(sum(isnan.(X_imputed))) / $(length(X_imputed))")
```

å‡ºåŠ›:
```
Missing values: 50 / 500
After imputation: 0 / 500
```

#### 6.4.4 MICEï¼ˆMultiple Imputation by Chained Equationsï¼‰

MICEã¯å„ç‰¹å¾´é‡ã‚’ä»–ã®ç‰¹å¾´é‡ã§äºˆæ¸¬ã—ã€åå¾©çš„ã«è£œå®Œã™ã‚‹ [^15]ã€‚

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

1. åˆæœŸåŒ–: å¹³å‡å€¤è£œå®Œ
2. å„ç‰¹å¾´é‡ $j$ ã«ã¤ã„ã¦:
   a. $j$ ä»¥å¤–ã®ç‰¹å¾´é‡ã‚’èª¬æ˜å¤‰æ•°ã€$j$ ã‚’ç›®çš„å¤‰æ•°ã¨ã—ã¦å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´
   b. æ¬ æå€¤ã‚’äºˆæ¸¬å€¤ã§è£œå®Œ
3. åæŸã¾ã§ç¹°ã‚Šè¿”ã™ï¼ˆé€šå¸¸5-10å›ï¼‰
4. **è¤‡æ•°å›å®Ÿè¡Œ** â†’ è¤‡æ•°ã®è£œå®Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆ â†’ çµ±è¨ˆé‡ã®ä¸ç¢ºå®Ÿæ€§ã‚’æ¨å®š

**æ•°å¼**:

$$
X_j^{(t+1)} = f_j(X_{-j}^{(t)}, \theta_j) + \epsilon
$$

ã“ã“ã§ $X_{-j}$ ã¯ $j$ ä»¥å¤–ã®å…¨ç‰¹å¾´é‡ã€$f_j$ ã¯å›å¸°ãƒ¢ãƒ‡ãƒ«ã€$\epsilon$ ã¯ãƒã‚¤ã‚ºã€‚

**Juliaå®Ÿè£…ï¼ˆç°¡æ˜“ç‰ˆï¼‰**:

```julia
using GLM, DataFrames

function mice_impute(X::Matrix{Float64}, n_iter::Int=10, m::Int=5)
    n, d = size(X)
    imputed_datasets = []

    for _ in 1:m  # Generate m imputed datasets
        X_imputed = copy(X)

        # Initialize with mean imputation
        for j in 1:d
            col = X_imputed[:, j]
            if any(isnan.(col))
                mean_val = mean(filter(!isnan, col))
                X_imputed[isnan.(col), j] .= mean_val
            end
        end

        # Iterative imputation
        for iter in 1:n_iter
            for j in 1:d
                missing_mask_j = isnan.(X[:, j])
                if !any(missing_mask_j)
                    continue
                end

                # Observed rows for feature j
                obs_idx = findall(.!missing_mask_j)
                miss_idx = findall(missing_mask_j)

                # Build regression model: X_j ~ X_{-j}
                X_obs = X_imputed[obs_idx, :]
                y_obs = X_obs[:, j]
                X_pred = X_obs[:, setdiff(1:d, j)]

                # Fit linear model
                df = DataFrame(X_pred, :auto)
                df.y = y_obs
                formula = Term(:y) ~ sum(Term.(names(df)[1:end-1]))
                model = lm(formula, df)

                # Predict missing values
                X_miss = X_imputed[miss_idx, setdiff(1:d, j)]
                df_miss = DataFrame(X_miss, :auto)
                y_pred = predict(model, df_miss)

                X_imputed[miss_idx, j] = y_pred
            end
        end

        push!(imputed_datasets, X_imputed)
    end

    # Return mean of m imputed datasets
    return mean(imputed_datasets)
end

# Example
X_mice = mice_impute(X, 10, 5)
println("MICE imputation completed")
```

#### 6.4.5 æ¬ æå€¤ã®å¯è¦–åŒ–

```julia
using Plots

function plot_missing_pattern(X::Matrix{Float64})
    missing_mask = isnan.(X)
    heatmap(missing_mask',
        xlabel="Sample",
        ylabel="Feature",
        title="Missing Data Pattern",
        color=:grays,
        clim=(0, 1))
end

# Visualize
plot_missing_pattern(X)
```

æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¯è¦–åŒ–ã§ã€MCAR/MAR/MNARã‚’è¨ºæ–­ã§ãã‚‹:

- **ãƒ©ãƒ³ãƒ€ãƒ ãªç‚¹åœ¨**: MCAR
- **ç‰¹å®šã®è¡Œ/åˆ—ã«é›†ä¸­**: MARï¼ˆè¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã«ä¾å­˜ï¼‰
- **æ§‹é€ çš„ãƒ‘ã‚¿ãƒ¼ãƒ³**: MNARï¼ˆæ¬ æå€¤è‡ªä½“ã«ä¾å­˜ï¼‰

### 6.5 æœ€æ–°ç ”ç©¶å‹•å‘ï¼ˆ2024-2026ï¼‰

#### 6.5.1 LLMã«ã‚ˆã‚‹è‡ªå‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³

GPT-4/Claudeã‚’ä½¿ã£ãŸè‡ªå‹•ãƒ©ãƒ™ãƒªãƒ³ã‚°ãŒå®Ÿç”¨åŒ–ã€‚

```python
# LLM-based data annotation
from openai import OpenAI
client = OpenAI()

def annotate_with_llm(text, classes):
    prompt = f"""Classify the following text into one of {classes}:

Text: {text}

Answer with only the class name."""

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )

    return response.choices[0].message.content

# Example
text = "The movie was absolutely terrible, worst I've ever seen"
label = annotate_with_llm(text, ["positive", "negative"])
print(f"Label: {label}")  # negative
```

**ç²¾åº¦**: Human baseline 95% â†’ GPT-4 93% (Stanfordç ”ç©¶)ã€‚ã‚³ã‚¹ãƒˆã¯äººé–“ã®1/100ã€‚

#### 6.4.2 Synthetic Data Generation: ãƒ‡ãƒ¼ã‚¿ãŒç„¡é™ã«ç”Ÿæˆã§ãã‚‹æ™‚ä»£

**Flamingo/Stable Diffusion**ã«ã‚ˆã‚‹ç”»åƒç”Ÿæˆ + LLMã«ã‚ˆã‚‹ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆ â†’ **åˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**ã€‚

ä¾‹: **Synthetic ImageNet** â€” Stable Diffusionã§1Mç”»åƒç”Ÿæˆ â†’ ResNet-50è¨“ç·´ â†’ å®Ÿãƒ‡ãƒ¼ã‚¿ã®95%ç²¾åº¦é”æˆã€‚

**æ•°å¼**:

$$
\mathcal{D}_{\text{synthetic}} = \{(\text{GenerateImage}(\text{prompt}_i), \text{label}_i)\}_{i=1}^N
$$

**åˆ©ç‚¹**:

- ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼å•é¡Œå›é¿ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ä¸è¦ï¼‰
- ãƒ­ãƒ³ã‚°ãƒ†ãƒ¼ãƒ«åˆ†å¸ƒã®è£œå®Œï¼ˆç¨€ãªã‚¯ãƒ©ã‚¹ã‚’å¤§é‡ç”Ÿæˆï¼‰
- ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã®æ¥µé™å½¢æ…‹

### 6.6 ä»Šå›ã®å­¦ã³ï¼ˆ3ã¤ã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆï¼‰

1. **ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãŒè¨“ç·´ã‚’æ¡é•ã„ã«åŠ é€Ÿã™ã‚‹**

æ¨™æº–åŒ– $z = \frac{x - \mu}{\sigma}$ ã ã‘ã§ã€å­¦ç¿’ç‡ã‚’10000å€ã«ã§ãã€åæŸãŒåŠ‡çš„ã«é€Ÿããªã‚‹ã€‚ãƒ‡ãƒ¼ã‚¿ã®æ‰±ã„æ–¹ã‚’çŸ¥ã‚‰ãªã‘ã‚Œã°ã€ã©ã‚Œã ã‘å„ªã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚‚ç„¡æ„å‘³ã ã€‚

2. **ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã¯çµ±åˆæˆ¦ç•¥ã§å¯¾å‡¦ã™ã‚‹**

SMOTEï¼ˆã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆï¼‰+ Focal Lossï¼ˆé›£ã—ã„ä¾‹ã«é›†ä¸­ï¼‰+ Class Weightingï¼ˆæå¤±ã®é‡ã¿ä»˜ã‘ï¼‰ã®çµ„ã¿åˆã‚ã›ã§ã€å°‘æ•°æ´¾ã‚¯ãƒ©ã‚¹ã®Recallã‚’15.6å€æ”¹å–„ã§ããŸã€‚å˜ä¸€æ‰‹æ³•ã§ã¯ä¸ååˆ†ã€çµ±åˆãŒéµã ã€‚

3. **HuggingFace Datasets + Julia Arrow = ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼å‡¦ç†**

Pythonï¼ˆHF Datasetsï¼‰ã¨Juliaï¼ˆArrow.jlï¼‰ã®é€£æºã§ã€æ•°GBç´šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’RAMã‚³ãƒ”ãƒ¼ãªã—ã§å‡¦ç†ã§ãã‚‹ã€‚ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åŠ¹ç‡åŒ–ã¯ã€ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã¨åŒã˜ãã‚‰ã„é‡è¦ã ã€‚

### 6.7 FAQ

:::details Q1. æ¨™æº–åŒ–ã¨BatchNormã¯ä½•ãŒé•ã†ï¼Ÿ

**æ¨™æº–åŒ–**: ãƒ‡ãƒ¼ã‚¿å…¨ä½“ï¼ˆè¨“ç·´ã‚»ãƒƒãƒˆï¼‰ã®çµ±è¨ˆé‡ $\mu, \sigma$ ã§ä¸€åº¦å¤‰æ› â†’ è¨“ç·´å‰ã®å‰å‡¦ç†ã€‚

**BatchNorm**: ãƒŸãƒ‹ãƒãƒƒãƒã”ã¨ã«çµ±è¨ˆé‡ã‚’è¨ˆç®— â†’ å„å±¤ã§å‹•çš„ã«æ­£è¦åŒ– â†’ è¨“ç·´ä¸­ã®å†…éƒ¨å‡¦ç†ã€‚

$$
\begin{aligned}
\text{Standardization:} \quad & z = \frac{x - \mu_{\text{å…¨ãƒ‡ãƒ¼ã‚¿}}}{\sigma_{\text{å…¨ãƒ‡ãƒ¼ã‚¿}}} \\
\text{BatchNorm:} \quad & z = \frac{x - \mu_{\text{ãƒãƒƒãƒ}}}{\sigma_{\text{ãƒãƒƒãƒ}}}
\end{aligned}
$$

ä¸¡æ–¹ä½¿ã†ã®ãŒä¸€èˆ¬çš„ï¼ˆå‰å‡¦ç†ã§æ¨™æº–åŒ– + å„å±¤ã§BatchNormï¼‰ã€‚
:::

:::details Q2. SMOTEã¯é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã§ã‚‚æœ‰åŠ¹ï¼Ÿ

**æ³¨æ„**: é«˜æ¬¡å…ƒï¼ˆ>100æ¬¡å…ƒï¼‰ã§ã¯SMOTEã®åŠ¹æœãŒè–„ã‚Œã‚‹ã€‚ç†ç”±:

- ç·šå½¢è£œé–“ $\mathbf{x}_{\text{new}} = \mathbf{x}_i + \lambda(\mathbf{x}_{\text{nn}} - \mathbf{x}_i)$ ã¯ã€é«˜æ¬¡å…ƒç©ºé–“ã§ã¯ã€Œæ„å‘³ã®ã‚ã‚‹ã€ä¸­é–“ç‚¹ã‚’ç”Ÿæˆã—ã«ãã„ï¼ˆæ¬¡å…ƒã®å‘ªã„ï¼‰
- k-æœ€è¿‘å‚ãŒé ããªã‚Šã€è£œé–“ãŒç„¡æ„å‘³ã«ãªã‚‹

**å¯¾ç­–**:

- **Borderline-SMOTE**: æ±ºå®šå¢ƒç•Œä»˜è¿‘ã®ã¿ç”Ÿæˆ
- **ADASYN**: å¯†åº¦ã«å¿œã˜ã¦ç”Ÿæˆæ•°èª¿æ•´
- **Deep SMOTE**: Autoencoderã‚„VAEã®æ½œåœ¨ç©ºé–“ã§è£œé–“ï¼ˆä½æ¬¡å…ƒåŒ–å¾Œã« SMOTEï¼‰
:::

:::details Q3. Focal Lossã®$\gamma$ã¯ã©ã†é¸ã¶ï¼Ÿ

**æ¨å¥¨å€¤**: $\gamma = 2.0$ï¼ˆLin et al. 2017åŸè«–æ–‡ [^6]ï¼‰

**èª¿æ•´æ–¹é‡**:

- $\gamma = 0$: æ¨™æº–ã®Cross-Entropyï¼ˆç°¡å˜ãªã‚µãƒ³ãƒ—ãƒ«ã‚‚ç­‰ã—ãæ‰±ã†ï¼‰
- $\gamma = 1$: è»½åº¦ã®ç°¡å˜ã‚µãƒ³ãƒ—ãƒ«å‰Šæ¸›ï¼ˆä¸å‡è¡¡æ¯” < 10:1ï¼‰
- $\gamma = 2$: æ¨™æº–ï¼ˆä¸å‡è¡¡æ¯” 10-100:1ï¼‰
- $\gamma = 5$: æ¥µç«¯ãªä¸å‡è¡¡ï¼ˆ100:1ä»¥ä¸Šï¼‰

**å®Ÿé¨“**:

```julia
for Î³ in [0, 1, 2, 5]
    focal = FocalLoss(Î±, Î³)
    # Train and evaluate
    println("Î³=$Î³: F1=$(metrics["f1"])")
end
```

ä¸€èˆ¬ã« $\gamma \in [2, 3]$ ãŒæœ€ã‚‚å®‰å®šã™ã‚‹ã€‚
:::

:::details Q4. ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã¯ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«ã‚‚é©ç”¨ã™ã‚‹ï¼Ÿ

**çµ¶å¯¾ã«NO**ã€‚ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã¯**è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿**ã«é©ç”¨ã™ã‚‹ã€‚

ç†ç”±:

- ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¯ã€ŒæœªçŸ¥ãƒ‡ãƒ¼ã‚¿ã€ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹ â†’ æ‹¡å¼µã™ã‚‹ã¨éåº¦ã«æœ‰åˆ©ãªè©•ä¾¡ã«ãªã‚‹
- æ±åŒ–æ€§èƒ½ã‚’æ­£ã—ãæ¸¬å®šã§ããªããªã‚‹

**Test Time Augmentation (TTA)**ã®ä¾‹å¤–:

æ¨è«–æ™‚ã«è¤‡æ•°ã®æ‹¡å¼µç‰ˆã§äºˆæ¸¬ã—ã€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã™ã‚‹æ‰‹æ³•ã¯å­˜åœ¨ã™ã‚‹ãŒã€ã“ã‚Œã¯è©•ä¾¡ã§ã¯ãªãæ¨è«–ã®æŠ€è¡“ã€‚

$$
\hat{y} = \frac{1}{K}\sum_{k=1}^K f(\text{aug}_k(\mathbf{x}))
$$

è©•ä¾¡æ™‚ã¯TTA**ãªã—**ã§è¨ˆæ¸¬ã™ã‚‹ã€‚
:::

:::details Q5. DVCã¨Git LFSã®é•ã„ã¯ï¼Ÿ

| è¦³ç‚¹ | DVC | Git LFS |
|:-----|:----|:--------|
| **ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸** | ä»»æ„ï¼ˆS3/GCS/Azure/NASï¼‰ | GitHub LFSå°‚ç”¨ã‚µãƒ¼ãƒãƒ¼ |
| **ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³** | âœ… dvc.yaml ã§å®šç¾©å¯èƒ½ | âŒ ãªã— |
| **å†ç¾æ€§** | âœ… ã‚³ãƒ¼ãƒ‰+ãƒ‡ãƒ¼ã‚¿+ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµ±åˆ | âš ï¸ ãƒ‡ãƒ¼ã‚¿ã®ã¿ |
| **ã‚³ã‚¹ãƒˆ** | è‡ªå‰ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼ˆå®‰ä¾¡ï¼‰ | GitHubèª²é‡‘ï¼ˆé«˜é¡ï¼‰ |
| **å­¦ç¿’æ›²ç·š** | ã‚„ã‚„æ€¥ | ç·©ã‚„ã‹ï¼ˆGitæ‹¡å¼µï¼‰ |

**æ¨å¥¨**: æœ¬æ ¼çš„ãªMLãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ â†’ DVCã€‚å°è¦æ¨¡/å€‹äºº â†’ Git LFSã€‚
:::

### 6.8 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ1é€±é–“ãƒ—ãƒ©ãƒ³ï¼‰

| æ—¥ | å†…å®¹ | æ™‚é–“ | é‡è¦åº¦ |
|:---|:-----|:-----|:-------|
| 1æ—¥ç›® | Zone 0-2ï¼ˆã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã€œç›´æ„Ÿï¼‰ | 30åˆ† | â˜…â˜…â˜… |
| 2æ—¥ç›® | Zone 3ï¼ˆæ•°å¼ä¿®è¡Œ å‰åŠ: æ¨™æº–åŒ–ãƒ»One-Hotãƒ»Class Weightingï¼‰ | 60åˆ† | â˜…â˜…â˜… |
| 3æ—¥ç›® | Zone 3ï¼ˆæ•°å¼ä¿®è¡Œ å¾ŒåŠ: Focal Lossãƒ»SMOTEãƒ»Boss Battleï¼‰ | 60åˆ† | â˜…â˜…â˜… |
| 4æ—¥ç›® | Zone 4ï¼ˆå®Ÿè£…: HF Datasetsãƒ»Juliaçµ±åˆãƒ»å‰å‡¦ç†å®Ÿè£…ï¼‰ | 90åˆ† | â˜…â˜…â˜… |
| 5æ—¥ç›® | Zone 5ï¼ˆå®Ÿé¨“: ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ€§èƒ½æ¤œè¨¼ï¼‰ | 60åˆ† | â˜…â˜…â˜… |
| 6æ—¥ç›® | Zone 6ï¼ˆç™ºå±•: æœ€æ–°ç ”ç©¶ãƒ»DVCï¼‰+ å¾©ç¿’ | 60åˆ† | â˜…â˜… |
| 7æ—¥ç›® | ç·å¾©ç¿’ + è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ + å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ | 90åˆ† | â˜…â˜…â˜… |

**é‡ç‚¹å¾©ç¿’ãƒã‚¤ãƒ³ãƒˆ**:

- [ ] æ¨™æº–åŒ–ã®æ•°å¼ $z = \frac{x - \mu}{\sigma}$ ã‚’æš—è¨˜ã—ã€Juliaå®Ÿè£…ã‚’å†ç¾ã§ãã‚‹
- [ ] Focal Loss $\text{FL}(p_t) = -(1 - p_t)^\gamma \log(p_t)$ ã®ç›´æ„Ÿã‚’èª¬æ˜ã§ãã‚‹
- [ ] SMOTEè£œé–“ $\mathbf{x}_{\text{new}} = \mathbf{x}_i + \lambda(\mathbf{x}_{\text{nn}} - \mathbf{x}_i)$ ã‚’å®Ÿè£…ã§ãã‚‹
- [ ] HuggingFace Datasets â†’ Julia Arrowã® ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼çµ±åˆã‚’å®Ÿè£…ã§ãã‚‹
- [ ] ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã€Baseline vs Combined ã®æ€§èƒ½å·®ã‚’å®Ÿé¨“ã§ç¤ºã›ã‚‹

### 6.9 é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼

```julia
# Self-assessment checklist
checklist = Dict(
    "æ¨™æº–åŒ–ã®æ•°å¼ã‚’å°å‡ºã§ãã‚‹" => false,
    "Focal Lossã®å‹¾é…ã‚’å°å‡ºã§ãã‚‹" => false,
    "SMOTEã‚’ã‚¹ã‚¯ãƒ©ãƒƒãƒå®Ÿè£…ã§ãã‚‹" => false,
    "HF Datasetsâ†’Julia Arrowçµ±åˆã‚’å®Ÿè£…ã§ãã‚‹" => false,
    "ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®Ÿé¨“ã‚’å†ç¾ã§ãã‚‹" => false
)

# Mark as completed
checklist["æ¨™æº–åŒ–ã®æ•°å¼ã‚’å°å‡ºã§ãã‚‹"] = true

# Print progress
total = length(checklist)
completed = sum(values(checklist))
progress = completed / total * 100

println("Progress: $(completed)/$(total) ($(round(progress, digits=1))%)")
for (task, done) in checklist
    status = done ? "âœ…" : "â¬œ"
    println("$status $task")
end
```

### 6.10 æ¬¡å›äºˆå‘Š: ç¬¬22å›ã€Œãƒã‚¤ãƒ†ã‚£ãƒ–ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å®Œå…¨ç‰ˆã€

ç¬¬20å›ã§VAE/GAN/Transformerã‚’å®Ÿè£…ã—ã€ç¬¬21å›ã§ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’å­¦ã‚“ã ã€‚æ¬¡ã¯**ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’çµ±åˆ**ã™ã‚‹ã€‚

**æ¬¡å›ã®å†…å®¹**:

- Vision-Languageãƒ¢ãƒ‡ãƒ«ã®ç†è«–ï¼ˆCLIP/BLIP-2/LLaVA/Qwen-VLï¼‰
- Cross-Modal Attentionã®æ•°å­¦ï¼ˆ$\text{Attention}(Q_{\text{text}}, K_{\text{image}}, V_{\text{image}})$ï¼‰
- Contrastive Learningå®Œå…¨ç‰ˆï¼ˆInfoNCE losså°å‡ºï¼‰
- âš¡ CLIP Juliaè¨“ç·´å®Ÿè£…
- ğŸ¦€ SmolVLM2 Rustæ¨è«–å®Ÿè£…
- VQAãƒ»Image Captioningè©•ä¾¡

**æ¥ç¶š**:

- ç¬¬21å›: ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®åŸºç›¤ã‚’å›ºã‚ãŸ
- **ç¬¬22å›**: ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆç”»åƒ+ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã‚’æ‰±ã†
- ç¬¬23å›: Fine-tuningï¼ˆäº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®é©å¿œï¼‰
- ç¬¬24å›: çµ±è¨ˆå­¦ãƒ»å› æœæ¨è«–ï¼ˆå®Ÿé¨“è¨­è¨ˆã®ç§‘å­¦ï¼‰

```mermaid
graph LR
    A["ğŸ“Š ç¬¬21å›<br/>Data Science"] --> B["ğŸ–¼ï¸ ç¬¬22å›<br/>Multimodal"]
    B --> C["ğŸ¯ ç¬¬23å›<br/>Fine-tuning"]
    C --> D["ğŸ“ˆ ç¬¬24å›<br/>Statistics"]
    style A fill:#e8f5e9
    style B fill:#fff9c4
```

**æº–å‚™ã™ã¹ãã“ã¨**:

- HuggingFace Datasetsã§ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆCOCO Captions / VQAv2ï¼‰ã‚’ãƒ­ãƒ¼ãƒ‰
- ç”»åƒåŸ‹ã‚è¾¼ã¿ï¼ˆViTï¼‰ã¨ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ï¼ˆBERTï¼‰ã®æ¬¡å…ƒã‚’æƒãˆã‚‹æ–¹æ³•ã‚’è€ƒãˆã‚‹
- Contrastive Lossï¼ˆå¯¾ç…§å­¦ç¿’ï¼‰ã®ç›´æ„Ÿã‚’æ´ã‚€ï¼ˆä¼¼ãŸç”»åƒ-ãƒ†ã‚­ã‚¹ãƒˆãƒšã‚¢ã®è·é›¢ã‚’è¿‘ã¥ã‘ã‚‹ï¼‰

### 6.11 ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **ã€Œãƒ‡ãƒ¼ã‚¿ãªãã—ã¦å­¦ç¿’ãªã—ã€ã¯å½“ãŸã‚Šå‰ã ã€‚ã ãŒæœ¬å½“ã®å•ã„ã¯ â€” ãƒ‡ãƒ¼ã‚¿ã®**è³ª**ã‚’ã©ã†å®šç¾©ã™ã‚‹ã‹ï¼Ÿ**

å¾“æ¥ã®æ©Ÿæ¢°å­¦ç¿’ã¯ã€Œãƒ‡ãƒ¼ã‚¿ã¯ä¸ãˆã‚‰ã‚Œã‚‹ã‚‚ã®ã€ã¨ã—ã¦æ‰±ã£ã¦ããŸã€‚ã ãŒ2025å¹´ã€åˆæˆãƒ‡ãƒ¼ã‚¿ãƒ»LLMã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»Active Learningã®æ™‚ä»£ã«ã€ãƒ‡ãƒ¼ã‚¿ã¯ã€Œä½œã‚‹ã‚‚ã®ã€ã«ãªã£ãŸã€‚

**å•ã„**:

1. **åˆæˆãƒ‡ãƒ¼ã‚¿ã¯ã€Œæœ¬ç‰©ã€ã‹ï¼Ÿ** Stable Diffusionã§ç”Ÿæˆã—ãŸç”»åƒã§ImageNetç²¾åº¦95%é”æˆã§ãã‚‹ãªã‚‰ã€å®Ÿãƒ‡ãƒ¼ã‚¿ã¯ä¸è¦ãªã®ã‹ï¼Ÿ
2. **LLMã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã¯äººé–“ã‚’è¶…ãˆã‚‹ã‹ï¼Ÿ** GPT-4ã®ç²¾åº¦ãŒäººé–“ã®93% vs 95%ãªã‚‰ã€ã‚³ã‚¹ãƒˆ1/100ã§ååˆ†ã§ã¯ï¼Ÿ
3. **ãƒ‡ãƒ¼ã‚¿å“è³ªã®é™ç•Œåç›Šã¯ï¼Ÿ** ãƒ‡ãƒ¼ã‚¿ã‚’ã©ã“ã¾ã§ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚Œã°ã€Œååˆ†ã€ã‹ï¼Ÿéå‰°ãªã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¯éå­¦ç¿’ã‚’æ‹›ãã®ã§ã¯ï¼Ÿ

**è­°è«–ã®è¦–ç‚¹**:

- **åˆæˆãƒ‡ãƒ¼ã‚¿ã®å±é™ºæ€§**: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ã‚¢ã‚¹ãŒãƒ‡ãƒ¼ã‚¿ã«æ··å…¥ â†’ ãƒ¢ãƒ‡ãƒ«ãŒç¾å®Ÿã‚’åæ˜ ã—ãªããªã‚‹
- **Human-in-the-loop**: å®Œå…¨è‡ªå‹•åŒ–ã¯ä¸å¯èƒ½ã€äººé–“ã®åˆ¤æ–­ãŒæœ€çµ‚é˜²è¡›ç·š
- **åˆ†å¸ƒã‚·ãƒ•ãƒˆ**: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆåˆ†å¸ƒãŒä¹–é›¢ã™ã‚‹å•é¡Œã¯ã€ã©ã‚“ãªã«å“è³ªã‚’ä¸Šã’ã¦ã‚‚è§£æ±ºã—ãªã„

**æ­´å²çš„æ–‡è„ˆ**:

- **1950å¹´ä»£**: ãƒ‡ãƒ¼ã‚¿ã¯æ‰‹ä½œæ¥­ã§åé›†ï¼ˆæ•°ç™¾ã‚µãƒ³ãƒ—ãƒ«ï¼‰
- **1990å¹´ä»£**: ImageNetç™»å ´ï¼ˆ1400ä¸‡æšã®æ‰‹å‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
- **2020å¹´ä»£**: LAION-5Bï¼ˆ50å„„æšã®è‡ªå‹•åé›†ï¼‰
- **2025å¹´**: åˆæˆãƒ‡ãƒ¼ã‚¿ãŒä¸»æµã«ãªã‚‹ï¼Ÿ

:::details æ­´å²çš„è¦³ç‚¹: ãƒ‡ãƒ¼ã‚¿åé›†ã®é€²åŒ–

| æ™‚ä»£ | ãƒ‡ãƒ¼ã‚¿è¦æ¨¡ | åé›†æ–¹æ³• | ã‚³ã‚¹ãƒˆ | å“è³ª |
|:-----|:----------|:--------|:-------|:-----|
| 1950-1980 | æ•°ç™¾ã€œæ•°åƒ | æ‰‹å‹•å…¥åŠ› | æ¥µé«˜ | æ¥µé«˜ |
| 1980-2000 | æ•°ä¸‡ã€œæ•°åä¸‡ | æ‰‹å‹•+ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° | é«˜ | é«˜ |
| 2000-2020 | æ•°ç™¾ä¸‡ã€œæ•°åå„„ | ã‚¯ãƒ©ã‚¦ãƒ‰ã‚½ãƒ¼ã‚·ãƒ³ã‚° | ä¸­ | ä¸­ |
| 2020-2025 | æ•°åå„„ã€œæ•°å…† | è‡ªå‹•åé›†+LLMãƒ•ã‚£ãƒ«ã‚¿ | ä½ | ä¸­ã€œä½ |
| **2025-** | **ç„¡é™ï¼ˆåˆæˆï¼‰** | **ç”Ÿæˆãƒ¢ãƒ‡ãƒ«** | **æ¥µä½** | **ï¼Ÿ** |

åˆæˆãƒ‡ãƒ¼ã‚¿ã®ã€Œå“è³ªã€ã‚’ã©ã†å®šç¾©ã™ã‚‹ã‹ãŒã€æ¬¡ä¸–ä»£AIã®éµã ã€‚
:::

:::message
**é€²æ—: 100% å®Œäº†** ğŸ‰ ç¬¬21å›ã€Œãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ & HuggingFace Datasetsã€å®Œèµ°ï¼ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®å…¨ã‚µã‚¤ã‚¯ãƒ«ã‚’ãƒã‚¹ã‚¿ãƒ¼ã—ãŸã€‚æ¬¡å›ã¯ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®çµ±åˆã¸ã€‚
:::

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Lhoest, Q., et al. (2021). "Datasets: A Community Library for Natural Language Processing". *Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations*, 175-184.
@[card](https://github.com/huggingface/datasets)

[^2]: Apache Arrow Development Team. (2024). "Apache Arrow: A Cross-Language Development Platform for In-Memory Data".
@[card](https://arrow.apache.org/)

[^3]: Bouchet-Valat, M., et al. (2024). "DataFrames.jl: Flexible and Fast Tabular Data in Julia". *Journal of Statistical Software*, 107(4), 1-32.
@[card](https://dataframes.juliadata.org/stable/)

[^4]: Ng, A. (2021). "A Chat with Andrew on MLOps: From Model-centric to Data-centric AI". *DeepLearning.AI Blog*.
@[card](https://www.deeplearning.ai/the-batch/issue-80/)

[^5]: Cui, Y., Jia, M., Lin, T.-Y., Song, Y., & Belongie, S. (2019). "Class-Balanced Loss Based on Effective Number of Samples". *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, 9268-9277.
@[card](https://arxiv.org/abs/1901.05555)

[^6]: Lin, T.-Y., Goyal, P., Girshick, R., He, K., & DollÃ¡r, P. (2017). "Focal Loss for Dense Object Detection". *Proceedings of the IEEE International Conference on Computer Vision (ICCV)*, 2980-2988.
@[card](https://arxiv.org/abs/1708.02002)

[^7]: Chawla, N. V., Bowyer, K. W., Hall, L. O., & Kegelmeyer, W. P. (2002). "SMOTE: Synthetic Minority Over-sampling Technique". *Journal of Artificial Intelligence Research*, 16, 321-357.
@[card](https://jair.org/index.php/jair/article/view/10302)

[^8]: Cubuk, E. D., Zoph, B., Mane, D., Vasudevan, V., & Le, Q. V. (2019). "AutoAugment: Learning Augmentation Strategies from Data". *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, 113-123.
@[card](https://arxiv.org/abs/1805.09501)

[^9]: Cubuk, E. D., Zoph, B., Shlens, J., & Le, Q. V. (2020). "RandAugment: Practical Automated Data Augmentation with a Reduced Search Space". *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)*, 702-703.
@[card](https://arxiv.org/abs/1909.13719)

[^10]: Stocker, C. (2017). "Augmentor.jl: A Julia Package for Image Augmentation". *GitHub*.
@[card](https://github.com/Evizero/Augmentor.jl)

[^11]: MÃ¼ller, S. G., & Hutter, F. (2021). "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation". *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*, 774-782.
@[card](https://arxiv.org/abs/2103.10158)

[^12]: Li, Y., Hu, G., Wang, Y., Hospedales, T., Robertson, N. M., & Yang, Y. (2020). "DADA: Differentiable Automatic Data Augmentation". *ECCV 2020*.
@[card](https://arxiv.org/abs/2003.03780)

[^13]: Kuprieiev, R., et al. (2024). "DVC: Data Version Control - Git for Data & Models".
@[card](https://dvc.org/)

### æ•™ç§‘æ›¸

- Murphy, K. P. (2023). *Probabilistic Machine Learning: Advanced Topics*. MIT Press. [https://probml.github.io/pml-book/](https://probml.github.io/pml-book/)
- GÃ©ron, A. (2022). *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow* (3rd ed.). O'Reilly Media.
- Bezanson, J., Edelman, A., Karpinski, S., & Shah, V. B. (2017). "Julia: A Fresh Approach to Numerical Computing". *SIAM Review*, 59(1), 65-98. [https://julialang.org/research/](https://julialang.org/research/)

---

## è¨˜æ³•è¦ç´„

æœ¬è¬›ç¾©ã§ä½¿ç”¨ã—ãŸæ•°å­¦è¨˜å·ã®ä¸€è¦§ã€‚

| è¨˜å· | æ„å‘³ | ä¾‹ |
|:-----|:-----|:---|
| $\mathbf{x}$ | ãƒ‡ãƒ¼ã‚¿ãƒ™ã‚¯ãƒˆãƒ« | $\mathbf{x} \in \mathbb{R}^d$ |
| $\mu$ | å¹³å‡ | $\mu = \frac{1}{n}\sum_{i=1}^n x_i$ |
| $\sigma$ | æ¨™æº–åå·® | $\sigma = \sqrt{\text{Var}[x]}$ |
| $z$ | æ¨™æº–åŒ–å¤‰æ•° | $z = \frac{x - \mu}{\sigma}$ |
| $y$ | ãƒ©ãƒ™ãƒ« | $y \in \{0, 1, \ldots, K-1\}$ |
| $\mathbf{e}_y$ | One-hotãƒ™ã‚¯ãƒˆãƒ« | $\mathbf{e}_y \in \mathbb{R}^K$ |
| $p_t$ | æ­£è§£ã‚¯ãƒ©ã‚¹ã®äºˆæ¸¬ç¢ºç‡ | $p_t = p_\theta(y \mid \mathbf{x})$ |
| $\gamma$ | Focal Loss focusing parameter | é€šå¸¸ $\gamma = 2$ |
| $\alpha$ | ã‚¯ãƒ©ã‚¹é‡ã¿ | $\alpha_k = \frac{1 - \beta}{1 - \beta^{N_k}}$ |
| $\lambda$ | SMOTEè£œé–“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | $\lambda \sim \text{Uniform}(0, 1)$ |
| $N_k$ | ã‚¯ãƒ©ã‚¹ $k$ ã®ã‚µãƒ³ãƒ—ãƒ«æ•° | $N = \sum_{k=0}^{K-1} N_k$ |
| $\rho$ | ä¸å‡è¡¡æ¯” | $\rho = \frac{\max_k N_k}{\min_k N_k}$ |
| $\mathcal{L}$ | æå¤±é–¢æ•° | $\mathcal{L}_{\text{CE}}, \mathcal{L}_{\text{FL}}$ |
| $\theta$ | ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | $\theta \in \mathbb{R}^p$ |
| $\mathbb{E}[\cdot]$ | æœŸå¾…å€¤ | $\mathbb{E}_{x \sim p}[f(x)]$ |
| $\text{KL}[p \| q]$ | KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ | åˆ†å¸ƒé–“ã®è·é›¢ |

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚