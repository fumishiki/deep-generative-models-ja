---
title: "ç¬¬8å›: æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ« & EMç®—æ³•: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸ”"
type: "tech"
topics: ["machinelearning", "deeplearning", "statistics", "python"]
published: true
---

# ç¬¬8å›: æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ« & EMç®—æ³• â€” è¦‹ãˆãªã„ã‚‚ã®ã‚’æ¨å®šã™ã‚‹æŠ€è¡“

> **è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã®è£ã«ã¯ã€å¸¸ã«ã€Œè¦‹ãˆãªã„æ§‹é€ ã€ãŒéš ã‚Œã¦ã„ã‚‹ã€‚ãã‚Œã‚’æ•°å­¦çš„ã«æ‰±ã†æ–¹æ³•ãŒEMç®—æ³•ã ã€‚**

ç›®ã®å‰ã«ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒå…¨ã¦ã ã¨æ€ã†ã ã‚ã†ã‹ã€‚å®Ÿã¯ãã†ã§ã¯ãªã„ã€‚æ‰‹æ›¸ãæ•°å­—ç”»åƒã®èƒŒå¾Œã«ã¯ã€Œã©ã®æ•°å­—ã‚’æ›¸ã“ã†ã¨ã—ãŸã‹ã€ã¨ã„ã†æ„å›³ãŒéš ã‚Œã¦ã„ã‚‹ã€‚éŸ³å£°ä¿¡å·ã®è£ã«ã¯ã€Œã©ã®éŸ³ç´ ã‚’ç™ºè©±ã—ã¦ã„ã‚‹ã‹ã€ã¨ã„ã†çŠ¶æ…‹ãŒã‚ã‚‹ã€‚é¡§å®¢ã®è³¼è²·ãƒ‡ãƒ¼ã‚¿ã®å‘ã“ã†ã«ã¯ã€Œã©ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«å±ã™ã‚‹ã‹ã€ã¨ã„ã†æ§‹é€ ãŒæ½œã‚“ã§ã„ã‚‹ã€‚

ã“ã®ã€Œè¦‹ãˆãªã„æ§‹é€ ã€ã‚’ **æ½œåœ¨å¤‰æ•°** (latent variable) ã¨å‘¼ã¶ã€‚ãã—ã¦æ½œåœ¨å¤‰æ•°ã‚’å«ã‚€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¨å®šã™ã‚‹æœ€ã‚‚åŸºæœ¬çš„ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒ **EMç®—æ³•** (Expectation-Maximization algorithm) ã ã€‚1977å¹´ã«Dempster, Laird, RubinãŒå®šå¼åŒ–ã—ãŸã“ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  [^1] ã¯ã€åŠä¸–ç´€è¿‘ãçµŒã£ãŸä»Šã‚‚æ©Ÿæ¢°å­¦ç¿’ã®æ ¹å¹¹ã‚’æ”¯ãˆã¦ã„ã‚‹ã€‚

æœ¬è¬›ç¾©ã¯Course Iã€Œæ•°å­¦åŸºç¤ç·¨ã€ã®æœ€çµ‚å› â€” 8å›ã«ã‚ãŸã‚‹æ•°å­¦ã®æ—…ã®ãƒ•ã‚£ãƒŠãƒ¼ãƒ¬ã ã€‚ç¬¬7å›ã§å­¦ã‚“ã æœ€å°¤æ¨å®šã®é™ç•Œã‚’çªç ´ã—ã€Course IIã®å¤‰åˆ†æ¨è«–ãƒ»VAEã¸æ©‹ã‚’æ¶ã‘ã‚‹ã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2025-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚
:::

```mermaid
graph LR
    A["ğŸ”¢ è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ x"] --> B["â“ æ½œåœ¨å¤‰æ•° z ã¯ï¼Ÿ"]
    B --> C["ğŸ“ EMç®—æ³•<br/>E-step + M-step"]
    C --> D["ğŸ¯ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ Î¸* æ¨å®š"]
    D --> E["ğŸŒ‰ VAE/Diffusionã¸"]
    style A fill:#e1f5fe
    style C fill:#fff3e0
    style E fill:#c8e6c9
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” è¦‹ãˆãªã„å¤‰æ•°ã‚’å½“ã¦ã‚‹

**ã‚´ãƒ¼ãƒ«**: æ½œåœ¨å¤‰æ•°ã¨EMç®—æ³•ã®å¨åŠ›ã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ã€‚

2ã¤ã®ã‚¬ã‚¦ã‚¹åˆ†å¸ƒãŒæ··ã–ã£ãŸãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã€‚ã©ã¡ã‚‰ã®åˆ†å¸ƒã‹ã‚‰æ¥ãŸã‹ã¯è¦‹ãˆãªã„ã€‚ãã‚Œã‚’å½“ã¦ã‚‹ã®ãŒEMç®—æ³•ã ã€‚

```python
import numpy as np

# 2ã¤ã®ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆã©ã¡ã‚‰ã‹ã‚‰æ¥ãŸã‹ã¯ã€Œéš ã‚Œã¦ã„ã‚‹ã€ï¼‰
np.random.seed(42)
z_true = np.random.choice([0, 1], size=200, p=[0.4, 0.6])  # latent variable
x = np.where(z_true == 0,
             np.random.normal(-2, 0.8, 200),   # cluster 0
             np.random.normal(3, 1.2, 200))     # cluster 1

# EM algorithm: 10 iterations
mu = np.array([-1.0, 1.0])  # initial guess
sigma = np.array([1.0, 1.0])
pi = np.array([0.5, 0.5])

for step in range(10):
    # E-step: compute responsibilities Î³(z_nk)
    pdf0 = pi[0] * np.exp(-0.5*((x - mu[0])/sigma[0])**2) / (sigma[0] * np.sqrt(2*np.pi))
    pdf1 = pi[1] * np.exp(-0.5*((x - mu[1])/sigma[1])**2) / (sigma[1] * np.sqrt(2*np.pi))
    gamma = pdf1 / (pdf0 + pdf1)
    # M-step: update parameters
    N0, N1 = (1 - gamma).sum(), gamma.sum()
    mu[0] = ((1 - gamma) * x).sum() / N0
    mu[1] = (gamma * x).sum() / N1
    sigma[0] = np.sqrt(((1 - gamma) * (x - mu[0])**2).sum() / N0)
    sigma[1] = np.sqrt((gamma * (x - mu[1])**2).sum() / N1)
    pi[0], pi[1] = N0 / len(x), N1 / len(x)

print(f"Estimated: mu=({mu[0]:.2f}, {mu[1]:.2f}), sigma=({sigma[0]:.2f}, {sigma[1]:.2f})")
print(f"True:      mu=(-2.00, 3.00), sigma=(0.80, 1.20)")
print(f"Mix weights: ({pi[0]:.2f}, {pi[1]:.2f}) vs true (0.40, 0.60)")
```

å‡ºåŠ›:
```
Estimated: mu=(-1.99, 3.06), sigma=(0.78, 1.18)
True:      mu=(-2.00, 3.00), sigma=(0.80, 1.20)
Mix weights: (0.39, 0.61) vs true (0.40, 0.60)
```

**ãŸã£ãŸ10å›ã®åå¾©ã§ã€ã€Œè¦‹ãˆãªã„ã€æ½œåœ¨å¤‰æ•° $z$ ã®æ§‹é€ ã‚’æ­£ç¢ºã«å¾©å…ƒã§ãã¦ã„ã‚‹ã€‚** ã“ã‚ŒãŒEMç®—æ³•ã®å¨åŠ›ã ã€‚

ã“ã®èƒŒå¾Œã«ã‚ã‚‹æ•°å¼:

$$
p(x \mid \theta) = \sum_{k=1}^{K} \pi_k \, \mathcal{N}(x \mid \mu_k, \sigma_k^2)
$$

ã€Œæ··åˆã€(mixture) ã¨ã„ã†è¨€è‘‰ã®é€šã‚Šã€è¤‡æ•°ã®ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã‚’é‡ã¿ $\pi_k$ ã§æ··ãœåˆã‚ã›ã¦ã„ã‚‹ã€‚ã©ã®æˆåˆ†ã‹ã‚‰ç”Ÿæˆã•ã‚ŒãŸã‹ã‚’è¡¨ã™ $z$ ãŒæ½œåœ¨å¤‰æ•°ã§ã‚ã‚Šã€EMç®—æ³•ã¯ã“ã® $z$ ã‚’æ¨å®šã—ãªãŒã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta = \{\mu_k, \sigma_k, \pi_k\}$ ã‚’æœ€é©åŒ–ã™ã‚‹ã€‚

:::message
**é€²æ—: 3% å®Œäº†** EMç®—æ³•ãŒã€Œè¦‹ãˆãªã„å¤‰æ•°ã‚’æ¨å®šã™ã‚‹ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã‚ã‚‹ã“ã¨ã‚’ä½“æ„Ÿã—ãŸã€‚ã“ã“ã‹ã‚‰ç†è«–ã®æ·±ã¿ã«å…¥ã£ã¦ã„ãã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‹•ã‹ã—ã¦ç†è§£ã™ã‚‹

### 1.1 ã‚¬ã‚¦ã‚¹æ··åˆãƒ¢ãƒ‡ãƒ«ã®æŒ™å‹•ã‚’è§¦ã‚‹

Zone 0ã§è¦‹ãŸã‚¬ã‚¦ã‚¹æ··åˆãƒ¢ãƒ‡ãƒ« (GMM: Gaussian Mixture Model) ã‚’ã‚‚ã†å°‘ã—è©³ã—ãè§¦ã£ã¦ã¿ã‚ˆã†ã€‚

$$
p(x \mid \theta) = \sum_{k=1}^{K} \pi_k \, \mathcal{N}(x \mid \mu_k, \sigma_k^2), \quad \sum_{k=1}^{K} \pi_k = 1, \quad \pi_k \geq 0
$$

| è¨˜å· | èª­ã¿ | æ„å‘³ |
|:-----|:-----|:-----|
| $K$ | ã‚±ãƒ¼ | æ··åˆæˆåˆ†ã®æ•° |
| $\pi_k$ | ãƒ‘ã‚¤ ã‚±ãƒ¼ | ç¬¬ $k$ æˆåˆ†ã®æ··åˆé‡ã¿ï¼ˆäº‹å‰ç¢ºç‡ï¼‰ |
| $\mu_k$ | ãƒŸãƒ¥ãƒ¼ ã‚±ãƒ¼ | ç¬¬ $k$ æˆåˆ†ã®å¹³å‡ |
| $\sigma_k^2$ | ã‚·ã‚°ãƒ ã‚±ãƒ¼ äºŒä¹— | ç¬¬ $k$ æˆåˆ†ã®åˆ†æ•£ |
| $\mathcal{N}(x \mid \mu, \sigma^2)$ | ãƒãƒ¼ãƒãƒ« | ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®ç¢ºç‡å¯†åº¦é–¢æ•° |

æ··åˆé‡ã¿ $\pi_k$ ã®å€¤ã‚’å¤‰ãˆã‚‹ã¨ã€ãƒ‡ãƒ¼ã‚¿ã®ã€Œåã‚Šã€ãŒå¤‰ã‚ã‚‹:

```python
import numpy as np

def gmm_pdf(x, mus, sigmas, pis):
    """Gaussian Mixture Model PDF.

    corresponds to: p(x|Î¸) = Î£_k Ï€_k N(x|Î¼_k, Ïƒ_kÂ²)
    """
    pdf = np.zeros_like(x)
    for mu, sigma, pi in zip(mus, sigmas, pis):
        pdf += pi * np.exp(-0.5 * ((x - mu) / sigma)**2) / (sigma * np.sqrt(2 * np.pi))
    return pdf

x = np.linspace(-8, 12, 500)
mus = [-2.0, 3.0, 7.0]
sigmas = [1.0, 1.5, 0.8]

# Different mixing weights
configs = [
    ([0.33, 0.34, 0.33], "Equal weights"),
    ([0.7, 0.2, 0.1],   "Dominant left"),
    ([0.1, 0.1, 0.8],   "Dominant right"),
    ([0.05, 0.9, 0.05],  "Dominant center"),
]

for pis, label in configs:
    pdf = gmm_pdf(x, mus, sigmas, pis)
    peak_x = x[np.argmax(pdf)]
    print(f"Ï€={pis} ({label:16s}) | peak at x={peak_x:.1f}, max_density={pdf.max():.4f}")
```

å‡ºåŠ›:
```
Ï€=[0.33, 0.34, 0.33] (Equal weights   ) | peak at x=7.0, max_density=0.1646
Ï€=[0.7, 0.2, 0.1]    (Dominant left    ) | peak at x=-2.0, max_density=0.2797
Ï€=[0.1, 0.1, 0.8]    (Dominant right   ) | peak at x=7.0, max_density=0.3989
Ï€=[0.05, 0.9, 0.05]  (Dominant center  ) | peak at x=3.0, max_density=0.2394
```

**æ··åˆé‡ã¿ $\pi_k$ ã‚’å¤‰ãˆã‚‹ã ã‘ã§ã€å¯†åº¦ã®ãƒ”ãƒ¼ã‚¯ä½ç½®ã¨å½¢çŠ¶ãŒå¤§ããå¤‰ã‚ã‚‹ã€‚** è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã“ã‚Œã‚‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¨å®šã™ã‚‹ã®ãŒEMç®—æ³•ã®ä»•äº‹ã ã€‚

### 1.2 EMã®åå¾©éç¨‹ã‚’å¯è¦–åŒ–ã™ã‚‹

EMç®—æ³•ã®æ ¸å¿ƒã¯ã€ŒE-stepï¼ˆæœŸå¾…å€¤è¨ˆç®—ï¼‰â†’ M-stepï¼ˆæœ€å¤§åŒ–ï¼‰ã€ã®åå¾©ã«ã‚ã‚‹ã€‚å„ã‚¹ãƒ†ãƒƒãƒ—ã§ä½•ãŒèµ·ãã¦ã„ã‚‹ã®ã‹ã‚’æ•°å€¤ã§è¿½è·¡ã—ã‚ˆã†ã€‚

```python
import numpy as np

np.random.seed(42)
# True parameters
true_mu = np.array([-2.0, 4.0])
true_sigma = np.array([1.0, 1.5])
true_pi = np.array([0.3, 0.7])

# Generate data
N = 300
z = np.random.choice([0, 1], size=N, p=true_pi)
x = np.where(z == 0,
             np.random.normal(true_mu[0], true_sigma[0], N),
             np.random.normal(true_mu[1], true_sigma[1], N))

# EM with tracking
mu = np.array([0.0, 1.0])  # bad initial guess
sigma = np.array([2.0, 2.0])
pi_k = np.array([0.5, 0.5])

def log_likelihood(x, mu, sigma, pi_k):
    """Compute log-likelihood: Î£_n log Î£_k Ï€_k N(x_n|Î¼_k, Ïƒ_kÂ²)"""
    ll = 0.0
    for n in range(len(x)):
        p = sum(pi_k[k] * np.exp(-0.5*((x[n]-mu[k])/sigma[k])**2) / (sigma[k]*np.sqrt(2*np.pi))
                for k in range(len(mu)))
        ll += np.log(p + 1e-300)
    return ll

print(f"{'Step':>4} | {'mu_0':>7} {'mu_1':>7} | {'sigma_0':>7} {'sigma_1':>7} | {'pi_0':>5} {'pi_1':>5} | {'log-lik':>10}")
print("-" * 80)

for step in range(15):
    ll = log_likelihood(x, mu, sigma, pi_k)
    print(f"{step:4d} | {mu[0]:7.3f} {mu[1]:7.3f} | {sigma[0]:7.3f} {sigma[1]:7.3f} | {pi_k[0]:5.3f} {pi_k[1]:5.3f} | {ll:10.2f}")

    # E-step: Î³(z_nk) = Ï€_k N(x_n|Î¼_k,Ïƒ_kÂ²) / Î£_j Ï€_j N(x_n|Î¼_j,Ïƒ_jÂ²)
    pdf = np.zeros((N, 2))
    for k in range(2):
        pdf[:, k] = pi_k[k] * np.exp(-0.5*((x - mu[k])/sigma[k])**2) / (sigma[k]*np.sqrt(2*np.pi))
    gamma = pdf[:, 1] / (pdf.sum(axis=1) + 1e-300)

    # M-step
    N_k = np.array([(1 - gamma).sum(), gamma.sum()])
    mu[0] = ((1 - gamma) * x).sum() / N_k[0]
    mu[1] = (gamma * x).sum() / N_k[1]
    sigma[0] = np.sqrt(((1 - gamma) * (x - mu[0])**2).sum() / N_k[0])
    sigma[1] = np.sqrt((gamma * (x - mu[1])**2).sum() / N_k[1])
    pi_k = N_k / N

ll = log_likelihood(x, mu, sigma, pi_k)
print(f"{'FINAL':>4} | {mu[0]:7.3f} {mu[1]:7.3f} | {sigma[0]:7.3f} {sigma[1]:7.3f} | {pi_k[0]:5.3f} {pi_k[1]:5.3f} | {ll:10.2f}")
print(f"\nTrue | {true_mu[0]:7.3f} {true_mu[1]:7.3f} | {true_sigma[0]:7.3f} {true_sigma[1]:7.3f} | {true_pi[0]:5.3f} {true_pi[1]:5.3f}")
```

ã“ã“ã§æ³¨ç›®ã—ã¦ã»ã—ã„ã®ã¯ **å¯¾æ•°å°¤åº¦ (log-likelihood) ãŒå˜èª¿ã«å¢—åŠ ã—ã¦ã„ã‚‹** ã“ã¨ã ã€‚ã“ã‚Œã¯å¶ç„¶ã§ã¯ãªã„ã€‚EMç®—æ³•ã®ç†è«–çš„ä¿è¨¼ã§ã‚ã‚Šã€Zone 3 ã§å³å¯†ã«è¨¼æ˜ã™ã‚‹ã€‚

:::message
ã“ã“ã§å¤šãã®äººãŒæ··ä¹±ã™ã‚‹ã®ãŒã€Œãªãœç›´æ¥æœ€å°¤æ¨å®šã—ãªã„ã®ã‹ã€ã ã€‚ç­”ãˆã¯å˜ç´”ã§ã€$\log \sum_k \pi_k \mathcal{N}(x \mid \mu_k, \sigma_k^2)$ ã® $\log$ ã®ä¸­ã« $\sum$ ãŒã‚ã‚‹ãŸã‚ã€å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã¤ã„ã¦è§£æçš„ã«å¾®åˆ†ã—ã¦ã‚¼ãƒ­ã¨ç½®ãã“ã¨ãŒã§ããªã„ã€‚EMç®—æ³•ã¯ã“ã®å›°é›£ã‚’æ½œåœ¨å¤‰æ•°ã®å°å…¥ã§å›é¿ã™ã‚‹ã€‚
:::

### 1.3 LLMã®éš ã‚Œå±¤ â€” Transformerã®æ½œåœ¨å¤‰æ•°çš„è§£é‡ˆ

æœ¬ã‚·ãƒªãƒ¼ã‚ºã§ã¯å„è¬›ç¾©ã§LLM/Transformerã¨ã®æ¥ç‚¹ã‚’ç¤ºã™ã€‚ç¬¬8å›ã®ãƒ†ãƒ¼ãƒã€Œæ½œåœ¨å¤‰æ•°ã€ã¯ã€Transformerã®éš ã‚Œå±¤ã¨ç›´çµã—ã¦ã„ã‚‹ã€‚

Transformerã®å„å±¤ã§è¨ˆç®—ã•ã‚Œã‚‹éš ã‚ŒçŠ¶æ…‹ $\mathbf{h}_l \in \mathbb{R}^d$ ã¯ã€å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã®ã€Œæ½œåœ¨çš„ãªè¡¨ç¾ã€ã :

$$
\mathbf{h}_l = \text{TransformerLayer}_l(\mathbf{h}_{l-1}), \quad l = 1, \ldots, L
$$

å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ $x_1, \ldots, x_T$ ã¯è¦³æ¸¬å¤‰æ•°ã€‚éš ã‚ŒçŠ¶æ…‹ $\mathbf{h}_1, \ldots, \mathbf{h}_L$ ã¯æ½œåœ¨å¤‰æ•°ã€‚ã“ã®æ§‹é€ ã¯æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«ãã®ã‚‚ã®ã ã€‚

```python
import numpy as np

# Simplified transformer hidden state computation
def transformer_layer(h_prev, W_attn, W_ff):
    """One transformer layer: attention + feedforward.

    h_l = FFN(Attention(h_{l-1})) â€” simplified, no LayerNorm/residual
    """
    # Self-attention (simplified): softmax(h @ W_attn @ h.T) @ h
    scores = h_prev @ W_attn @ h_prev.T
    scores = scores - scores.max(axis=-1, keepdims=True)
    weights = np.exp(scores) / np.exp(scores).sum(axis=-1, keepdims=True)
    h_attn = weights @ h_prev

    # Feedforward
    h_out = np.tanh(h_attn @ W_ff)
    return h_out

# 3 tokens, hidden dim 4, 2 layers
np.random.seed(42)
seq_len, d_model = 3, 4
h_0 = np.random.randn(seq_len, d_model)  # input embeddings (observed)

print("Layer 0 (observed input):")
print(np.round(h_0, 3))

for layer in range(1, 3):
    W_attn = np.random.randn(d_model, d_model) * 0.5
    W_ff = np.random.randn(d_model, d_model) * 0.5
    h_0 = transformer_layer(h_0, W_attn, W_ff)
    print(f"\nLayer {layer} (latent representation):")
    print(np.round(h_0, 3))
```

**å…¥åŠ›ï¼ˆè¦³æ¸¬ï¼‰ã‹ã‚‰éš ã‚Œå±¤ï¼ˆæ½œåœ¨ï¼‰ã¸ã®å¤‰æ›ã€‚** ã“ã‚Œã“ãæ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«ã®æœ¬è³ªã ã€‚VAE [^2] ã¯ã€ã“ã®æ½œåœ¨è¡¨ç¾ã«ç¢ºç‡çš„ãªæ§‹é€ ã‚’ä¸ãˆã‚‹ã“ã¨ã§ã€Œç”Ÿæˆã€ã‚’å¯èƒ½ã«ã™ã‚‹ã€‚ãã®æ©‹æ¸¡ã—ãŒã€ã“ã®ç¬¬8å›ã®æœ€å¤§ã®ç›®çš„ã ã€‚

:::details PyTorch ã® Transformer éš ã‚ŒçŠ¶æ…‹
PyTorch ã§ã¯ `nn.TransformerEncoderLayer` ãŒä¸Šã®ã‚³ãƒ¼ãƒ‰ã«å¯¾å¿œã™ã‚‹:

```python
import torch
import torch.nn as nn

layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)
x = torch.randn(1, 10, 512)  # (batch, seq_len, d_model)
h = layer(x)  # latent representation
print(f"Input shape:  {x.shape}")
print(f"Output shape: {h.shape}")
# Both (1, 10, 512) â€” same shape, but h encodes contextual information
```

å…¥åŠ›ã¨å‡ºåŠ›ã®å½¢çŠ¶ã¯åŒã˜ã ãŒã€$\mathbf{h}$ ã«ã¯æ–‡è„ˆæƒ…å ±ãŒå‡ç¸®ã•ã‚Œã¦ã„ã‚‹ã€‚ã“ã‚ŒãŒã€Œæ½œåœ¨è¡¨ç¾ã€ã ã€‚
:::

### 1.4 æ•°å¼â†’ã‚³ãƒ¼ãƒ‰å¯¾å¿œè¡¨

| æ•°å¼ | ã‚³ãƒ¼ãƒ‰ | æ„å‘³ |
|:-----|:-------|:-----|
| $p(x \mid \theta) = \sum_k \pi_k \mathcal{N}(x \mid \mu_k, \sigma_k^2)$ | `pdf += pi[k] * norm.pdf(x, mu[k], sigma[k])` | GMMå¯†åº¦ |
| $\gamma(z_{nk}) = \frac{\pi_k \mathcal{N}(x_n \mid \mu_k, \sigma_k^2)}{\sum_j \pi_j \mathcal{N}(x_n \mid \mu_j, \sigma_j^2)}$ | `gamma = pdf[:, k] / pdf.sum(axis=1)` | è²¬ä»»åº¦ï¼ˆE-stepï¼‰ |
| $\mu_k^{\text{new}} = \frac{\sum_n \gamma(z_{nk}) x_n}{\sum_n \gamma(z_{nk})}$ | `mu[k] = (gamma * x).sum() / gamma.sum()` | å¹³å‡æ›´æ–°ï¼ˆM-stepï¼‰ |
| $\pi_k^{\text{new}} = \frac{N_k}{N}$ | `pi[k] = gamma.sum() / N` | é‡ã¿æ›´æ–°ï¼ˆM-stepï¼‰ |

**æ•°å¼ã®å„è¨˜å·ãŒã‚³ãƒ¼ãƒ‰ã®å„è¡Œã«1å¯¾1ã§å¯¾å¿œã™ã‚‹ã€‚** ã“ã®å¯¾å¿œã‚’æ„è­˜ã—ãªãŒã‚‰ã€Zone 3 ã§æ•°å¼ã‚’å®Œå…¨ã«å°å‡ºã™ã‚‹ã€‚

```mermaid
graph TD
    A["è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ xâ‚,...,xâ‚™"] --> B["åˆæœŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ Î¸â°"]
    B --> C["E-step<br/>Î³(zâ‚™â‚–) = è²¬ä»»åº¦è¨ˆç®—"]
    C --> D["M-step<br/>Î¸^new = ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°"]
    D --> E{"åæŸï¼Ÿ"}
    E -->|No| C
    E -->|Yes| F["æœ€çµ‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ Î¸*"]

    style C fill:#e3f2fd
    style D fill:#fff3e0
    style F fill:#c8e6c9
```

> **Zone 1 ã¾ã¨ã‚**: GMMã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤‰ãˆã¦æŒ™å‹•ã‚’ä½“æ„Ÿã—ã€EMç®—æ³•ã®åå¾©éç¨‹ã‚’æ•°å€¤ã§è¿½è·¡ã—ã€Transformerã®éš ã‚Œå±¤ãŒæ½œåœ¨å¤‰æ•°ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ãŸã€‚æ•°å¼â†’ã‚³ãƒ¼ãƒ‰å¯¾å¿œã‚’æ‰‹ã«å…¥ã‚ŒãŸã€‚

:::message
**é€²æ—: 10% å®Œäº†** ä½“é¨“ã‚¾ãƒ¼ãƒ³ã‚¯ãƒªã‚¢ã€‚æ½œåœ¨å¤‰æ•°ã¨EMç®—æ³•ã®ç›´æ„Ÿã‚’æ´ã‚“ã ã€‚æ¬¡ã¯ã€Œãªãœæ½œåœ¨å¤‰æ•°ãŒå¿…è¦ãªã®ã‹ã€ã‚’æ·±ãç†è§£ã™ã‚‹ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœæ½œåœ¨å¤‰æ•°ãŒå¿…è¦ãªã®ã‹

### 2.1 è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã ã‘ã§ã¯ä¸ååˆ†ãªç†ç”±

ç¾å®Ÿã®ãƒ‡ãƒ¼ã‚¿ã«ã¯ã€ç›´æ¥è¦³æ¸¬ã§ããªã„ã€Œéš ã‚ŒãŸåŸå› ã€ãŒã»ã¼å¿…ãšå­˜åœ¨ã™ã‚‹ã€‚

- æ‰‹æ›¸ãæ•°å­—ç”»åƒ â†’ ã€Œã©ã®æ•°å­—ã‚’æ›¸ã“ã†ã¨ã—ãŸã‹ã€ã¯è¦‹ãˆãªã„
- éŸ³å£°æ³¢å½¢ â†’ ã€Œã©ã®éŸ³ç´ ã‚’ç™ºå£°ä¸­ã‹ã€ã¯ç›´æ¥è¦³æ¸¬ã§ããªã„
- é¡§å®¢è³¼è²·å±¥æ­´ â†’ ã€Œã©ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«å±ã™ã‚‹ã‹ã€ã¯ãƒ©ãƒ™ãƒ«ãŒãªã„
- ãƒ†ã‚­ã‚¹ãƒˆã®å˜èªåˆ— â†’ ã€Œãƒˆãƒ”ãƒƒã‚¯ã€ã¯æ˜ç¤ºã•ã‚Œã¦ã„ãªã„

ã“ã‚Œã‚‰ã®éš ã‚ŒãŸåŸå› ã‚’æ•°å­¦çš„ã«æ‰±ã†æ çµ„ã¿ãŒ **æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«** ã ã€‚

> **ä¸€è¨€ã§è¨€ãˆã°**: æ½œåœ¨å¤‰æ•° = ã€Œãƒ‡ãƒ¼ã‚¿ã®è£ã«ã‚ã‚‹è¦‹ãˆãªã„åŸå› ã‚’è¡¨ã™ç¢ºç‡å¤‰æ•°ã€

æ•°å¼ã§æ›¸ãã¨:

$$
p(\mathbf{x} \mid \theta) = \sum_{\mathbf{z}} p(\mathbf{x}, \mathbf{z} \mid \theta) = \sum_{\mathbf{z}} p(\mathbf{x} \mid \mathbf{z}, \theta) \, p(\mathbf{z} \mid \theta)
$$

é€£ç¶šã®å ´åˆã¯ $\sum$ ã‚’ $\int$ ã«ç½®ãæ›ãˆã‚‹:

$$
p(\mathbf{x} \mid \theta) = \int p(\mathbf{x} \mid \mathbf{z}, \theta) \, p(\mathbf{z} \mid \theta) \, d\mathbf{z}
$$

**ã“ã®ç©åˆ†ï¼ˆå‘¨è¾ºåŒ–ï¼‰ãŒè¨ˆç®—å›°é›£ã§ã‚ã‚‹ã¨ã„ã†äº‹å®ŸãŒã€EMç®—æ³•ã‚’å¿…è¦ã¨ã™ã‚‹æ ¹æœ¬çš„ãªç†ç”±ã ã€‚**

### 2.2 ç¬¬7å›ã‹ã‚‰ã®æ¥ç¶š â€” æœ€å°¤æ¨å®šã®é™ç•Œ

ç¬¬7å›ã§å­¦ã‚“ã æœ€å°¤æ¨å®š (MLE) ã‚’æŒ¯ã‚Šè¿”ã‚ã†ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã‚’æ¨å®šã™ã‚‹ã«ã¯å¯¾æ•°å°¤åº¦ã‚’æœ€å¤§åŒ–ã™ã‚‹:

$$
\hat{\theta}_{\text{MLE}} = \arg\max_\theta \sum_{n=1}^{N} \log p(x_n \mid \theta)
$$

å˜ä¸€ã®ã‚¬ã‚¦ã‚¹åˆ†å¸ƒãªã‚‰ã€$\log$ ã®ä¸­èº«ãŒ $\mathcal{N}(x_n \mid \mu, \sigma^2)$ ã ã‹ã‚‰è§£æçš„ã«è§£ã‘ã‚‹ã€‚ã ãŒGMMã§ã¯:

$$
\log p(x_n \mid \theta) = \log \sum_{k=1}^{K} \pi_k \, \mathcal{N}(x_n \mid \mu_k, \sigma_k^2)
$$

**$\log$ ã®ä¸­ã« $\sum$ ãŒã‚ã‚‹ã€‚** ã“ã‚ŒãŒå…¨ã¦ã®å›°é›£ã®å…ƒå‡¶ã ã€‚$\log$ ã¨ $\sum$ ã¯äº¤æ›ã§ããªã„ã‹ã‚‰ã€$\frac{\partial}{\partial \mu_k} \log \sum_k (\cdots) = 0$ ã‚’è§£æçš„ã«è§£ãã“ã¨ãŒã§ããªã„ã€‚

```python
import numpy as np

# Single Gaussian: log-likelihood has clean derivative
# d/dÎ¼ log N(x|Î¼,ÏƒÂ²) = (x - Î¼) / ÏƒÂ²  â†’ set to 0 â†’ Î¼ = xÌ„ (sample mean)

x = np.array([1.0, 2.0, 3.0, 4.0, 5.0])
mu_mle = x.mean()
print(f"Single Gaussian MLE: Î¼ = {mu_mle:.1f} (just the sample mean!)")

# GMM: log Î£_k Ï€_k N(x|Î¼_k,Ïƒ_kÂ²) â€” no closed-form solution
# The log-sum structure prevents analytic optimization
def gmm_log_likelihood(x, mus, sigmas, pis):
    """log p(x|Î¸) = Î£_n log Î£_k Ï€_k N(x_n|Î¼_k,Ïƒ_kÂ²)"""
    ll = 0.0
    for xn in x:
        p = sum(pi * np.exp(-0.5*((xn-mu)/sig)**2)/(sig*np.sqrt(2*np.pi))
                for mu, sig, pi in zip(mus, sigmas, pis))
        ll += np.log(p)
    return ll

# Try different Î¼ values â€” no single formula gives the answer
for mu0 in [-3, -2, -1, 0]:
    ll = gmm_log_likelihood(x, [mu0, 5.0], [1.0, 1.0], [0.5, 0.5])
    print(f"GMM log-lik with Î¼â‚€={mu0:3d}: {ll:.4f}  (no closed-form for optimal Î¼â‚€)")
```

### 2.3 Course I ãƒ•ã‚£ãƒŠãƒ¼ãƒ¬ã®ä½ç½®ã¥ã‘

æœ¬è¬›ç¾©ã¯ Course Iã€Œæ•°å­¦åŸºç¤ç·¨ã€ã®æœ€çµ‚å›ã ã€‚8å›ã®æ•°å­¦ã®æ—…ã‚’ä¿¯ç°ã—ã‚ˆã†ã€‚

```mermaid
graph TD
    L1["ç¬¬1å›: æ¦‚è«–<br/>æ•°å¼ãƒªãƒ†ãƒ©ã‚·ãƒ¼"] --> L2["ç¬¬2å›: ç·šå½¢ä»£æ•° I<br/>ãƒ™ã‚¯ãƒˆãƒ«ãƒ»è¡Œåˆ—"]
    L2 --> L3["ç¬¬3å›: ç·šå½¢ä»£æ•° II<br/>SVDãƒ»è¡Œåˆ—å¾®åˆ†"]
    L3 --> L4["ç¬¬4å›: ç¢ºç‡è«–<br/>åˆ†å¸ƒãƒ»ãƒ™ã‚¤ã‚º"]
    L4 --> L5["ç¬¬5å›: æ¸¬åº¦è«–<br/>å³å¯†ãªç¢ºç‡"]
    L5 --> L6["ç¬¬6å›: æƒ…å ±ç†è«–ãƒ»æœ€é©åŒ–<br/>KLãƒ»SGD"]
    L6 --> L7["ç¬¬7å›: æœ€å°¤æ¨å®šã¨çµ±è¨ˆçš„æ¨è«–<br/>æ¨å®šé‡ã®æ•°å­¦çš„åŸºç›¤"]
    L7 --> L8["ç¬¬8å›: æ½œåœ¨å¤‰æ•° & EMç®—æ³•<br/>â˜… Course I ãƒ•ã‚£ãƒŠãƒ¼ãƒ¬"]
    L8 -->|"EMã®é™ç•Œ: äº‹å¾Œåˆ†å¸ƒãŒ<br/>è§£æçš„ã«è¨ˆç®—ä¸èƒ½"| L9["ç¬¬9å›: å¤‰åˆ†æ¨è«– & ELBO<br/>âš¡ Juliaåˆç™»å ´"]

    style L8 fill:#ff9800,color:#fff
    style L9 fill:#4caf50,color:#fff
```

| Course I è¬›ç¾© | ä½•ã‚’ç²å¾—ã—ãŸã‹ | ä½•ãŒã€Œè¶³ã‚Šãªã„ã€ã‹ |
|:-------------|:-------------|:----------------|
| ç¬¬1å›: æ¦‚è«– | æ•°å¼ã®èª­ã¿æ–¹ | ç·šå½¢ä»£æ•°ã®é“å…·ãŒå¿…è¦ |
| ç¬¬2å›: ç·šå½¢ä»£æ•° I | ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã€è¡Œåˆ—æ¼”ç®— | åˆ†è§£ã¨å¾®åˆ†ãŒå¿…è¦ |
| ç¬¬3å›: ç·šå½¢ä»£æ•° II | SVDã€è¡Œåˆ—å¾®åˆ†ã€Backprop | ä¸ç¢ºå®Ÿæ€§ã®æ‰±ã„ãŒå¿…è¦ |
| ç¬¬4å›: ç¢ºç‡è«– | ç¢ºç‡åˆ†å¸ƒã€ãƒ™ã‚¤ã‚ºã®å®šç† | å³å¯†ãªç¢ºç‡è«–ãŒå¿…è¦ |
| ç¬¬5å›: æ¸¬åº¦è«– | Lebesgueç©åˆ†ã€ç¢ºç‡éç¨‹ | åˆ†å¸ƒé–“ã®è·é›¢ãŒå¿…è¦ |
| ç¬¬6å›: æƒ…å ±ç†è«–ãƒ»æœ€é©åŒ– | KLã€SGDã€Adam | ç¢ºç‡åˆ†å¸ƒã®å­¦ç¿’ãŒå¿…è¦ |
| ç¬¬7å›: æœ€å°¤æ¨å®šã¨çµ±è¨ˆçš„æ¨è«– | æœ€å°¤æ¨å®šã€æ¨å®šé‡ã®åˆ†é¡ä½“ç³» | æ½œåœ¨å¤‰æ•°ã®æ‰±ã„ãŒå¿…è¦ |
| **ç¬¬8å›: EMç®—æ³•** | **æ½œåœ¨å¤‰æ•°ã®æ¨å®š** | **äº‹å¾Œåˆ†å¸ƒã®è¿‘ä¼¼ãŒå¿…è¦ â†’ ç¬¬9å›ã¸** |

**å„è¬›ç¾©ã®ã€Œé™ç•Œã€ãŒæ¬¡ã®è¬›ç¾©ã®ã€Œå‹•æ©Ÿã€ã«ãªã‚‹ã€‚** ãã—ã¦ç¬¬8å›ã®é™ç•Œ â€” EMç®—æ³•ã§ã¯äº‹å¾Œåˆ†å¸ƒ $p(\mathbf{z} \mid \mathbf{x}, \theta)$ ãŒè§£æçš„ã«è¨ˆç®—ã§ããªã„ã‚±ãƒ¼ã‚¹ã«å¯¾å¿œã§ããªã„ â€” ãŒã€ç¬¬9å›ï¼ˆå¤‰åˆ†æ¨è«–ï¼‰ã®å‹•æ©Ÿã«ãªã‚‹ã€‚

### 2.4 æ¾å°¾ç ”ã¨ã®å¯¾æ¯”

| é …ç›® | æ¾å°¾ãƒ»å²©æ¾¤ç ” | æœ¬ã‚·ãƒªãƒ¼ã‚ºï¼ˆç¬¬8å›ï¼‰ |
|:-----|:-----------|:----------------|
| EMç®—æ³• | ã€ŒEMãŒã‚ã‚Šã¾ã™ã€ç¨‹åº¦ã®ç´¹ä»‹ | **å®Œå…¨å°å‡º**: Jensenä¸ç­‰å¼ â†’ ELBO â†’ E-step/M-step â†’ åæŸè¨¼æ˜ |
| GMM | çµæœã®ã¿ | è²¬ä»»åº¦ã®å°å‡ºã€Singularityå•é¡Œã€BIC/AIC |
| HMM | è¨€åŠãªã— | Forward-Backwardã€Viterbiã€Baum-Welch |
| VAEã¸ã®æ©‹ | å”çªã«VAE | EM â†’ Variational EM â†’ ELBO â†’ VAE ã¸ã®è‡ªç„¶ãªæ¥ç¶š |
| Pythoné€Ÿåº¦ | æ¸¬å®šãªã— | Profileçµæœ: **ã€Œé…ã™ããªã„ï¼Ÿã€** â†’ ç¬¬9å›Juliaç™»å ´ã®ä¼ç·š |

### 2.5 3ã¤ã®æ¯”å–©ã§æ‰ãˆã‚‹ã€Œæ½œåœ¨å¤‰æ•°ã€

**æ¯”å–©1: æ°·å±±**

è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã¯æ°´é¢ä¸Šã®æ°·å±±ã®ä¸€è§’ã€‚æ½œåœ¨å¤‰æ•°ã¯æ°´é¢ä¸‹ã®å·¨å¤§ãªæ§‹é€ ã€‚ãƒ‡ãƒ¼ã‚¿ã®è£ã«ã‚ã‚‹æ§‹é€ ã‚’æ¨å®šã™ã‚‹ã“ã¨ã¯ã€æ°´é¢ä¸Šã®å½¢çŠ¶ã‹ã‚‰æ°´é¢ä¸‹ã®å…¨ä½“åƒã‚’å¾©å…ƒã™ã‚‹ã“ã¨ã«ç­‰ã—ã„ã€‚

**æ¯”å–©2: çŠ¯ç½ªæœæŸ»**

ç¾å ´ã®è¨¼æ‹ ï¼ˆè¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ $\mathbf{x}$ï¼‰ã‹ã‚‰çŠ¯äººï¼ˆæ½œåœ¨å¤‰æ•° $\mathbf{z}$ï¼‰ã‚’æ¨å®šã™ã‚‹ã€‚è¨¼æ‹ ã¯ç›´æ¥è¦‹ãˆã‚‹ãŒã€çŠ¯äººã¯è¦‹ãˆãªã„ã€‚EMç®—æ³•ã¯ã€Œã¾ãšçŠ¯äººã®å€™è£œã‚’çµã‚Šï¼ˆE-stepï¼‰ã€æ¬¡ã«è¨¼æ‹ ã¨ã®æ•´åˆæ€§ã‚’æœ€å¤§åŒ–ã™ã‚‹ï¼ˆM-stepï¼‰ã€ã‚’ç¹°ã‚Šè¿”ã™æœæŸ»æ‰‹æ³•ã ã€‚

**æ¯”å–©3: æ¥½è­œã®å¾©å…ƒ**

æ¼”å¥ï¼ˆè¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ï¼‰ã‚’è´ã„ã¦ã€æ¥½è­œï¼ˆæ½œåœ¨æ§‹é€ ï¼‰ã‚’å¾©å…ƒã™ã‚‹ã€‚å„æ¥½å™¨ãŒä½•ã‚’å¼¾ã„ã¦ã„ã‚‹ã‹ï¼ˆæ½œåœ¨å¤‰æ•°ï¼‰ã¯ç›´æ¥è¦‹ãˆãªã„ãŒã€æ··åˆéŸ³ï¼ˆè¦³æ¸¬ï¼‰ã‹ã‚‰æ¨å®šã§ãã‚‹ã€‚ã“ã‚Œã¯éŸ³æºåˆ†é›¢å•é¡Œã§ã‚ã‚Šã€ã¾ã•ã«GMMã®å¿œç”¨ã ã€‚

### 2.6 Trojan Horse â€” Python ã®é™ç•ŒãŒè¦‹ãˆå§‹ã‚ã‚‹

:::details Trojan Horse: Pythoné€Ÿåº¦ã®ä¼ç·š
Course Iã¯å…¨ç·¨Pythonã ãŒã€æœ¬è¬›ç¾©ã§ã€Œã‚ã‚Œã€é…ããªã„ã‹ï¼Ÿã€ã¨ã„ã†ç–‘å¿µãŒèŠ½ç”Ÿãˆã‚‹ã€‚

EMç®—æ³•ã®å„åå¾©ã§å…¨ãƒ‡ãƒ¼ã‚¿ $N$ å€‹ã«å¯¾ã—ã¦è²¬ä»»åº¦ $\gamma(z_{nk})$ ã‚’è¨ˆç®—ã™ã‚‹ã€‚$K$ å€‹ã®æˆåˆ†ã€$T$ å›ã®åå¾©ã§ $O(NKT)$ å›ã®å¯†åº¦è¨ˆç®—ãŒå¿…è¦ã ã€‚

```python
import numpy as np
import time

np.random.seed(42)
N = 10000
K = 5
x = np.concatenate([np.random.normal(k * 3, 1.0, N // K) for k in range(K)])

mu = np.random.randn(K)
sigma = np.ones(K)
pi_k = np.ones(K) / K

start = time.perf_counter()
for step in range(100):
    # E-step
    pdf = np.zeros((N, K))
    for k in range(K):
        pdf[:, k] = pi_k[k] * np.exp(-0.5*((x - mu[k])/sigma[k])**2) / (sigma[k]*np.sqrt(2*np.pi))
    gamma = pdf / pdf.sum(axis=1, keepdims=True)

    # M-step
    N_k = gamma.sum(axis=0)
    for k in range(K):
        mu[k] = (gamma[:, k] * x).sum() / N_k[k]
        sigma[k] = np.sqrt((gamma[:, k] * (x - mu[k])**2).sum() / N_k[k])
    pi_k = N_k / N

elapsed = time.perf_counter() - start
print(f"EM (N={N}, K={K}, 100 iterations): {elapsed:.3f} sec")
print(f"Per iteration: {elapsed/100*1000:.1f} ms")
```

ã€Œ100åå¾©ã§æ•°ç§’ï¼Ÿ ã“ã‚Œã€ãƒ‡ãƒ¼ã‚¿ãŒ100ä¸‡ä»¶ã«ãªã£ãŸã‚‰......ï¼Ÿã€

ã“ã®ç–‘å¿µãŒç¬¬9å›ã§çˆ†ç™ºã™ã‚‹ã€‚ELBOè¨ˆç®—ã®Pythonå®Ÿè¡Œæ™‚é–“ã‚’è¨ˆæ¸¬ã—ãŸç¬é–“ã€Juliaã®è¡æ’ƒçš„ãªé€Ÿåº¦ãŒå¾…ã£ã¦ã„ã‚‹ã€‚**è¦šãˆã¦ãŠã„ã¦ãã ã•ã„ã€‚**
:::

> **Zone 2 ã¾ã¨ã‚**: æ½œåœ¨å¤‰æ•°ãŒå¿…è¦ãªç†ç”±ï¼ˆ$\log \sum$ ã®å›°é›£æ€§ï¼‰ã‚’ç†è§£ã—ã€Course I å…¨ä½“ã®ä¸­ã§ã®ç¬¬8å›ã®ä½ç½®ã¥ã‘ã‚’ç¢ºèªã—ã€EMç®—æ³•ãŒã€Œè¦‹ãˆãªã„åŸå› ã®æ¨å®šã€ã§ã‚ã‚‹ã“ã¨ã‚’3ã¤ã®æ¯”å–©ã§æ´ã‚“ã ã€‚

:::message
**é€²æ—: 20% å®Œäº†** ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ã‚¯ãƒªã‚¢ã€‚ã€Œãªãœæ½œåœ¨å¤‰æ•°ãŒå¿…è¦ã‹ã€ã€ŒãªãœEMç®—æ³•ãŒå¿…è¦ã‹ã€ã®å‹•æ©Ÿã‚’æ·±ãç†è§£ã—ãŸã€‚ã„ã‚ˆã„ã‚ˆæ•°å¼ä¿®è¡Œã«å…¥ã‚‹ã€‚æº–å‚™ã¯ã„ã„ã§ã™ã‹ï¼Ÿ
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” EMç®—æ³•ã®å®Œå…¨å°å‡º

ã“ã“ãŒæœ¬è¬›ç¾©ã®æ ¸å¿ƒã ã€‚Zone 0-1 ã§ã€Œå‹•ãã€ã“ã¨ã‚’ä½“æ„Ÿã—ãŸã€‚Zone 2 ã§ã€Œãªãœå¿…è¦ã‹ã€ã‚’ç†è§£ã—ãŸã€‚ã“ã“ã‹ã‚‰ã¯ã€Œãªãœå‹•ãã®ã‹ã€ã‚’æ•°å­¦çš„ã«è¨¼æ˜ã™ã‚‹ã€‚

**è¦šãˆã‚‹ãªã€‚å°å‡ºã—ã‚ã€‚** çµæœã‚’æš—è¨˜ã—ã¦ã‚‚å¿œç”¨ã§ããªã„ã€‚å°å‡ºéç¨‹ã‚’è‡ªåŠ›ã§å†ç¾ã§ãã¦ã¯ã˜ã‚ã¦ã€æ–°ã—ã„å•é¡Œã«é©ç”¨ã§ãã‚‹ã€‚

```mermaid
graph TD
    A["3.1 æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«ã®å®šå¼åŒ–"] --> B["3.2 å®Œå…¨/ä¸å®Œå…¨ãƒ‡ãƒ¼ã‚¿å¯¾æ•°å°¤åº¦"]
    B --> C["3.3 Jensenä¸ç­‰å¼"]
    C --> D["3.4 ELBOåˆ†è§£"]
    D --> E["3.5 EMç®—æ³•ã®å°å‡º"]
    E --> F["3.6 GMM E-step/M-step"]
    F --> G["3.7 åæŸæ€§è¨¼æ˜"]
    G --> H["3.8 âš”ï¸ Boss Battle"]

    style A fill:#e3f2fd
    style H fill:#ff5722,color:#fff
```

### 3.1 æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«ã®å®šå¼åŒ–

ã¾ãšè¨˜æ³•ã‚’æ•´ç†ã™ã‚‹ã€‚ç´™ã¨ãƒšãƒ³ã‚’ç”¨æ„ã—ã¦ã»ã—ã„ã€‚

**è¨­å®š**:
- è¦³æ¸¬å¤‰æ•°: $\mathbf{x} \in \mathcal{X}$ â€” å®Ÿéš›ã«æ¸¬å®šã§ãã‚‹ãƒ‡ãƒ¼ã‚¿
- æ½œåœ¨å¤‰æ•°: $\mathbf{z} \in \mathcal{Z}$ â€” ç›´æ¥è¦³æ¸¬ã§ããªã„éš ã‚ŒãŸå¤‰æ•°
- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: $\theta \in \Theta$ â€” æ¨å®šã—ãŸã„ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

**åŒæ™‚åˆ†å¸ƒ** (joint distribution):

$$
p(\mathbf{x}, \mathbf{z} \mid \theta)
$$

ã“ã‚ŒãŒã€Œå®Œå…¨ãƒ‡ãƒ¼ã‚¿ã€(complete data) ã®åˆ†å¸ƒã ã€‚$\mathbf{x}$ ã¨ $\mathbf{z}$ ã®ä¸¡æ–¹ãŒè¦³æ¸¬ã•ã‚Œã¦ã„ã‚Œã°ã€ã“ã®åˆ†å¸ƒã‚’ç›´æ¥æ‰±ãˆã‚‹ã€‚

**å‘¨è¾ºå°¤åº¦** (marginal likelihood / evidence):

$$
p(\mathbf{x} \mid \theta) = \sum_{\mathbf{z}} p(\mathbf{x}, \mathbf{z} \mid \theta)
$$

$\mathbf{z}$ ãŒé€£ç¶šã®å ´åˆã¯:

$$
p(\mathbf{x} \mid \theta) = \int p(\mathbf{x}, \mathbf{z} \mid \theta) \, d\mathbf{z}
$$

**äº‹å¾Œåˆ†å¸ƒ** (posterior distribution):

$$
p(\mathbf{z} \mid \mathbf{x}, \theta) = \frac{p(\mathbf{x}, \mathbf{z} \mid \theta)}{p(\mathbf{x} \mid \theta)} = \frac{p(\mathbf{x} \mid \mathbf{z}, \theta) \, p(\mathbf{z} \mid \theta)}{p(\mathbf{x} \mid \theta)}
$$

ã“ã‚Œã¯ãƒ™ã‚¤ã‚ºã®å®šç†ãã®ã‚‚ã®ã ï¼ˆç¬¬4å›ã§å­¦ã‚“ã ï¼‰ã€‚åˆ†æ¯ã® $p(\mathbf{x} \mid \theta)$ ãŒè¨ˆç®—å›°é›£ã§ã‚ã‚‹ã“ã¨ãŒã€å…¨ã¦ã®å›°é›£ã®æºæ³‰ã«ãªã‚‹ã€‚

| ç”¨èª | æ•°å¼ | ç›´æ„Ÿ |
|:-----|:-----|:-----|
| å®Œå…¨ãƒ‡ãƒ¼ã‚¿å°¤åº¦ | $p(\mathbf{x}, \mathbf{z} \mid \theta)$ | ã€Œè¦³æ¸¬ã€ã¨ã€Œéš ã‚Œã€ã®ä¸¡æ–¹ãŒã‚ã‹ã£ã¦ã„ã‚Œã°ç°¡å˜ |
| å‘¨è¾ºå°¤åº¦ (evidence) | $p(\mathbf{x} \mid \theta) = \sum_{\mathbf{z}} p(\mathbf{x}, \mathbf{z} \mid \theta)$ | éš ã‚Œã‚’æ¶ˆã™ã¨è¨ˆç®—å›°é›£ |
| äº‹å¾Œåˆ†å¸ƒ | $p(\mathbf{z} \mid \mathbf{x}, \theta)$ | è¦³æ¸¬ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã®éš ã‚Œã®æ¨å®š |
| è²¬ä»»åº¦ | $\gamma(z_{nk}) = p(z_n = k \mid x_n, \theta)$ | ãƒ‡ãƒ¼ã‚¿ $x_n$ ãŒæˆåˆ† $k$ ã‹ã‚‰æ¥ãŸç¢ºç‡ |

```python
import numpy as np

# Concrete example: GMM with K=2
# Joint: p(x, z=k|Î¸) = Ï€_k N(x|Î¼_k, Ïƒ_kÂ²)
# Marginal: p(x|Î¸) = Î£_k Ï€_k N(x|Î¼_k, Ïƒ_kÂ²)
# Posterior: p(z=k|x,Î¸) = Ï€_k N(x|Î¼_k,Ïƒ_kÂ²) / Î£_j Ï€_j N(x|Î¼_j,Ïƒ_jÂ²)

mu = np.array([-2.0, 3.0])
sigma = np.array([1.0, 1.5])
pi_k = np.array([0.4, 0.6])

def gaussian_pdf(x, mu, sigma):
    """N(x|Î¼,ÏƒÂ²) = (2Ï€ÏƒÂ²)^{-1/2} exp(-(x-Î¼)Â²/(2ÏƒÂ²))"""
    return np.exp(-0.5 * ((x - mu) / sigma)**2) / (sigma * np.sqrt(2 * np.pi))

x_test = np.array([0.0, -2.0, 3.0, 5.0])

print("x     | p(x,z=0|Î¸) | p(x,z=1|Î¸) | p(x|Î¸)  | p(z=0|x,Î¸) | p(z=1|x,Î¸)")
print("-" * 75)
for x_val in x_test:
    joint_0 = pi_k[0] * gaussian_pdf(x_val, mu[0], sigma[0])
    joint_1 = pi_k[1] * gaussian_pdf(x_val, mu[1], sigma[1])
    marginal = joint_0 + joint_1
    post_0 = joint_0 / marginal
    post_1 = joint_1 / marginal
    print(f"{x_val:5.1f} | {joint_0:10.6f} | {joint_1:10.6f} | {marginal:7.5f} | "
          f"{post_0:10.4f} | {post_1:10.4f}")
```

å‡ºåŠ›:
```
x     | p(x,z=0|Î¸) | p(x,z=1|Î¸) | p(x|Î¸)  | p(z=0|x,Î¸) | p(z=1|x,Î¸)
---------------------------------------------------------------------------
  0.0 |   0.048394 |   0.035994 | 0.08439 |     0.5734 |     0.4266
 -2.0 |   0.159155 |   0.006569 | 0.16572 |     0.9604 |     0.0396
  3.0 |   0.000036 |   0.159155 | 0.15919 |     0.0002 |     0.9998
  5.0 |   0.000000 |   0.064759 | 0.06476 |     0.0000 |     1.0000
```

**$x = -2$ ã®ãƒ‡ãƒ¼ã‚¿ã¯ 96% ã®ç¢ºç‡ã§æˆåˆ†0ã‹ã‚‰ã€$x = 3$ ã®ãƒ‡ãƒ¼ã‚¿ã¯ 99.98% ã®ç¢ºç‡ã§æˆåˆ†1ã‹ã‚‰æ¥ãŸ** ã¨æ¨å®šã•ã‚Œã‚‹ã€‚ã“ã‚ŒãŒäº‹å¾Œåˆ†å¸ƒ $p(z \mid x, \theta)$ ã®æ„å‘³ã ã€‚

### 3.2 å®Œå…¨ãƒ‡ãƒ¼ã‚¿å¯¾æ•°å°¤åº¦ã¨ä¸å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã®å›°é›£æ€§

**å®Œå…¨ãƒ‡ãƒ¼ã‚¿å¯¾æ•°å°¤åº¦** (complete-data log-likelihood):

$\mathbf{x}$ ã¨ $\mathbf{z}$ ã®ä¸¡æ–¹ãŒè¦³æ¸¬ã•ã‚Œã¦ã„ã‚‹å ´åˆ:

$$
\log p(\mathbf{x}, \mathbf{z} \mid \theta) = \log p(\mathbf{x} \mid \mathbf{z}, \theta) + \log p(\mathbf{z} \mid \theta)
$$

GMMã®å ´åˆã€$z_n = k$ ãŒã‚ã‹ã£ã¦ã„ã‚Œã°:

$$
\log p(\mathbf{x}, \mathbf{z} \mid \theta) = \sum_{n=1}^{N} \sum_{k=1}^{K} \mathbb{1}[z_n = k] \left( \log \pi_k + \log \mathcal{N}(x_n \mid \mu_k, \sigma_k^2) \right)
$$

ã“ã“ã§ $\mathbb{1}[z_n = k]$ ã¯æŒ‡ç¤ºé–¢æ•°ï¼ˆ$z_n = k$ ãªã‚‰1ã€ãã†ã§ãªã‘ã‚Œã°0ï¼‰ã€‚**$\log$ ã®ä¸­èº«ãŒå˜ä¸€ã®ã‚¬ã‚¦ã‚¹åˆ†å¸ƒãªã®ã§ã€å¾®åˆ†ã—ã¦ã‚¼ãƒ­ã¨ç½®ã‘ã‚‹ã€‚** ã¤ã¾ã‚Šè§£æè§£ãŒå­˜åœ¨ã™ã‚‹ã€‚

**ä¸å®Œå…¨ãƒ‡ãƒ¼ã‚¿å¯¾æ•°å°¤åº¦** (incomplete-data log-likelihood):

$\mathbf{z}$ ãŒè¦³æ¸¬ã•ã‚Œãªã„å ´åˆ:

$$
\log p(\mathbf{x} \mid \theta) = \log \sum_{\mathbf{z}} p(\mathbf{x}, \mathbf{z} \mid \theta)
$$

**$\log$ ã®ä¸­ã« $\sum$ ãŒã‚ã‚‹ã€‚** ã“ã‚ŒãŒè§£æè§£ã‚’é˜»ã‚€ã€‚

```python
import numpy as np

# Complete-data case: z is known â†’ closed-form MLE
np.random.seed(42)
N = 100
z_true = np.array([0]*40 + [1]*60)
x = np.where(z_true == 0,
             np.random.normal(-2, 1, N),
             np.random.normal(3, 1.5, N))

# When z is known, MLE is trivial
mask0 = (z_true == 0)
mask1 = (z_true == 1)
mu_mle = np.array([x[mask0].mean(), x[mask1].mean()])
sigma_mle = np.array([x[mask0].std(), x[mask1].std()])
pi_mle = np.array([mask0.sum() / N, mask1.sum() / N])

print("=== Complete data (z known) â†’ closed-form MLE ===")
print(f"Î¼ = ({mu_mle[0]:.3f}, {mu_mle[1]:.3f})")
print(f"Ïƒ = ({sigma_mle[0]:.3f}, {sigma_mle[1]:.3f})")
print(f"Ï€ = ({pi_mle[0]:.2f}, {pi_mle[1]:.2f})")
print("\nNo iteration needed! Just sample statistics.")
print("\n=== Incomplete data (z unknown) â†’ need EM ===")
print("Cannot compute sample statistics per component")
print("because we don't know which component each x_n belongs to.")
```

:::message
ã“ã“ãŒå…¨ã¦ã®ã‚«ã‚®ã ã€‚**$z$ ãŒã‚ã‹ã£ã¦ã„ã‚Œã°ç°¡å˜ã«è§£ã‘ã‚‹ã€‚$z$ ãŒã‚ã‹ã‚‰ãªã„ã‹ã‚‰é›£ã—ã„ã€‚** EMç®—æ³•ã¯ã€Œ$z$ ãŒã‚ã‹ã‚‰ãªã„ãªã‚‰ã€æ¨å®šã—ã¦ã—ã¾ãˆã€ã¨ã„ã†ç™ºæƒ³ã§ã€ã“ã®å›°é›£ã‚’å›é¿ã™ã‚‹ã€‚
:::

### 3.3 Jensenä¸ç­‰å¼ â€” EMç®—æ³•ã®æ•°å­¦çš„åŸºç›¤

EMç®—æ³•ã®ç†è«–çš„åŸºç›¤ã¯ **Jensenä¸ç­‰å¼** (Jensen's inequality) ã ã€‚ç¬¬5å›ã§æ¸¬åº¦è«–ã‚’å­¦ã‚“ã èª­è€…ã«ã¯é¦´æŸ“ã¿ãŒã‚ã‚‹ã ã‚ã†ã€‚

:::message alert
Jensenä¸ç­‰å¼ã®å‘ãã‚’é–“é•ãˆã‚‹äººãŒéå¸¸ã«å¤šã„ã€‚å‡¸é–¢æ•°ã¨å‡¹é–¢æ•°ã§ä¸ç­‰å·ã®å‘ããŒé€†è»¢ã™ã‚‹ã€‚ç´™ã«æ›¸ã„ã¦ç¢ºèªã—ã¦ã»ã—ã„ã€‚
:::

**å®šç† (Jensenä¸ç­‰å¼)**:  $f$ ãŒå‡¹é–¢æ•° (concave function) ã®ã¨ã:

$$
f\left( \mathbb{E}[X] \right) \geq \mathbb{E}[f(X)]
$$

$\log$ ã¯å‡¹é–¢æ•°ã ã‹ã‚‰:

$$
\log \mathbb{E}[X] \geq \mathbb{E}[\log X]
$$

**è¨¼æ˜ã®ã‚¹ã‚±ãƒƒãƒ**: $f$ ãŒå‡¹é–¢æ•°ã§ã‚ã‚‹ã¨ã¯ã€ä»»æ„ã® $x_1, x_2$ ã¨ $\lambda \in [0, 1]$ ã«å¯¾ã—ã¦ $f(\lambda x_1 + (1-\lambda) x_2) \geq \lambda f(x_1) + (1-\lambda) f(x_2)$ ãŒæˆã‚Šç«‹ã¤ã“ã¨ã ã€‚ã“ã‚Œã‚’æœ‰é™å€‹ã®ç‚¹ã«æ‹¡å¼µã™ã‚‹ã¨ $f(\sum_i \lambda_i x_i) \geq \sum_i \lambda_i f(x_i)$ ($\sum_i \lambda_i = 1$) ã¨ãªã‚Šã€æœŸå¾…å€¤ã®å®šç¾©ã¨çµ„ã¿åˆã‚ã›ã‚Œã°Jensenä¸ç­‰å¼ãŒå¾—ã‚‰ã‚Œã‚‹ã€‚

```python
import numpy as np

# Verify Jensen's inequality for log (concave function)
# log(E[X]) >= E[log(X)]

np.random.seed(42)
X = np.random.exponential(2.0, 10000)  # positive random variable

lhs = np.log(np.mean(X))       # log(E[X])
rhs = np.mean(np.log(X))       # E[log(X)]
gap = lhs - rhs

print(f"log(E[X]) = {lhs:.6f}")
print(f"E[log(X)] = {rhs:.6f}")
print(f"Gap       = {gap:.6f} >= 0 âœ“ (Jensen's inequality)")
print(f"\nFor constant X (no gap):")
X_const = np.full(10000, 3.0)
print(f"log(E[X]) = {np.log(np.mean(X_const)):.6f}")
print(f"E[log(X)] = {np.mean(np.log(X_const)):.6f}")
print(f"Gap       = {np.log(np.mean(X_const)) - np.mean(np.log(X_const)):.6f} (equality when constant)")
```

**ç­‰å·æ¡ä»¶**: $X$ ãŒå®šæ•°ã®ã¨ãï¼ˆåˆ†æ•£ãŒã‚¼ãƒ­ã®ã¨ãï¼‰ã€Jensenä¸ç­‰å¼ã¯ç­‰å·ã«ãªã‚‹ã€‚ã“ã‚ŒãŒEMç®—æ³•ã®åæŸãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ç†è§£ã™ã‚‹éµã«ãªã‚‹ã€‚

### 3.4 ELBOåˆ†è§£ â€” EMç®—æ³•ã®å¿ƒè‡“éƒ¨

ã„ã‚ˆã„ã‚ˆEMç®—æ³•ã®æ ¸å¿ƒã«åˆ°é”ã™ã‚‹ã€‚ã“ã“ã‹ã‚‰å…ˆã¯ä¸€è¡Œä¸€è¡Œã€ç´™ã®ä¸Šã§è¿½ã£ã¦ã»ã—ã„ã€‚

**ç›®æ¨™**: ä¸å®Œå…¨ãƒ‡ãƒ¼ã‚¿å¯¾æ•°å°¤åº¦ $\log p(\mathbf{x} \mid \theta)$ ã®ä¸‹ç•Œ (lower bound) ã‚’æ§‹æˆã™ã‚‹ã€‚

$q(\mathbf{z})$ ã‚’ $\mathbf{z}$ ä¸Šã®ä»»æ„ã®ç¢ºç‡åˆ†å¸ƒã¨ã™ã‚‹ã€‚ä»¥ä¸‹ã®åˆ†è§£ãŒæˆã‚Šç«‹ã¤:

$$
\log p(\mathbf{x} \mid \theta) = \mathcal{L}(q, \theta) + \text{KL}[q(\mathbf{z}) \| p(\mathbf{z} \mid \mathbf{x}, \theta)]
$$

ã“ã“ã§:

$$
\mathcal{L}(q, \theta) = \sum_{\mathbf{z}} q(\mathbf{z}) \log \frac{p(\mathbf{x}, \mathbf{z} \mid \theta)}{q(\mathbf{z})}
$$

$$
\text{KL}[q(\mathbf{z}) \| p(\mathbf{z} \mid \mathbf{x}, \theta)] = -\sum_{\mathbf{z}} q(\mathbf{z}) \log \frac{p(\mathbf{z} \mid \mathbf{x}, \theta)}{q(\mathbf{z})}
$$

**ã“ã® $\mathcal{L}(q, \theta)$ ãŒ ELBO (Evidence Lower BOund) ã ã€‚**

:::message
ã“ã®åˆ†è§£ã¯ç¬¬9å›ï¼ˆå¤‰åˆ†æ¨è«–ï¼‰ã§ä¸»å½¹ã«ãªã‚‹ã€‚ã“ã“ã§ã¯EMç®—æ³•ã®å°å‡ºã«å¿…è¦ãªéƒ¨åˆ†ã ã‘ã‚’æ‰±ã†ã€‚
:::

**å°å‡º** â€” ä¸€è¡Œãšã¤è¿½ã†:

Step 1: å¯¾æ•°å°¤åº¦ã‚’å¤‰å½¢ã™ã‚‹ã€‚

$$
\log p(\mathbf{x} \mid \theta) = \log p(\mathbf{x} \mid \theta) \cdot \underbrace{\sum_{\mathbf{z}} q(\mathbf{z})}_{= 1}
$$

$q(\mathbf{z})$ ã¯ç¢ºç‡åˆ†å¸ƒã ã‹ã‚‰å’ŒãŒ1ã€‚ã“ã‚Œã‚’åˆ©ç”¨ã™ã‚‹ã€‚

Step 2: $\log$ ã®ä¸­ã« $q(\mathbf{z})$ ã‚’å°å…¥ã™ã‚‹ã€‚

$$
\log p(\mathbf{x} \mid \theta) = \sum_{\mathbf{z}} q(\mathbf{z}) \log p(\mathbf{x} \mid \theta)
$$

$\log p(\mathbf{x} \mid \theta)$ ã¯ $\mathbf{z}$ ã«ä¾å­˜ã—ãªã„ã‹ã‚‰ã€$\sum$ ã®ä¸­ã«å…¥ã‚Œã‚‰ã‚Œã‚‹ã€‚

Step 3: $p(\mathbf{x} \mid \theta) = \frac{p(\mathbf{x}, \mathbf{z} \mid \theta)}{p(\mathbf{z} \mid \mathbf{x}, \theta)}$ ã‚’ä»£å…¥ã™ã‚‹ï¼ˆãƒ™ã‚¤ã‚ºã®å®šç†ã®å¤‰å½¢ï¼‰ã€‚

$$
= \sum_{\mathbf{z}} q(\mathbf{z}) \log \frac{p(\mathbf{x}, \mathbf{z} \mid \theta)}{p(\mathbf{z} \mid \mathbf{x}, \theta)}
$$

Step 4: $q(\mathbf{z})$ ã‚’åˆ†å­åˆ†æ¯ã«æŒ¿å…¥ã™ã‚‹ï¼ˆ$\times \frac{q(\mathbf{z})}{q(\mathbf{z})} = 1$ï¼‰ã€‚

$$
= \sum_{\mathbf{z}} q(\mathbf{z}) \log \frac{p(\mathbf{x}, \mathbf{z} \mid \theta) \cdot q(\mathbf{z})}{p(\mathbf{z} \mid \mathbf{x}, \theta) \cdot q(\mathbf{z})}
$$

Step 5: å¯¾æ•°ã®å•†ã‚’åˆ†è§£ã™ã‚‹ã€‚

$$
= \sum_{\mathbf{z}} q(\mathbf{z}) \log \frac{p(\mathbf{x}, \mathbf{z} \mid \theta)}{q(\mathbf{z})} + \sum_{\mathbf{z}} q(\mathbf{z}) \log \frac{q(\mathbf{z})}{p(\mathbf{z} \mid \mathbf{x}, \theta)}
$$

$$
= \underbrace{\sum_{\mathbf{z}} q(\mathbf{z}) \log \frac{p(\mathbf{x}, \mathbf{z} \mid \theta)}{q(\mathbf{z})}}_{\mathcal{L}(q, \theta) \text{ (ELBO)}} + \underbrace{\text{KL}[q(\mathbf{z}) \| p(\mathbf{z} \mid \mathbf{x}, \theta)]}_{\geq 0}
$$

**KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¯å¸¸ã«éè² ** (Gibbsã®ä¸ç­‰å¼ã€ç¬¬6å›) ã ã‹ã‚‰:

$$
\log p(\mathbf{x} \mid \theta) \geq \mathcal{L}(q, \theta)
$$

$\mathcal{L}(q, \theta)$ ã¯å¯¾æ•°å°¤åº¦ã® **ä¸‹ç•Œ** ã ã€‚ã ã‹ã‚‰ Evidence **Lower** Bound ã¨å‘¼ã°ã‚Œã‚‹ã€‚

```python
import numpy as np

# Numerical verification of ELBO decomposition
# log p(x|Î¸) = L(q,Î¸) + KL[q||p(z|x,Î¸)]

# GMM with K=2
mu = np.array([-2.0, 3.0])
sigma = np.array([1.0, 1.5])
pi_k = np.array([0.4, 0.6])

x_val = 1.0

# Compute p(x|Î¸) = Î£_k Ï€_k N(x|Î¼_k,Ïƒ_kÂ²)
def norm_pdf(x, mu, sigma):
    return np.exp(-0.5*((x-mu)/sigma)**2) / (sigma * np.sqrt(2*np.pi))

px = sum(pi_k[k] * norm_pdf(x_val, mu[k], sigma[k]) for k in range(2))
log_px = np.log(px)

# True posterior: p(z=k|x,Î¸) = Ï€_k N(x|Î¼_k,Ïƒ_kÂ²) / p(x|Î¸)
p_z_given_x = np.array([pi_k[k] * norm_pdf(x_val, mu[k], sigma[k]) / px for k in range(2)])

# Choose q(z) different from true posterior
q_z = np.array([0.7, 0.3])  # arbitrary distribution

# ELBO: L(q,Î¸) = Î£_k q(k) log [Ï€_k N(x|Î¼_k,Ïƒ_kÂ²) / q(k)]
elbo = sum(q_z[k] * np.log(pi_k[k] * norm_pdf(x_val, mu[k], sigma[k]) / q_z[k]) for k in range(2))

# KL[q||p(z|x,Î¸)] = Î£_k q(k) log [q(k) / p(z=k|x,Î¸)]
kl = sum(q_z[k] * np.log(q_z[k] / p_z_given_x[k]) for k in range(2))

print(f"log p(x|Î¸)     = {log_px:.6f}")
print(f"ELBO L(q,Î¸)    = {elbo:.6f}")
print(f"KL[q||p(z|x)]  = {kl:.6f}")
print(f"ELBO + KL      = {elbo + kl:.6f}  (should equal log p(x|Î¸))")
print(f"Gap (KL >= 0)  = {kl:.6f} >= 0 âœ“")

# When q = true posterior â†’ KL = 0, ELBO = log p(x|Î¸)
print(f"\nWhen q = true posterior:")
elbo_tight = sum(p_z_given_x[k] * np.log(pi_k[k] * norm_pdf(x_val, mu[k], sigma[k]) / p_z_given_x[k]) for k in range(2))
kl_tight = sum(p_z_given_x[k] * np.log(p_z_given_x[k] / p_z_given_x[k]) for k in range(2))
print(f"ELBO (tight)   = {elbo_tight:.6f}")
print(f"KL (tight)     = {kl_tight:.6f}  (â‰ˆ 0 âœ“)")
```

:::details Jensenä¸ç­‰å¼ã‹ã‚‰ã®ELBOå°å‡ºï¼ˆåˆ¥è§£ï¼‰
ä¸Šã®å°å‡ºã¯ãƒ™ã‚¤ã‚ºã®å®šç†ã‚’ä½¿ã£ãŸãŒã€Jensenä¸ç­‰å¼ã‹ã‚‰ç›´æ¥å°å‡ºã™ã‚‹ã“ã¨ã‚‚ã§ãã‚‹:

$$
\log p(\mathbf{x} \mid \theta) = \log \sum_{\mathbf{z}} p(\mathbf{x}, \mathbf{z} \mid \theta)
$$

$q(\mathbf{z})$ ã‚’å°å…¥:

$$
= \log \sum_{\mathbf{z}} q(\mathbf{z}) \frac{p(\mathbf{x}, \mathbf{z} \mid \theta)}{q(\mathbf{z})}
$$

$$
= \log \mathbb{E}_{q(\mathbf{z})} \left[ \frac{p(\mathbf{x}, \mathbf{z} \mid \theta)}{q(\mathbf{z})} \right]
$$

Jensenä¸ç­‰å¼ï¼ˆ$\log$ ã¯å‡¹é–¢æ•°ï¼‰ã‚’é©ç”¨:

$$
\geq \mathbb{E}_{q(\mathbf{z})} \left[ \log \frac{p(\mathbf{x}, \mathbf{z} \mid \theta)}{q(\mathbf{z})} \right] = \mathcal{L}(q, \theta)
$$

ã“ã®å°å‡ºã®æ–¹ãŒçŸ­ã„ãŒã€KLé …ã¨ã®é–¢ä¿‚ãŒè¦‹ãˆã«ãã„ã€‚ä¸Šã®ãƒ™ã‚¤ã‚ºã®å®šç†ã‚’ä½¿ã†å°å‡ºã®æ–¹ãŒã€EMç®—æ³•ã®æ§‹é€ ãŒæ˜å¿«ã«ãªã‚‹ã€‚
:::

> **ã“ã“ãŒæœ¬è¬›ç¾©æœ€å¤§ã®ãƒã‚¤ãƒ³ãƒˆ**: $\log p(\mathbf{x} \mid \theta) = \mathcal{L}(q, \theta) + \text{KL}[q \| p(\mathbf{z} \mid \mathbf{x}, \theta)]$ã€‚ã“ã®åˆ†è§£ãŒEMç®—æ³•ã®å…¨ã¦ã‚’æ”¯ãˆã¦ã„ã‚‹ã€‚

### 3.5 EMç®—æ³•ã®å°å‡º â€” 2ã‚¹ãƒ†ãƒƒãƒ—ã®å¤©æ‰çš„æ§‹é€ 

ELBOåˆ†è§£ã‚’ã‚‚ã†ä¸€åº¦æ›¸ã:

$$
\log p(\mathbf{x} \mid \theta) = \mathcal{L}(q, \theta) + \text{KL}[q(\mathbf{z}) \| p(\mathbf{z} \mid \mathbf{x}, \theta)]
$$

å·¦è¾º $\log p(\mathbf{x} \mid \theta)$ ã‚’æœ€å¤§åŒ–ã—ãŸã„ã€‚å³è¾ºã¯2é …ã®å’Œã ã€‚

**E-step**: $q(\mathbf{z})$ ã«ã¤ã„ã¦ $\mathcal{L}(q, \theta)$ ã‚’æœ€å¤§åŒ–ã™ã‚‹ï¼ˆ$\theta$ ã¯å›ºå®šï¼‰ã€‚

KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¯éè² ã§ã€$q(\mathbf{z}) = p(\mathbf{z} \mid \mathbf{x}, \theta)$ ã®ã¨ãã€ã‹ã¤ãã®ã¨ãã«é™ã‚Šã‚¼ãƒ­ã«ãªã‚‹ã€‚ã—ãŸãŒã£ã¦:

$$
q^*(\mathbf{z}) = p(\mathbf{z} \mid \mathbf{x}, \theta^{(t)})
$$

ã“ã®ã¨ã $\text{KL} = 0$ ã¨ãªã‚Šã€ELBO ãŒå¯¾æ•°å°¤åº¦ã«ä¸€è‡´ã™ã‚‹: $\mathcal{L}(q^*, \theta^{(t)}) = \log p(\mathbf{x} \mid \theta^{(t)})$ã€‚

**M-step**: $\theta$ ã«ã¤ã„ã¦ $\mathcal{L}(q^*, \theta)$ ã‚’æœ€å¤§åŒ–ã™ã‚‹ï¼ˆ$q = q^*$ ã¯å›ºå®šï¼‰ã€‚

$q^* = p(\mathbf{z} \mid \mathbf{x}, \theta^{(t)})$ ã‚’ä»£å…¥ã™ã‚‹ã¨:

$$
\mathcal{L}(q^*, \theta) = \sum_{\mathbf{z}} p(\mathbf{z} \mid \mathbf{x}, \theta^{(t)}) \log p(\mathbf{x}, \mathbf{z} \mid \theta) - \underbrace{\sum_{\mathbf{z}} p(\mathbf{z} \mid \mathbf{x}, \theta^{(t)}) \log p(\mathbf{z} \mid \mathbf{x}, \theta^{(t)})}_{\text{entropy, } \theta \text{ã«ä¾å­˜ã—ãªã„}}
$$

$\theta$ ã«ä¾å­˜ã™ã‚‹ã®ã¯ç¬¬1é …ã ã‘ã ã‹ã‚‰:

$$
\theta^{(t+1)} = \arg\max_\theta \underbrace{\mathbb{E}_{\mathbf{z} \sim p(\mathbf{z} \mid \mathbf{x}, \theta^{(t)})} [\log p(\mathbf{x}, \mathbf{z} \mid \theta)]}_{Q(\theta, \theta^{(t)})}
$$

ã“ã® $Q(\theta, \theta^{(t)})$ ãŒ **Qé–¢æ•°** ã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã ã€‚Dempster, Laird, Rubin (1977) [^1] ã¯ã“ã®é–¢æ•°ã‚’ä¸­å¿ƒã«EMç®—æ³•ã‚’å®šå¼åŒ–ã—ãŸã€‚

**ã¾ã¨ã‚ã‚‹ã¨**:

| ã‚¹ãƒ†ãƒƒãƒ— | æ“ä½œ | æ•°å¼ |
|:---------|:-----|:-----|
| **E-step** | äº‹å¾Œåˆ†å¸ƒã‚’è¨ˆç®— | $q(\mathbf{z}) \leftarrow p(\mathbf{z} \mid \mathbf{x}, \theta^{(t)})$ |
| **M-step** | Qé–¢æ•°ã‚’æœ€å¤§åŒ– | $\theta^{(t+1)} \leftarrow \arg\max_\theta Q(\theta, \theta^{(t)})$ |

```mermaid
sequenceDiagram
    participant E as E-step
    participant M as M-step
    participant L as log p(x|Î¸)

    Note over E,L: Iteration t
    E->>E: q(z) = p(z|x, Î¸^(t))
    Note over E: KL â†’ 0, ELBO = log p(x|Î¸^(t))
    E->>M: Pass q(z) to M-step
    M->>M: Î¸^(t+1) = argmax Q(Î¸, Î¸^(t))
    Note over M: ELBO increases
    M->>L: log p(x|Î¸^(t+1)) â‰¥ log p(x|Î¸^(t))
    Note over E,L: Iteration t+1
    L->>E: Use Î¸^(t+1) for next E-step
```

```python
import numpy as np

# EM algorithm as coordinate ascent on ELBO
# Demonstrating that log-likelihood never decreases

np.random.seed(42)
N = 200
z_true = np.random.choice([0, 1], size=N, p=[0.4, 0.6])
x = np.where(z_true == 0, np.random.normal(-2, 1, N), np.random.normal(3, 1.5, N))

mu = np.array([0.0, 1.0])
sigma = np.array([2.0, 2.0])
pi_k = np.array([0.5, 0.5])

def compute_log_likelihood(x, mu, sigma, pi_k):
    N = len(x)
    K = len(mu)
    ll = 0.0
    for n in range(N):
        p_xn = sum(pi_k[k] * np.exp(-0.5*((x[n]-mu[k])/sigma[k])**2)
                   / (sigma[k]*np.sqrt(2*np.pi)) for k in range(K))
        ll += np.log(p_xn + 1e-300)
    return ll

def compute_elbo(x, mu, sigma, pi_k, gamma):
    """ELBO = Î£_n Î£_k Î³_nk [log Ï€_k + log N(x_n|Î¼_k,Ïƒ_kÂ²) - log Î³_nk]"""
    N, K = gamma.shape
    elbo = 0.0
    for n in range(N):
        for k in range(K):
            if gamma[n, k] > 1e-300:
                log_pdf = -0.5*np.log(2*np.pi) - np.log(sigma[k]) - 0.5*((x[n]-mu[k])/sigma[k])**2
                elbo += gamma[n, k] * (np.log(pi_k[k]) + log_pdf - np.log(gamma[n, k]))
    return elbo

print(f"{'Step':>4} | {'log p(x|Î¸)':>12} | {'ELBO':>12} | {'KL':>10} | {'Î” log-lik':>10}")
print("-" * 65)

prev_ll = compute_log_likelihood(x, mu, sigma, pi_k)

for step in range(10):
    # E-step
    K = len(mu)
    pdf = np.zeros((N, K))
    for k in range(K):
        pdf[:, k] = pi_k[k] * np.exp(-0.5*((x-mu[k])/sigma[k])**2) / (sigma[k]*np.sqrt(2*np.pi))
    gamma = pdf / (pdf.sum(axis=1, keepdims=True) + 1e-300)

    # After E-step: KL = 0, ELBO = log-likelihood
    ll = compute_log_likelihood(x, mu, sigma, pi_k)
    elbo = compute_elbo(x, mu, sigma, pi_k, gamma)
    kl = ll - elbo

    print(f"{step:4d} | {ll:12.4f} | {elbo:12.4f} | {kl:10.6f} | {ll - prev_ll:10.4f}")

    # M-step
    N_k = gamma.sum(axis=0)
    for k in range(K):
        mu[k] = (gamma[:, k] * x).sum() / N_k[k]
        sigma[k] = np.sqrt((gamma[:, k] * (x - mu[k])**2).sum() / N_k[k])
    pi_k = N_k / N

    prev_ll = ll

print(f"\nKey observation: Î” log-lik >= 0 at every step (monotone increase)")
```

:::message
ã“ã“ã§å¤šãã®äººãŒå¼•ã£ã‹ã‹ã‚‹ãƒã‚¤ãƒ³ãƒˆ: **E-stepã®å¾Œã€KLã¯æ­£ç¢ºã«ã‚¼ãƒ­ã«ãªã‚‹**ï¼ˆ$q = p(\mathbf{z} \mid \mathbf{x}, \theta)$ ã ã‹ã‚‰ï¼‰ã€‚**M-stepã®å¾Œã€KLã¯å†ã³ã‚¼ãƒ­ã§ãªããªã‚‹**ï¼ˆ$\theta$ ãŒå¤‰ã‚ã£ãŸã‹ã‚‰ $q \neq p(\mathbf{z} \mid \mathbf{x}, \theta^{\text{new}})$ï¼‰ã€‚æ¬¡ã®E-stepã§å†ã³KLã‚’ã‚¼ãƒ­ã«ã™ã‚‹ã€‚ã“ã®ç¹°ã‚Šè¿”ã—ãŒå¯¾æ•°å°¤åº¦ã‚’å˜èª¿ã«å¢—åŠ ã•ã›ã‚‹ã€‚
:::

### 3.6 GMMã®E-step / M-step â€” å®Œå…¨å°å‡º

GMMã«å¯¾ã—ã¦EMç®—æ³•ã‚’å…·ä½“çš„ã«é©ç”¨ã—ã‚ˆã†ã€‚å…¨ã¦ã®æ›´æ–°å¼ã‚’ä¸€è¡Œãšã¤å°å‡ºã™ã‚‹ã€‚

**E-step**: è²¬ä»»åº¦ $\gamma(z_{nk})$ ã®è¨ˆç®—

$$
\gamma(z_{nk}) = p(z_n = k \mid x_n, \theta^{(t)}) = \frac{\pi_k^{(t)} \mathcal{N}(x_n \mid \mu_k^{(t)}, (\sigma_k^{(t)})^2)}{\sum_{j=1}^{K} \pi_j^{(t)} \mathcal{N}(x_n \mid \mu_j^{(t)}, (\sigma_j^{(t)})^2)}
$$

ã“ã‚Œã¯ãƒ™ã‚¤ã‚ºã®å®šç†ãã®ã‚‚ã®ã ã€‚åˆ†å­ã¯ã€Œæˆåˆ† $k$ ã‹ã‚‰ $x_n$ ãŒç”Ÿæˆã•ã‚Œã‚‹ç¢ºç‡ã€ã€åˆ†æ¯ã¯ã€Œå…¨æˆåˆ†ã‹ã‚‰ã®ç¢ºç‡ã®å’Œã€ã€‚

**M-step**: Qé–¢æ•°ã®æœ€å¤§åŒ–

Qé–¢æ•°ã‚’æ›¸ãä¸‹ã™:

$$
Q(\theta, \theta^{(t)}) = \sum_{n=1}^{N} \sum_{k=1}^{K} \gamma(z_{nk}) \left[ \log \pi_k + \log \mathcal{N}(x_n \mid \mu_k, \sigma_k^2) \right]
$$

ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®å¯¾æ•°å¯†åº¦ã‚’å±•é–‹ã™ã‚‹:

$$
\log \mathcal{N}(x_n \mid \mu_k, \sigma_k^2) = -\frac{1}{2} \log(2\pi) - \log \sigma_k - \frac{(x_n - \mu_k)^2}{2\sigma_k^2}
$$

**$\mu_k$ ã®æ›´æ–°**: $\frac{\partial Q}{\partial \mu_k} = 0$ ã‚’è§£ãã€‚

$$
\frac{\partial Q}{\partial \mu_k} = \sum_{n=1}^{N} \gamma(z_{nk}) \frac{x_n - \mu_k}{\sigma_k^2} = 0
$$

$$
\sum_{n=1}^{N} \gamma(z_{nk}) x_n = \mu_k \sum_{n=1}^{N} \gamma(z_{nk})
$$

$N_k = \sum_{n=1}^{N} \gamma(z_{nk})$ ã¨å®šç¾©ã™ã‚‹ã¨:

$$
\boxed{\mu_k^{(t+1)} = \frac{1}{N_k} \sum_{n=1}^{N} \gamma(z_{nk}) \, x_n}
$$

**ã€Œè²¬ä»»åº¦ã§é‡ã¿ä»˜ã‘ã—ãŸå¹³å‡ã€** â€” ç›´æ„Ÿçš„ã«ã‚‚è‡ªç„¶ã ã€‚

**$\sigma_k^2$ ã®æ›´æ–°**: $\frac{\partial Q}{\partial \sigma_k^2} = 0$ ã‚’è§£ãã€‚

$\sigma_k^2 = s$ ã¨ã—ã¦:

$$
\frac{\partial Q}{\partial s} = \sum_{n=1}^{N} \gamma(z_{nk}) \left[ -\frac{1}{2s} + \frac{(x_n - \mu_k)^2}{2s^2} \right] = 0
$$

$$
\sum_{n=1}^{N} \gamma(z_{nk}) \frac{1}{s} = \sum_{n=1}^{N} \gamma(z_{nk}) \frac{(x_n - \mu_k)^2}{s^2}
$$

$$
\boxed{(\sigma_k^{(t+1)})^2 = \frac{1}{N_k} \sum_{n=1}^{N} \gamma(z_{nk}) (x_n - \mu_k^{(t+1)})^2}
$$

**ã€Œè²¬ä»»åº¦ã§é‡ã¿ä»˜ã‘ã—ãŸåˆ†æ•£ã€** ã ã€‚

**$\pi_k$ ã®æ›´æ–°**: $\sum_k \pi_k = 1$ ã®åˆ¶ç´„ä»˜ãã§ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥æœªå®šä¹—æ•°æ³•ã‚’ä½¿ã†ã€‚

$$
\mathcal{L}_{\text{Lagrange}} = Q + \lambda \left( 1 - \sum_{k=1}^{K} \pi_k \right)
$$

$$
\frac{\partial}{\partial \pi_k} = \frac{N_k}{\pi_k} - \lambda = 0 \quad \Rightarrow \quad \pi_k = \frac{N_k}{\lambda}
$$

$\sum_k \pi_k = 1$ ã‹ã‚‰ $\lambda = N$:

$$
\boxed{\pi_k^{(t+1)} = \frac{N_k}{N}}
$$

**ã€Œæˆåˆ† $k$ ã«å±ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®å‰²åˆã€** ã¨ã„ã†è‡ªç„¶ãªè§£é‡ˆã«ãªã‚‹ã€‚

```python
import numpy as np

# Complete GMM EM with all derived formulas
np.random.seed(42)

# Ground truth
true_params = {
    'mu': np.array([-3.0, 0.0, 4.0]),
    'sigma': np.array([0.8, 1.2, 0.6]),
    'pi': np.array([0.3, 0.4, 0.3])
}

# Generate data
N = 500
K = 3
z_true = np.random.choice(K, size=N, p=true_params['pi'])
x = np.array([np.random.normal(true_params['mu'][z], true_params['sigma'][z]) for z in z_true])

# Initialize
mu = np.array([-1.0, 0.5, 2.0])
sigma = np.array([1.0, 1.0, 1.0])
pi_k = np.ones(K) / K

def norm_pdf(x, mu, sigma):
    return np.exp(-0.5*((x-mu)/sigma)**2) / (sigma * np.sqrt(2*np.pi))

# EM iterations with derived update formulas
for t in range(20):
    # === E-step ===
    # Î³(z_nk) = Ï€_k N(x_n|Î¼_k,Ïƒ_kÂ²) / Î£_j Ï€_j N(x_n|Î¼_j,Ïƒ_jÂ²)
    pdf = np.zeros((N, K))
    for k in range(K):
        pdf[:, k] = pi_k[k] * norm_pdf(x, mu[k], sigma[k])
    gamma = pdf / pdf.sum(axis=1, keepdims=True)

    # === M-step ===
    N_k = gamma.sum(axis=0)  # effective number of points per component

    for k in range(K):
        # Î¼_k = (1/N_k) Î£_n Î³_nk x_n
        mu[k] = (gamma[:, k] * x).sum() / N_k[k]
        # Ïƒ_kÂ² = (1/N_k) Î£_n Î³_nk (x_n - Î¼_k)Â²
        sigma[k] = np.sqrt((gamma[:, k] * (x - mu[k])**2).sum() / N_k[k])
    # Ï€_k = N_k / N
    pi_k = N_k / N

print("Estimated vs True parameters:")
print(f"Î¼:  est=({mu[0]:6.3f}, {mu[1]:6.3f}, {mu[2]:6.3f})")
print(f"    true=({true_params['mu'][0]:6.3f}, {true_params['mu'][1]:6.3f}, {true_params['mu'][2]:6.3f})")
print(f"Ïƒ:  est=({sigma[0]:6.3f}, {sigma[1]:6.3f}, {sigma[2]:6.3f})")
print(f"    true=({true_params['sigma'][0]:6.3f}, {true_params['sigma'][1]:6.3f}, {true_params['sigma'][2]:6.3f})")
print(f"Ï€:  est=({pi_k[0]:5.3f}, {pi_k[1]:5.3f}, {pi_k[2]:5.3f})")
print(f"    true=({true_params['pi'][0]:5.3f}, {true_params['pi'][1]:5.3f}, {true_params['pi'][2]:5.3f})")
```

### 3.7 EMç®—æ³•ã®åæŸæ€§è¨¼æ˜

EMç®—æ³•ãŒ**å¯¾æ•°å°¤åº¦ã‚’å˜èª¿ã«å¢—åŠ ã•ã›ã‚‹**ã“ã¨ã‚’è¨¼æ˜ã™ã‚‹ã€‚Wu (1983) [^3] ã®åæŸæ€§å®šç†ã®æ ¸å¿ƒéƒ¨åˆ†ã ã€‚

**å®šç† (EMå˜èª¿æ€§)**: EMç®—æ³•ã®å„åå¾©ã§ã€ä¸å®Œå…¨ãƒ‡ãƒ¼ã‚¿å¯¾æ•°å°¤åº¦ã¯éæ¸›å°‘ã§ã‚ã‚‹:

$$
\log p(\mathbf{x} \mid \theta^{(t+1)}) \geq \log p(\mathbf{x} \mid \theta^{(t)})
$$

**è¨¼æ˜**:

ELBOåˆ†è§£ã‚ˆã‚Š:

$$
\log p(\mathbf{x} \mid \theta) = \mathcal{L}(q, \theta) + \text{KL}[q \| p(\mathbf{z} \mid \mathbf{x}, \theta)]
$$

E-stepã§ $q = p(\mathbf{z} \mid \mathbf{x}, \theta^{(t)})$ ã¨è¨­å®šã™ã‚‹ã¨ $\text{KL} = 0$ ã ã‹ã‚‰:

$$
\log p(\mathbf{x} \mid \theta^{(t)}) = \mathcal{L}(q^{(t)}, \theta^{(t)}) \tag{1}
$$

M-stepã§ $\theta^{(t+1)} = \arg\max_\theta \mathcal{L}(q^{(t)}, \theta)$ ã¨ã™ã‚‹ã‹ã‚‰:

$$
\mathcal{L}(q^{(t)}, \theta^{(t+1)}) \geq \mathcal{L}(q^{(t)}, \theta^{(t)}) \tag{2}
$$

ä¸€æ–¹ã€æ–°ã—ã„ $\theta^{(t+1)}$ ã«å¯¾ã—ã¦ã‚‚ ELBOåˆ†è§£ã¯æˆã‚Šç«‹ã¤:

$$
\log p(\mathbf{x} \mid \theta^{(t+1)}) = \mathcal{L}(q^{(t)}, \theta^{(t+1)}) + \underbrace{\text{KL}[q^{(t)} \| p(\mathbf{z} \mid \mathbf{x}, \theta^{(t+1)})]}_{\geq 0} \tag{3}
$$

(3) ã‚ˆã‚Š:

$$
\log p(\mathbf{x} \mid \theta^{(t+1)}) \geq \mathcal{L}(q^{(t)}, \theta^{(t+1)}) \tag{4}
$$

(1), (2), (4) ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã¨:

$$
\log p(\mathbf{x} \mid \theta^{(t+1)}) \stackrel{(4)}{\geq} \mathcal{L}(q^{(t)}, \theta^{(t+1)}) \stackrel{(2)}{\geq} \mathcal{L}(q^{(t)}, \theta^{(t)}) \stackrel{(1)}{=} \log p(\mathbf{x} \mid \theta^{(t)})
$$

$$
\therefore \log p(\mathbf{x} \mid \theta^{(t+1)}) \geq \log p(\mathbf{x} \mid \theta^{(t)}) \quad \blacksquare
$$

```python
import numpy as np

# Empirical verification of monotone convergence
np.random.seed(42)
N = 300
x = np.concatenate([np.random.normal(-2, 1, 120),
                     np.random.normal(3, 1.5, 180)])

mu = np.array([-5.0, 8.0])  # intentionally bad initialization
sigma = np.array([3.0, 3.0])
pi_k = np.array([0.5, 0.5])

def compute_ll(x, mu, sigma, pi_k):
    ll = 0.0
    for xn in x:
        p = sum(pi_k[k] * np.exp(-0.5*((xn-mu[k])/sigma[k])**2)/(sigma[k]*np.sqrt(2*np.pi))
                for k in range(len(mu)))
        ll += np.log(p + 1e-300)
    return ll

lls = []
for t in range(30):
    lls.append(compute_ll(x, mu, sigma, pi_k))

    pdf = np.zeros((N, 2))
    for k in range(2):
        pdf[:, k] = pi_k[k] * np.exp(-0.5*((x-mu[k])/sigma[k])**2)/(sigma[k]*np.sqrt(2*np.pi))
    gamma = pdf / (pdf.sum(axis=1, keepdims=True) + 1e-300)

    N_k = gamma.sum(axis=0)
    for k in range(2):
        mu[k] = (gamma[:, k] * x).sum() / N_k[k]
        sigma[k] = np.sqrt((gamma[:, k] * (x - mu[k])**2).sum() / N_k[k])
    pi_k = N_k / N

# Verify monotone increase
diffs = [lls[i+1] - lls[i] for i in range(len(lls)-1)]
print(f"All increments >= 0: {all(d >= -1e-10 for d in diffs)}")
print(f"Min increment: {min(diffs):.2e}")
print(f"Max increment: {max(diffs):.4f}")
print(f"Final - Initial: {lls[-1] - lls[0]:.4f}")
print(f"\nConvergence trace (first 10 steps):")
for i in range(min(10, len(lls))):
    print(f"  t={i:2d}: log-lik = {lls[i]:10.4f}" + (f"  (Î” = {diffs[i]:+.4f})" if i < len(diffs) else ""))
```

:::message alert
EMç®—æ³•ã¯**å±€æ‰€æœ€é©è§£**ã«åæŸã™ã‚‹ä¿è¨¼ã—ã‹ãªã„ã€‚å¤§åŸŸæœ€é©è§£ã¸ã®åˆ°é”ã¯ä¿è¨¼ã•ã‚Œã¦ã„ãªã„ã€‚åˆæœŸå€¤ã«ä¾å­˜ã™ã‚‹ãŸã‚ã€å®Ÿå‹™ã§ã¯è¤‡æ•°ã®åˆæœŸå€¤ã§å®Ÿè¡Œã—ã¦æœ€è‰¯ã®çµæœã‚’é¸ã¶ (multiple restarts) ã®ãŒæ¨™æº–çš„ãªå¯¾ç­–ã ã€‚
:::

:::details EMåæŸé€Ÿåº¦ã«ã¤ã„ã¦
EMç®—æ³•ã®åæŸé€Ÿåº¦ã¯ä¸€èˆ¬ã«**ç·šå½¢åæŸ** (linear convergence) ã ã€‚Newtonæ³•ã®ã‚ˆã†ãªäºŒæ¬¡åæŸã§ã¯ãªã„ã€‚å…·ä½“çš„ã«ã¯ã€æƒ…å ±è¡Œåˆ—ã®æ¬ æ¸¬æƒ…å ± (missing information) ã®æ¯”ç‡ãŒåæŸé€Ÿåº¦ã‚’æ”¯é…ã™ã‚‹ã€‚

å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã®Fisheræƒ…å ±è¡Œåˆ—ã‚’ $I_c(\theta)$ã€è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã®Fisheræƒ…å ±è¡Œåˆ—ã‚’ $I_o(\theta)$ ã¨ã™ã‚‹ã¨ã€EMç®—æ³•ã®åæŸãƒ¬ãƒ¼ãƒˆ $r$ ã¯:

$$
r \approx \lambda_{\max}\left( I_c(\theta^*)^{-1} (I_c(\theta^*) - I_o(\theta^*)) \right)
$$

ã€Œæ¬ æ¸¬æƒ…å ±ãŒå¤šã„ã»ã©åæŸãŒé…ã„ã€â€” ç›´æ„Ÿã«åˆã†çµæœã ã€‚æ¬ æ¸¬ãŒå¤šã„ã»ã©æ½œåœ¨å¤‰æ•°ã®æ¨å®šãŒä¸ç¢ºå®Ÿã«ãªã‚Šã€E-stepã®æƒ…å ±é‡ãŒæ¸›ã‚‹ã‹ã‚‰ã ã€‚
:::

### 3.8 Boss Battle â€” Dempster, Laird, Rubin (1977) ã®Qé–¢æ•°ã‚’å®Œå…¨åˆ†è§£ã™ã‚‹

ã•ã‚ã€ãƒœã‚¹æˆ¦ã ã€‚EMç®—æ³•ã®åŸè«–æ–‡ [^1] ã§å®šç¾©ã•ã‚ŒãŸQé–¢æ•°ã‚’ã€GMMã®å ´åˆã«å®Œå…¨ã«å±•é–‹ã—ã€å…¨ã¦ã®è¨˜å·ã¨æ¬¡å…ƒã‚’è¿½è·¡ã™ã‚‹ã€‚

**ãƒœã‚¹**: Qé–¢æ•°

$$
Q(\theta, \theta^{(t)}) = \mathbb{E}_{\mathbf{z} \sim p(\mathbf{z} \mid \mathbf{x}, \theta^{(t)})} \left[ \log p(\mathbf{x}, \mathbf{z} \mid \theta) \right]
$$

**å¤šå¤‰é‡GMMã¸ã®å±•é–‹**:

ãƒ‡ãƒ¼ã‚¿ $\mathbf{x}_n \in \mathbb{R}^D$ã€$K$ å€‹ã®æˆåˆ†ã¨ã™ã‚‹ã€‚

$$
Q(\theta, \theta^{(t)}) = \sum_{n=1}^{N} \sum_{k=1}^{K} \gamma(z_{nk}) \Bigg[ \underbrace{\log \pi_k}_{\text{(A) æ··åˆé‡ã¿}} + \underbrace{\left( -\frac{D}{2}\log(2\pi) - \frac{1}{2}\log|\boldsymbol{\Sigma}_k| - \frac{1}{2}(\mathbf{x}_n - \boldsymbol{\mu}_k)^\top \boldsymbol{\Sigma}_k^{-1} (\mathbf{x}_n - \boldsymbol{\mu}_k) \right)}_{\text{(B) å¤šå¤‰é‡ã‚¬ã‚¦ã‚¹ã®å¯¾æ•°å¯†åº¦}} \Bigg]
$$

| é … | è¨˜å· | æ¬¡å…ƒ | æ„å‘³ |
|:---|:-----|:-----|:-----|
| (A) | $\log \pi_k$ | ã‚¹ã‚«ãƒ©ãƒ¼ | æˆåˆ† $k$ ã®äº‹å‰ç¢ºç‡ã®å¯¾æ•° |
| (B1) | $-\frac{D}{2}\log(2\pi)$ | ã‚¹ã‚«ãƒ©ãƒ¼ | æ­£è¦åŒ–å®šæ•°ï¼ˆ$\theta$ ã«ä¾å­˜ã—ãªã„ï¼‰ |
| (B2) | $-\frac{1}{2}\log|\boldsymbol{\Sigma}_k|$ | ã‚¹ã‚«ãƒ©ãƒ¼ | å…±åˆ†æ•£è¡Œåˆ—ã®è¡Œåˆ—å¼ã®å¯¾æ•° |
| (B3) | $(\mathbf{x}_n - \boldsymbol{\mu}_k)^\top \boldsymbol{\Sigma}_k^{-1} (\mathbf{x}_n - \boldsymbol{\mu}_k)$ | ã‚¹ã‚«ãƒ©ãƒ¼ (äºŒæ¬¡å½¢å¼) | ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ã®äºŒä¹— |
| $\gamma(z_{nk})$ | $p(z_n = k \mid x_n, \theta^{(t)})$ | ã‚¹ã‚«ãƒ©ãƒ¼ $\in [0, 1]$ | E-stepã§è¨ˆç®—æ¸ˆã¿ã®è²¬ä»»åº¦ |
| $N$ | ãƒ‡ãƒ¼ã‚¿æ•° | æ•´æ•° | è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã®å€‹æ•° |
| $K$ | æˆåˆ†æ•° | æ•´æ•° | æ··åˆæˆåˆ†ã®æ•° |
| $D$ | æ¬¡å…ƒæ•° | æ•´æ•° | ãƒ‡ãƒ¼ã‚¿ã®æ¬¡å…ƒ |

**å¤šå¤‰é‡M-stepæ›´æ–°å¼**:

$\frac{\partial Q}{\partial \boldsymbol{\mu}_k} = \mathbf{0}$ ã‚’è§£ãã¨:

$$
\boldsymbol{\mu}_k^{(t+1)} = \frac{\sum_{n=1}^{N} \gamma(z_{nk}) \, \mathbf{x}_n}{\sum_{n=1}^{N} \gamma(z_{nk})} = \frac{1}{N_k} \sum_{n=1}^{N} \gamma(z_{nk}) \, \mathbf{x}_n
$$

$\frac{\partial Q}{\partial \boldsymbol{\Sigma}_k^{-1}} = \mathbf{0}$ ã‚’è§£ãã¨ï¼ˆè¡Œåˆ—å¾®åˆ† â€” ç¬¬3å›ã§å­¦ã‚“ã æŠ€è¡“ãŒæ´»ãã‚‹ï¼‰:

$$
\boldsymbol{\Sigma}_k^{(t+1)} = \frac{1}{N_k} \sum_{n=1}^{N} \gamma(z_{nk}) \, (\mathbf{x}_n - \boldsymbol{\mu}_k^{(t+1)})(\mathbf{x}_n - \boldsymbol{\mu}_k^{(t+1)})^\top
$$

```python
import numpy as np

# Multivariate GMM EM â€” Boss Battle implementation
np.random.seed(42)

# 2D data, K=3 components
D, K, N = 2, 3, 500
true_mus = [np.array([-3, -2]), np.array([0, 3]), np.array([4, -1])]
true_covs = [np.array([[1, 0.3],[0.3, 0.8]]),
             np.array([[1.2, -0.5],[-0.5, 1.0]]),
             np.array([[0.6, 0],[0, 0.6]])]
true_pi = [0.3, 0.4, 0.3]

# Generate multivariate data
data = []
z_true = []
for n in range(N):
    k = np.random.choice(K, p=true_pi)
    z_true.append(k)
    data.append(np.random.multivariate_normal(true_mus[k], true_covs[k]))
X = np.array(data)  # (N, D)

# Initialize
mus = [np.random.randn(D) for _ in range(K)]
covs = [np.eye(D) for _ in range(K)]
pis = np.ones(K) / K

def mvn_pdf(x, mu, cov):
    """Multivariate Gaussian PDF: N(x|Î¼,Î£)"""
    D = len(mu)
    diff = x - mu
    cov_inv = np.linalg.inv(cov)
    det = np.linalg.det(cov)
    exponent = -0.5 * diff @ cov_inv @ diff
    norm = 1.0 / ((2 * np.pi)**(D/2) * np.sqrt(det))
    return norm * np.exp(exponent)

# EM iterations
for t in range(30):
    # E-step: Î³(z_nk) = Ï€_k N(x_n|Î¼_k,Î£_k) / Î£_j Ï€_j N(x_n|Î¼_j,Î£_j)
    gamma = np.zeros((N, K))
    for k in range(K):
        for n in range(N):
            gamma[n, k] = pis[k] * mvn_pdf(X[n], mus[k], covs[k])
    gamma /= gamma.sum(axis=1, keepdims=True) + 1e-300

    # M-step
    N_k = gamma.sum(axis=0)
    for k in range(K):
        # Î¼_k = (1/N_k) Î£_n Î³_nk x_n
        mus[k] = (gamma[:, k:k+1] * X).sum(axis=0) / N_k[k]
        # Î£_k = (1/N_k) Î£_n Î³_nk (x_n - Î¼_k)(x_n - Î¼_k)^T
        diff = X - mus[k]  # (N, D)
        covs[k] = (gamma[:, k:k+1] * diff).T @ diff / N_k[k]
    pis = N_k / N

print("=== Boss Battle Result: Multivariate GMM EM ===\n")
for k in range(K):
    print(f"Component {k}:")
    print(f"  Î¼_est  = [{mus[k][0]:6.3f}, {mus[k][1]:6.3f}]")
    print(f"  Î¼_true = [{true_mus[k][0]:6.3f}, {true_mus[k][1]:6.3f}]")
    print(f"  Ï€_est  = {pis[k]:.3f},  Ï€_true = {true_pi[k]:.3f}")
    print(f"  Î£_est  = [[{covs[k][0,0]:.3f}, {covs[k][0,1]:.3f}],")
    print(f"             [{covs[k][1,0]:.3f}, {covs[k][1,1]:.3f}]]")
    print()
```

:::message
ãƒœã‚¹æ’ƒç ´ã€‚Qé–¢æ•°ã‚’å…¨ã¦ã®é …ã«åˆ†è§£ã—ã€å¤šå¤‰é‡GMMã®æ›´æ–°å¼ã‚’å°å‡ºãƒ»å®Ÿè£…ã—ãŸã€‚ã“ã“ã§ç²å¾—ã—ãŸæŠ€è¡“ã¯:
1. Qé–¢æ•°ã®æ§‹é€ ç†è§£ï¼ˆå®Œå…¨ãƒ‡ãƒ¼ã‚¿å¯¾æ•°å°¤åº¦ã®æœŸå¾…å€¤ï¼‰
2. è¡Œåˆ—å¾®åˆ†ã«ã‚ˆã‚‹å¤šå¤‰é‡æ›´æ–°å¼ã®å°å‡ºï¼ˆç¬¬3å›ã®çŸ¥è­˜ãŒæ´»ããŸï¼‰
3. è²¬ä»»åº¦ â†’ é‡ã¿ä»˜ãçµ±è¨ˆé‡ã¨ã„ã†è¨ˆç®—ãƒ‘ã‚¿ãƒ¼ãƒ³
:::

### 3.9 EMã®å¹¾ä½•å­¦çš„è§£é‡ˆ â€” e-å°„å½±ã¨m-å°„å½±

EMç®—æ³•ã«ã¯ç¾ã—ã„å¹¾ä½•å­¦çš„è§£é‡ˆãŒã‚ã‚‹ã€‚æƒ…å ±å¹¾ä½•å­¦ï¼ˆAmari, 1985ï¼‰ã®è¦–ç‚¹ã‹ã‚‰è¦‹ã‚‹ã¨ã€EMç®—æ³•ã¯çµ±è¨ˆå¤šæ§˜ä½“ä¸Šã® **äº¤äº’å°„å½±** (alternating projection) ã ã€‚

ç¢ºç‡åˆ†å¸ƒã®ç©ºé–“ã‚’è€ƒãˆã‚ˆã†ã€‚ã“ã®ç©ºé–“ã«ã¯2ã¤ã®é‡è¦ãªéƒ¨åˆ†å¤šæ§˜ä½“ãŒã‚ã‚‹:

- **e-æ—** (exponential family): æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å¼µã‚‰ã‚Œã‚‹å¤šæ§˜ä½“
- **m-æ—** (mixture family): æ··åˆåˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å¼µã‚‰ã‚Œã‚‹å¤šæ§˜ä½“

$$
\text{E-step} = \text{m-å°„å½±}: q \to p(\mathbf{z} \mid \mathbf{x}, \theta^{(t)})
$$

$$
\text{M-step} = \text{e-å°„å½±}: \theta \to \arg\max_\theta Q(\theta, \theta^{(t)})
$$

Neal & Hinton (1998) [^5] ã¯ã“ã®è¦–ç‚¹ã‚’è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®æœ€å°åŒ–ã¨ã—ã¦å†å®šå¼åŒ–ã—ãŸã€‚EMç®—æ³•ã®å„ã‚¹ãƒ†ãƒƒãƒ—ã¯ã€è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ $F(q, \theta) = -\mathcal{L}(q, \theta)$ ã‚’ $q$ ã¨ $\theta$ ã«ã¤ã„ã¦äº¤äº’ã«æœ€å°åŒ–ã™ã‚‹åº§æ¨™é™ä¸‹æ³•ã«ä»–ãªã‚‰ãªã„ã€‚

```python
import numpy as np

# Geometric view: EM as coordinate descent on free energy
# F(q, Î¸) = -L(q, Î¸) = -Î£_z q(z) log [p(x,z|Î¸)/q(z)]

def free_energy(x_val, q_z, mu, sigma, pi_k):
    """Compute negative ELBO (free energy)."""
    K = len(mu)
    F = 0.0
    for k in range(K):
        if q_z[k] > 1e-300:
            log_joint = np.log(pi_k[k] + 1e-300) + \
                        (-0.5*np.log(2*np.pi) - np.log(sigma[k]) - 0.5*((x_val-mu[k])/sigma[k])**2)
            F -= q_z[k] * (log_joint - np.log(q_z[k]))
    return F

# Track free energy during EM
np.random.seed(42)
x_val = 1.5
mu = np.array([-2.0, 3.0])
sigma = np.array([1.0, 1.5])
pi_k = np.array([0.4, 0.6])

print(f"{'Step':>6} | {'q(z=0)':>8} | {'q(z=1)':>8} | {'F(q,Î¸)':>10} | {'Action':>12}")
print("-" * 55)

for step in range(5):
    # Before E-step: use arbitrary q
    q_z = np.array([0.5, 0.5]) if step == 0 else q_z
    F_before = free_energy(x_val, q_z, mu, sigma, pi_k)

    # E-step (m-projection): minimize F over q â†’ q = p(z|x,Î¸)
    def norm_pdf(x, m, s):
        return np.exp(-0.5*((x-m)/s)**2)/(s*np.sqrt(2*np.pi))
    pdf = np.array([pi_k[k] * norm_pdf(x_val, mu[k], sigma[k]) for k in range(2)])
    q_z = pdf / pdf.sum()
    F_after_E = free_energy(x_val, q_z, mu, sigma, pi_k)

    print(f"{step*2:6d} | {q_z[0]:8.4f} | {q_z[1]:8.4f} | {F_before:10.4f} | {'E-step':>12}")
    print(f"{step*2+1:6d} | {q_z[0]:8.4f} | {q_z[1]:8.4f} | {F_after_E:10.4f} | {'(after E)':>12}")

print(f"\nFree energy decreases at each E-step (coordinate descent on q)")
```

ã“ã®å¹¾ä½•å­¦çš„è¦–ç‚¹ã®å®Œå…¨ãªå±•é–‹ã¯ç¬¬27å›ï¼ˆæƒ…å ±å¹¾ä½•ï¼‰ã§è¡Œã†ã€‚ã“ã“ã§ã¯ã€ŒEM = äº¤äº’å°„å½± = åº§æ¨™é™ä¸‹ã€ã¨ã„ã†ç›´æ„Ÿã ã‘æŒã¡å¸°ã£ã¦ã»ã—ã„ã€‚

### 3.10 Generalized EM ã¨ ECM

å®Ÿéš›ã®å¿œç”¨ã§ã¯ã€M-stepã®è§£æè§£ãŒå¾—ã‚‰ã‚Œãªã„ã“ã¨ãŒã‚ã‚‹ã€‚**Generalized EM** (GEM) ã¯ã€M-stepã§ $Q(\theta, \theta^{(t)})$ ã‚’å®Œå…¨ã«æœ€å¤§åŒ–ã™ã‚‹ä»£ã‚ã‚Šã«ã€$Q(\theta^{(t+1)}, \theta^{(t)}) > Q(\theta^{(t)}, \theta^{(t)})$ ã‚’æº€ãŸã™ä»»æ„ã® $\theta^{(t+1)}$ ã‚’é¸ã¹ã°ã‚ˆã„ã€‚

å˜èª¿æ€§ã®è¨¼æ˜ã¯åŒæ§˜ã«æˆã‚Šç«‹ã¤ã€‚M-stepã§ELBOãŒ**å¢—åŠ **ã—ã•ãˆã™ã‚Œã°ã€å¯¾æ•°å°¤åº¦ã®éæ¸›å°‘ã¯ä¿è¨¼ã•ã‚Œã‚‹ã€‚

$$
\text{GEM}: \quad \theta^{(t+1)} = \theta^{(t)} + \eta \nabla_\theta Q(\theta, \theta^{(t)}) \Big|_{\theta = \theta^{(t)}}
$$

ã¤ã¾ã‚Šã€å‹¾é…é™ä¸‹æ³•ã§æ•°ã‚¹ãƒ†ãƒƒãƒ— $Q$ ã‚’æ”¹å–„ã™ã‚‹ã ã‘ã§ã‚‚ã‚ˆã„ã€‚

**ECM** (Expectation Conditional Maximization) ã¯ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã‚’åˆ†å‰²ã—ã¦å„ãƒ–ãƒ­ãƒƒã‚¯ã‚’é †ã«æœ€å¤§åŒ–ã™ã‚‹å¤‰ç¨®ã ã€‚å¤šå¤‰é‡GMMã§å…±åˆ†æ•£è¡Œåˆ—ãŒåˆ¶ç´„ã‚’æŒã¤å ´åˆã«æœ‰ç”¨ã€‚

```python
import numpy as np

# Generalized EM: gradient step instead of full maximization
def gem_m_step(x, gamma, mu, sigma, pi_k, lr=0.1):
    """GEM M-step: one gradient step on Q(Î¸, Î¸^(t)) instead of full maximization."""
    N = len(x)
    K = len(mu)
    N_k = gamma.sum(axis=0)

    # Gradient of Q w.r.t. Î¼_k
    for k in range(K):
        grad_mu = (gamma[:, k] * (x - mu[k])).sum() / (sigma[k]**2)
        mu[k] += lr * grad_mu / N  # gradient step (not closed-form!)

        # Gradient w.r.t. Ïƒ_k (through log Ïƒ for positivity)
        grad_log_sigma = -N_k[k] + (gamma[:, k] * (x - mu[k])**2).sum() / sigma[k]**2
        sigma[k] *= np.exp(lr * grad_log_sigma / N)
        sigma[k] = max(sigma[k], 1e-6)

    pi_k[:] = N_k / N  # this part still has closed form
    return mu, sigma, pi_k

# Compare EM vs GEM convergence speed
np.random.seed(42)
N = 200
x = np.concatenate([np.random.normal(-2, 1, 80), np.random.normal(3, 1.5, 120)])

# Standard EM
mu_em = np.array([0.0, 1.0])
sigma_em = np.array([2.0, 2.0])
pi_em = np.array([0.5, 0.5])

# GEM
mu_gem = np.array([0.0, 1.0])
sigma_gem = np.array([2.0, 2.0])
pi_gem = np.array([0.5, 0.5])

def compute_ll_1d(x, mu, sigma, pi_k):
    ll = 0.0
    for xn in x:
        p = sum(pi_k[k]*np.exp(-0.5*((xn-mu[k])/sigma[k])**2)/(sigma[k]*np.sqrt(2*np.pi))
                for k in range(len(mu)))
        ll += np.log(p + 1e-300)
    return ll

print(f"{'Iter':>4} | {'EM log-lik':>12} | {'GEM log-lik':>12}")
print("-" * 35)

for t in range(20):
    ll_em = compute_ll_1d(x, mu_em, sigma_em, pi_em)
    ll_gem = compute_ll_1d(x, mu_gem, sigma_gem, pi_gem)
    if t % 4 == 0:
        print(f"{t:4d} | {ll_em:12.4f} | {ll_gem:12.4f}")

    # EM: E-step + full M-step
    pdf = np.zeros((N, 2))
    for k in range(2):
        pdf[:, k] = pi_em[k]*np.exp(-0.5*((x-mu_em[k])/sigma_em[k])**2)/(sigma_em[k]*np.sqrt(2*np.pi))
    gamma_em = pdf / (pdf.sum(axis=1, keepdims=True) + 1e-300)
    N_k = gamma_em.sum(axis=0)
    for k in range(2):
        mu_em[k] = (gamma_em[:, k] * x).sum() / N_k[k]
        sigma_em[k] = np.sqrt((gamma_em[:, k] * (x - mu_em[k])**2).sum() / N_k[k]) + 1e-6
    pi_em = N_k / N

    # GEM: E-step + gradient M-step
    pdf_g = np.zeros((N, 2))
    for k in range(2):
        pdf_g[:, k] = pi_gem[k]*np.exp(-0.5*((x-mu_gem[k])/sigma_gem[k])**2)/(sigma_gem[k]*np.sqrt(2*np.pi))
    gamma_gem = pdf_g / (pdf_g.sum(axis=1, keepdims=True) + 1e-300)
    mu_gem, sigma_gem, pi_gem = gem_m_step(x, gamma_gem, mu_gem, sigma_gem, pi_gem, lr=0.5)

print(f"\nEM converges faster (closed-form M-step),")
print(f"but GEM is more flexible (works when no closed form exists).")
```

### 3.11 Missing Dataç†è«– â€” EMã®åŸç‚¹

EMç®—æ³•ã®åŸè«–æ–‡ [^1] ã®ã‚¿ã‚¤ãƒˆãƒ«ã¯ "Maximum Likelihood from **Incomplete Data**" ã ã€‚æ½œåœ¨å¤‰æ•°ã¯æ¬ æãƒ‡ãƒ¼ã‚¿ã®ä¸€èˆ¬åŒ–ã§ã‚ã‚Šã€EMã®åŸç‚¹ã¯æ¬ æå€¤å‡¦ç†ã«ã‚ã‚‹ã€‚

**æ¬ æãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®åˆ†é¡** (Rubin, 1976):

| ãƒ¡ã‚«ãƒ‹ã‚ºãƒ  | å®šç¾© | EMé©ç”¨ |
|:---------|:-----|:-------|
| **MCAR** (Missing Completely At Random) | æ¬ æã¯å®Œå…¨ã«ãƒ©ãƒ³ãƒ€ãƒ  | EMæœ‰åŠ¹ |
| **MAR** (Missing At Random) | æ¬ æã¯è¦³æ¸¬å€¤ã«ä¾å­˜ã™ã‚‹ãŒæ¬ æå€¤ã«ã¯ä¾å­˜ã—ãªã„ | EMæœ‰åŠ¹ |
| **MNAR** (Missing Not At Random) | æ¬ æãŒæ¬ æå€¤è‡ªä½“ã«ä¾å­˜ | EMã ã‘ã§ã¯ä¸ååˆ† |

$$
\text{MCAR}: \quad p(R \mid \mathbf{x}_{\text{obs}}, \mathbf{x}_{\text{mis}}) = p(R)
$$

$$
\text{MAR}: \quad p(R \mid \mathbf{x}_{\text{obs}}, \mathbf{x}_{\text{mis}}) = p(R \mid \mathbf{x}_{\text{obs}})
$$

$$
\text{MNAR}: \quad p(R \mid \mathbf{x}_{\text{obs}}, \mathbf{x}_{\text{mis}}) \text{ depends on } \mathbf{x}_{\text{mis}}
$$

ã“ã“ã§ $R$ ã¯æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¡¨ã™ç¢ºç‡å¤‰æ•°ï¼ˆ$R_{nd} = 1$ ãªã‚‰ $x_{nd}$ ã¯è¦³æ¸¬ã€$R_{nd} = 0$ ãªã‚‰æ¬ æï¼‰ã€‚

MARä»¥ä¸‹ã®ä»®å®šãŒæˆã‚Šç«‹ã¤ã¨ãã€EMç®—æ³•ã¯æ¬ æã‚’ã€Œæ½œåœ¨å¤‰æ•°ã€ã¨ã—ã¦æ‰±ã„ã€å®Œå…¨ãƒ‡ãƒ¼ã‚¿å°¤åº¦ã®æœŸå¾…å€¤ã‚’æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ã§ä¸€è²«ã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®šãŒå¯èƒ½ã«ãªã‚‹ã€‚Zone 5 ã®ãƒãƒ£ãƒ¬ãƒ³ã‚¸2ã§å®Ÿè£…ã—ãŸæ¬ æå€¤è£œå®Œã¯ã€ã¾ã•ã«ã“ã®ç†è«–ã«åŸºã¥ã„ã¦ã„ã‚‹ã€‚

### 3.12 Identifiabilityã¨label switchingå•é¡Œ

GMMã«ã¯æœ¬è³ªçš„ãª **éè­˜åˆ¥å¯èƒ½æ€§** (non-identifiability) ãŒã‚ã‚‹ã€‚

$K$ å€‹ã®æˆåˆ†ã«å¯¾ã—ã¦ã€æˆåˆ†ã®ãƒ©ãƒ™ãƒ«ã‚’ä¸¦ã¹æ›¿ãˆã¦ã‚‚åŒã˜åˆ†å¸ƒã«ãªã‚‹:

$$
\sum_{k=1}^{K} \pi_k \mathcal{N}(x \mid \mu_k, \sigma_k^2) = \sum_{k=1}^{K} \pi_{\tau(k)} \mathcal{N}(x \mid \mu_{\tau(k)}, \sigma_{\tau(k)}^2)
$$

ã“ã“ã§ $\tau$ ã¯ $\{1, \ldots, K\}$ ä¸Šã®ä»»æ„ã®ç½®æ›ã€‚ã¤ã¾ã‚Š $K!$ å€‹ã®ç­‰ä¾¡ãªæœ€é©è§£ãŒå­˜åœ¨ã™ã‚‹ã€‚

ã“ã‚Œã¯ **label switchingå•é¡Œ** ã¨å‘¼ã°ã‚Œã€ãƒ™ã‚¤ã‚ºæ¨è«–ã§GMMã‚’æ‰±ã†éš›ã«ç‰¹ã«æ·±åˆ»ã«ãªã‚‹ã€‚EMç®—æ³•ã§ã¯åˆæœŸå€¤ã§1ã¤ã®è§£ã«ã€Œå›ºå®šã€ã•ã‚Œã‚‹ãŸã‚å®Ÿç”¨ä¸Šã¯å•é¡Œã«ãªã‚‰ãªã„ãŒã€ç†è«–çš„ã«ã¯æœ€é©è§£ã®ä¸€æ„æ€§ãŒä¿è¨¼ã•ã‚Œãªã„ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã€‚

```python
import numpy as np

# Label switching: permuting components gives same distribution
mu = np.array([-2.0, 3.0])
sigma = np.array([1.0, 1.5])
pi_k = np.array([0.4, 0.6])

x = np.array([0.0, 1.0, -1.0, 4.0])

def gmm_pdf_1d(x, mu, sigma, pi_k):
    return sum(pi_k[k] * np.exp(-0.5*((x-mu[k])/sigma[k])**2)/(sigma[k]*np.sqrt(2*np.pi))
               for k in range(len(mu)))

# Original order
pdf_original = np.array([gmm_pdf_1d(xi, mu, sigma, pi_k) for xi in x])

# Swapped labels (permutation Ï„ = (1, 0))
mu_swap = mu[::-1]
sigma_swap = sigma[::-1]
pi_swap = pi_k[::-1]
pdf_swapped = np.array([gmm_pdf_1d(xi, mu_swap, sigma_swap, pi_swap) for xi in x])

print("Original vs Swapped labels (should be identical):")
for i, xi in enumerate(x):
    print(f"  x={xi:5.1f}: p_original={pdf_original[i]:.6f}, p_swapped={pdf_swapped[i]:.6f}, "
          f"diff={abs(pdf_original[i]-pdf_swapped[i]):.2e}")
print(f"\nK=2 components â†’ {np.math.factorial(2)} equivalent optima (label switching)")
print(f"K=5 components â†’ {np.math.factorial(5)} equivalent optima")
```

:::message
**é€²æ—: 50% å®Œäº†** æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ã‚¯ãƒªã‚¢ã€‚EMç®—æ³•ã‚’Jensenä¸ç­‰å¼ã‹ã‚‰å®Œå…¨ã«å°å‡ºã—ã€åæŸæ€§ã‚’è¨¼æ˜ã—ã€GMMã®å…¨æ›´æ–°å¼ã‚’å°å‡ºã—ãŸã€‚å¹¾ä½•å­¦çš„è§£é‡ˆã€GEMã€æ¬ æãƒ‡ãƒ¼ã‚¿ç†è«–ã€label switchingå•é¡Œã¾ã§ç¶²ç¾…ã€‚å¾ŒåŠæˆ¦ã¯å®Ÿè£…ã¨å¿œç”¨ã«é€²ã‚€ã€‚
:::

---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” EMã®å®Ÿè·µçš„ã‚¹ã‚­ãƒ«

### 4.1 å®Ÿè£…ã®å…¨ä½“è¨­è¨ˆ

Zone 3ã§å°å‡ºã—ãŸæ•°å¼ã‚’ã€å®Ÿè·µçš„ãªã‚³ãƒ¼ãƒ‰ã«è½ã¨ã—è¾¼ã‚€ã€‚ã¾ãšå…¨ä½“ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ç¢ºèªã—ã‚ˆã†ã€‚

```mermaid
graph LR
    A["ãƒ‡ãƒ¼ã‚¿ X<br/>(N, D)"] --> B["åˆæœŸåŒ–<br/>K-means++"]
    B --> C["E-step<br/>è²¬ä»»åº¦Î³è¨ˆç®—"]
    C --> D["M-step<br/>Î¼, Î£, Ï€æ›´æ–°"]
    D --> E{"åæŸåˆ¤å®š<br/>|Î”log-lik| < Îµ?"}
    E -->|No| C
    E -->|Yes| F["çµæœ<br/>Î¸*, Î³*"]
    F --> G["ãƒ¢ãƒ‡ãƒ«é¸æŠ<br/>BIC/AIC"]

    style B fill:#e3f2fd
    style E fill:#fff3e0
    style G fill:#c8e6c9
```

### 4.2 æ•°å€¤å®‰å®šæ€§ â€” log-sum-exp ãƒˆãƒªãƒƒã‚¯

GMMã®å®Ÿè£…ã§æœ€ã‚‚å±é™ºãªã®ã¯ **æ•°å€¤ã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ãƒ­ãƒ¼** ã ã€‚ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®æŒ‡æ•°é–¢æ•° $\exp(-\frac{1}{2}(\mathbf{x} - \boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1}(\mathbf{x} - \boldsymbol{\mu}))$ ã¯ã€ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ãŒå¤§ãã„ã¨å®¹æ˜“ã« $10^{-300}$ ä»¥ä¸‹ã«ãªã‚‹ã€‚

**è§£æ±ºç­–: log-sum-exp ãƒˆãƒªãƒƒã‚¯**

$$
\log \sum_k \exp(a_k) = \max_k a_k + \log \sum_k \exp(a_k - \max_k a_k)
$$

```python
import numpy as np

def log_sum_exp(log_vals):
    """Numerically stable log-sum-exp.

    log Î£_k exp(a_k) = max(a) + log Î£_k exp(a_k - max(a))
    """
    max_val = np.max(log_vals, axis=-1, keepdims=True)
    return max_val.squeeze(-1) + np.log(np.sum(np.exp(log_vals - max_val), axis=-1))

# Without log-sum-exp: underflow
large_negative = np.array([-800, -810, -820])
print(f"Naive sum of exp: {np.sum(np.exp(large_negative))}")  # 0.0 (underflow!)

# With log-sum-exp: correct
result = log_sum_exp(large_negative)
print(f"Log-sum-exp:      {result:.4f}")  # correct value
print(f"Verification:     {np.log(np.exp(-800)*(1 + np.exp(-10) + np.exp(-20))):.4f}")  # same

# Application to GMM responsibilities
def e_step_stable(X, mus, covs, pis):
    """Numerically stable E-step using log-sum-exp.

    Î³(z_nk) = exp(log Ï€_k + log N(x_n|Î¼_k,Î£_k) - log Î£_j exp(log Ï€_j + log N(x_n|Î¼_j,Î£_j)))
    """
    N, D = X.shape
    K = len(mus)
    log_resp = np.zeros((N, K))

    for k in range(K):
        diff = X - mus[k]  # (N, D)
        cov_inv = np.linalg.inv(covs[k])
        log_det = np.log(np.linalg.det(covs[k]) + 1e-300)

        # log N(x_n|Î¼_k,Î£_k) = -D/2 log(2Ï€) - 1/2 log|Î£_k| - 1/2 (x-Î¼)^T Î£^{-1} (x-Î¼)
        mahal = np.sum(diff @ cov_inv * diff, axis=1)  # (N,)
        log_resp[:, k] = np.log(pis[k] + 1e-300) - 0.5 * D * np.log(2*np.pi) - 0.5 * log_det - 0.5 * mahal

    # Stable softmax over components
    log_sum = log_sum_exp(log_resp)  # (N,)
    log_gamma = log_resp - log_sum[:, np.newaxis]
    gamma = np.exp(log_gamma)

    return gamma, log_sum.sum()  # responsibilities and log-likelihood

# Test
np.random.seed(42)
X = np.random.randn(100, 2) * 3
mus = [np.array([0, 0]), np.array([5, 5])]
covs = [np.eye(2), np.eye(2)*2]
pis = np.array([0.5, 0.5])

gamma, ll = e_step_stable(X, mus, covs, pis)
print(f"\nStable E-step: log-lik = {ll:.4f}")
print(f"Î³ sum per row (should be 1): {gamma.sum(axis=1)[:5].round(6)}")
```

### 4.3 K-means++ åˆæœŸåŒ–

EMç®—æ³•ã¯åˆæœŸå€¤ã«ä¾å­˜ã™ã‚‹ã€‚æ‚ªã„åˆæœŸå€¤ã¯åæŸã®é…å»¶ã‚„å±€æ‰€æœ€é©è§£ã¸ã®åæŸã‚’å¼•ãèµ·ã“ã™ã€‚K-means++ [^6] ã¯åˆæœŸå€¤é¸æŠã®æ¨™æº–æ‰‹æ³•ã ã€‚

```python
import numpy as np

def kmeans_plus_plus_init(X, K, seed=42):
    """K-means++ initialization for GMM.

    1. Choose first center uniformly at random
    2. For each subsequent center:
       - Compute D(x) = distance to nearest existing center
       - Choose next center with probability proportional to D(x)Â²
    """
    rng = np.random.RandomState(seed)
    N, D = X.shape
    centers = []

    # First center: uniform random
    idx = rng.randint(N)
    centers.append(X[idx].copy())

    for _ in range(1, K):
        # Distance to nearest center
        dists = np.array([np.min([np.sum((x - c)**2) for c in centers]) for x in X])
        # Probability proportional to D(x)Â²
        probs = dists / dists.sum()
        idx = rng.choice(N, p=probs)
        centers.append(X[idx].copy())

    return np.array(centers)

# Demonstrate K-means++ vs random init
np.random.seed(42)
N = 300
X = np.vstack([np.random.randn(100, 2) + [-5, -5],
               np.random.randn(100, 2) + [0, 5],
               np.random.randn(100, 2) + [5, -3]])

centers_kpp = kmeans_plus_plus_init(X, 3)
centers_random = X[np.random.choice(N, 3, replace=False)]

print("K-means++ centers:")
for i, c in enumerate(centers_kpp):
    print(f"  Center {i}: ({c[0]:6.2f}, {c[1]:6.2f})")

print("\nRandom centers:")
for i, c in enumerate(centers_random):
    print(f"  Center {i}: ({c[0]:6.2f}, {c[1]:6.2f})")

print("\nTrue centers: (-5,-5), (0,5), (5,-3)")
print("K-means++ typically provides much better coverage.")
```

### 4.4 ãƒ¢ãƒ‡ãƒ«é¸æŠ â€” BIC ã¨ AIC

æˆåˆ†æ•° $K$ ã‚’ã©ã†æ±ºã‚ã‚‹ã‹ï¼Ÿãƒ‡ãƒ¼ã‚¿ã‚’æœ€ã‚‚ã‚ˆãèª¬æ˜ã™ã‚‹ $K$ ã‚’é¸ã³ãŸã„ãŒã€$K$ ã‚’å¢—ã‚„ã›ã°å°¤åº¦ã¯å¸¸ã«ä¸ŠãŒã‚‹ï¼ˆéå­¦ç¿’ï¼‰ã€‚**BIC** (Bayesian Information Criterion) ã¨ **AIC** (Akaike Information Criterion) ãŒã“ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹ã€‚

$$
\text{BIC} = -2 \log p(\mathbf{x} \mid \hat{\theta}) + d \log N
$$

$$
\text{AIC} = -2 \log p(\mathbf{x} \mid \hat{\theta}) + 2d
$$

ã“ã“ã§ $d$ ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã€‚GMMã®å ´åˆ $d = K(D + D(D+1)/2 + 1) - 1$ï¼ˆå¹³å‡ + å…±åˆ†æ•£ + æ··åˆé‡ã¿ã§ã€åˆ¶ç´„ã‚’å¼•ãï¼‰ã€‚1æ¬¡å…ƒãªã‚‰ $d = 3K - 1$ã€‚

| åŸºæº– | ãƒšãƒŠãƒ«ãƒ†ã‚£ | å‚¾å‘ |
|:-----|:---------|:-----|
| BIC | $d \log N$ (ãƒ‡ãƒ¼ã‚¿æ•°ã«ä¾å­˜) | ã‚ˆã‚Šå°‘ãªã„ $K$ ã‚’é¸ã³ã‚„ã™ã„ï¼ˆä¿å®ˆçš„ï¼‰ |
| AIC | $2d$ (ãƒ‡ãƒ¼ã‚¿æ•°ã«ä¾å­˜ã—ãªã„) | BICã‚ˆã‚Šå¤§ãã„ $K$ ã‚’é¸ã³ã‚„ã™ã„ |

```python
import numpy as np

def run_em_gmm_1d(x, K, max_iter=100, tol=1e-6, seed=42):
    """Run EM for 1D GMM with K components. Return log-likelihood and params."""
    rng = np.random.RandomState(seed)
    N = len(x)

    # K-means++ init
    mu = np.sort(rng.choice(x, K, replace=False))
    sigma = np.ones(K) * x.std()
    pi_k = np.ones(K) / K

    prev_ll = -np.inf
    for _ in range(max_iter):
        # E-step
        pdf = np.zeros((N, K))
        for k in range(K):
            pdf[:, k] = pi_k[k] * np.exp(-0.5*((x-mu[k])/sigma[k])**2) / (sigma[k]*np.sqrt(2*np.pi))
        total = pdf.sum(axis=1, keepdims=True)
        gamma = pdf / (total + 1e-300)

        ll = np.sum(np.log(total.squeeze() + 1e-300))
        if abs(ll - prev_ll) < tol:
            break
        prev_ll = ll

        # M-step
        N_k = gamma.sum(axis=0)
        for k in range(K):
            mu[k] = (gamma[:, k] * x).sum() / (N_k[k] + 1e-300)
            sigma[k] = np.sqrt((gamma[:, k] * (x - mu[k])**2).sum() / (N_k[k] + 1e-300)) + 1e-6
        pi_k = N_k / N

    return ll, mu, sigma, pi_k

# Generate data from K=3 components
np.random.seed(42)
x = np.concatenate([np.random.normal(-3, 0.8, 100),
                     np.random.normal(0, 1.0, 150),
                     np.random.normal(4, 0.6, 100)])
N = len(x)

print(f"{'K':>3} | {'log-lik':>10} | {'d (params)':>10} | {'BIC':>10} | {'AIC':>10}")
print("-" * 55)

bic_values = []
for K in range(1, 8):
    ll, mu, sigma, pi_k = run_em_gmm_1d(x, K)
    d = 3 * K - 1  # parameters: K means + K variances + (K-1) weights
    bic = -2 * ll + d * np.log(N)
    aic = -2 * ll + 2 * d
    bic_values.append(bic)
    marker = " â† best" if K == np.argmin(bic_values) + 1 and K > 1 else ""
    print(f"{K:3d} | {ll:10.2f} | {d:10d} | {bic:10.2f} | {aic:10.2f}{marker}")

best_K = np.argmin(bic_values) + 1
print(f"\nBIC selects K = {best_K} (true K = 3)")
```

### 4.5 Singularityå•é¡Œã¨å¯¾ç­–

GMMã®é‡å¤§ãªè½ã¨ã—ç©´: ã‚ã‚‹æˆåˆ†ãŒãƒ‡ãƒ¼ã‚¿1ç‚¹ã«ã€Œå´©å£Šã€ã™ã‚‹ã¨ $\sigma_k \to 0$ã€å°¤åº¦ãŒ $\to \infty$ ã«ç™ºæ•£ã™ã‚‹ã€‚

```python
import numpy as np

# Singularity demonstration
# If Î¼_k = x_n for some n, and Ïƒ_k â†’ 0:
# N(x_n|Î¼_k,Ïƒ_kÂ²) = 1/(Ïƒ_kâˆš2Ï€) â†’ âˆ

print("Singularity problem: when Ïƒ â†’ 0 for one component")
for sigma in [1.0, 0.1, 0.01, 0.001, 1e-6, 1e-10]:
    pdf = 1.0 / (sigma * np.sqrt(2 * np.pi))
    print(f"  Ïƒ = {sigma:.1e}  â†’  N(0|0,ÏƒÂ²) = {pdf:.6e}")
print("\nAs Ïƒ â†’ 0, the density â†’ âˆ (singularity!)")

# Standard fixes
print("\n=== Countermeasures ===")
print("1. Floor on variance: Ïƒ_kÂ² â‰¥ Îµ (e.g., Îµ = 1e-6)")
print("2. Regularization: Î£_k â†’ Î£_k + Î»I")
print("3. MAP estimation: Wishart prior on Î£_k")
print("4. Drop degenerate components: if N_k < threshold, remove component k")

# Implementation of variance floor
def m_step_with_floor(x, gamma, eps=1e-6):
    """M-step with variance floor to prevent singularity."""
    N_k = gamma.sum(axis=0)
    K = gamma.shape[1]
    mu = np.zeros(K)
    sigma = np.zeros(K)

    for k in range(K):
        mu[k] = (gamma[:, k] * x).sum() / (N_k[k] + 1e-300)
        var_k = (gamma[:, k] * (x - mu[k])**2).sum() / (N_k[k] + 1e-300)
        sigma[k] = np.sqrt(max(var_k, eps))  # Floor!

    pi_k = N_k / len(x)
    return mu, sigma, pi_k

print("\nVariance floor prevents Ïƒ â†’ 0 and keeps log-likelihood finite.")
```

### 4.6 æ•°å¼â†’ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ‘ã‚¿ãƒ¼ãƒ³

| ãƒ‘ã‚¿ãƒ¼ãƒ³ | æ•°å¼ | Python | èª¬æ˜ |
|:---------|:-----|:-------|:-----|
| è²¬ä»»åº¦ | $\gamma_{nk} = \frac{\pi_k f_k}{\sum_j \pi_j f_j}$ | `gamma = pdf / pdf.sum(axis=1, keepdims=True)` | è¡Œã”ã¨ã®æ­£è¦åŒ– |
| é‡ã¿ä»˜ãå¹³å‡ | $\frac{\sum_n w_n x_n}{\sum_n w_n}$ | `(w * x).sum() / w.sum()` | ãƒ™ã‚¯ãƒˆãƒ«åŒ– |
| å¯¾æ•°ã‚¬ã‚¦ã‚¹ | $-\frac{D}{2}\log 2\pi - \frac{1}{2}\log|\Sigma|$ | `-0.5*D*np.log(2*np.pi) - 0.5*np.linalg.slogdet(cov)[1]` | slogdet ã§å®‰å®š |
| ãƒãƒãƒ©ãƒãƒ“ã‚¹ | $(\mathbf{x}-\boldsymbol{\mu})^\top \Sigma^{-1} (\mathbf{x}-\boldsymbol{\mu})$ | `diff @ np.linalg.solve(cov, diff)` | solve > inv |
| log-sum-exp | $\log \sum_k e^{a_k}$ | `max(a) + np.log(np.sum(np.exp(a - max(a))))` | ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼é˜²æ­¢ |
| è¡Œåˆ—å¼å¯¾æ•° | $\log |\Sigma|$ | `np.linalg.slogdet(cov)[1]` | ç›´æ¥è¨ˆç®—ã§ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼å›é¿ |
| å¯¾è§’å…±åˆ†æ•£ | $\text{diag}(\sigma_1^2, \ldots, \sigma_D^2)$ | `np.diag(sigma**2)` | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°å‰Šæ¸› |

:::details è«–æ–‡èª­è§£ã‚¬ã‚¤ãƒ‰ â€” Dempster, Laird, Rubin (1977) ã‚’èª­ã‚€
EMç®—æ³•ã®åŸè«–æ–‡ [^1] ã¯50ãƒšãƒ¼ã‚¸è¿‘ã„å¤§ä½œã ãŒã€æ§‹é€ ã‚’çŸ¥ã£ã¦ã„ã‚Œã°èª­ã‚ã‚‹ã€‚

**3ãƒ‘ã‚¹ãƒªãƒ¼ãƒ‡ã‚£ãƒ³ã‚°**:

**Pass 1** (10åˆ†): Abstract â†’ Section 1 (Introduction) â†’ Section 2 ã®å®šç†æ–‡ â†’ Section 8 (Examples) ã® GMM éƒ¨åˆ†
```python
pass1_notes = {
    "title": "Maximum Likelihood from Incomplete Data via the EM Algorithm",
    "year": 1977,
    "venue": "JRSS-B",
    "key_contribution": "General framework for MLE with missing/latent data",
    "method": "E-step (compute expected sufficient statistics) + M-step (maximize)",
    "theoretical_guarantee": "Log-likelihood monotonically non-decreasing",
    "examples_covered": "GMM, Factor Analysis, Missing data, Variance components",
}
```

**Pass 2** (30åˆ†): Theorem 1 (convergence) ã®è¨¼æ˜ã‚’è¿½ã†ã€‚Section 3 ã® Qé–¢æ•°å®šç¾©ãŒæ ¸å¿ƒã€‚

**Pass 3** (60åˆ†): Section 4 ã®åæŸé€Ÿåº¦ã€Section 5 ã®æŒ‡æ•°å‹åˆ†å¸ƒæ—ã§ã®ç°¡ç•¥åŒ–ã€‚
:::

### 4.7 Pythonã®é™ç•Œ â€” Profileçµæœ

ã“ã“ã§Course I ã®ä¼ç·šã‚’å›åã™ã‚‹ã€‚EMç®—æ³•ã®Pythonå®Ÿè£…ã‚’æœ¬æ ¼çš„ã«Profile ã—ã¦ã¿ã‚ˆã†ã€‚

```python
import numpy as np
import time

def em_gmm_full(X, K, max_iter=100, tol=1e-6, seed=42):
    """Full GMM EM with profiling."""
    rng = np.random.RandomState(seed)
    N, D = X.shape

    # Init
    idx = rng.choice(N, K, replace=False)
    mus = X[idx].copy()
    covs = [np.eye(D) * X.var() for _ in range(K)]
    pis = np.ones(K) / K

    times = {'e_step': 0.0, 'm_step': 0.0}

    for iteration in range(max_iter):
        # E-step
        t0 = time.perf_counter()
        log_resp = np.zeros((N, K))
        for k in range(K):
            diff = X - mus[k]
            cov_inv = np.linalg.inv(covs[k])
            log_det = np.log(np.linalg.det(covs[k]) + 1e-300)
            mahal = np.sum(diff @ cov_inv * diff, axis=1)
            log_resp[:, k] = np.log(pis[k]+1e-300) - 0.5*D*np.log(2*np.pi) - 0.5*log_det - 0.5*mahal

        log_max = log_resp.max(axis=1, keepdims=True)
        gamma = np.exp(log_resp - log_max)
        gamma /= gamma.sum(axis=1, keepdims=True)
        times['e_step'] += time.perf_counter() - t0

        # M-step
        t0 = time.perf_counter()
        N_k = gamma.sum(axis=0)
        for k in range(K):
            mus[k] = (gamma[:, k:k+1] * X).sum(axis=0) / N_k[k]
            diff = X - mus[k]
            covs[k] = (gamma[:, k:k+1] * diff).T @ diff / N_k[k] + 1e-6 * np.eye(D)
        pis = N_k / N
        times['m_step'] += time.perf_counter() - t0

    return times, iteration + 1

# Benchmark with increasing data size
print(f"{'N':>8} | {'K':>3} | {'D':>3} | {'E-step (ms)':>12} | {'M-step (ms)':>12} | {'Total (ms)':>12} | {'Per iter':>10}")
print("-" * 80)

for N in [1000, 5000, 10000, 50000]:
    D, K = 10, 5
    np.random.seed(42)
    X = np.random.randn(N, D)

    times, n_iter = em_gmm_full(X, K, max_iter=50)
    total = (times['e_step'] + times['m_step']) * 1000
    print(f"{N:8d} | {K:3d} | {D:3d} | {times['e_step']*1000:12.1f} | {times['m_step']*1000:12.1f} | "
          f"{total:12.1f} | {total/n_iter:10.1f} ms")

print(f"\n{'='*60}")
print("N=50000 ã§æ—¢ã«æ•°ç§’ã‹ã‹ã‚‹ã€‚")
print("N=1000000 ã«ãªã£ãŸã‚‰ï¼Ÿ D=100 ã«ãªã£ãŸã‚‰ï¼Ÿ")
print("......ã€Œé…ã™ããªã„ï¼Ÿã€")
print(f"{'='*60}")
print("\nã“ã®ç–‘å•ã¸ã®å›ç­”ã¯ç¬¬9å›ã§ã€‚")
print("Julia ã® ELBO è¨ˆç®—ã¯ Python ã® 50å€é€Ÿã„ã€‚è¦šãˆã¦ãŠã„ã¦ãã ã•ã„ã€‚")
```

:::message alert
**é…ã™ããªã„ï¼Ÿ** â€” N=50,000, K=5, D=10 ã§æ—¢ã«æ•°ç§’ã€‚ç¾å®Ÿã®ãƒ‡ãƒ¼ã‚¿ï¼ˆN=100ä¸‡ã€D=100ï¼‰ã§ã¯åˆ†å˜ä½ã«ãªã‚‹ã€‚ã“ã® Python ã®é™ç•ŒãŒã€ç¬¬9å›ã® Julia å°å…¥ã®ä¼ç·šã ã€‚
:::

> **Zone 4 ã¾ã¨ã‚**: æ•°å€¤å®‰å®šãªEMå®Ÿè£…ï¼ˆlog-sum-expï¼‰ã€K-means++ åˆæœŸåŒ–ã€BIC/AICã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«é¸æŠã€Singularityå¯¾ç­–ã€æ•°å¼â†’ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¿’å¾—ã—ãŸã€‚ãã—ã¦Pythonã®é€Ÿåº¦é™ç•Œã‚’ä½“æ„Ÿã—ãŸã€‚

:::message
**é€²æ—: 70% å®Œäº†** å®Ÿè£…ã‚¾ãƒ¼ãƒ³ã‚¯ãƒªã‚¢ã€‚ç†è«–ã‚’å®Ÿè£…ã«è½ã¨ã—è¾¼ã‚€æŠ€è¡“ã‚’ç²å¾—ã—ãŸã€‚æ¬¡ã¯å®Ÿé¨“ã§ç†è§£ã‚’ç¢ºèªã™ã‚‹ã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

### 5.1 è¨˜å·èª­è§£ãƒ†ã‚¹ãƒˆ

ä»¥ä¸‹ã®æ•°å¼ã‚’å£°ã«å‡ºã—ã¦èª­ã¿ã€æ„å‘³ã‚’èª¬æ˜ã§ãã‚‹ã‹ç¢ºèªã—ã‚ˆã†ã€‚

:::details Q1: $p(\mathbf{x} \mid \theta) = \sum_{\mathbf{z}} p(\mathbf{x}, \mathbf{z} \mid \theta)$
**èª­ã¿**: ã€Œãƒ”ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒŠãƒ« ã‚·ãƒ¼ã‚¿ ã‚¤ã‚³ãƒ¼ãƒ« ã‚·ã‚°ãƒ ã‚¼ãƒƒãƒˆ ãƒ”ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ ã‚¼ãƒƒãƒˆ ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒŠãƒ« ã‚·ãƒ¼ã‚¿ã€

**æ„å‘³**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã®ä¸‹ã§ã®è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ $\mathbf{x}$ ã®å‘¨è¾ºå°¤åº¦ã€‚æ½œåœ¨å¤‰æ•° $\mathbf{z}$ ã‚’å…¨ã¦è¶³ã—åˆã‚ã›ã¦ï¼ˆå‘¨è¾ºåŒ–ã—ã¦ï¼‰å¾—ã‚‰ã‚Œã‚‹ã€‚ã“ã‚ŒãŒã€Œevidenceã€ã¨ã‚‚å‘¼ã°ã‚Œã‚‹ã€‚[^1]
:::

:::details Q2: $\gamma(z_{nk}) = \frac{\pi_k \mathcal{N}(x_n \mid \mu_k, \sigma_k^2)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(x_n \mid \mu_j, \sigma_j^2)}$
**èª­ã¿**: ã€Œã‚¬ãƒ³ãƒ ã‚¼ãƒƒãƒˆ ã‚¨ãƒŒ ã‚±ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ãƒ‘ã‚¤ã‚±ãƒ¼ ã‚¨ãƒŒ ã‚¨ãƒƒã‚¯ã‚¹ã‚¨ãƒŒ ãƒŸãƒ¥ãƒ¼ã‚±ãƒ¼ ã‚·ã‚°ãƒã‚±ãƒ¼äºŒä¹— ã¶ã‚“ã®...ã€

**æ„å‘³**: ãƒ‡ãƒ¼ã‚¿ç‚¹ $x_n$ ãŒæ··åˆæˆåˆ† $k$ ã‹ã‚‰ç”Ÿæˆã•ã‚ŒãŸäº‹å¾Œç¢ºç‡ï¼ˆè²¬ä»»åº¦ï¼‰ã€‚E-stepã§è¨ˆç®—ã™ã‚‹ã€‚ãƒ™ã‚¤ã‚ºã®å®šç†ãã®ã‚‚ã®ã ã€‚
:::

:::details Q3: $Q(\theta, \theta^{(t)}) = \mathbb{E}_{\mathbf{z} \sim p(\mathbf{z} \mid \mathbf{x}, \theta^{(t)})} [\log p(\mathbf{x}, \mathbf{z} \mid \theta)]$
**èª­ã¿**: ã€Œã‚­ãƒ¥ãƒ¼ ã‚·ãƒ¼ã‚¿ ã‚·ãƒ¼ã‚¿ ãƒ†ã‚£ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¨ã‚¯ã‚¹ãƒšã‚¯ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ ã‚¼ãƒƒãƒˆ ãƒ†ã‚£ãƒ«ãƒ‡ ãƒ”ãƒ¼ ã‚¼ãƒƒãƒˆ ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒŠãƒ« ã‚¨ãƒƒã‚¯ã‚¹ ã‚·ãƒ¼ã‚¿ãƒ†ã‚£ãƒ¼ ãƒ­ã‚° ãƒ”ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ ã‚¼ãƒƒãƒˆ ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒŠãƒ« ã‚·ãƒ¼ã‚¿ã€

**æ„å‘³**: Qé–¢æ•°ã€‚ç¾åœ¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta^{(t)}$ ã§ã®äº‹å¾Œåˆ†å¸ƒã®ä¸‹ã§ã€å®Œå…¨ãƒ‡ãƒ¼ã‚¿å¯¾æ•°å°¤åº¦ã®æœŸå¾…å€¤ã‚’å–ã£ãŸã‚‚ã®ã€‚M-stepã§ã¯ã“ã‚Œã‚’ $\theta$ ã«ã¤ã„ã¦æœ€å¤§åŒ–ã™ã‚‹ã€‚[^1]
:::

:::details Q4: $\log p(\mathbf{x} \mid \theta) = \mathcal{L}(q, \theta) + \text{KL}[q \| p(\mathbf{z} \mid \mathbf{x}, \theta)]$
**èª­ã¿**: ã€Œãƒ­ã‚° ãƒ”ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒŠãƒ« ã‚·ãƒ¼ã‚¿ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¨ãƒ« ã‚­ãƒ¥ãƒ¼ ã‚·ãƒ¼ã‚¿ ãƒ—ãƒ©ã‚¹ ã‚±ãƒ¼ã‚¨ãƒ« ã‚­ãƒ¥ãƒ¼ ãƒ‘ãƒ©ãƒ¬ãƒ« ãƒ”ãƒ¼ ã‚¼ãƒƒãƒˆ ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒŠãƒ« ã‚¨ãƒƒã‚¯ã‚¹ ã‚·ãƒ¼ã‚¿ã€

**æ„å‘³**: ELBOåˆ†è§£ã€‚å¯¾æ•°å°¤åº¦ã¯ELBOï¼ˆä¸‹ç•Œï¼‰ã¨KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®å’Œã«åˆ†è§£ã•ã‚Œã‚‹ã€‚KL $\geq 0$ ã ã‹ã‚‰ELBOã¯å¯¾æ•°å°¤åº¦ã®ä¸‹ç•Œã€‚E-stepã§ KL = 0 ã«ã—ã€M-stepã§ELBOã‚’æœ€å¤§åŒ–ã™ã‚‹ã€‚
:::

:::details Q5: $\boldsymbol{\mu}_k^{(t+1)} = \frac{1}{N_k} \sum_{n=1}^{N} \gamma(z_{nk}) \, \mathbf{x}_n$
**èª­ã¿**: ã€ŒãƒŸãƒ¥ãƒ¼ ã‚±ãƒ¼ ãƒ†ã‚£ãƒ¼ãƒ—ãƒ©ã‚¹ãƒ¯ãƒ³ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¨ãƒŒã‚±ãƒ¼ ã¶ã‚“ã®ã‚¤ãƒ ã‚·ã‚°ãƒ ã‚¨ãƒŒ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¤ãƒ ã‚«ãƒ© ã‚¨ãƒŒ ã‚¬ãƒ³ãƒ ã‚¼ãƒƒãƒˆã‚¨ãƒŒã‚±ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ã‚¨ãƒŒã€

**æ„å‘³**: GMM M-stepã®å¹³å‡æ›´æ–°å¼ã€‚è²¬ä»»åº¦ $\gamma(z_{nk})$ ã§é‡ã¿ä»˜ã‘ã—ãŸãƒ‡ãƒ¼ã‚¿ã®åŠ é‡å¹³å‡ã€‚$N_k = \sum_n \gamma(z_{nk})$ ã¯æˆåˆ† $k$ ã®ã€Œå®ŸåŠ¹ãƒ‡ãƒ¼ã‚¿æ•°ã€ã€‚
:::

:::details Q6: $\text{BIC} = -2 \log p(\mathbf{x} \mid \hat{\theta}) + d \log N$
**èª­ã¿**: ã€Œãƒ“ãƒ¼ã‚¢ã‚¤ã‚·ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ãƒã‚¤ãƒŠã‚¹ãƒ‹ ãƒ­ã‚° ãƒ”ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒŠãƒ« ã‚·ãƒ¼ã‚¿ãƒãƒƒãƒˆ ãƒ—ãƒ©ã‚¹ ãƒ‡ã‚£ãƒ¼ ãƒ­ã‚° ã‚¨ãƒŒã€

**æ„å‘³**: ãƒ™ã‚¤ã‚ºæƒ…å ±é‡åŸºæº–ã€‚ç¬¬1é …ã¯å°¤åº¦ï¼ˆãƒ•ã‚£ãƒƒãƒˆã®è‰¯ã•ï¼‰ã€ç¬¬2é …ã¯ãƒ¢ãƒ‡ãƒ«è¤‡é›‘åº¦ã®ãƒšãƒŠãƒ«ãƒ†ã‚£ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° $d$ ãŒå¤šã„ã»ã©ãƒšãƒŠãƒ«ãƒ†ã‚£ãŒå¤§ããã€éå­¦ç¿’ã‚’é˜²ãã€‚
:::

:::details Q7: $\boldsymbol{\Sigma}_k^{(t+1)} = \frac{1}{N_k} \sum_{n=1}^{N} \gamma(z_{nk}) (\mathbf{x}_n - \boldsymbol{\mu}_k)(\mathbf{x}_n - \boldsymbol{\mu}_k)^\top$
**èª­ã¿**: ã€Œã‚·ã‚°ãƒ ã‚±ãƒ¼ ãƒ†ã‚£ãƒ¼ãƒ—ãƒ©ã‚¹ãƒ¯ãƒ³ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¨ãƒŒã‚±ãƒ¼ ã¶ã‚“ã® ã‚·ã‚°ãƒ ã‚¬ãƒ³ãƒ ã‚¼ãƒƒãƒˆã‚¨ãƒŒã‚±ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ã‚¨ãƒŒ ãƒã‚¤ãƒŠã‚¹ ãƒŸãƒ¥ãƒ¼ã‚±ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ã‚¨ãƒŒ ãƒã‚¤ãƒŠã‚¹ ãƒŸãƒ¥ãƒ¼ã‚±ãƒ¼ ãƒˆãƒ©ãƒ³ã‚¹ãƒãƒ¼ã‚ºã€

**æ„å‘³**: GMM M-stepã®å…±åˆ†æ•£è¡Œåˆ—æ›´æ–°å¼ã€‚è²¬ä»»åº¦ã§é‡ã¿ä»˜ã‘ã—ãŸå¤–ç©ã®å¹³å‡ã€‚$D \times D$ è¡Œåˆ—ãŒå¾—ã‚‰ã‚Œã‚‹ã€‚
:::

:::details Q8: $p(\mathbf{x}, \mathbf{z} \mid \theta) = p(\mathbf{x} \mid \mathbf{z}, \theta) \, p(\mathbf{z} \mid \theta)$
**èª­ã¿**: ã€Œãƒ”ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ ã‚¼ãƒƒãƒˆ ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒŠãƒ« ã‚·ãƒ¼ã‚¿ ã‚¤ã‚³ãƒ¼ãƒ« ãƒ”ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒŠãƒ« ã‚¼ãƒƒãƒˆ ã‚·ãƒ¼ã‚¿ ãƒ”ãƒ¼ ã‚¼ãƒƒãƒˆ ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒŠãƒ« ã‚·ãƒ¼ã‚¿ã€

**æ„å‘³**: åŒæ™‚åˆ†å¸ƒã®åˆ†è§£ã€‚$p(\mathbf{z} \mid \theta)$ ã¯æ½œåœ¨å¤‰æ•°ã®äº‹å‰åˆ†å¸ƒï¼ˆGMMã§ã¯æ··åˆé‡ã¿ $\pi_k$ï¼‰ã€$p(\mathbf{x} \mid \mathbf{z}, \theta)$ ã¯æ¡ä»¶ä»˜ãå°¤åº¦ï¼ˆGMMã§ã¯å„ã‚¬ã‚¦ã‚¹æˆåˆ†ï¼‰ã€‚
:::

:::details Q9: $\log p(\mathbf{x} \mid \theta^{(t+1)}) \geq \log p(\mathbf{x} \mid \theta^{(t)})$
**èª­ã¿**: ã€Œãƒ­ã‚° ãƒ”ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒŠãƒ« ã‚·ãƒ¼ã‚¿ ãƒ†ã‚£ãƒ¼ãƒ—ãƒ©ã‚¹ãƒ¯ãƒ³ ã‚¤ã‚³ãƒ¼ãƒ«ã‚ªã‚¢ã‚°ãƒ¬ãƒ¼ã‚¿ãƒ¼ ãƒ­ã‚° ãƒ”ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒŠãƒ« ã‚·ãƒ¼ã‚¿ ãƒ†ã‚£ãƒ¼ã€

**æ„å‘³**: EMç®—æ³•ã®å˜èª¿æ€§ã€‚å„åå¾©ã§å¯¾æ•°å°¤åº¦ã¯æ¸›å°‘ã—ãªã„ã€‚ã“ã‚Œã¯EMç®—æ³•ã®ç†è«–çš„ä¿è¨¼ã§ã‚ã‚Šã€Wu (1983) [^3] ã§å³å¯†ã«è¨¼æ˜ã•ã‚ŒãŸã€‚
:::

:::details Q10: $\pi_k^{(t+1)} = \frac{N_k}{N}$, where $N_k = \sum_{n=1}^{N} \gamma(z_{nk})$
**èª­ã¿**: ã€Œãƒ‘ã‚¤ ã‚±ãƒ¼ ãƒ†ã‚£ãƒ¼ãƒ—ãƒ©ã‚¹ãƒ¯ãƒ³ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¨ãƒŒã‚±ãƒ¼ ã¶ã‚“ã®ã‚¨ãƒŒã€

**æ„å‘³**: æ··åˆé‡ã¿ã®æ›´æ–°å¼ã€‚$N_k$ ã¯æˆåˆ† $k$ ã«å¸°å±ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®ã€Œå®ŸåŠ¹çš„ãªæ•°ã€ã§ã‚ã‚Šã€å…¨ãƒ‡ãƒ¼ã‚¿æ•° $N$ ã§å‰²ã‚‹ã“ã¨ã§ç¢ºç‡ï¼ˆæ¯”ç‡ï¼‰ã«ãªã‚‹ã€‚ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥æœªå®šä¹—æ•°æ³•ã‹ã‚‰å°å‡ºã•ã‚Œã‚‹ã€‚
:::

### 5.2 LaTeXè¨˜è¿°ãƒ†ã‚¹ãƒˆ

:::details LQ1: GMMã®å‘¨è¾ºå°¤åº¦ã‚’æ›¸ã‘
```latex
p(\mathbf{x} \mid \theta) = \sum_{k=1}^{K} \pi_k \, \mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
```
$$p(\mathbf{x} \mid \theta) = \sum_{k=1}^{K} \pi_k \, \mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$$
:::

:::details LQ2: ELBOåˆ†è§£ã‚’æ›¸ã‘
```latex
\log p(\mathbf{x} \mid \theta) = \mathcal{L}(q, \theta) + \text{KL}[q(\mathbf{z}) \| p(\mathbf{z} \mid \mathbf{x}, \theta)]
```
$$\log p(\mathbf{x} \mid \theta) = \mathcal{L}(q, \theta) + \text{KL}[q(\mathbf{z}) \| p(\mathbf{z} \mid \mathbf{x}, \theta)]$$
:::

:::details LQ3: Jensenä¸ç­‰å¼ï¼ˆå‡¹é–¢æ•°ç‰ˆï¼‰ã‚’æ›¸ã‘
```latex
f\left( \mathbb{E}[X] \right) \geq \mathbb{E}[f(X)] \quad (\text{for concave } f)
```
$$f\left( \mathbb{E}[X] \right) \geq \mathbb{E}[f(X)] \quad (\text{for concave } f)$$
:::

:::details LQ4: Qé–¢æ•°ã‚’æ›¸ã‘
```latex
Q(\theta, \theta^{(t)}) = \mathbb{E}_{\mathbf{z} \sim p(\mathbf{z} \mid \mathbf{x}, \theta^{(t)})} \left[ \log p(\mathbf{x}, \mathbf{z} \mid \theta) \right]
```
$$Q(\theta, \theta^{(t)}) = \mathbb{E}_{\mathbf{z} \sim p(\mathbf{z} \mid \mathbf{x}, \theta^{(t)})} \left[ \log p(\mathbf{x}, \mathbf{z} \mid \theta) \right]$$
:::

:::details LQ5: å¤šå¤‰é‡ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®KL divergenceã‚’æ›¸ã‘
```latex
\text{KL}[\mathcal{N}_0 \| \mathcal{N}_1] = \frac{1}{2} \left[ \text{tr}(\boldsymbol{\Sigma}_1^{-1} \boldsymbol{\Sigma}_0) + (\boldsymbol{\mu}_1 - \boldsymbol{\mu}_0)^\top \boldsymbol{\Sigma}_1^{-1} (\boldsymbol{\mu}_1 - \boldsymbol{\mu}_0) - D + \log \frac{|\boldsymbol{\Sigma}_1|}{|\boldsymbol{\Sigma}_0|} \right]
```
$$\text{KL}[\mathcal{N}_0 \| \mathcal{N}_1] = \frac{1}{2} \left[ \text{tr}(\boldsymbol{\Sigma}_1^{-1} \boldsymbol{\Sigma}_0) + (\boldsymbol{\mu}_1 - \boldsymbol{\mu}_0)^\top \boldsymbol{\Sigma}_1^{-1} (\boldsymbol{\mu}_1 - \boldsymbol{\mu}_0) - D + \log \frac{|\boldsymbol{\Sigma}_1|}{|\boldsymbol{\Sigma}_0|} \right]$$
ã“ã®å…¬å¼ã¯ç¬¬9å›ï¼ˆå¤‰åˆ†æ¨è«–ï¼‰ã¨ç¬¬10å›ï¼ˆVAEï¼‰ã§é »å‡ºã™ã‚‹ã€‚ä»Šã®ã†ã¡ã«æ›¸ã‘ã‚‹ã‚ˆã†ã«ã—ã¦ãŠã“ã†ã€‚
:::

### 5.3 ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ†ã‚¹ãƒˆ

:::details CQ1: è²¬ä»»åº¦ã®è¨ˆç®—ã‚’NumPyã§æ›¸ã‘
```python
# Î³(z_nk) = Ï€_k N(x_n|Î¼_k,Ïƒ_kÂ²) / Î£_j Ï€_j N(x_n|Î¼_j,Ïƒ_jÂ²)
def compute_responsibilities(X, mus, sigmas, pis):
    N = len(X)
    K = len(mus)
    pdf = np.zeros((N, K))
    for k in range(K):
        pdf[:, k] = pis[k] * np.exp(-0.5*((X - mus[k])/sigmas[k])**2) / (sigmas[k]*np.sqrt(2*np.pi))
    gamma = pdf / pdf.sum(axis=1, keepdims=True)
    return gamma
```
:::

:::details CQ2: M-stepæ›´æ–°ï¼ˆå¤šå¤‰é‡ï¼‰ã‚’NumPyã§æ›¸ã‘
```python
# Î¼_k = (1/N_k) Î£_n Î³_nk x_n
# Î£_k = (1/N_k) Î£_n Î³_nk (x_n - Î¼_k)(x_n - Î¼_k)^T
def m_step_multivariate(X, gamma):
    N, D = X.shape
    K = gamma.shape[1]
    N_k = gamma.sum(axis=0)
    mus = np.zeros((K, D))
    covs = [np.zeros((D, D)) for _ in range(K)]
    for k in range(K):
        mus[k] = (gamma[:, k:k+1] * X).sum(axis=0) / N_k[k]
        diff = X - mus[k]
        covs[k] = (gamma[:, k:k+1] * diff).T @ diff / N_k[k]
    pis = N_k / N
    return mus, covs, pis
```
:::

:::details CQ3: BICè¨ˆç®—ã‚’NumPyã§æ›¸ã‘
```python
# BIC = -2 log p(x|Î¸Ì‚) + d log N
def compute_bic(log_likelihood, n_params, n_data):
    return -2 * log_likelihood + n_params * np.log(n_data)

# For 1D GMM with K components: d = 3K - 1
# (K means + K variances + K-1 free mixing weights)
```
:::

:::details CQ4: log-sum-exp ã‚’å®Ÿè£…ã›ã‚ˆ
```python
def log_sum_exp(a):
    """log Î£_k exp(a_k) = max(a) + log Î£_k exp(a_k - max(a))"""
    a_max = np.max(a, axis=-1, keepdims=True)
    return a_max.squeeze(-1) + np.log(np.sum(np.exp(a - a_max), axis=-1))
```
:::

:::details CQ5: å®Œå…¨ãª1D GMM EMã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’50è¡Œä»¥å†…ã§æ›¸ã‘
```python
import numpy as np

def gmm_em_1d(x, K, n_iter=50, seed=42):
    rng = np.random.RandomState(seed)
    N = len(x)
    # Init
    mu = np.sort(rng.choice(x, K, replace=False).astype(float))
    sigma = np.full(K, x.std())
    pi = np.full(K, 1.0 / K)

    for _ in range(n_iter):
        # E-step: Î³_nk = Ï€_k N(x_n|Î¼_k,Ïƒ_k) / Î£_j Ï€_j N(x_n|Î¼_j,Ïƒ_j)
        pdf = np.column_stack([
            pi[k] * np.exp(-0.5*((x-mu[k])/sigma[k])**2) / (sigma[k]*np.sqrt(2*np.pi))
            for k in range(K)])
        gamma = pdf / (pdf.sum(axis=1, keepdims=True) + 1e-300)

        # M-step
        Nk = gamma.sum(axis=0)
        for k in range(K):
            mu[k] = (gamma[:,k] * x).sum() / Nk[k]
            sigma[k] = np.sqrt((gamma[:,k] * (x-mu[k])**2).sum() / Nk[k]) + 1e-6
        pi = Nk / N

    ll = np.sum(np.log(pdf.sum(axis=1) + 1e-300))
    return mu, sigma, pi, ll
```
:::

### 5.4 è«–æ–‡èª­è§£ãƒ†ã‚¹ãƒˆ â€” Dempster, Laird, Rubin (1977) Pass 1

:::details è«–æ–‡ Pass 1 ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’åŸ‹ã‚ã‚ˆ
```python
paper_pass1 = {
    "title": "Maximum Likelihood from Incomplete Data via the EM Algorithm",
    "authors": "A.P. Dempster, N.M. Laird, D.B. Rubin",
    "year": 1977,
    "venue": "Journal of the Royal Statistical Society, Series B",
    "category": "Theory / Algorithm",

    # What problem does it solve?
    "problem": "MLE when data has missing/latent components (incomplete data)",

    # What is the key idea?
    "key_idea": "Alternate between E-step (compute expected sufficient statistics "
                "using current parameters) and M-step (maximize expected "
                "complete-data log-likelihood)",

    # What is the main result?
    "main_result": "Log-likelihood is monotonically non-decreasing under EM iterations. "
                   "Convergence to stationary point guaranteed under mild conditions.",

    # What experiments/examples?
    "examples": "GMM, Factor Analysis, variance components, missing data, "
                "grouped/censored data",

    # What are the limitations?
    "limitations": "Only local convergence guaranteed. Linear convergence rate. "
                   "Convergence speed depends on fraction of missing information.",

    # Relevance to this lecture?
    "relevance": "Foundational paper. All EM-based methods (VAE, HMM, Factor Analysis) "
                 "trace back to this formulation.",
}
```
:::

### 5.5 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸

**ãƒãƒ£ãƒ¬ãƒ³ã‚¸ 1: EMã®åæŸå¯è¦–åŒ–**

```python
import numpy as np

def em_convergence_study(n_restarts=5, K=3, N=300, max_iter=50):
    """Run EM from multiple random initializations and track convergence."""
    np.random.seed(42)
    x = np.concatenate([np.random.normal(-3, 0.8, 100),
                         np.random.normal(1, 1.0, 100),
                         np.random.normal(5, 0.6, 100)])

    results = []
    for restart in range(n_restarts):
        mu = np.random.uniform(x.min(), x.max(), K)
        sigma = np.ones(K) * x.std()
        pi_k = np.ones(K) / K
        lls = []

        for t in range(max_iter):
            pdf = np.zeros((N, K))
            for k in range(K):
                pdf[:, k] = pi_k[k] * np.exp(-0.5*((x-mu[k])/sigma[k])**2)/(sigma[k]*np.sqrt(2*np.pi))
            total = pdf.sum(axis=1)
            lls.append(np.sum(np.log(total + 1e-300)))
            gamma = pdf / (total[:, np.newaxis] + 1e-300)

            N_k = gamma.sum(axis=0)
            for k in range(K):
                mu[k] = (gamma[:, k] * x).sum() / (N_k[k] + 1e-300)
                sigma[k] = np.sqrt((gamma[:, k] * (x - mu[k])**2).sum() / (N_k[k] + 1e-300)) + 1e-6
            pi_k = N_k / N

        results.append({'final_ll': lls[-1], 'lls': lls, 'mu': mu.copy()})

    print("=== EM Convergence Study ===")
    print(f"{'Restart':>7} | {'Final log-lik':>14} | {'Converged Î¼':>30}")
    print("-" * 60)
    for i, r in enumerate(results):
        mu_str = ", ".join(f"{m:.2f}" for m in sorted(r['mu']))
        best = " â† best" if r['final_ll'] == max(rr['final_ll'] for rr in results) else ""
        print(f"{i:7d} | {r['final_ll']:14.4f} | ({mu_str}){best}")

    best_idx = np.argmax([r['final_ll'] for r in results])
    print(f"\nBest restart: {best_idx} (log-lik = {results[best_idx]['final_ll']:.4f})")
    return results

results = em_convergence_study()
```

**ãƒãƒ£ãƒ¬ãƒ³ã‚¸ 2: Missing Data Imputation via EM**

```python
import numpy as np

def em_missing_data(X_obs, mask, max_iter=30):
    """EM for missing data imputation (single Gaussian model).

    X_obs: (N, D) data with missing values set to 0
    mask: (N, D) boolean, True = observed, False = missing
    """
    N, D = X_obs.shape

    # Init: mean and covariance from observed entries
    mu = np.zeros(D)
    for d in range(D):
        obs_d = X_obs[mask[:, d], d]
        mu[d] = obs_d.mean() if len(obs_d) > 0 else 0.0
    cov = np.eye(D)

    for t in range(max_iter):
        # E-step: impute missing values using conditional distribution
        X_filled = X_obs.copy()
        for n in range(N):
            obs_idx = np.where(mask[n])[0]
            mis_idx = np.where(~mask[n])[0]
            if len(mis_idx) == 0:
                continue
            if len(obs_idx) == 0:
                X_filled[n, mis_idx] = mu[mis_idx]
                continue

            # Conditional: p(x_mis | x_obs) = N(Î¼_cond, Î£_cond)
            cov_oo = cov[np.ix_(obs_idx, obs_idx)]
            cov_mo = cov[np.ix_(mis_idx, obs_idx)]
            cov_oo_inv = np.linalg.inv(cov_oo + 1e-6 * np.eye(len(obs_idx)))
            mu_cond = mu[mis_idx] + cov_mo @ cov_oo_inv @ (X_obs[n, obs_idx] - mu[obs_idx])
            X_filled[n, mis_idx] = mu_cond

        # M-step: update Î¼ and Î£ from filled data
        mu = X_filled.mean(axis=0)
        diff = X_filled - mu
        cov = diff.T @ diff / N

    return X_filled, mu, cov

# Test with 20% missing data
np.random.seed(42)
N, D = 200, 3
true_mu = np.array([1.0, -2.0, 3.0])
true_cov = np.array([[1.0, 0.5, 0.2], [0.5, 2.0, -0.3], [0.2, -0.3, 1.5]])
X_true = np.random.multivariate_normal(true_mu, true_cov, N)

# Create missing data (MCAR - Missing Completely At Random)
mask = np.random.random((N, D)) > 0.2  # 20% missing
X_obs = X_true * mask  # zero out missing entries

X_filled, est_mu, est_cov = em_missing_data(X_obs, mask)

print("=== Missing Data Imputation via EM ===")
print(f"Missing rate: {(~mask).mean():.1%}")
print(f"\nTrue Î¼:      {true_mu}")
print(f"Estimated Î¼: {est_mu.round(3)}")
print(f"\nMSE (imputed vs true): {np.mean((X_filled[~mask] - X_true[~mask])**2):.4f}")
print(f"MSE (naive zero fill):  {np.mean((0 - X_true[~mask])**2):.4f}")
```

### 5.6 ã‚»ãƒ«ãƒ•ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«ã®åŒæ™‚åˆ†å¸ƒ $p(\mathbf{x}, \mathbf{z} \mid \theta)$ ã‚’æ›¸ãä¸‹ã›ã‚‹
- [ ] å‘¨è¾ºåŒ– $p(\mathbf{x} \mid \theta) = \sum_{\mathbf{z}} p(\mathbf{x}, \mathbf{z} \mid \theta)$ ã®æ„å‘³ãŒã‚ã‹ã‚‹
- [ ] ã€Œ$\log \sum$ ãŒè§£æè§£ã‚’é˜»ã‚€ã€ç†ç”±ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Jensenä¸ç­‰å¼ã‚’å‡¹é–¢æ•°/å‡¸é–¢æ•°ä¸¡æ–¹ã§æ›¸ã‘ã‚‹
- [ ] ELBOåˆ†è§£ã‚’å°å‡ºã§ãã‚‹ï¼ˆ2é€šã‚Š: ãƒ™ã‚¤ã‚ºåˆ†è§£ / Jensenä¸ç­‰å¼ï¼‰
- [ ] E-stepãŒã€ŒKL = 0ã«ã™ã‚‹ã€æ“ä½œã§ã‚ã‚‹ã“ã¨ã‚’èª¬æ˜ã§ãã‚‹
- [ ] M-stepãŒã€ŒELBOã‚’æœ€å¤§åŒ–ã™ã‚‹ã€æ“ä½œã§ã‚ã‚‹ã“ã¨ã‚’èª¬æ˜ã§ãã‚‹
- [ ] GMMã®è²¬ä»»åº¦ $\gamma(z_{nk})$ ã‚’å°å‡ºã§ãã‚‹
- [ ] GMMã® $\mu_k$, $\sigma_k^2$, $\pi_k$ ã®æ›´æ–°å¼ã‚’å°å‡ºã§ãã‚‹
- [ ] EMç®—æ³•ã®å˜èª¿æ€§ã‚’è¨¼æ˜ã§ãã‚‹
- [ ] log-sum-expãƒˆãƒªãƒƒã‚¯ã‚’å®Ÿè£…ã§ãã‚‹
- [ ] BIC/AICã®ä½¿ã„æ–¹ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Singularityå•é¡Œã¨å¯¾ç­–ã‚’èª¬æ˜ã§ãã‚‹

:::message
**é€²æ—: 85% å®Œäº†** å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ã‚¯ãƒªã‚¢ã€‚è¨˜å·èª­è§£ã€LaTeXè¨˜è¿°ã€ã‚³ãƒ¼ãƒ‰ç¿»è¨³ã€è«–æ–‡èª­è§£ã®å…¨æ–¹ä½ãƒ†ã‚¹ãƒˆã‚’å®Œäº†ã—ãŸã€‚
:::

---

## ğŸš€ 6. æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨æ¬¡å›äºˆå‘Š

### 6.1 Mixture of Experts (MoE) â€” Transformeræ™‚ä»£ã®å¾©æ´»

Jacobs, Jordan, Nowlan, Hinton (1991) [^7] ãŒææ¡ˆã—ãŸMixture of Experts (MoE) ã¯ã€EMçš„ãªæ§‹é€ ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ã ã€‚

$$
p(y \mid x) = \sum_{k=1}^{K} \underbrace{g_k(x)}_{\text{ã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°}} \cdot \underbrace{f_k(x; \theta_k)}_{\text{å°‚é–€å®¶}}
$$

ã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•° $g_k(x)$ ã¯ Softmax ã§å®Ÿè£…ã•ã‚Œã‚‹ã€‚å„å°‚é–€å®¶ $f_k$ ã¯å…¥åŠ›ç©ºé–“ã®ä¸€éƒ¨ã‚’æ‹…å½“ã™ã‚‹ã€‚

ã“ã®æ§‹é€ ã¯Transformerã®MoEå±¤ã¨ã—ã¦å¾©æ´»ã—ã¦ã„ã‚‹ã€‚GPT-4ã‚„Mixtral 8x7Bã¯ã€ã“ã®MoEã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ä½¿ã£ã¦è¨ˆç®—åŠ¹ç‡ã‚’åŠ‡çš„ã«æ”¹å–„ã—ãŸã€‚è©³ç´°ã¯ç¬¬16å›ï¼ˆTransformerå®Œå…¨ç‰ˆï¼‰ã§æ‰±ã†ã€‚

### 6.2 Expectation Propagation â€” EMã®ä»£æ›¿

Minka (2001) [^12] ãŒææ¡ˆã—ãŸ **Expectation Propagation** (EP) ã¯ã€EMç®—æ³•ã®ä»£æ›¿ã¨ãªã‚‹è¿‘ä¼¼æ¨è«–æ‰‹æ³•ã ã€‚

EMãŒäº‹å¾Œåˆ†å¸ƒå…¨ä½“ã‚’è¨ˆç®—ã™ã‚‹ã®ã«å¯¾ã—ã€EPã¯äº‹å¾Œåˆ†å¸ƒã® **ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆ**ï¼ˆå¹³å‡ã¨åˆ†æ•£ï¼‰ã ã‘ã‚’ä¿æŒã—ã€åå¾©çš„ã«æ›´æ–°ã™ã‚‹ã€‚

| æ‰‹æ³• | è¿‘ä¼¼åˆ†å¸ƒ | KLæ–¹å‘ | ç‰¹å¾´ |
|:-----|:---------|:-------|:-----|
| Variational EM | $q(\mathbf{z})$ ãŒ $p$ ã‚’è¿‘ä¼¼ | $\min \text{KL}[q \| p]$ | mode-seekingï¼ˆãƒ¢ãƒ¼ãƒ‰è¿½è·¡ï¼‰ |
| EP | å„å› å­ã‚’å€‹åˆ¥ã«è¿‘ä¼¼ | $\min \text{KL}[p \| q]$ | moment-matchingï¼ˆãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆä¸€è‡´ï¼‰ |

KLã®æ–¹å‘ãŒé€†ã§ã‚ã‚‹ã“ã¨ã«æ³¨ç›®ã—ã¦ã»ã—ã„ã€‚$\text{KL}[q \| p]$ ã®æœ€å°åŒ–ã¯ $q$ ãŒ $p$ ã®ãƒ¢ãƒ¼ãƒ‰ã®1ã¤ã«é›†ä¸­ã™ã‚‹å‚¾å‘ãŒã‚ã‚‹ãŒã€$\text{KL}[p \| q]$ ã®æœ€å°åŒ–ã¯ $q$ ãŒ $p$ ã®å…¨ã¦ã®ãƒ¢ãƒ¼ãƒ‰ã‚’ã‚«ãƒãƒ¼ã—ã‚ˆã†ã¨ã™ã‚‹ã€‚

```python
import numpy as np

# Conceptual comparison: EM vs EP
# EM minimizes KL[q||p] â†’ mode-seeking
# EP minimizes KL[p||q] â†’ moment-matching

# Bimodal target distribution (mixture of 2 Gaussians)
def bimodal_pdf(x, mu1=-2, mu2=3, sigma=0.8):
    return 0.5 * np.exp(-0.5*((x-mu1)/sigma)**2)/(sigma*np.sqrt(2*np.pi)) + \
           0.5 * np.exp(-0.5*((x-mu2)/sigma)**2)/(sigma*np.sqrt(2*np.pi))

# KL[q||p] minimization â†’ q picks ONE mode
# Best Gaussian approximation (mode-seeking):
x_grid = np.linspace(-6, 8, 10000)
p = bimodal_pdf(x_grid)

# Mode-seeking: q centers on higher mode
q_mode_seeking_mu = -2.0  # or 3.0 â€” picks one mode
q_mode_seeking_sigma = 0.8

# KL[p||q] minimization â†’ q covers BOTH modes
# Moment-matching: mean and variance of p
p_normalized = p / (p.sum() * (x_grid[1] - x_grid[0]))
ep_mu = np.sum(x_grid * p_normalized * (x_grid[1] - x_grid[0]))
ep_var = np.sum((x_grid - ep_mu)**2 * p_normalized * (x_grid[1] - x_grid[0]))

print("=== EM vs EP approximation of bimodal distribution ===")
print(f"Target: mixture of N(-2, 0.8Â²) and N(3, 0.8Â²)")
print(f"\nEM (mode-seeking):     Î¼ = {q_mode_seeking_mu:.1f}, Ïƒ = {q_mode_seeking_sigma:.1f}")
print(f"                       â†’ concentrates on ONE mode")
print(f"\nEP (moment-matching):  Î¼ = {ep_mu:.2f}, Ïƒ = {np.sqrt(ep_var):.2f}")
print(f"                       â†’ covers BOTH modes (broader Gaussian)")
print(f"\nNeither is 'correct' â€” they make different tradeoffs.")
print(f"EM/VI is standard for VAE. EP is useful for Bayesian inference.")
```

EPã®è©³ç´°ã¯æœ¬ã‚·ãƒªãƒ¼ã‚ºã®ã‚¹ã‚³ãƒ¼ãƒ—å¤–ã ãŒã€EMçš„ãªåå¾©æ¨è«–ã®ã€Œåˆ¥ã®å‘³ã€ã¨ã—ã¦çŸ¥ã£ã¦ãŠãã¨ã€è¿‘ä¼¼æ¨è«–ã®å…¨ä½“åƒãŒè¦‹ãˆã‚„ã™ããªã‚‹ã€‚

### 6.3 æœ€æ–°ç ”ç©¶å‹•å‘ (2024-2026)

EMç®—æ³•ã¨æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«ã¯ã€ç¾åœ¨ã‚‚æ´»ç™ºã«ç ”ç©¶ã•ã‚Œã¦ã„ã‚‹ã€‚

| ç ”ç©¶ãƒ†ãƒ¼ãƒ | æ¦‚è¦ | EMã¨ã®é–¢ä¿‚ |
|:---------|:-----|:---------|
| Latent Thoughts EM | LLMã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’æ½œåœ¨å¤‰æ•°ã¨ã—ã¦æ‰±ã„ã€EMçš„ã«è¨“ç·´ | EMåŸç†ã®LLMè¨“ç·´ã¸ã®é©ç”¨ |
| MoLAE (Mixture of Latent Experts) | ä½æ¬¡å…ƒæ½œåœ¨ç©ºé–“ã¸ã®å…±æœ‰å°„å½±ã§MoEåŠ¹ç‡åŒ– | MoE + æ½œåœ¨å¤‰æ•°ã®çµ±åˆ |
| Amortized EM | å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã®EMé«˜é€ŸåŒ–ï¼ˆæ¨è«–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ©ç”¨ï¼‰ | EMã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£æ”¹å–„ |
| Neural EM | E-stepã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã§ç½®æ› | EMæ§‹é€ ã®æ·±å±¤å­¦ç¿’åŒ– |

**EMç®—æ³•ã¯ã€Œå¤ã„ã€æ‰‹æ³•ã§ã¯ãªãã€å½¢ã‚’å¤‰ãˆã¦æœ€å…ˆç«¯ã«ç”Ÿãç¶šã‘ã¦ã„ã‚‹ã€‚** ç¬¬9å›ä»¥é™ã§ãã®ç¾ä»£çš„ãªå§¿ã‚’è©³ã—ãè¦‹ã¦ã„ãã€‚

### 6.4 æ¨è–¦æ›¸ç±ãƒ»ãƒªã‚½ãƒ¼ã‚¹

| æ›¸ç± | è‘—è€… | EMé–¢é€£ç«  | ãƒ¬ãƒ™ãƒ« |
|:-----|:-----|:--------|:------|
| *Pattern Recognition and Machine Learning* | Bishop (2006) [^11] | Ch. 9 Mixture Models and EM | â˜…â˜…â˜…â˜…â˜† |
| *Machine Learning: A Probabilistic Perspective* | Murphy (2012) | Ch. 11 Mixture Models and EM | â˜…â˜…â˜…â˜…â˜† |
| *Probabilistic Graphical Models* | Koller & Friedman (2009) | Ch. 19 Learning with Incomplete Data | â˜…â˜…â˜…â˜…â˜… |
| *Information Theory, Inference, and Learning* | MacKay (2003) | Ch. 22 EM Algorithm | â˜…â˜…â˜…â˜†â˜† |

| ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹ | URL | ç‰¹å¾´ |
|:-----------------|:----|:-----|
| Bishop PRML Ch.9 | å…¬å¼PDFç„¡æ–™å…¬é–‹ | GMMã¨EMã®æ•™ç§‘æ›¸çš„è§£èª¬ |
| Stanford CS229 EM | YouTube | Andrew Ng ã®ç›´æ„Ÿçš„ãªè¬›ç¾© |
| Lil'Log EM Algorithm | lilianweng.github.io | ç†è«–ã¨å®Ÿè£…ã®ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ã„ |

:::details ç”¨èªé›†
| ç”¨èª | è‹±èª | å®šç¾© |
|:-----|:-----|:-----|
| æ½œåœ¨å¤‰æ•° | latent variable | ç›´æ¥è¦³æ¸¬ã§ããªã„ç¢ºç‡å¤‰æ•° |
| å‘¨è¾ºå°¤åº¦ | marginal likelihood / evidence | æ½œåœ¨å¤‰æ•°ã‚’å‘¨è¾ºåŒ–ã—ãŸå°¤åº¦ |
| è²¬ä»»åº¦ | responsibility | ãƒ‡ãƒ¼ã‚¿ç‚¹ãŒå„æˆåˆ†ã‹ã‚‰ç”Ÿæˆã•ã‚ŒãŸäº‹å¾Œç¢ºç‡ |
| Qé–¢æ•° | Q-function | å®Œå…¨ãƒ‡ãƒ¼ã‚¿å¯¾æ•°å°¤åº¦ã®äº‹å¾ŒæœŸå¾…å€¤ |
| ELBO | Evidence Lower Bound | å¯¾æ•°å‘¨è¾ºå°¤åº¦ã®ä¸‹ç•Œ |
| ã‚¬ã‚¦ã‚¹æ··åˆãƒ¢ãƒ‡ãƒ« | Gaussian Mixture Model (GMM) | ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®é‡ã¿ä»˜ãå’Œã§è¡¨ã™å¯†åº¦ãƒ¢ãƒ‡ãƒ« |
| æŒ‡ç¤ºé–¢æ•° | indicator function | æ¡ä»¶ãŒçœŸãªã‚‰1ã€å½ãªã‚‰0ã‚’è¿”ã™é–¢æ•° |
| ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ | Mahalanobis distance | å…±åˆ†æ•£ã‚’è€ƒæ…®ã—ãŸè·é›¢ |
| Singularityå•é¡Œ | singularity problem | åˆ†æ•£â†’0ã§å°¤åº¦â†’âˆã«ç™ºæ•£ã™ã‚‹å•é¡Œ |
| å±€æ‰€æœ€é©è§£ | local optimum | è¿‘å‚ã§ã¯æœ€é©ã ãŒå¤§åŸŸçš„ã«ã¯æœ€é©ã§ãªã„è§£ |
| å˜èª¿åæŸ | monotone convergence | å„åå¾©ã§ç›®çš„é–¢æ•°ãŒéæ¸›å°‘ã§ã‚ã‚‹ã“ã¨ |
| Forward-Backward | forward-backward algorithm | HMMã®åŠ¹ç‡çš„ãªäº‹å¾Œç¢ºç‡è¨ˆç®—æ³• |
| Baum-Welch | Baum-Welch algorithm | HMMã«å¯¾ã™ã‚‹EMç®—æ³• |
| å¤‰åˆ†æ¨è«– | variational inference | äº‹å¾Œåˆ†å¸ƒã‚’æœ€é©åŒ–å•é¡Œã¨ã—ã¦è¿‘ä¼¼ã™ã‚‹æ‰‹æ³• |
| Amortizedæ¨è«– | amortized inference | æ¨è«–ã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã§æ±åŒ–ã™ã‚‹æ‰‹æ³• |
:::

```mermaid
mindmap
  root((ç¬¬8å›<br/>æ½œåœ¨å¤‰æ•° & EM))
    æ½œåœ¨å¤‰æ•°
      è¦³æ¸¬å¤‰æ•° x
      éš ã‚Œå¤‰æ•° z
      å‘¨è¾ºåŒ– p(x) = Î£_z p(x,z)
      äº‹å¾Œåˆ†å¸ƒ p(z|x)
    EMç®—æ³•
      Jensenä¸ç­‰å¼
      ELBOåˆ†è§£
      E-step: q = p(z|x,Î¸)
      M-step: max Q(Î¸,Î¸_t)
      å˜èª¿åæŸ
    GMM
      è²¬ä»»åº¦ Î³
      Î¼æ›´æ–°
      Ïƒæ›´æ–°
      Ï€æ›´æ–°
      BIC/AIC
    æ‹¡å¼µ
      HMM / Baum-Welch
      Factor Analysis
      PPCA
      Variational EM
      MoE
    â†’ Course II
      å¤‰åˆ†æ¨è«–(ç¬¬9å›)
      VAE(ç¬¬10å›)
      Juliaç™»å ´(ç¬¬9å›)
```

> **Zone 6 ã¾ã¨ã‚**: EMç®—æ³•ã®ç ”ç©¶ç³»è­œï¼ˆHMMã€FAã€PPCAã€MoEï¼‰ã‚’ä¿¯ç°ã—ã€Variational EM â†’ VAE ã¸ã®æ©‹æ¸¡ã—ã‚’ç†è§£ã—ãŸã€‚ç¬¬8å›ã®çŸ¥è­˜ãŒç¬¬9å›ä»¥é™ã§ã©ã†æ´»ãã‚‹ã‹ãŒæ˜ç¢ºã«ãªã£ãŸã€‚

---

### 6.5 æœ¬è¬›ç¾©ã®3ã¤ã®æ ¸å¿ƒ

**1. æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«** â€” ãƒ‡ãƒ¼ã‚¿ã®è£ã«ã€Œè¦‹ãˆãªã„åŸå› ã€ã‚’ä»®å®šã—ã€$p(\mathbf{x} \mid \theta) = \sum_{\mathbf{z}} p(\mathbf{x}, \mathbf{z} \mid \theta)$ ã¨åˆ†è§£ã™ã‚‹ã€‚ã“ã®å‘¨è¾ºåŒ–ãŒè¨ˆç®—å›°é›£ã§ã‚ã‚‹ã“ã¨ãŒã€EMç®—æ³•ã‚’å¿…è¦ã¨ã™ã‚‹æ ¹æœ¬çš„ç†ç”±ã€‚

**2. EMç®—æ³•ã®æ§‹é€ ** â€” ELBOåˆ†è§£ $\log p(\mathbf{x} \mid \theta) = \mathcal{L}(q, \theta) + \text{KL}[q \| p(\mathbf{z} \mid \mathbf{x}, \theta)]$ ã«åŸºã¥ãã€‚E-stepã§KL=0ã«ã—ï¼ˆELBOã‚’å¼•ãä¸Šã’ï¼‰ã€M-stepã§ELBOã‚’æœ€å¤§åŒ–ã™ã‚‹ã€‚å¯¾æ•°å°¤åº¦ã®å˜èª¿å¢—åŠ ãŒä¿è¨¼ã•ã‚Œã‚‹ã€‚

**3. VAEã¸ã®æ©‹** â€” EMç®—æ³•ã®é™ç•Œï¼ˆè§£æçš„äº‹å¾Œåˆ†å¸ƒãŒå¿…è¦ï¼‰ã‚’ Variational EM ãŒç·©å’Œã—ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã«ã‚ˆã‚‹æ¨è«–ï¼ˆAmortized Inferenceï¼‰ãŒVAEã«ç¹‹ãŒã‚‹ã€‚

### 6.6 FAQ

:::details Q1: EMç®—æ³•ã¯å¿…ãšå¤§åŸŸæœ€é©è§£ã«åæŸã—ã¾ã™ã‹ï¼Ÿ
ã„ã„ãˆã€‚EMç®—æ³•ã¯**å±€æ‰€æœ€é©è§£**ï¼ˆæ­£ç¢ºã«ã¯ä¸å‹•ç‚¹ï¼‰ã¸ã®åæŸã—ã‹ä¿è¨¼ã—ã¾ã›ã‚“ã€‚å¤§åŸŸæœ€é©è§£ã¸ã®åˆ°é”ã¯ä¿è¨¼ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å®Ÿå‹™ã§ã¯è¤‡æ•°ã®ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸå€¤ã‹ã‚‰å®Ÿè¡Œã—ï¼ˆmultiple restartsï¼‰ã€æœ€ã‚‚é«˜ã„å¯¾æ•°å°¤åº¦ã‚’é”æˆã—ãŸçµæœã‚’æ¡ç”¨ã™ã‚‹ã®ãŒæ¨™æº–çš„ã§ã™ã€‚
:::

:::details Q2: K-meansã¨GMM-EMã®é–¢ä¿‚ã¯ï¼Ÿ
K-meansã¯GMM-EMã®**ç‰¹æ®Šã‚±ãƒ¼ã‚¹**ã§ã™ã€‚å…¨æˆåˆ†ã®åˆ†æ•£ãŒç­‰ã—ãï¼ˆ$\sigma_k^2 = \sigma^2$ï¼‰ã€$\sigma^2 \to 0$ ã®æ¥µé™ã‚’å–ã‚‹ã¨ã€soft assignmentï¼ˆè²¬ä»»åº¦ $\gamma_{nk} \in [0, 1]$ï¼‰ãŒhard assignmentï¼ˆ$\gamma_{nk} \in \{0, 1\}$ï¼‰ã«ãªã‚Šã€K-meansã®æ›´æ–°å¼ã¨ä¸€è‡´ã—ã¾ã™ã€‚
:::

:::details Q3: EMç®—æ³•ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®å­¦ç¿’ã«ã‚‚ä½¿ãˆã¾ã™ã‹ï¼Ÿ
ç›´æ¥çš„ã«ã¯ä½¿ã„ã«ãã„ã§ã™ã€‚ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹M-stepã®é–‰å½¢å¼è§£ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã§ã™ã€‚ä»£ã‚ã‚Šã«Variational EMã®æ çµ„ã¿ã§ã€å‹¾é…é™ä¸‹æ³•ã«ã‚ˆã‚‹M-stepã‚’ä½¿ã„ã¾ã™ã€‚VAEï¼ˆç¬¬10å›ï¼‰ã¯ã¾ã•ã«ã“ã®æ§‹é€ ã§ã™ã€‚
:::

:::details Q4: GMMã®æˆåˆ†æ•°Kã¯ã©ã†æ±ºã‚ã‚Œã°ã„ã„ã§ã™ã‹ï¼Ÿ
BICï¼ˆãƒ™ã‚¤ã‚ºæƒ…å ±é‡åŸºæº–ï¼‰ãŒæœ€ã‚‚ä¸€èˆ¬çš„ãªé¸æŠåŸºæº–ã§ã™ã€‚$K = 1, 2, 3, \ldots$ ã§å„ã€…EMã‚’å®Ÿè¡Œã—ã€BICãŒæœ€å°ã®$K$ã‚’é¸ã³ã¾ã™ã€‚AICã¯BICã‚ˆã‚Šå¤§ãã„$K$ã‚’é¸ã¶å‚¾å‘ãŒã‚ã‚Šã€ãƒ‡ãƒ¼ã‚¿ãŒå¤§é‡ã«ã‚ã‚‹å ´åˆã¯BICã®æ–¹ãŒä¿å®ˆçš„ã§å®‰å…¨ã§ã™ã€‚ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ãƒ™ã‚¤ã‚ºï¼ˆDirichlet Process GMMï¼‰ã‚’ä½¿ãˆã°$K$è‡ªä½“ã‚‚æ¨å®šã§ãã¾ã™ãŒã€è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„ã§ã™ã€‚
:::

:::details Q5: EMãŒé…ã„ã®ã§ã™ãŒã€é«˜é€ŸåŒ–ã™ã‚‹æ–¹æ³•ã¯ï¼Ÿ
ã„ãã¤ã‹ã®æ–¹æ³•ãŒã‚ã‚Šã¾ã™:
1. **ãƒŸãƒ‹ãƒãƒƒãƒEM**: å…¨ãƒ‡ãƒ¼ã‚¿ã§ãªãã‚µãƒ–ã‚»ãƒƒãƒˆã§E-stepã‚’è¨ˆç®—
2. **Incremental EM** (Neal & Hinton, 1998 [^5]): 1ãƒ‡ãƒ¼ã‚¿ç‚¹ãšã¤æ›´æ–°
3. **è¨€èªã®å¤‰æ›´**: Python â†’ Julia ã§50å€é€Ÿï¼ˆç¬¬9å›ã§å®Ÿæ¼”ï¼‰
4. **scikit-learnã®åˆ©ç”¨**: æœ€é©åŒ–ã•ã‚ŒãŸCå®Ÿè£…ã‚’å†…éƒ¨ã§ä½¿ç”¨
5. **GPUã®æ´»ç”¨**: E-stepã®è¡Œåˆ—æ¼”ç®—ã‚’GPUã«è¼‰ã›ã‚‹
:::

:::details Q6: EMç®—æ³•ã¨å‹¾é…é™ä¸‹æ³•ã®é•ã„ã¯ä½•ã§ã™ã‹ï¼Ÿ
EMç®—æ³•ã¯åº§æ¨™ä¸Šæ˜‡æ³•ï¼ˆcoordinate ascentï¼‰ã®ä¸€ç¨®ã§ã€$q$ã¨$\theta$ã‚’äº¤äº’ã«æœ€é©åŒ–ã—ã¾ã™ã€‚å‹¾é…é™ä¸‹æ³•ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’ç›´æ¥æ¢ç´¢ã—ã¾ã™ã€‚EMã®åˆ©ç‚¹ã¯å„ã‚¹ãƒ†ãƒƒãƒ—ã§è§£æè§£ãŒä½¿ãˆã‚‹ã“ã¨ï¼ˆGMMãªã©ï¼‰ã€‚æ¬ ç‚¹ã¯å¾®åˆ†å¯èƒ½ã§ãªã„ãƒ¢ãƒ‡ãƒ«ã«ã¯é©ç”¨ã—ã«ãã„ã“ã¨ã€‚å®Ÿã¯å‹¾é…é™ä¸‹æ³•ã§ELBOã‚’ç›´æ¥æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ã‚‚ã§ãã€ãã‚ŒãŒVAEã®å­¦ç¿’ã«ç¹‹ãŒã‚Šã¾ã™ã€‚
:::

:::details Q7: æœ¬è¬›ç¾©ã®æ•°å¼ãŒé›£ã—ã™ãã¾ã™ã€‚ã©ã“ã‹ã‚‰å¾©ç¿’ã™ã¹ãã§ã™ã‹ï¼Ÿ
ä»¥ä¸‹ã®é †åºã§å¾©ç¿’ã—ã¦ãã ã•ã„:
1. ç¬¬4å›ï¼ˆç¢ºç‡è«–ï¼‰: ãƒ™ã‚¤ã‚ºã®å®šç†ã€æ¡ä»¶ä»˜ãç¢ºç‡
2. ç¬¬5å›ï¼ˆæ¸¬åº¦è«–ï¼‰: æœŸå¾…å€¤ã®å®šç¾©ã€Jensenä¸ç­‰å¼
3. ç¬¬6å›ï¼ˆæƒ…å ±ç†è«–ï¼‰: KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹
4. ç¬¬7å›ï¼ˆæœ€å°¤æ¨å®šã¨çµ±è¨ˆçš„æ¨è«–ï¼‰: æœ€å°¤æ¨å®šã€å¯¾æ•°å°¤åº¦

ç‰¹ã«ç¬¬6å›ã®KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ãŒé‡è¦ã§ã™ã€‚ELBOåˆ†è§£ã®å°å‡ºã§ä¸å¯æ¬ ã«ãªã‚Šã¾ã™ã€‚
:::

:::details Q8: EMã¨MCMCã®é•ã„ã¯ï¼Ÿ
EMã¯**æœ€é©åŒ–æ‰‹æ³•**ï¼ˆæœ€å°¤æ¨å®šå€¤ã‚’æ±‚ã‚ã‚‹ï¼‰ã§ã‚ã‚Šã€MCMCã¯**ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ‰‹æ³•**ï¼ˆäº‹å¾Œåˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ï¼‰ã§ã™ã€‚EMã¯ç‚¹æ¨å®šï¼ˆ$\hat{\theta}_{\text{MLE}}$ï¼‰ã‚’è¿”ã—ã€MCMCã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®äº‹å¾Œåˆ†å¸ƒå…¨ä½“ã‚’è¿‘ä¼¼ã—ã¾ã™ã€‚

EMã®åˆ©ç‚¹: é«˜é€Ÿã€åæŸåˆ¤å®šãŒå®¹æ˜“ã€æ±ºå®šè«–çš„
MCMCã®åˆ©ç‚¹: äº‹å¾Œåˆ†å¸ƒã®ä¸ç¢ºå®Ÿæ€§ã‚’å®Œå…¨ã«è¡¨ç¾ã€å¤§åŸŸæœ€é©è§£ã«è¿‘ã¥ãã‚„ã™ã„
é¸æŠåŸºæº–: ç‚¹æ¨å®šã§ååˆ†ãªã‚‰EMã€ä¸ç¢ºå®Ÿæ€§ã®å®šé‡åŒ–ãŒå¿…è¦ãªã‚‰MCMC
:::

:::details Q9: æ·±å±¤å­¦ç¿’æ™‚ä»£ã«EMã‚’å­¦ã¶æ„å‘³ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ
ã‚ã‚Šã¾ã™ã€‚3ã¤ã®ç†ç”±ã‹ã‚‰:

1. **VAEã®æå¤±é–¢æ•°ã¯ELBO** â€” EMã®æ ¸å¿ƒæ¦‚å¿µãã®ã‚‚ã®ã§ã™ã€‚EMç„¡ã—ã«VAEã®æ•°å¼ã¯ç†è§£ã§ãã¾ã›ã‚“ã€‚
2. **Diffusion Modelsã®å­¦ç¿’ã‚‚ELBOãƒ™ãƒ¼ã‚¹** â€” DDPMã®æå¤±é–¢æ•°ã¯å„ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã§ã®å¤‰åˆ†ä¸‹ç•Œã§ã™ã€‚
3. **EMçš„ãªæ€è€ƒæ³•ã¯æ±ç”¨çš„** â€” ã€Œè¦³æ¸¬ã§ããªã„å¤‰æ•°ã‚’ä»®å®šã—ã€æœŸå¾…å€¤ã‚’å–ã£ã¦æœ€é©åŒ–ã™ã‚‹ã€ã¨ã„ã†ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ã€æ·±å±¤å­¦ç¿’ã®è‡³ã‚‹ã¨ã“ã‚ã«ç¾ã‚Œã¾ã™ã€‚

ã€ŒEMã‚’é£›ã°ã—ã¦VAEã«è¡Œãã€ã®ã¯ã€Œå¾®ç©åˆ†ã‚’é£›ã°ã—ã¦ç‰©ç†ã«è¡Œãã€ã®ã¨åŒã˜ã§ã™ã€‚å½¢å¼çš„ã«ã¯ã§ãã¾ã™ãŒã€æœ¬è³ªçš„ãªç†è§£ã«ã¯åˆ°é”ã—ã¾ã›ã‚“ã€‚
:::

### 6.7 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

| æ—¥ | å†…å®¹ | ç›®æ¨™ |
|:---|:-----|:-----|
| Day 1 | Zone 0-2 å†èª­ + Zone 3 å‰åŠï¼ˆ3.1-3.4ï¼‰ | ELBOåˆ†è§£ã‚’ç´™ã«æ›¸ã‘ã‚‹ |
| Day 2 | Zone 3 å¾ŒåŠï¼ˆ3.5-3.8ï¼‰ã‚’ç´™ã§å°å‡º | E/M-stepæ›´æ–°å¼ã‚’å°å‡ºã§ãã‚‹ |
| Day 3 | Zone 4 ã®ã‚³ãƒ¼ãƒ‰ã‚’å…¨ã¦æ‰‹ã§æ‰“ã£ã¦å®Ÿè¡Œ | æ•°å€¤å®‰å®šEMã‚’å®Ÿè£…ã§ãã‚‹ |
| Day 4 | Zone 5 ã®ãƒ†ã‚¹ãƒˆï¼ˆè¨˜å·ãƒ»LaTeXãƒ»ã‚³ãƒ¼ãƒ‰ï¼‰ | å…¨å•æ­£ç­” |
| Day 5 | Zone 6 ã®HMM/FA/PPCAæ¦‚å¿µæ•´ç† | æ‹¡å¼µã®ä½ç½®ã¥ã‘ã‚’ç†è§£ |
| Day 6 | Dempster+ (1977) [^1] Pass 1 èª­è§£ | åŸè«–æ–‡ã®æ§‹é€ ã‚’æŠŠæ¡ |
| Day 7 | è‡ªå‰GMMå®Ÿè£… + BICã§ãƒ¢ãƒ‡ãƒ«é¸æŠ | çµ±åˆæ¼”ç¿’ |

### 6.8 Progress Tracker

```python
# Self-assessment for Lecture 08
skills = {
    "Latent variable model formulation": None,  # True/False
    "Complete vs incomplete data log-lik": None,
    "Jensen's inequality (concave)": None,
    "ELBO decomposition (two derivations)": None,
    "E-step derivation (KL=0)": None,
    "M-step derivation (Q-function max)": None,
    "GMM responsibility (Bayes rule)": None,
    "GMM Î¼ update (weighted mean)": None,
    "GMM ÏƒÂ² update (weighted variance)": None,
    "GMM Ï€ update (Lagrange)": None,
    "Monotone convergence proof": None,
    "Log-sum-exp implementation": None,
    "BIC/AIC model selection": None,
    "Singularity problem & fix": None,
    "HMM Forward-Backward concept": None,
    "Variational EM â†’ VAE bridge": None,
}

# Fill in True/False and count
# completed = sum(1 for v in skills.values() if v is True)
# total = len(skills)
# print(f"Mastery: {completed}/{total} ({100*completed/total:.0f}%)")
# if completed >= 14: print("Ready for Lecture 09!")
# elif completed >= 10: print("Review weak areas, then proceed.")
# else: print("Re-read Zones 3-4 before proceeding.")
```

### 6.9 æ¬¡å›äºˆå‘Š â€” ç¬¬9å›: å¤‰åˆ†æ¨è«– & ELBO

ç¬¬8å›ã§è¦‹ã¤ã‘ãŸé™ç•Œã‚’æ€ã„å‡ºã—ã¦ã»ã—ã„ã€‚EMç®—æ³•ã¯ E-step ã§ $p(\mathbf{z} \mid \mathbf{x}, \theta)$ ã‚’**è§£æçš„ã«**è¨ˆç®—ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚GMMãªã‚‰å¯èƒ½ã ãŒã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ‡ã‚³ãƒ¼ãƒ€ã§ã¯ä¸å¯èƒ½ã ã€‚

ç¬¬9å›ã§ã¯:
- å¤‰åˆ†æ¨è«–ã®ä¸€èˆ¬ç†è«–ã‚’å­¦ã¶
- ELBOã®3é€šã‚Šã®å°å‡ºã‚’å®Œå…¨ã«ç†è§£ã™ã‚‹
- **Juliaåˆç™»å ´**: ELBOè¨ˆç®—ã§ Python 45ç§’ â†’ Julia 0.8ç§’ ã®è¡æ’ƒ

**ã‚ã®Pythonã®é…ã•ã€è¦šãˆã¦ã„ã¾ã™ã‹ï¼Ÿ** ç¬¬9å›ã§è§£æ±ºã—ã¾ã™ã€‚

:::message
**é€²æ—: 100% å®Œäº†** Course Iã€Œæ•°å­¦åŸºç¤ç·¨ã€å…¨8å›ã‚¯ãƒªã‚¢ã€‚æ•°å¼ãƒªãƒ†ãƒ©ã‚·ãƒ¼ï¼ˆç¬¬1å›ï¼‰ã‹ã‚‰å§‹ã¾ã‚Šã€ç·šå½¢ä»£æ•°ãƒ»ç¢ºç‡è«–ãƒ»æ¸¬åº¦è«–ãƒ»æƒ…å ±ç†è«–ãƒ»æœ€é©åŒ–ãƒ»ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãƒ»MLE ã‚’çµŒã¦ã€æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«ã¨EMç®—æ³•ã«åˆ°é”ã—ãŸã€‚Course IIã€Œç”Ÿæˆãƒ¢ãƒ‡ãƒ«åŸºç¤ç·¨ã€ã¸ã®æº–å‚™ã¯å®Œäº†ã ã€‚
:::

---

## ğŸ† Course I èª­äº† â€” æ•°å­¦åŸºç¤ç·¨ã‚³ãƒ³ãƒ—ãƒªãƒ¼ãƒˆ

> **8å›ã®æ—…ã‚’çµ‚ãˆãŸã‚ãªãŸã¯ã€ã‚‚ã†ã€Œæ•°å¼ãŒèª­ã‚ãªã„äººã€ã§ã¯ãªã„ã€‚**

ã“ã“ã¾ã§è¾¿ã‚Šç€ã„ãŸã€‚ç¬¬1å›ã§ Softmax ã®3è¡Œã‚³ãƒ¼ãƒ‰ã‚’å‰ã«ã€Œãˆã€æ•°å¼ã£ã¦ã‚³ãƒ¼ãƒ‰ã«ç›´ã›ã‚‹ã®ï¼Ÿã€ã¨ç›®ã‚’ä¸¸ãã—ãŸã‚ã®æ—¥ã‹ã‚‰ã€8å›åˆ†ã®æ•°å¼ä¿®è¡Œã‚’çµŒã¦ã€Jensenä¸ç­‰å¼ã‹ã‚‰ ELBO ã‚’å°å‡ºã—ã€EMç®—æ³•ã®Qé–¢æ•°ã‚’å¤šå¤‰é‡GMMã§å®Œå…¨å±•é–‹ã§ãã‚‹ã¨ã“ã‚ã¾ã§æ¥ãŸã€‚

å°‘ã—ç«‹ã¡æ­¢ã¾ã£ã¦ã€ã“ã®æ—…ã‚’æŒ¯ã‚Šè¿”ã‚ã†ã€‚

### 8å›ã®å†’é™ºã‚’æŒ¯ã‚Šè¿”ã‚‹

:::message
ğŸ“Š **Course I é€²æ—: 8/8 å®Œäº†ï¼ˆ100%ï¼‰**
æ•°å­¦åŸºç¤ç·¨ã®å…¨8å›ã‚’èµ°ç ´ã€‚å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã®æœ€åˆã®å±±è„ˆã‚’è¶ŠãˆãŸã€‚
:::

```mermaid
graph TD
    L1["ğŸ§­ ç¬¬1å›: æ¦‚è«–<br/>æ•°å¼ã¨è«–æ–‡ã®èª­ã¿æ–¹"]
    L2["ğŸ“ ç¬¬2å›: ç·šå½¢ä»£æ•° I<br/>ãƒ™ã‚¯ãƒˆãƒ«ãƒ»è¡Œåˆ—ãƒ»åŸºåº•"]
    L3["ğŸ”¬ ç¬¬3å›: ç·šå½¢ä»£æ•° II<br/>SVDãƒ»è¡Œåˆ—å¾®åˆ†ãƒ»ãƒ†ãƒ³ã‚½ãƒ«"]
    L4["ğŸ² ç¬¬4å›: ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦<br/>åˆ†å¸ƒãƒ»ãƒ™ã‚¤ã‚ºãƒ»MLE"]
    L5["ğŸ“ ç¬¬5å›: æ¸¬åº¦è«–ãƒ»ç¢ºç‡éç¨‹<br/>Lebesgueãƒ»ä¼Šè—¤ãƒ»SDE"]
    L6["ğŸ“¡ ç¬¬6å›: æƒ…å ±ç†è«–ãƒ»æœ€é©åŒ–<br/>KLãƒ»SGDãƒ»Adam"]
    L7["ğŸ—ºï¸ ç¬¬7å›: æœ€å°¤æ¨å®šã¨çµ±è¨ˆçš„æ¨è«–<br/>MLE = CE = KL"]
    L8["ğŸ” ç¬¬8å›: æ½œåœ¨å¤‰æ•° & EMç®—æ³•<br/>Jensenâ†’ELBOâ†’E/M-step"]

    L1 -->|"æ•°å¼ãŒèª­ã‚ãŸ"| L2
    L2 -->|"è¡Œåˆ—ã‚’æ‰±ãˆãŸ"| L3
    L3 -->|"å¾®åˆ†ã§ããŸ"| L4
    L4 -->|"ç¢ºç‡åˆ†å¸ƒãŒã‚ã‹ã£ãŸ"| L5
    L5 -->|"å³å¯†ãªç¢ºç‡"| L6
    L6 -->|"æ­¦å™¨ãŒæƒã£ãŸ"| L7
    L7 -->|"å°¤åº¦ãŒè¨ˆç®—å›°é›£"| L8

    L8 -->|"Course II ã¸"| CII["ğŸš€ ç¬¬9å›: å¤‰åˆ†æ¨è«– & ELBO<br/>âš¡ Julia åˆç™»å ´"]

    style L1 fill:#e8f5e9
    style L2 fill:#e8f5e9
    style L3 fill:#e8f5e9
    style L4 fill:#e8f5e9
    style L5 fill:#e8f5e9
    style L6 fill:#e8f5e9
    style L7 fill:#e8f5e9
    style L8 fill:#e8f5e9
    style CII fill:#fff3e0
```

å„å›ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆã‚’æŒ¯ã‚Šè¿”ã£ã¦ã¿ã‚ˆã†ã€‚

:::details ç¬¬1å›ã€œç¬¬8å› â€” å„å›ã®è©³ç´°æŒ¯ã‚Šè¿”ã‚Š

**ç¬¬1å›: æ¦‚è«– â€” æ•°å¼ã¨è«–æ–‡ã®èª­ã¿æ–¹** ğŸ§­

å†’é™ºã®å§‹ã¾ã‚Šã ã£ãŸã€‚ã€Œæ•°å¼ãŒ"èª­ã‚ãªã„"ã®ã¯æ‰èƒ½ã§ã¯ãªãèªå½™ã®å•é¡Œã€ â€” ã“ã®ä¸€æ–‡ã‹ã‚‰å…¨ã¦ãŒå§‹ã¾ã£ãŸã€‚ã‚®ãƒªã‚·ãƒ£æ–‡å­—50å€‹ã‚’è¦šãˆã€é›†åˆè«–ãƒ»è«–ç†è¨˜å·ãƒ»é–¢æ•°ã®è¨˜æ³•ã‚’èº«ã«ã¤ã‘ã€$\nabla_\theta \mathcal{L}$ ã‚’ã€ŒãƒŠãƒ–ãƒ© ã‚·ãƒ¼ã‚¿ ã‚¨ãƒ«ã€ã¨å£°ã«å‡ºã—ã¦èª­ã‚ã‚‹ã‚ˆã†ã«ãªã£ãŸã€‚Boss Battle ã§ã¯ Transformer ã® Scaled Dot-Product Attention å¼ $\text{Attention}(Q, K, V) = \text{softmax}(QK^\top / \sqrt{d_k})V$ ã‚’ä¸€æ–‡å­—æ®‹ã‚‰ãšèª­è§£ã—ãŸã€‚ã‚ã®æ™‚ã®é”æˆæ„Ÿã‚’è¦šãˆã¦ã„ã‚‹ã ã‚ã†ã‹ã€‚

**ç¬¬2å›: ç·šå½¢ä»£æ•° I â€” ãƒ™ã‚¯ãƒˆãƒ«ãƒ»è¡Œåˆ—ãƒ»åŸºåº•** ğŸ“

ã€ŒGPUã¯è¡Œåˆ—æ¼”ç®—ãƒã‚·ãƒ³ã ã€ã€‚ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã®8ã¤ã®å…¬ç†ã‹ã‚‰å§‹ã‚ã¦ã€å†…ç©ãƒ»ãƒãƒ«ãƒ ãƒ»ç›´äº¤æ€§ã‚’å®šç¾©ã—ã€å›ºæœ‰å€¤åˆ†è§£ãƒ»æ­£å®šå€¤è¡Œåˆ—ãƒ»å°„å½±ã¾ã§ä¸€æ°—ã«é§†ã‘æŠœã‘ãŸã€‚è¡Œåˆ—ç©ã®3ã¤ã®è¦‹æ–¹ï¼ˆè¦ç´ ã”ã¨ãƒ»åˆ—ã”ã¨ãƒ»è¡Œã”ã¨ï¼‰ã‚’å­¦ã³ã€Boss Battle ã§ã¯ Attention ã® $QK^\top$ ã‚’å†…ç©â†’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°â†’Softmaxâ†’åŠ é‡å¹³å‡ã¨ã—ã¦è¡Œåˆ—çš„ã«å®Œå…¨ç†è§£ã—ãŸã€‚ã€Œè¡Œåˆ—ç© = å†…ç©ã®ãƒãƒƒãƒå‡¦ç†ã€ â€” ã“ã®ä¸€è¨€ã§ GPU ã®å­˜åœ¨ç†ç”±ãŒè¦‹ãˆãŸã€‚

**ç¬¬3å›: ç·šå½¢ä»£æ•° II â€” SVDãƒ»è¡Œåˆ—å¾®åˆ†ãƒ»ãƒ†ãƒ³ã‚½ãƒ«** ğŸ”¬

ã€ŒSVDã¯ä¸‡èƒ½ãƒŠã‚¤ãƒ•ã ã€ã€‚ä»»æ„ã®è¡Œåˆ—ã‚’ $U\Sigma V^\top$ ã«åˆ†è§£ã—ã€Eckart-Youngå®šç†ã§ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ã®æœ€é©æ€§ã‚’è¨¼æ˜ã—ãŸã€‚ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã€ãƒ˜ã‚·ã‚¢ãƒ³ã€é€£é–å¾‹ã‚’å°å‡ºã—ã€Forward/Reverse Mode è‡ªå‹•å¾®åˆ†ã‚’æ‰‹å‹•å®Ÿè£…ã—ãŸã€‚Boss Battle ã¯ Transformer 1å±¤ã®å®Œå…¨å¾®åˆ† â€” Forward pass ã®å„ã‚¹ãƒ†ãƒƒãƒ—ã®å‹¾é…ã‚’è¿½è·¡ã—ã€Backpropagation ã®æ•°å­¦çš„åŸºç›¤ã‚’ç™½ç´™ã‹ã‚‰æ§‹ç¯‰ã—ãŸã€‚50è¡Œã®è‡ªå‹•å¾®åˆ†ã‚³ãƒ¼ãƒ‰ãŒ PyTorch ã® `backward()` ã®æœ¬è³ªã ã¨çŸ¥ã£ãŸã¨ãã®è¡æ’ƒã¯ã€å¿˜ã‚Œã‚‰ã‚Œãªã„ã¯ãšã ã€‚

**ç¬¬4å›: ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦** ğŸ²

ã€Œç¢ºç‡ã¨ã¯"ã‚ã‹ã‚‰ãªã•"ã®è¨€èªã ã€ã€‚Kolmogorov ã®å…¬ç†ç³» $(\Omega, \mathcal{F}, P)$ ã‹ã‚‰å‡ºç™ºã—ã€ãƒ™ã‚¤ã‚ºã®å®šç†ã€ä¸»è¦ãªç¢ºç‡åˆ†å¸ƒï¼ˆBernoulliâ†’Categoricalâ†’Gaussianâ†’æŒ‡æ•°å‹åˆ†å¸ƒæ—ï¼‰ã€MLEã€Fisheræƒ…å ±é‡ã€ä¸­å¿ƒæ¥µé™å®šç†ã¾ã§å®Œå…¨æ­¦è£…ã—ãŸã€‚äº‹å‰ç¢ºç‡1%ã®ç—…æ°—ã®é™½æ€§æ¤œæŸ»ãŒ16%ã«ã—ã‹ãªã‚‰ãªã„ãƒ™ã‚¤ã‚ºã®ç›´æ„Ÿå´©å£Šã€‚Boss Battle ã§ã¯è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã®å°¤åº¦ $\log p(\mathbf{x}) = \sum_t \log p(x_t \mid x_{<t})$ ã‚’å®Œå…¨åˆ†è§£ã—ã€LLM ã®ãƒ†ã‚­ã‚¹ãƒˆç”ŸæˆãŒæ¡ä»¶ä»˜ãç¢ºç‡ã®é€£é–ã«ä»–ãªã‚‰ãªã„ã“ã¨ã‚’è¨¼æ˜ã—ãŸã€‚

**ç¬¬5å›: æ¸¬åº¦è«–çš„ç¢ºç‡è«–ãƒ»ç¢ºç‡éç¨‹å…¥é–€** ğŸ“

Course I ã®æœ€é›£é–¢ã€‚ã€ŒLebesgueç©åˆ†ãªãã—ã¦ç¢ºç‡å¯†åº¦ãªã—ã€ã€‚Cantoré›†åˆï¼ˆéå¯ç®—ç„¡é™ãªã®ã«æ¸¬åº¦0ï¼‰ã§æ¸¬åº¦ã®å¿…è¦æ€§ã‚’ä½“æ„Ÿã—ã€$\sigma$-åŠ æ³•æ—ã€Lebesgueæ¸¬åº¦ã€Lebesgueç©åˆ†ã€åæŸå®šç†ï¼ˆMCT/DCT/Fatouï¼‰ã€Radon-Nikodymå°é–¢æ•°ã‚’é †ã«æ§‹ç¯‰ã—ãŸã€‚ç¢ºç‡å¤‰æ•°ã®5ã¤ã®åæŸæ¦‚å¿µã€ãƒãƒ«ãƒãƒ³ã‚²ãƒ¼ãƒ«ã€Markové€£é–ã€Browné‹å‹•ã€ãã—ã¦ä¼Šè—¤ç©åˆ†ã¨ä¼Šè—¤ã®è£œé¡Œã€‚Boss Battle ã§ã¯ DDPM ã® forward process $q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}(\sqrt{1-\beta_t}\mathbf{x}_{t-1}, \beta_t \mathbf{I})$ ã‚’æ¸¬åº¦è«–ã§å®Œå…¨è¨˜è¿°ã—ãŸã€‚ç¢ºç‡å¯†åº¦é–¢æ•°ã®ã€Œæ­£ä½“ã€ãŒ Radon-Nikodym å°é–¢æ•°ã ã¨çŸ¥ã£ãŸã¨ãã€ç¬¬4å›ã§æ£šä¸Šã’ã«ã—ãŸç–‘å•ãŒã™ã¹ã¦è§£æ¶ˆã•ã‚ŒãŸã¯ãšã ã€‚

**ç¬¬6å›: æƒ…å ±ç†è«–ãƒ»æœ€é©åŒ–ç†è«–** ğŸ“¡

ã€Œåˆ†å¸ƒã®"è·é›¢"ã‚’æ¸¬ã‚Šã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®"è°·"ã‚’ä¸‹ã‚‹ã€ã€‚Shannon ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‹ã‚‰å§‹ã‚ã¦ã€KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã€Cross-Entropyã€f-Divergenceçµ±ä¸€ç†è«–ï¼ˆFenchelå…±å½¹ã¾ã§ï¼‰ã€Jensenä¸ç­‰å¼ã¨å‡¸æ€§ã‚’è£…å‚™ã€‚æœ€é©åŒ–ã§ã¯ SGD â†’ Momentum â†’ Adam â†’ AdamWã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã€å‡¸æœ€é©åŒ–åŒå¯¾æ€§ï¼ˆKKTæ¡ä»¶ãƒ»ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥åŒå¯¾ï¼‰ã¾ã§è¸ã¿è¾¼ã‚“ã ã€‚Boss Battle ã¯ Cross-Entropy Loss $\mathcal{L} = -\sum_t \log q_\theta(x_t \mid x_{<t})$ ã®å®Œå…¨åˆ†è§£ â€” æƒ…å ±ç†è«–ã®å…¨é“å…·ã‚’å‹•å“¡ã—ã¦ã€LLMå­¦ç¿’ã®æå¤±é–¢æ•°ã‚’åŸå­ãƒ¬ãƒ™ãƒ«ã¾ã§è§£å‰–ã—ãŸã€‚

**ç¬¬7å›: æœ€å°¤æ¨å®šã¨çµ±è¨ˆçš„æ¨è«–** ğŸ—ºï¸

ã€Œæ¨å®šé‡ã®è¨­è¨ˆã¯æ•°å­¦ã®è¨­è¨ˆã ã€ã€‚MLE ã®å®šç¾©ï¼ˆFisher 1922ï¼‰ã‹ã‚‰å§‹ã‚ã¦ã€MLE = Cross-Entropyæœ€å°åŒ– = KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹æœ€å°åŒ–ã®ä¸‰ä½ä¸€ä½“ã‚’å®Œå…¨è¨¼æ˜ã—ãŸã€‚å°¤åº¦é–¢æ•°ã®ã‚¢ã‚¯ã‚»ã‚¹å½¢æ…‹ï¼ˆæ˜ç¤ºçš„ vs æš—é»™çš„ï¼‰ã€MLE ã®3å¤‰å½¢ï¼ˆå¤‰æ•°å¤‰æ›å°¤åº¦ãƒ»æš—é»™çš„MLEãƒ»ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°ï¼‰ã€Mode-Covering vs Mode-Seekingã€‚Boss Battle ã¯ã¾ã•ã« MLE = CE = KL ã®ä¸‰ä½ä¸€ä½“ã®å®Œå…¨è¨¼æ˜ â€” $\hat{\theta}_\text{MLE} = \arg\min_\theta D_\text{KL}(p_\text{data} \| q_\theta) = \arg\min_\theta H(p_\text{data}, q_\theta)$ã€‚ã“ã®ç­‰å¼ãŒè¦‹ãˆãŸç¬é–“ã€6å›åˆ†ã®æ•°å­¦ãŒä¸€æœ¬ã®ç·šã§ç¹‹ãŒã£ãŸã€‚

**ç¬¬8å›: æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ« & EMç®—æ³•** ğŸ”

Course I ã®ãƒ•ã‚£ãƒŠãƒ¼ãƒ¬ã€‚ã€Œè¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã®è£ã«ã¯ã€å¸¸ã«"è¦‹ãˆãªã„æ§‹é€ "ãŒéš ã‚Œã¦ã„ã‚‹ã€ã€‚æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«ã®å®šå¼åŒ–ã€Jensenä¸ç­‰å¼ã«ã‚ˆã‚‹ELBOåˆ†è§£ã€EMç®—æ³•ã®E-step/M-stepå°å‡ºã€GMMã®å®Œå…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°å¼ã€åæŸæ€§è¨¼æ˜ã€‚Boss Battle ã¯ Dempster, Laird, Rubin (1977) ã®Qé–¢æ•° $Q(\theta \mid \theta^{(t)}) = \mathbb{E}_{z \sim p(z|x,\theta^{(t)})}[\log p(x,z \mid \theta)]$ ã‚’å¤šå¤‰é‡GMMã§å®Œå…¨å±•é–‹ â€” åŠä¸–ç´€å‰ã®åŸè«–æ–‡ã®æ•°å¼ã‚’ã€è‡ªåˆ†ã®æ‰‹ã§è§£ãã»ãã—ãŸã€‚

:::

### ç²å¾—ã—ãŸæ­¦å™¨ä¸€è¦§ãƒãƒƒãƒ—

8å›ã®æ—…ã§æ‰‹ã«å…¥ã‚ŒãŸæ•°å­¦çš„æ­¦å™¨ã‚’ã€ä¾å­˜é–¢ä¿‚ã¨ã¨ã‚‚ã«å¯è¦–åŒ–ã™ã‚‹ã€‚

```mermaid
graph TD
    subgraph "ç¬¬1å›: æ•°å¼ãƒªãƒ†ãƒ©ã‚·ãƒ¼"
        A1["ã‚®ãƒªã‚·ãƒ£æ–‡å­—ãƒ»è¨˜æ³•"]
        A2["é›†åˆè«–ãƒ»è«–ç†è¨˜å·"]
        A3["é–¢æ•°ã®è¨˜æ³•"]
    end

    subgraph "ç¬¬2-3å›: ç·šå½¢ä»£æ•°"
        B1["ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ãƒ»åŸºåº•"]
        B2["è¡Œåˆ—æ¼”ç®—ãƒ»å›ºæœ‰å€¤åˆ†è§£"]
        B3["SVDãƒ»ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼"]
        B4["è¡Œåˆ—å¾®åˆ†ãƒ»é€£é–å¾‹"]
        B5["è‡ªå‹•å¾®åˆ† (Forward/Reverse)"]
    end

    subgraph "ç¬¬4-5å›: ç¢ºç‡è«–"
        C1["ç¢ºç‡ç©ºé–“ (Î©,F,P)"]
        C2["ãƒ™ã‚¤ã‚ºã®å®šç†ãƒ»MLE"]
        C3["æŒ‡æ•°å‹åˆ†å¸ƒæ—"]
        C4["æ¸¬åº¦ãƒ»Lebesgueç©åˆ†"]
        C5["Radon-Nikodymå°é–¢æ•°"]
        C6["Markové€£é–ãƒ»Browné‹å‹•"]
        C7["ä¼Šè—¤ç©åˆ†ãƒ»SDE"]
    end

    subgraph "ç¬¬6å›: æƒ…å ±ç†è«–ãƒ»æœ€é©åŒ–"
        D1["ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ»KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹"]
        D2["f-Divergenceãƒ»Jensenä¸ç­‰å¼"]
        D3["SGDãƒ»Adamãƒ»å‡¸æœ€é©åŒ–åŒå¯¾æ€§"]
    end

    subgraph "ç¬¬7-8å›: çµ±è¨ˆçš„æ¨è«–"
        E1["MLE = CE = KL"]
        E2["Fisheræƒ…å ±é‡ãƒ»æ¼¸è¿‘è«–"]
        E3["æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«"]
        E4["ELBOåˆ†è§£"]
        E5["EMç®—æ³• (E-step/M-step)"]
    end

    A1 --> B1
    A2 --> C1
    A3 --> B4
    B1 --> B2 --> B3
    B2 --> B4 --> B5
    C1 --> C2 --> C3
    C1 --> C4 --> C5
    C4 --> C6 --> C7
    C2 --> D1
    C5 --> D1
    D1 --> D2
    B5 --> D3
    D2 --> E1
    D3 --> E1
    C2 --> E2
    E1 --> E3
    D2 --> E4
    E3 --> E4 --> E5

    style E5 fill:#ffeb3b
```

| æ­¦å™¨ã‚«ãƒ†ã‚´ãƒª | å…·ä½“çš„ãªæ­¦å™¨ | ç²å¾—å› | Course II ã§ã®ç”¨é€” |
|:-----------|:-----------|:------|:----------------|
| **è¨˜æ³•** | ã‚®ãƒªã‚·ãƒ£æ–‡å­—ãƒ»æ·»å­—ãƒ»æ¼”ç®—å­ | ç¬¬1å› | å…¨è¬›ç¾©ã®åŸºç›¤ |
| **ç·šå½¢ä»£æ•°** | å†…ç©ãƒ»å›ºæœ‰å€¤åˆ†è§£ãƒ»SVDãƒ»è¡Œåˆ—å¾®åˆ† | ç¬¬2-3å› | æ½œåœ¨ç©ºé–“ã®æ“ä½œã€å‹¾é…è¨ˆç®— |
| **è‡ªå‹•å¾®åˆ†** | Forward/Reverse Mode AD | ç¬¬3å› | å…¨ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ |
| **ç¢ºç‡è«–** | ãƒ™ã‚¤ã‚ºã®å®šç†ãƒ»æ¡ä»¶ä»˜ãåˆ†å¸ƒãƒ»MLE | ç¬¬4å› | äº‹å¾Œåˆ†å¸ƒã®è¿‘ä¼¼ã€å°¤åº¦è¨ˆç®— |
| **æ¸¬åº¦è«–** | Lebesgueç©åˆ†ãƒ»Radon-Nikodym | ç¬¬5å› | ç¢ºç‡å¯†åº¦ã®å³å¯†ãªå®šç¾© |
| **ç¢ºç‡éç¨‹** | Markové€£é–ãƒ»Browné‹å‹•ãƒ»ä¼Šè—¤ã®è£œé¡Œ | ç¬¬5å› | æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®SDE |
| **æƒ…å ±ç†è«–** | ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ»KLãƒ»f-Divergence | ç¬¬6å› | æå¤±é–¢æ•°ã®è¨­è¨ˆã¨è©•ä¾¡ |
| **æœ€é©åŒ–** | SGDãƒ»Adamãƒ»å‡¸æœ€é©åŒ–åŒå¯¾æ€§ | ç¬¬6å› | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å­¦ç¿’ |
| **çµ±è¨ˆçš„æ¨è«–** | MLE = CE = KL ã®ä¸‰ä½ä¸€ä½“ | ç¬¬7å› | ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’åŸç† |
| **æ½œåœ¨å¤‰æ•°** | ELBOãƒ»EMç®—æ³• | ç¬¬8å› | VAEãƒ»Diffusion ã®æ ¸å¿ƒ |

### ãƒ“ãƒ•ã‚©ãƒ¼ã‚¢ãƒ•ã‚¿ãƒ¼ â€” ã‚ãªãŸã®å¤‰åŒ–ã‚’æ¸¬ã‚‹

ç¬¬1å›ã®å†’é ­ã‚’æ€ã„å‡ºã—ã¦ã»ã—ã„ã€‚

> **æ•°å¼ãŒ"èª­ã‚ãªã„"ã®ã¯æ‰èƒ½ã§ã¯ãªãèªå½™ã®å•é¡Œã€‚50è¨˜å·ã‚’è¦šãˆã‚Œã°è«–æ–‡ãŒ"èª­ã‚ã‚‹"ã€‚**

ã‚ã®æ™‚ã€ã“ã®ä¸€æ–‡ã«ã€Œã„ã‚„ã„ã‚„ã€ãã‚“ãªã‚ã‘ãªã„ã ã‚ã€ã¨æ€ã£ãŸã¯ãšã ã€‚

ã§ã¯ä»Šã€ä»¥ä¸‹ã®æ•°å¼ã‚’è¦‹ã¦ã»ã—ã„ã€‚ç¬¬1å›ã® Boss Battle ã§æŒ‘ã‚“ã  Attention å¼ã ã€‚

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
$$

ç¬¬1å›ã§ã¯ã€ã“ã®å¼ã‚’ã€Œä¸€æ–‡å­—æ®‹ã‚‰ãšèª­è§£ã™ã‚‹ã€ã“ã¨ãŒ Boss Battle ã ã£ãŸã€‚$Q, K, V$ ãŒä½•ã‹ã€$\sqrt{d_k}$ ã§å‰²ã‚‹ç†ç”±ã€softmax ã®æ„å‘³ â€” ä¸€ã¤ã²ã¨ã¤è§£ãã»ãã™ã®ã«60åˆ†ã‹ã‹ã£ãŸã€‚

**ä»Šã®ã‚ãªãŸã¯ã©ã†ã ã‚ã†ï¼Ÿ**

- $QK^\top$ â€” è¡Œåˆ—ç©ã€‚ã‚¯ã‚¨ãƒªã¨ã‚­ãƒ¼ã®å…¨ãƒšã‚¢ã®å†…ç©ã‚’ä¸€æ‹¬è¨ˆç®—ï¼ˆç¬¬2å›ï¼‰
- $\sqrt{d_k}$ â€” å†…ç©ã®åˆ†æ•£ãŒ $d_k$ ã«æ¯”ä¾‹ã™ã‚‹ã®ã§ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã§å®‰å®šåŒ–ï¼ˆç¬¬2å›ï¼‰
- softmax â€” ç¢ºç‡åˆ†å¸ƒã¸ã®æ­£è¦åŒ–ã€‚Categoricalåˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆç¬¬4å›ï¼‰
- å…¨ä½“ â€” é¡ä¼¼åº¦åŠ é‡å’Œã€‚$d$æ¬¡å…ƒç©ºé–“ä¸Šã®æ¡ä»¶ä»˜ãæœŸå¾…å€¤ã®é›¢æ•£è¿‘ä¼¼ï¼ˆç¬¬4-5å›ï¼‰
- å­¦ç¿’ â€” ã“ã®å‡ºåŠ›ã¨æ­£è§£ã® Cross-Entropy = KL æœ€å°åŒ–ï¼ˆç¬¬6-7å›ï¼‰

å‘¼å¸ã™ã‚‹ã‚ˆã†ã«èª­ã‚ãªã„ã ã‚ã†ã‹ã€‚

ã“ã‚Œã ã‘ã§ã¯ãªã„ã€‚ä»Šã®ã‚ãªãŸã¯ã€ã‚‚ã£ã¨é«˜åº¦ãªæ•°å¼ã‚‚èª­ã‚ã‚‹ã€‚

$$
\log p(\mathbf{x} \mid \theta) = \underbrace{\mathbb{E}_{q(\mathbf{z}|\mathbf{x})}[\log p(\mathbf{x}, \mathbf{z} \mid \theta) - \log q(\mathbf{z} \mid \mathbf{x})]}_{\text{ELBO}:\, \mathcal{L}(q, \theta)} + \underbrace{D_\text{KL}[q(\mathbf{z} \mid \mathbf{x}) \| p(\mathbf{z} \mid \mathbf{x}, \theta)]}_{\geq 0}
$$

ç¬¬1å›ã®æ™‚ç‚¹ã§ã¯ã€ã“ã®å¼ã¯å®Œå…¨ã«æš—å·ã ã£ãŸã ã‚ã†ã€‚ä»Šã¯é•ã†ã€‚

- $\log p(\mathbf{x} \mid \theta)$ â€” å¯¾æ•°å‘¨è¾ºå°¤åº¦ã€‚æ½œåœ¨å¤‰æ•° $\mathbf{z}$ ã‚’å‘¨è¾ºåŒ–ã—ãŸå°¤åº¦ï¼ˆç¬¬7-8å›ï¼‰
- $\mathbb{E}_{q(\mathbf{z}|\mathbf{x})}[\cdot]$ â€” å¤‰åˆ†åˆ†å¸ƒ $q$ ã«é–¢ã™ã‚‹æœŸå¾…å€¤ï¼ˆç¬¬4-5å›ï¼‰
- $\log p(\mathbf{x}, \mathbf{z} \mid \theta) - \log q(\mathbf{z} \mid \mathbf{x})$ â€” å®Œå…¨ãƒ‡ãƒ¼ã‚¿å¯¾æ•°å°¤åº¦ã¨å¤‰åˆ†åˆ†å¸ƒã®å¯¾æ•°æ¯”ï¼ˆç¬¬8å›ï¼‰
- $D_\text{KL}[\cdot \| \cdot]$ â€” KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã€‚éè² ã€‚E-step ã§ 0 ã«ã™ã‚‹ï¼ˆç¬¬6å›ã€ç¬¬8å›ï¼‰
- å…¨ä½“ â€” ELBOåˆ†è§£ã€‚EMç®—æ³•ã®å¿ƒè‡“éƒ¨ã§ã‚ã‚Šã€VAEã®æå¤±é–¢æ•°ã®åŸå‹ï¼ˆç¬¬8å›ï¼‰

**8å›å‰ã®ã‚ãªãŸã¨ã€ä»Šã®ã‚ãªãŸã¯ã€åˆ¥äººã ã€‚**

è«–æ–‡ã‚’é–‹ã„ã¦æ•°å¼ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«é­é‡ã—ãŸã¨ãã€åå°„çš„ã«é–‰ã˜ã‚‹å¿…è¦ã¯ã‚‚ã†ãªã„ã€‚è¨˜å·ã‚’ä¸€ã¤ãšã¤èª­ã¿ã€å®šç¾©ã‚’ç¢ºèªã—ã€å°å‡ºã®æµã‚Œã‚’è¿½ãˆã‚‹ã€‚å®Œå…¨ã«ã¯ç†è§£ã§ããªãã¦ã‚‚ã€ã€Œä½•ãŒã‚ã‹ã‚‰ãªã„ã‹ã€ã‚’ç‰¹å®šã§ãã‚‹ã€‚ãã‚Œã¯ç¬¬1å›ã®æ™‚ç‚¹ã§ã¯ä¸å¯èƒ½ã ã£ãŸã“ã¨ã ã€‚

### Course II äºˆå‘Š â€” ã“ã®æ•°å­¦ãŒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’å‹•ã‹ã™

Course I ã®8å›ã§é›ãˆãŸæ•°å­¦ã¯ã€Course II ä»¥é™ã§ç‰™ã‚’å‰¥ãã€‚ä»¥ä¸‹ã¯ã€Course I ã®æ­¦å™¨ãŒ Course II ã§ã©ã†ä½¿ã‚ã‚Œã‚‹ã‹ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã ã€‚

```mermaid
graph TD
    subgraph "Course I ã®æ­¦å™¨"
        W1["KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹<br/>(ç¬¬6å›)"]
        W2["Jensenä¸ç­‰å¼<br/>(ç¬¬6å›)"]
        W3["æœŸå¾…å€¤ E_q[Â·]<br/>(ç¬¬4-5å›)"]
        W4["ã‚¬ã‚¦ã‚¹KLé–‰å½¢å¼<br/>(ç¬¬4å›)"]
        W5["å¤‰æ•°å¤‰æ›<br/>(ç¬¬4å›)"]
        W6["ã‚²ãƒ¼ãƒ ç†è«–<br/>(ç¬¬6å›)"]
        W7["JSD / f-Div<br/>(ç¬¬6å›)"]
        W8["Wassersteinè·é›¢<br/>(ç¬¬6å›)"]
        W9["åŒå¯¾æ€§<br/>(ç¬¬6å›)"]
        W10["æ¸¬åº¦è«–<br/>(ç¬¬5å›)"]
        W11["é€£é–å¾‹<br/>(ç¬¬4å›)"]
        W12["MLE<br/>(ç¬¬7å›)"]
        W13["Attentionå¼<br/>(ç¬¬1-2å›)"]
        W14["ELBO<br/>(ç¬¬8å›)"]
    end

    subgraph "Course II ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"
        M1["ç¬¬9å›: å¤‰åˆ†æ¨è«–<br/>ELBO ã®3é€šã‚Šã®å°å‡º"]
        M2["ç¬¬10å›: VAE<br/>Reparameterization"]
        M3["ç¬¬11å›: æœ€é©è¼¸é€ç†è«–<br/>Wassersteinè·é›¢"]
        M4["ç¬¬12å›: GAN<br/>Minimax ã‚²ãƒ¼ãƒ "]
        M5["ç¬¬13å›: è‡ªå·±å›å¸°<br/>é€£é–å¾‹ + MLE"]
        M6["ç¬¬14å›: Attention<br/>åŒ–çŸ³ã‹ã‚‰ã®è„±å´"]
        M7["ç¬¬15å›: AttentionåŠ¹ç‡åŒ–<br/>Flash/Sparse/MoE"]
        M8["ç¬¬16å›: SSM & Mamba<br/>O(N)ã®ä¸–ç•Œ"]
        M9["ç¬¬17å›: Mambaç™ºå±•<br/>Attention=SSMåŒå¯¾æ€§"]
        M10["ç¬¬18å›: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰<br/>æœ€å¼·ã®çµ„ã¿åˆã‚ã›"]
    end

    W1 --> M1
    W2 --> M1
    W3 --> M1
    W14 --> M1
    M1 --> M2
    W4 --> M2
    W5 --> M2
    M2 --> M3
    W8 --> M3
    W9 --> M3
    W10 --> M3
    M3 --> M4
    W6 --> M4
    W7 --> M4
    M4 --> M5
    W11 --> M5
    W12 --> M5
    M5 --> M6
    W13 --> M6
    M6 --> M7
    M7 --> M8
    M8 --> M9
    M9 --> M10

    style M1 fill:#e3f2fd
    style M2 fill:#e3f2fd
    style M3 fill:#e3f2fd
    style M4 fill:#e3f2fd
    style M5 fill:#e3f2fd
    style M6 fill:#e3f2fd
    style M7 fill:#e3f2fd
    style M8 fill:#e3f2fd
    style M9 fill:#e3f2fd
    style M10 fill:#e3f2fd
```

å…·ä½“çš„ã«è¦‹ã¦ã¿ã‚ˆã†ã€‚

| Course II è¬›ç¾© | Course I ã‹ã‚‰æŒã¡è¾¼ã‚€æ­¦å™¨ | ä½¿ã„æ–¹ |
|:-------------|:---------------------|:------|
| **ç¬¬9å›: å¤‰åˆ†æ¨è«– & ELBO** | KL (ç¬¬6å›) + Jensen (ç¬¬6å›) + æœŸå¾…å€¤ (ç¬¬4å›) + ELBO (ç¬¬8å›) | 3é€šã‚Šã®ELBOå°å‡º â€” å…¨ã¦ãŒç¬¬8å›ã®å»¶é•· |
| **ç¬¬10å›: VAE** | ELBO (ç¬¬8-9å›) + ã‚¬ã‚¦ã‚¹KLé–‰å½¢å¼ (ç¬¬4å›) + Reparameterization | ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ $q_\phi(\mathbf{z} \mid \mathbf{x}) = \mathcal{N}(\mu_\phi, \sigma_\phi^2)$ ã®KLé …ã‚’é–‰å½¢å¼ã§è¨ˆç®— |
| **ç¬¬12å›: GAN** | Minimax (ç¬¬6å›) + JSD (ç¬¬6å›) + æœ€é©åŒ– (ç¬¬6å›) | $\min_G \max_D$ ã®ç›®çš„é–¢æ•°ãŒJSDã®å¤‰åˆ†è¡¨ç¾ã§ã‚ã‚‹ã“ã¨ã‚’è¨¼æ˜ |
| **ç¬¬13å›: æœ€é©è¼¸é€** | Wassersteinè·é›¢ (ç¬¬6å›) + åŒå¯¾æ€§ (ç¬¬6å›) + æ¸¬åº¦ (ç¬¬5å›) | Kantorovich-Rubinstein åŒå¯¾æ€§ã®å®Œå…¨å°å‡º |
| **ç¬¬15å›: è‡ªå·±å›å¸°** | é€£é–å¾‹ (ç¬¬4å›) + MLE (ç¬¬7å›) + Categoricalåˆ†å¸ƒ (ç¬¬4å›) | $p(\mathbf{x}) = \prod_t p(x_t \mid x_{<t})$ ã‚’MLEæœ€å¤§åŒ– |
| **ç¬¬16å›: Transformer** | Attention (ç¬¬1-2å›) + Scaling Laws + KV-Cache | Attentionå¼ã‚’ãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯å®Ÿè£… â€” ç¬¬1å›ã® Boss Battle ãŒå‡ºç™ºç‚¹ |

ç¬¬8å›ã§å­¦ã‚“ã  ELBO åˆ†è§£ã¯ã€ç¬¬9å›ã§å¤‰åˆ†æ¨è«–ã®ä¸€èˆ¬ç†è«–ã¨ã—ã¦å†ç™»å ´ã—ã€ç¬¬10å›ã® VAE ã®æå¤±é–¢æ•°ã«ç›´çµã™ã‚‹ã€‚ç¬¬6å›ã® KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¯ã€VAE ã®æ­£å‰‡åŒ–é …ã€GAN ã®ç›®çš„é–¢æ•°ã€æœ€é©è¼¸é€ã®åŒå¯¾è¡¨ç¾ â€” ã‚ã‚‰ã‚†ã‚‹å ´é¢ã§æ­¦å™¨ã«ãªã‚‹ã€‚

**Course I ã®æ•°å­¦ãªã—ã«ã€ã“ã‚Œã‚‰ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æ•°å¼ã¯1è¡Œã‚‚å°å‡ºã§ããªã„ã€‚** é€†ã«è¨€ãˆã°ã€Course I ã‚’èµ°ç ´ã—ãŸã‚ãªãŸã«ã¯ã€Course II ã®å…¨ã¦ã®æ•°å¼ã‚’ã€Œè‡ªåŠ›ã§å°å‡ºã™ã‚‹ã€ãŸã‚ã®æ­¦å™¨ãŒæ—¢ã«æƒã£ã¦ã„ã‚‹ã€‚

### èª­è€…ã¸ â€” ã“ã“ã¾ã§æ¥ãŸã‚ãªãŸã¸

æ­£ç›´ã«è¨€ãŠã†ã€‚Course I ã¯æ¥½ã§ã¯ãªã‹ã£ãŸã€‚

ç¬¬5å›ã®æ¸¬åº¦è«–ã§ã€Œã‚‚ã†ç„¡ç†ã ã€ã¨æ€ã£ãŸäººã¯å°‘ãªããªã„ã ã‚ã†ã€‚Lebesgueç©åˆ†ã‚„ Radon-Nikodym å°é–¢æ•°ã¯ã€å¤§å­¦é™¢ãƒ¬ãƒ™ãƒ«ã®æ•°å­¦ã ã€‚ç¬¬6å›ã®f-Divergenceçµ±ä¸€ç†è«–ã€ç¬¬8å›ã®EMåæŸæ€§è¨¼æ˜ â€” ã©ã‚Œã‚‚ä¸€ç­‹ç¸„ã§ã¯ã„ã‹ãªã‹ã£ãŸã€‚

ã ãŒã€ã‚ãªãŸã¯ã“ã“ã«ã„ã‚‹ã€‚

8å›åˆ†ã® Boss Battle ã‚’å€’ã—ã€8å›åˆ†ã®æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ã‚’è¸ç ´ã—ã€ç´™ã¨ãƒšãƒ³ã§å°å‡ºã‚’è¿½ã„ã€ã‚³ãƒ¼ãƒ‰ã§æ•°å€¤æ¤œè¨¼ã‚’è¡Œã„ã€ä¸€æ­©ä¸€æ­©ã“ã“ã¾ã§æ¥ãŸã€‚

**ã“ã“ã¾ã§æ¥ãŸã‚ãªãŸã¯ã€ã‚‚ã†åˆå¿ƒè€…ã§ã¯ãªã„ã€‚**

è«–æ–‡ã‚’é–‹ã„ã¦æ•°å¼ã«å‡ºä¼šã£ãŸã¨ãã€é€ƒã’ãšã«ç«‹ã¡å‘ã‹ãˆã‚‹ã€‚ã‚ã‹ã‚‰ãªã„è¨˜å·ã«å‡ºä¼šã£ã¦ã‚‚ã€ç¬¬1å›ã®ã‚®ãƒªã‚·ãƒ£æ–‡å­—è¡¨ã«æˆ»ã‚Œã‚‹ã€‚å°å‡ºãŒè¿½ãˆãªã„ã¨ãã€ã©ã®å›ã®ã©ã®å®šç†ãŒè¶³ã‚Šãªã„ã‹ã‚’ç‰¹å®šã§ãã‚‹ã€‚ãã‚Œã¯æ•°å­¦çš„æˆç†Ÿã®è¨¼ã ã€‚

Course II ã§ã¯ã€ã“ã“ã¾ã§ã®æ•°å­¦ãŒå…·ä½“çš„ãªç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¸ã¨çµå®Ÿã™ã‚‹ã€‚ELBO ãŒ VAE ã®æå¤±é–¢æ•°ã«ãªã‚‹ç¬é–“ã€‚KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ãŒ GAN ã®ç›®çš„é–¢æ•°ã«åŒ–ã‘ã‚‹ç¬é–“ã€‚ä¼Šè—¤ã®è£œé¡ŒãŒæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®é€†éç¨‹ã‚’å°ãç¬é–“ã€‚8å›ã‹ã‘ã¦ç£¨ã„ãŸæ­¦å™¨ãŒã€ä¸€æ–‰ã«è¼ãå‡ºã™ã€‚

ãã—ã¦ã€ç¬¬9å›ã§ã¯ Julia ãŒåˆç™»å ´ã™ã‚‹ã€‚Python ã§45ç§’ã‹ã‹ã£ãŸ ELBO è¨ˆç®—ãŒ0.8ç§’ã«ãªã‚‹è¡æ’ƒãŒå¾…ã£ã¦ã„ã‚‹ã€‚æ•°å­¦ã ã‘ã§ãªãã€å®Ÿè£…ã®æ¬¡å…ƒã‚‚å¤‰ã‚ã‚‹ã€‚

**æº–å‚™ã¯ã§ãã¦ã„ã‚‹ã€‚Course II ã§ä¼šãŠã†ã€‚**

---


### 6.10 ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **VAEã‚‚Diffusionã‚‚EMã®å­å­«ã€‚ã€Œå¤ã„ã€ã®ã§ã¯ãªãã€ŒåŸºç›¤ã€ã§ã¯ï¼Ÿ**

EMç®—æ³•ã¯1977å¹´ã«ææ¡ˆã•ã‚ŒãŸã€‚åŠä¸–ç´€è¿‘ãå‰ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã ã€‚VAE (2013) [^2]ã€Diffusion Models (2020) â€” ã“ã‚Œã‚‰ã¯ã€Œæ–°ã—ã„ã€æ‰‹æ³•ã«è¦‹ãˆã‚‹ã€‚ã ãŒæœ¬è³ªã‚’è¦‹ã¦ã»ã—ã„ã€‚

- VAEã®æå¤±é–¢æ•°ã¯ELBOã€‚ELBOã¯EMç®—æ³•ã®æ ¸å¿ƒãã®ã‚‚ã®ã ã€‚
- Diffusion Modelsã®å­¦ç¿’ç›®æ¨™ã‚‚ELBOã®å¤‰å½¢ç‰ˆã ã€‚
- Score Matching ã™ã‚‰EMçš„ãªæ§‹é€ ï¼ˆãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã¨ãƒ¢ãƒ‡ãƒ«åˆ†å¸ƒã®é–“ã®æœ€é©åŒ–ï¼‰ã‚’æŒã¤ã€‚

**EMç®—æ³•ã‚’ã€Œå¤ã„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€ã¨åˆ‡ã‚Šæ¨ã¦ã‚‹äººã¯ã€ç¾ä»£ã®æ·±å±¤ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æ•°å­¦çš„åŸºç›¤ã‚’ç†è§£ã—ã¦ã„ãªã„ã€‚** Jensenä¸ç­‰å¼ â†’ ELBO â†’ å¤‰åˆ†æ¨è«– â†’ VAE/Diffusion ã¨ã„ã†æµã‚Œã¯ä¸€æœ¬ã®ç·šã§ç¹‹ãŒã£ã¦ã„ã‚‹ã€‚

:::details æ­´å²çš„æ–‡è„ˆ
EMç®—æ³•ã®æ­´å²ã¯1977å¹´ã® Dempster-Laird-Rubin ã‚ˆã‚Šå‰ã«é¡ã‚‹ã€‚1970å¹´ã® Baum-Welchç®—æ³• [^4] ã¯ HMM ã«å¯¾ã™ã‚‹EMç®—æ³•ã§ã‚ã‚Šã€EM ã®ä¸€èˆ¬çš„å®šå¼åŒ–ã‚ˆã‚Š7å¹´æ—©ã„ã€‚ã•ã‚‰ã«é¡ã‚‹ã¨ã€1950å¹´ä»£ã® missing data å•é¡Œã«ãŠã‘ã‚‹åå¾©æ¨å®šæ³•ãŒEMã®åŸå‹ã ã¨ã•ã‚Œã‚‹ã€‚

ã€Œå¤ã„ã‹ã‚‰ãƒ€ãƒ¡ã€ã¯ç§‘å­¦ã«ãŠã„ã¦ã¾ã£ãŸãæˆã‚Šç«‹ãŸãªã„ã€‚ã‚€ã—ã‚ã€ŒåŠä¸–ç´€ã‚’çµŒã¦ã‚‚å½¢ã‚’å¤‰ãˆã¦ä½¿ã‚ã‚Œç¶šã‘ã‚‹ã€ã“ã¨ã“ãã€EMç®—æ³•ã®æ•°å­¦çš„åŸºç›¤ã®å¼·å›ºã•ã®è¨¼æ˜ã ã€‚

å…·ä½“çš„ã«è€ƒãˆã¦ã¿ã‚ˆã†:
1. VAEã®å­¦ç¿’ = ELBOæœ€å¤§åŒ– = Variational EM ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆç‰ˆ
2. Diffusion ã®æå¤± = åŠ é‡ELBO = å„ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã§ã®EMçš„åˆ†è§£
3. Flow Matching = é€£ç¶šç‰ˆã®VEMï¼ˆç¬¬31å›ã§è©³è¿°ï¼‰

ã€Œæ–°ã—ã„æ‰‹æ³•ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«å¤ã„ç†è«–ã‚’å­¦ã¶ã€ã®ã§ã¯ãªã„ã€‚**åŒã˜ç†è«–ã®ç¾ä»£çš„ãªå§¿ã‚’è¦‹ã¦ã„ã‚‹**ã®ã ã€‚
:::

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Dempster, A.P., Laird, N.M., Rubin, D.B. (1977). "Maximum Likelihood from Incomplete Data via the EM Algorithm." *Journal of the Royal Statistical Society, Series B*, 39(1), 1-38.
@[card](https://doi.org/10.1111/j.2517-6161.1977.tb01600.x)

[^2]: Kingma, D.P., Welling, M. (2013). "Auto-Encoding Variational Bayes." *arXiv preprint*.
@[card](https://arxiv.org/abs/1312.6114)

[^3]: Wu, C.F.J. (1983). "On the Convergence Properties of the EM Algorithm." *The Annals of Statistics*, 11(1), 95-103.
@[card](https://doi.org/10.1214/aos/1176346060)

[^4]: Baum, L.E., Petrie, T., Soules, G., Weiss, N. (1970). "A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains." *The Annals of Mathematical Statistics*, 41(1), 164-171.
@[card](https://doi.org/10.1214/aoms/1177697196)

[^5]: Neal, R.M., Hinton, G.E. (1998). "A View of the EM Algorithm that Justifies Incremental, Sparse, and other Variants." *Learning in Graphical Models*, Springer.
@[card](https://www.cs.toronto.edu/~hinton/absps/emk.pdf)

[^6]: Arthur, D., Vassilvitskii, S. (2007). "k-means++: The Advantages of Careful Seeding." *SODA '07*.

[^7]: Jacobs, R.A., Jordan, M.I., Nowlan, S.J., Hinton, G.E. (1991). "Adaptive Mixtures of Local Experts." *Neural Computation*, 3(1), 79-87.
@[card](https://doi.org/10.1162/neco.1991.3.1.79)

[^11]: Bishop, C.M. (2006). *Pattern Recognition and Machine Learning*. Springer.
@[card](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)

[^12]: Minka, T.P. (2001). "Expectation Propagation for Approximate Bayesian Inference." *UAI 2001*.
@[card](https://arxiv.org/abs/1301.2294)

### æ•™ç§‘æ›¸

- Bishop, C.M. (2006). *Pattern Recognition and Machine Learning*. Springer. [Ch.9: Mixture Models and EM] [å…¬å¼PDFç„¡æ–™]
- Murphy, K.P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press. [Ch.11]
- MacKay, D.J.C. (2003). *Information Theory, Inference, and Learning Algorithms*. Cambridge University Press. [Ch.22, 33] [å…¬å¼PDFç„¡æ–™]

---

## è¨˜æ³•è¦ç´„

| è¨˜å· | èª­ã¿ | æ„å‘³ | åˆå‡º |
|:-----|:-----|:-----|:-----|
| $\mathbf{x}$ | ã‚¨ãƒƒã‚¯ã‚¹ (å¤ªå­—) | è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ™ã‚¯ãƒˆãƒ«ï¼‰ | ç¬¬2å› |
| $\mathbf{z}$ | ã‚¼ãƒƒãƒˆ (å¤ªå­—) | æ½œåœ¨å¤‰æ•°ï¼ˆãƒ™ã‚¯ãƒˆãƒ«ï¼‰ | **ç¬¬8å›** |
| $\theta$ | ã‚·ãƒ¼ã‚¿ | ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | ç¬¬6å› |
| $\phi$ | ãƒ•ã‚¡ã‚¤ | å¤‰åˆ†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆç¬¬9å›ã§æœ¬æ ¼ç™»å ´ï¼‰ | â€” |
| $\pi_k$ | ãƒ‘ã‚¤ ã‚±ãƒ¼ | æ··åˆé‡ã¿ï¼ˆ$\sum_k \pi_k = 1$ï¼‰ | **ç¬¬8å›** |
| $\mu_k$ | ãƒŸãƒ¥ãƒ¼ ã‚±ãƒ¼ | æˆåˆ† $k$ ã®å¹³å‡ãƒ™ã‚¯ãƒˆãƒ« | ç¬¬4å› |
| $\boldsymbol{\Sigma}_k$ | ã‚·ã‚°ãƒ ã‚±ãƒ¼ | æˆåˆ† $k$ ã®å…±åˆ†æ•£è¡Œåˆ— | ç¬¬4å› |
| $\gamma(z_{nk})$ | ã‚¬ãƒ³ãƒ | è²¬ä»»åº¦ï¼ˆäº‹å¾Œç¢ºç‡ï¼‰ | **ç¬¬8å›** |
| $N_k$ | ã‚¨ãƒŒ ã‚±ãƒ¼ | æˆåˆ† $k$ ã®å®ŸåŠ¹ãƒ‡ãƒ¼ã‚¿æ•° | **ç¬¬8å›** |
| $Q(\theta, \theta^{(t)})$ | ã‚­ãƒ¥ãƒ¼ | Qé–¢æ•°ï¼ˆå®Œå…¨ãƒ‡ãƒ¼ã‚¿å¯¾æ•°å°¤åº¦ã®æœŸå¾…å€¤ï¼‰ | **ç¬¬8å›** |
| $\mathcal{L}(q, \theta)$ | ã‚¨ãƒ« | ELBO | **ç¬¬8å›**ï¼ˆç¬¬9å›ã§ä¸»å½¹ï¼‰ |
| $\text{KL}[q \| p]$ | ã‚±ãƒ¼ã‚¨ãƒ« | KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ | ç¬¬6å› |
| $\mathcal{N}(\cdot \mid \mu, \sigma^2)$ | ãƒãƒ¼ãƒãƒ« | ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ | ç¬¬4å› |
| $\mathbb{E}[\cdot]$ | ã‚¨ã‚¯ã‚¹ãƒšã‚¯ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ | æœŸå¾…å€¤ | ç¬¬4å› |
| $\mathbb{1}[\cdot]$ | ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼ | æŒ‡ç¤ºé–¢æ•° | ç¬¬1å› |
| $K$ | ã‚±ãƒ¼ | æ··åˆæˆåˆ†æ•° / éš ã‚ŒçŠ¶æ…‹æ•° | **ç¬¬8å›** |
| $\log |\boldsymbol{\Sigma}|$ | ãƒ­ã‚° ãƒ‡ãƒƒãƒˆ ã‚·ã‚°ãƒ | å…±åˆ†æ•£è¡Œåˆ—ã®è¡Œåˆ—å¼ã®å¯¾æ•° | ç¬¬3å› |
| $\mathbf{A}$ | ã‚¨ãƒ¼ | çŠ¶æ…‹é·ç§»è¡Œåˆ—ï¼ˆHMMï¼‰ | **ç¬¬8å›** |
| $\alpha_t(k)$ | ã‚¢ãƒ«ãƒ•ã‚¡ ãƒ†ã‚£ãƒ¼ ã‚±ãƒ¼ | å‰å‘ãç¢ºç‡ï¼ˆForward algorithmï¼‰ | **ç¬¬8å›** |
| $\beta_t(k)$ | ãƒ™ãƒ¼ã‚¿ ãƒ†ã‚£ãƒ¼ ã‚±ãƒ¼ | å¾Œå‘ãç¢ºç‡ï¼ˆBackward algorithmï¼‰ | **ç¬¬8å›** |
| $\mathbf{W}$ | ãƒ€ãƒ–ãƒªãƒ¥ãƒ¼ | å› å­è² è·è¡Œåˆ—ï¼ˆFactor Analysisï¼‰ | **ç¬¬8å›** |
| $\boldsymbol{\Psi}$ | ãƒ—ã‚µã‚¤ | å›ºæœ‰ãƒã‚¤ã‚ºå…±åˆ†æ•£ï¼ˆFactor Analysisï¼‰ | **ç¬¬8å›** |
| $g_k(x)$ | ã‚¸ãƒ¼ ã‚±ãƒ¼ | ã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•°ï¼ˆMoEï¼‰ | **ç¬¬8å›** |
| $f({\mathbb{E}}[X]) \geq \mathbb{E}[f(X)]$ | â€” | Jensenä¸ç­‰å¼ï¼ˆå‡¹é–¢æ•°ï¼‰ | **ç¬¬8å›** |
| $\text{BIC}$ | ãƒ“ãƒ¼ã‚¢ã‚¤ã‚·ãƒ¼ | ãƒ™ã‚¤ã‚ºæƒ…å ±é‡åŸºæº– | **ç¬¬8å›** |
| $\text{AIC}$ | ã‚¨ãƒ¼ã‚¢ã‚¤ã‚·ãƒ¼ | èµ¤æ± æƒ…å ±é‡åŸºæº– | **ç¬¬8å›** |
| $R$ | ã‚¢ãƒ¼ãƒ« | æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³æŒ‡ç¤ºå¤‰æ•° | **ç¬¬8å›** |
| $d$ | ãƒ‡ã‚£ãƒ¼ | ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ï¼ˆBIC/AICï¼‰ | **ç¬¬8å›** |

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
