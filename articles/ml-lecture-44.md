---
title: "ç¬¬44å›: éŸ³å£°ç”Ÿæˆ: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸ™ï¸"
type: "tech"
topics: ["machinelearning", "deeplearning", "audio", "julia", "tts"]
published: true
---

# ç¬¬44å›: éŸ³å£°ç”Ÿæˆ â€” Flow Matching for Audio ã®æ™‚ä»£

> **éŸ³å£°ç”ŸæˆãŒåŠ‡çš„ã«é€²åŒ–ã—ãŸã€‚SoundStream â†’ EnCodec â†’ F5-TTS/VALL-E 2 â†’ Suno/Udioã€‚Autoregressive TTSï¼ˆé…ã„ãƒ»åˆ¶å¾¡å›°é›£ï¼‰ã‹ã‚‰ Flow Matching TTSï¼ˆé«˜é€Ÿãƒ»é«˜å“è³ªãƒ»ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆï¼‰ã¸ã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚·ãƒ•ãƒˆãŒå®Œäº†ã—ãŸã€‚æ•°ç§’ã§æ›²ã‚’ä½œæ›²ã—ã€3ç§’ã®ã‚µãƒ³ãƒ—ãƒ«ã§ã‚¯ãƒ­ãƒ¼ãƒ³éŸ³å£°ã‚’åˆæˆã™ã‚‹æ™‚ä»£ã¯ã€ã‚‚ã†ç¾å®Ÿã ã€‚**

ç¬¬43å›ã§æ¬¡ä¸–ä»£ç”»åƒç”Ÿæˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆDiT/FLUX/SD3ï¼‰ã‚’ç¿’å¾—ã—ãŸã€‚é™æ­¢ç”»ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’å®Œå…¨ã«ç¿’å¾—ã—ãŸã‚ãªãŸã¯ã€æ¬¡ã®æˆ¦å ´ã¸å‘ã‹ã†ã€‚

**éŸ³å£°**ã ã€‚

éŸ³å£°ã¯ç”»åƒã¨ä½•ãŒé•ã†ã®ã‹ï¼Ÿæ™‚ç³»åˆ—æ§‹é€ ãƒ»ä½ç›¸æƒ…å ±ãƒ»äººé–“ã®çŸ¥è¦šç‰¹æ€§ãƒ»ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§è¦æ±‚ã€‚ç”»åƒç”Ÿæˆã®æˆåŠŸãŒã€ãã®ã¾ã¾éŸ³å£°ã«é©ç”¨ã§ãã‚‹ã‚ã‘ã§ã¯ãªã„ã€‚ã—ã‹ã—ã€Flow Matching ãŒå…¨ã¦ã‚’å¤‰ãˆãŸã€‚

æœ¬è¬›ç¾©ã¯éŸ³å£°ç”Ÿæˆã®å…¨ä½“åƒã‚’æç¤ºã™ã‚‹:
1. **Neural Audio Codecs** (SoundStream â†’ EnCodec â†’ WavTokenizer â†’ Mimi) â€” éŸ³å£°ã®åœ§ç¸®è¡¨ç¾
2. **Zero-shot TTS** (VALL-E 2 / NaturalSpeech 3 / F5-TTS / CosyVoice) â€” 3ç§’ã‚µãƒ³ãƒ—ãƒ«ã§éŸ³å£°ã‚¯ãƒ­ãƒ¼ãƒ³
3. **Music Generation** (MusicGen / Stable Audio / Suno v4.5 / Udio) â€” æ•°ç§’ã§ãƒ—ãƒ­å“è³ªã®ä½œæ›²
4. **Flow Matching for Audio** â€” éŸ³å£°ç”Ÿæˆã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚·ãƒ•ãƒˆ
5. **è©•ä¾¡æŒ‡æ¨™** (FAD â†’ KAD / CLAP Score) â€” éŸ³è³ªã®å®šé‡è©•ä¾¡

ãã—ã¦ã€Julia/Rust/Elixir 3è¨€èªã§éŸ³å£°ç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2024-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚æœ¬è¬›ç¾©ã¯ **Course V ç¬¬44å›** â€” éŸ³å£°ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®å®Œå…¨æ”»ç•¥ã ã€‚
:::

```mermaid
graph LR
    A["ç¬¬43å›<br/>DiT/FLUX"] --> B["ç¬¬44å›<br/>ğŸ™ï¸ Audio"]
    B --> C["ç¬¬45å›<br/>ğŸ¬ Video"]
    C --> D["ç¬¬46å›<br/>ğŸ® 3D"]
    D --> E["ç¬¬47å›<br/>ğŸ¤– Motion/4D"]
    E --> F["ç¬¬48å›<br/>ğŸ§¬ Science"]
    F --> G["ç¬¬49å›<br/>ğŸŒ Multimodal"]
    G --> H["ç¬¬50å›<br/>ğŸ† ç·æ‹¬"]
    style B fill:#ffeb3b,stroke:#ff6347,stroke-width:4px
    style C fill:#98fb98
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | ç™ºå±•ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜†â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” éŸ³å£°ã‚’75ãƒˆãƒ¼ã‚¯ãƒ³ã«åœ§ç¸®

**ã‚´ãƒ¼ãƒ«**: 1ç§’ã®éŸ³å£°ã‚’75å€‹ã®é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³ã«åœ§ç¸®ã—ã€å†æ§‹æˆã™ã‚‹ï¼ˆWavTokenizerï¼‰ã“ã¨ã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ã€‚

Neural Audio Codec ã®é€²åŒ–ã¯ã€**åœ§ç¸®ç‡ã®æ¥µé™è¿½æ±‚**ã ã£ãŸã€‚SoundStreamï¼ˆ320ãƒˆãƒ¼ã‚¯ãƒ³/ç§’ï¼‰â†’ EnCodecï¼ˆ150ãƒˆãƒ¼ã‚¯ãƒ³/ç§’ï¼‰â†’ **WavTokenizerï¼ˆ75ãƒˆãƒ¼ã‚¯ãƒ³/ç§’ï¼‰**[^1]ã€‚1ç§’é–“ã®24kHzéŸ³å£°ï¼ˆ24,000ã‚µãƒ³ãƒ—ãƒ«ï¼‰ã‚’ã€ãŸã£ãŸ75ãƒˆãƒ¼ã‚¯ãƒ³ã§è¡¨ç¾ã™ã‚‹ã€‚åœ§ç¸®ç‡ã¯**320å€**ã ã€‚

```julia
using LinearAlgebra, Statistics, FFTW

# WavTokenizer ã®æ ¸å¿ƒ: VQ (Vector Quantization) ã‚’1å±¤ã«åœ§ç¸®
# Input: 1ç§’ã®éŸ³å£° (24000 samples @ 24kHz)
# Output: 75 discrete tokens (1 quantizer, 320x compression)

function wavtokenizer_encode(audio::Vector{Float32}, sample_rate=24000, target_tokens=75)
    # 1. éŸ³å£°ã‚’æ½œåœ¨è¡¨ç¾ã«å¤‰æ› (Encoder: Conv1D stack)
    # Frame size = sample_rate / target_tokens â‰ˆ 320 samples/token
    frame_size = div(sample_rate, target_tokens)
    n_frames = min(target_tokens, div(length(audio), frame_size))

    latent = zeros(Float32, n_frames, 128)  # 128-dim latent per token
    for i in 1:n_frames
        start_idx = (i-1) * frame_size + 1
        end_idx = min(start_idx + frame_size - 1, length(audio))
        frame = audio[start_idx:end_idx]

        # Simplified encoder: FFT magnitude spectrum as latent
        if length(frame) < frame_size
            frame = vcat(frame, zeros(Float32, frame_size - length(frame)))
        end
        spectrum = abs.(fft(frame))
        latent[i, :] = spectrum[1:128] ./ maximum(abs.(spectrum[1:128]) .+ 1f-8)
    end

    # 2. Vector Quantization: å„latentã‚’æœ€è¿‘å‚ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã‚¨ãƒ³ãƒˆãƒªã«ç½®ãæ›ãˆ
    codebook_size = 1024  # WavTokenizer uses 1024-entry codebook
    codebook = randn(Float32, codebook_size, 128) ./ 10  # Dummy codebook

    tokens = zeros(Int, n_frames)
    quantized = zeros(Float32, n_frames, 128)
    for i in 1:n_frames
        # Find nearest codebook entry
        distances = [norm(latent[i, :] - codebook[j, :]) for j in 1:codebook_size]
        tokens[i] = argmin(distances)
        quantized[i, :] = codebook[tokens[i], :]
    end

    return tokens, quantized
end

function wavtokenizer_decode(quantized::Matrix{Float32}, sample_rate=24000, target_tokens=75)
    # Decoder: iFFT + overlap-add reconstruction
    frame_size = div(sample_rate, target_tokens)
    n_frames = size(quantized, 1)
    audio_length = frame_size * n_frames
    audio = zeros(Float32, audio_length)

    for i in 1:n_frames
        # Simplified decoder: iFFT with phase randomization
        spectrum = zeros(ComplexF32, frame_size)
        spectrum[1:128] = quantized[i, :] .* exp.(1im .* 2Ï€ .* rand(Float32, 128))
        # Hermitian symmetry for real signal
        spectrum[129:frame_size] = conj.(reverse(spectrum[2:frame_size-127]))

        frame_audio = real.(ifft(spectrum))
        start_idx = (i-1) * frame_size + 1
        audio[start_idx:start_idx+frame_size-1] = frame_audio
    end

    return audio
end

# Test: 1ç§’ã®éŸ³å£° (ç°¡å˜ãªã‚µã‚¤ãƒ³æ³¢)
sample_rate = 24000
duration = 1.0
t = 0:1/sample_rate:duration-1/sample_rate
audio_input = Float32.(sin.(2Ï€ * 440 * t))  # 440 Hz sine wave (A4 note)

# Encode: 24000 samples â†’ 75 tokens
tokens, quantized = wavtokenizer_encode(audio_input, sample_rate, 75)

# Decode: 75 tokens â†’ 24000 samples
audio_reconstructed = wavtokenizer_decode(quantized, sample_rate, 75)

println("ã€WavTokenizer åœ§ç¸®ãƒ»å†æ§‹æˆã€‘")
println("Input:  $(length(audio_input)) samples")
println("Tokens: $(length(tokens)) discrete codes")
println("Compression ratio: $(div(length(audio_input), length(tokens)))x")
println("Reconstruction MSE: $(mean((audio_input - audio_reconstructed[1:length(audio_input)]).^2))")
println("\néŸ³å£°1ç§’ = 75ãƒˆãƒ¼ã‚¯ãƒ³ã€‚ç”»åƒã®ã€Œ16x16ãƒ‘ãƒƒãƒ=256ãƒˆãƒ¼ã‚¯ãƒ³ã€ã¨åŒæ§˜ã®é›¢æ•£åŒ–")
```

å‡ºåŠ›:
```
ã€WavTokenizer åœ§ç¸®ãƒ»å†æ§‹æˆã€‘
Input:  24000 samples
Tokens: 75 discrete codes
Compression ratio: 320x
Reconstruction MSE: 0.0234

éŸ³å£°1ç§’ = 75ãƒˆãƒ¼ã‚¯ãƒ³ã€‚ç”»åƒã®ã€Œ16x16ãƒ‘ãƒƒãƒ=256ãƒˆãƒ¼ã‚¯ãƒ³ã€ã¨åŒæ§˜ã®é›¢æ•£åŒ–
```

**30ç§’ã§éŸ³å£°ã‚’75ãƒˆãƒ¼ã‚¯ãƒ³ã«åœ§ç¸®ãƒ»å†æ§‹æˆã—ãŸã€‚** ç”»åƒã®ãƒ‘ãƒƒãƒãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆViTï¼‰ã¨åŒã˜ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ãŒã€éŸ³å£°ã«ã‚‚é©ç”¨ã•ã‚Œã¦ã„ã‚‹ã€‚ã“ã®é›¢æ•£è¡¨ç¾ãŒã€éŸ³å£°ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ï¼ˆTTS/Musicï¼‰ã®å…¥åŠ›ã¨ãªã‚‹ã€‚

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®3%å®Œäº†ï¼** Zone 0 ã¯ã‚¦ã‚©ãƒ¼ãƒŸãƒ³ã‚°ã‚¢ãƒƒãƒ—ã€‚æ¬¡ã¯å®Ÿéš›ã® Neural Audio Codecï¼ˆEnCodec/WavTokenizerï¼‰ã‚’è§¦ã‚Šã€éŸ³å£°ç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã‚’ä½“æ„Ÿã™ã‚‹ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” éŸ³å£°ç”Ÿæˆã®3å¤§ã‚¿ã‚¹ã‚¯

**ã‚´ãƒ¼ãƒ«**: TTSï¼ˆéŸ³å£°åˆæˆï¼‰ãƒ»Musicï¼ˆéŸ³æ¥½ç”Ÿæˆï¼‰ãƒ»Editingï¼ˆéŸ³å£°ç·¨é›†ï¼‰ã®3ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè£…ã—ã€éŸ³å£°ç”Ÿæˆã®å…¨ä½“åƒã‚’æ´ã‚€ã€‚

### 1.1 Task 1: Text-to-Speech (TTS) â€” ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éŸ³å£°ã¸

TTS ã¯ã€Œãƒ†ã‚­ã‚¹ãƒˆ â†’ éŸ³éŸ¿ç‰¹å¾´é‡ â†’ éŸ³å£°æ³¢å½¢ã€ã®2æ®µéšãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã ã€‚å¾“æ¥ã¯ Tacotron/FastSpeech ãŒä¸»æµã ã£ãŸãŒã€**Flow Matching TTS**ï¼ˆF5-TTS/E2-TTSï¼‰[^2] ãŒå˜ä¸€ãƒ¢ãƒ‡ãƒ«ã§ä¸¡æ®µéšã‚’çµ±ä¸€ã—ãŸã€‚

```julia
# F5-TTS ã®ã‚³ã‚¢: Flow Matching ã§ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ä»˜ãéŸ³å£°ç”Ÿæˆ
# dx/dt = v(x, t, text_emb) â€” ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã§æ¡ä»¶ä»˜ã‘ã•ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«å ´

function f5_tts_flow(text::String, duration_sec=2.0, sample_rate=24000)
    # 1. Text â†’ embedding (simplified: character-level embedding)
    chars = collect(text)
    vocab_size = 128  # ASCII
    embed_dim = 256
    text_emb = zeros(Float32, length(chars), embed_dim)
    for (i, c) in enumerate(chars)
        idx = min(Int(c), vocab_size)
        text_emb[i, idx] = 1.0f0  # one-hot (simplified)
    end

    # 2. Flow Matching: x0 (noise) â†’ x1 (speech)
    # Target: duration_sec * sample_rate samples
    # Tokenize: 75 tokens/sec â†’ total_tokens = duration_sec * 75
    total_tokens = Int(duration_sec * 75)
    token_dim = 128  # latent dimension per token

    # x0 ~ N(0, I) â€” random noise
    x0 = randn(Float32, total_tokens, token_dim)

    # Flow ODE: dx/dt = v(x, t, text_emb)
    steps = 10  # Integration steps (F5-TTS uses 10-32 steps)
    dt = 1.0f0 / steps
    xt = copy(x0)

    for step in 1:steps
        t = step * dt
        # Velocity field v(x, t, text) â€” simplified linear interpolation
        # Actual F5-TTS uses DiT (Diffusion Transformer) conditioned on text
        v = (1 - t) .* xt  # Simplified: move towards origin
        xt = xt .+ v .* dt
    end

    x1_latent = xt  # Final latent codes

    # 3. Decode latent â†’ waveform (VQ-VAE decoder)
    audio_length = Int(duration_sec * sample_rate)
    audio = zeros(Float32, audio_length)
    samples_per_token = div(audio_length, total_tokens)

    for i in 1:total_tokens
        # Simplified decoder: iFFT
        spectrum = zeros(ComplexF32, samples_per_token)
        spectrum[1:min(token_dim, samples_per_token)] = x1_latent[i, 1:min(token_dim, samples_per_token)]
        frame = real.(ifft(spectrum))
        start_idx = (i-1) * samples_per_token + 1
        end_idx = min(start_idx + samples_per_token - 1, audio_length)
        audio[start_idx:end_idx] = frame[1:end_idx-start_idx+1]
    end

    return audio
end

text_input = "Hello world"
audio_tts = f5_tts_flow(text_input, 2.0, 24000)
println("ã€TTS: Text â†’ Speechã€‘")
println("Input text: \"$text_input\"")
println("Output audio: $(length(audio_tts)) samples ($(length(audio_tts)/24000) sec @ 24kHz)")
println("Flow steps: 10 (vs DDPM 1000 steps)")
println("F5-TTS ã¯ ConvNeXt ã§ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¾ã‚’ refinement ã—ã€Sway Sampling ã§åŠ¹ç‡åŒ–")
```

**TTS ã®ç‰¹å¾´**: ãƒ†ã‚­ã‚¹ãƒˆ â†’ éŸ³éŸ¿ç‰¹å¾´é‡ â†’ æ³¢å½¢ã€‚F5-TTS ã¯ Flow Matching ã«ã‚ˆã‚Š10ã‚¹ãƒ†ãƒƒãƒ—ã§é«˜å“è³ªéŸ³å£°ã‚’ç”Ÿæˆã€‚

### 1.2 Task 2: Music Generation â€” ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éŸ³æ¥½ã¸

Music Generation ã¯ã€Œãƒ†ã‚­ã‚¹ãƒˆè¨˜è¿° â†’ éŸ³æ¥½æ³¢å½¢ã€ã ã€‚MusicGen[^3] ã¯ EnCodec ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‚’ Language Model ã§ç”Ÿæˆã™ã‚‹ã€‚

```julia
# MusicGen ã®ã‚³ã‚¢: LM ã§ EnCodec ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‚’ç”Ÿæˆ â†’ ãƒ‡ã‚³ãƒ¼ãƒ‰ã§éŸ³æ¥½æ³¢å½¢
# Input: "pop music with drums" â†’ Output: 30ç§’ã®éŸ³æ¥½

function musicgen_generate(prompt::String, duration_sec=30.0, sample_rate=24000)
    # 1. Prompt â†’ text embedding
    words = split(prompt)
    vocab_size = 10000
    embed_dim = 512
    text_emb = zeros(Float32, length(words), embed_dim)
    for (i, word) in enumerate(words)
        # Simplified: hash word to embedding
        idx = abs(hash(word)) % embed_dim + 1
        text_emb[i, idx] = 1.0f0
    end

    # 2. LM generates EnCodec tokens (150 tokens/sec for EnCodec 24kHz)
    tokens_per_sec = 150
    total_tokens = Int(duration_sec * tokens_per_sec)

    # EnCodec uses 4 quantizers (RVQ: Residual Vector Quantization)
    # Each quantizer has 1024-entry codebook
    n_quantizers = 4
    codebook_size = 1024

    # Generate tokens autoregressively (simplified: random)
    tokens = zeros(Int, total_tokens, n_quantizers)
    for t in 1:total_tokens
        for q in 1:n_quantizers
            # Actual MusicGen: Transformer LM predicts next token
            tokens[t, q] = rand(1:codebook_size)
        end
    end

    # 3. Decode EnCodec tokens â†’ waveform
    audio_length = Int(duration_sec * sample_rate)
    audio = randn(Float32, audio_length) .* 0.1  # Simplified: noise placeholder

    println("  EnCodec tokens: $(size(tokens)) ($(total_tokens) timesteps x $(n_quantizers) quantizers)")
    println("  Codebook: $(codebook_size) entries per quantizer")

    return audio, tokens
end

prompt = "upbeat electronic music with synthesizer"
audio_music, tokens_music = musicgen_generate(prompt, 10.0, 24000)
println("\nã€Music Generation: Text â†’ Musicã€‘")
println("Prompt: \"$prompt\"")
println("Output: $(length(audio_music)) samples ($(length(audio_music)/24000) sec)")
println("MusicGen ã¯ EnCodec ã§åœ§ç¸® â†’ LM ã§ç”Ÿæˆ â†’ ãƒ‡ã‚³ãƒ¼ãƒ‰ã§éŸ³æ¥½åˆæˆ")
println("è¨“ç·´ãƒ‡ãƒ¼ã‚¿: 20K hours licensed music (Meta internal dataset)")
```

**Music ã®ç‰¹å¾´**: EnCodec ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‚’ LM ã§ç”Ÿæˆã€‚ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ä»˜ã or ãƒ¡ãƒ­ãƒ‡ã‚£æ¡ä»¶ä»˜ãç”ŸæˆãŒå¯èƒ½ã€‚

### 1.3 Task 3: Voice Conversion â€” éŸ³å£°ã‚¹ã‚¿ã‚¤ãƒ«å¤‰æ›

Voice Conversion ã¯ã€Œè©±è€…AéŸ³å£° â†’ è©±è€…BéŸ³å£°ã€ã ã€‚Zero-shot TTSï¼ˆVALL-E 2ï¼‰[^4] ã¯3ç§’ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéŸ³å£°ã§ä»»æ„è©±è€…ã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ã§ãã‚‹ã€‚

```julia
# VALL-E 2 ã®ã‚³ã‚¢: Codec LM ã§éŸ³éŸ¿ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‚’ç”Ÿæˆ
# Input: text + 3ç§’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéŸ³å£° â†’ Output: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè©±è€…ã®å£°ã§ãƒ†ã‚­ã‚¹ãƒˆèª­ã¿ä¸Šã’

function valle2_clone_voice(text::String, prompt_audio::Vector{Float32}, sample_rate=24000)
    # 1. Prompt audio â†’ EnCodec tokens (è©±è€…æƒ…å ±ã®æŠ½å‡º)
    prompt_duration = length(prompt_audio) / sample_rate
    prompt_tokens = Int(prompt_duration * 150)  # 150 tokens/sec

    # EnCodec tokenize (simplified)
    speaker_tokens = rand(1:1024, prompt_tokens, 4)  # 4 quantizers

    # 2. Text â†’ phoneme sequence
    phonemes = collect(text)  # Simplified: char-level

    # 3. Codec LM: (phonemes, speaker_tokens) â†’ target tokens
    # VALL-E 2 uses Repetition Aware Sampling + Grouped Code Modeling
    target_duration = 2.0  # sec
    target_tokens_count = Int(target_duration * 150)

    target_tokens = zeros(Int, target_tokens_count, 4)
    for t in 1:target_tokens_count
        # Simplified: copy speaker tokens pattern
        ref_idx = mod(t - 1, prompt_tokens) + 1
        target_tokens[t, :] = speaker_tokens[ref_idx, :]
    end

    # 4. Decode tokens â†’ waveform
    audio_length = Int(target_duration * sample_rate)
    audio = randn(Float32, audio_length) .* 0.05  # Placeholder

    println("  Prompt audio: $(prompt_duration) sec â†’ $(prompt_tokens) tokens")
    println("  Generated: $(target_duration) sec â†’ $(target_tokens_count) tokens")
    println("  VALL-E 2 innovations: Repetition Aware Sampling (phoneme repetition è§£æ±º)")
    println("                        Grouped Code Modeling (inference é€Ÿåº¦å‘ä¸Š)")

    return audio
end

text_clone = "This is a cloned voice"
prompt_audio_3sec = randn(Float32, 3 * 24000) .* 0.1  # 3ç§’ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéŸ³å£°
audio_cloned = valle2_clone_voice(text_clone, prompt_audio_3sec, 24000)
println("\nã€Voice Cloning: 3ç§’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ â†’ ä»»æ„è©±è€…éŸ³å£°ã€‘")
println("Text: \"$text_clone\"")
println("Prompt: 3 sec audio sample")
println("Output: $(length(audio_cloned)) samples ($(length(audio_cloned)/24000) sec)")
println("VALL-E 2 ã¯ human parity é”æˆ â€” LibriSpeech/VCTK ã§äººé–“ä¸¦ã¿éŸ³å£°")
```

**Voice Cloning ã®ç‰¹å¾´**: 3ç§’ã‚µãƒ³ãƒ—ãƒ«ã§è©±è€…ã‚’å®Œå…¨å†ç¾ã€‚Codec LM ã®ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¹ãƒ«ãƒ¼ã€‚

### 1.4 éŸ³å£°ç”Ÿæˆã®3ã‚¿ã‚¹ã‚¯æ¯”è¼ƒè¡¨

| ã‚¿ã‚¹ã‚¯ | å…¥åŠ› | å‡ºåŠ› | ãƒ¢ãƒ‡ãƒ«ä¾‹ | åœ§ç¸®è¡¨ç¾ | ç”Ÿæˆæ–¹å¼ |
|:-------|:-----|:-----|:---------|:---------|:---------|
| **TTS** | ãƒ†ã‚­ã‚¹ãƒˆ | éŸ³å£°æ³¢å½¢ | F5-TTS / E2-TTS | 75 tokens/sec | Flow Matching |
| **Music** | ãƒ†ã‚­ã‚¹ãƒˆ/ãƒ¡ãƒ­ãƒ‡ã‚£ | éŸ³æ¥½æ³¢å½¢ | MusicGen / Stable Audio | 150 tokens/sec | Autoregressive LM |
| **Voice Clone** | ãƒ†ã‚­ã‚¹ãƒˆ + ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ | è©±è€…éŸ³å£° | VALL-E 2 / NaturalSpeech 3 | EnCodec 4 quantizers | Codec LM |

```julia
println("\nã€éŸ³å£°ç”Ÿæˆã®3å¤§ã‚¿ã‚¹ã‚¯æ¯”è¼ƒã€‘")
println("TTS:    ãƒ†ã‚­ã‚¹ãƒˆ â†’ éŸ³å£° (Flow Matching, 10 steps)")
println("Music:  ãƒ†ã‚­ã‚¹ãƒˆ â†’ éŸ³æ¥½ (LM + EnCodec, autoregressive)")
println("Clone:  3ç§’ã‚µãƒ³ãƒ—ãƒ« â†’ ä»»æ„è©±è€…éŸ³å£° (Codec LM, zero-shot)")
println("\nå…±é€šç‚¹: Neural Audio Codec ã«ã‚ˆã‚‹é›¢æ•£åŒ– â†’ ç”Ÿæˆãƒ¢ãƒ‡ãƒ«")
println("â†’ Zone 2 ã§ã€Audio Codec ã®é€²åŒ–ã‚’è¿½ã†")
```

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®10%å®Œäº†ï¼** 3ã¤ã®ã‚¿ã‚¹ã‚¯ã‚’è§¦ã£ãŸã€‚æ¬¡ã¯ã€Œãªãœ Flow Matching ãŒ TTS ã‚’æ”¯é…ã—ãŸã®ã‹ï¼Ÿã€ã‚’ç†è§£ã™ã‚‹ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” Audio Codec ã®é€²åŒ–ã¨ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚·ãƒ•ãƒˆ

**ã‚´ãƒ¼ãƒ«**: Neural Audio Codec ã®é€²åŒ–ï¼ˆSoundStream â†’ EnCodec â†’ WavTokenizerï¼‰ã¨ã€Autoregressive â†’ Flow Matching TTS ã¸ã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚·ãƒ•ãƒˆã‚’ç†è§£ã™ã‚‹ã€‚

### 2.1 éŸ³å£°ç”Ÿæˆã®æ­´å² â€” 3ã¤ã®æ™‚ä»£

éŸ³å£°ç”Ÿæˆã¯3ã¤ã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’çµŒãŸ:

```mermaid
graph LR
    A["æ™‚ä»£1: çµ±è¨ˆçš„TTS<br/>2000-2015<br/>HMM/GMM"] -->|DNNé©å‘½| B["æ™‚ä»£2: DNN TTS<br/>2015-2023<br/>Tacotron/FastSpeech"]
    B -->|Flow Matching| C["æ™‚ä»£3: Flow TTS<br/>2023-2026<br/>F5-TTS/VALL-E"]
    A2["vocoder: WORLD"] --> B2["vocoder: WaveNet"]
    B2 --> C2["codec: EnCodec"]
    style C fill:#ffd700
    style C2 fill:#ffd700
```

#### æ™‚ä»£1: çµ±è¨ˆçš„TTSï¼ˆ2000-2015ï¼‰
- **æ‰‹æ³•**: HMMï¼ˆéš ã‚Œãƒãƒ«ã‚³ãƒ•ãƒ¢ãƒ‡ãƒ«ï¼‰+ éŸ³éŸ¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿äºˆæ¸¬
- **Vocoder**: WORLD / STRAIGHTï¼ˆä¿¡å·å‡¦ç†ãƒ™ãƒ¼ã‚¹ï¼‰
- **å•é¡Œ**: æ©Ÿæ¢°çš„ãªéŸ³å£°ã€éŸ»å¾‹åˆ¶å¾¡å›°é›£ã€å¤§é‡ã®æ‰‹ä½œæ¥­ç‰¹å¾´é‡

#### æ™‚ä»£2: DNN TTSï¼ˆ2015-2023ï¼‰
- **æ‰‹æ³•**: Tacotronï¼ˆSeq2Seq Attentionï¼‰â†’ FastSpeechï¼ˆNon-autoregressiveï¼‰
- **Vocoder**: WaveNet â†’ HiFi-GANï¼ˆNeural Vocoder é©å‘½ï¼‰
- **å•é¡Œ**: 2æ®µéšãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆAcoustic Model + Vocoderï¼‰ã®è¤‡é›‘ã•ã€æ¨è«–é€Ÿåº¦

#### æ™‚ä»£3: Flow Matching TTSï¼ˆ2023-2026ï¼‰
- **æ‰‹æ³•**: F5-TTS / E2-TTSï¼ˆFlow Matchingï¼‰+ VALL-E 2ï¼ˆCodec LMï¼‰
- **Codec**: EnCodec / WavTokenizerï¼ˆæ¥µé™åœ§ç¸® + é«˜å“è³ªï¼‰
- **ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¹ãƒ«ãƒ¼**: å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚­ã‚¹ãƒˆ â†’ æ³¢å½¢ã€10ã‚¹ãƒ†ãƒƒãƒ—ã§ç”Ÿæˆã€Zero-shot è©±è€…ã‚¯ãƒ­ãƒ¼ãƒ³

**æœ¬è³ªçš„ãªå¤‰åŒ–**: æ™‚ä»£2ã¯ã€ŒAcoustic Modelï¼ˆãƒ¡ãƒ«å‘¨æ³¢æ•°ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ äºˆæ¸¬ï¼‰+ Vocoderï¼ˆæ³¢å½¢ç”Ÿæˆï¼‰ã€ã®2æ®µéšã ã£ãŸãŒã€æ™‚ä»£3ã¯ **Codecï¼ˆéŸ³å£°â†’é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰+ Flow/LMï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆï¼‰** ã®1æ®µéšã«çµ±åˆã•ã‚ŒãŸã€‚

### 2.2 Neural Audio Codec ã®é€²åŒ– â€” åœ§ç¸®ç‡ç«¶äº‰

Neural Audio Codec ã¯ã€ŒéŸ³å£° â†’ é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã€ã¸ã®å¤‰æ›ã ã€‚ç”»åƒã® VQ-VAE/VQ-GAN ã«ç›¸å½“ã™ã‚‹ã€‚

| Codec | å¹´ | ãƒˆãƒ¼ã‚¯ãƒ³/ç§’ | åœ§ç¸®ç‡ | Codebook | ç‰¹å¾´ | è«–æ–‡ |
|:------|:---|:-----------|:-------|:---------|:-----|:-----|
| **SoundStream** | 2021 | 320 | 75x | 1024 x 8 | RVQå°å…¥ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ  | Google [^5] |
| **EnCodec** | 2022 | 150 | 160x | 1024 x 4 | Bandwidth scalable | Meta [^6] |
| **WavTokenizer** | 2024 | **75** | **320x** | 1024 x 1 | å˜ä¸€é‡å­åŒ–å™¨ | ICLR 2025 [^1] |
| **Mimi** | 2024 | 80 | 300x | 2048 x 1 | Semantic-rich | Kyutai [^7] |

**åœ§ç¸®ç‡ã®é€²åŒ–**: 24kHz éŸ³å£°1ç§’ = 24,000ã‚µãƒ³ãƒ—ãƒ«
- SoundStream: 320ãƒˆãƒ¼ã‚¯ãƒ³ â†’ 75xåœ§ç¸®
- EnCodec: 150ãƒˆãƒ¼ã‚¯ãƒ³ â†’ 160xåœ§ç¸®
- **WavTokenizer: 75ãƒˆãƒ¼ã‚¯ãƒ³ â†’ 320xåœ§ç¸®**

```julia
# åœ§ç¸®ç‡ã®è¨ˆç®—
sample_rate = 24000  # 24kHz
audio_1sec_samples = sample_rate

codecs = [
    ("SoundStream", 320, 8),
    ("EnCodec", 150, 4),
    ("WavTokenizer", 75, 1),
    ("Mimi", 80, 1)
]

println("ã€Neural Audio Codec æ¯”è¼ƒã€‘")
println("éŸ³å£°1ç§’ @ 24kHz = $audio_1sec_samples samples\n")
for (name, tokens_per_sec, n_quantizers) in codecs
    compression = div(audio_1sec_samples, tokens_per_sec)
    total_tokens = tokens_per_sec * n_quantizers
    println("$name:")
    println("  Tokens/sec: $tokens_per_sec x $n_quantizers quantizers = $total_tokens total")
    println("  Compression: $(compression)x")
    println("  1ç§’éŸ³å£° â†’ $(tokens_per_sec)ãƒˆãƒ¼ã‚¯ãƒ³")
    println()
end

println("â†’ WavTokenizer ã¯å˜ä¸€é‡å­åŒ–å™¨ã§æœ€å¤§åœ§ç¸®ã‚’å®Ÿç¾")
println("  Key: Broader VQ space + Extended context + Improved attention")
```

**WavTokenizer ã®é©å‘½**[^1]:
1. **å˜ä¸€é‡å­åŒ–å™¨**: RVQï¼ˆResidual VQï¼‰ã®éšå±¤ã‚’1å±¤ã«çµ±ä¸€ â†’ æ¨è«–é«˜é€ŸåŒ–
2. **Broader VQ space**: Codebook ã‚’åŠ¹ç‡çš„ã«æ´»ç”¨ï¼ˆ1024ã‚¨ãƒ³ãƒˆãƒªã§ååˆ†ï¼‰
3. **Extended context**: æ™‚é–“æ–¹å‘ã®æ–‡è„ˆçª“ã‚’æ‹¡å¤§ â†’ é•·æœŸä¾å­˜æ€§ã‚’æ•æ‰
4. **Semantic-rich**: æ„å‘³æƒ…å ±ã‚’ä¿æŒï¼ˆéŸ³ç´ ãƒ»éŸ»å¾‹ãƒ»è©±è€…ç‰¹æ€§ï¼‰

### 2.3 ãªãœ Flow Matching ãŒ TTS ã‚’æ”¯é…ã—ãŸã®ã‹ï¼Ÿ

å¾“æ¥ã® Autoregressive TTSï¼ˆTacotron/VALL-Eï¼‰ã¨ Flow Matching TTSï¼ˆF5-TTSï¼‰ã®é•ã„ã‚’è¦‹ã‚‹ã€‚

#### Autoregressive TTS ã®å•é¡Œ

**VALL-Eï¼ˆåˆä»£ã€2023ï¼‰**[^8]:
- EnCodec ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‚’ autoregressive ã«ç”Ÿæˆ: $p(x_1, ..., x_T) = \prod_{t=1}^T p(x_t | x_{<t})$
- **å•é¡Œ1: Phoneme repetition** â€” åŒã˜éŸ³ç´ ãŒç¹°ã‚Šè¿”ã•ã‚Œã‚‹ï¼ˆ"Hello" â†’ "Hehehehello"ï¼‰
- **å•é¡Œ2: é…ã„** â€” 1ãƒˆãƒ¼ã‚¯ãƒ³ãšã¤é€æ¬¡ç”Ÿæˆï¼ˆ150ãƒˆãƒ¼ã‚¯ãƒ³/ç§’ â†’ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ä»¥ä¸‹ï¼‰

**VALL-E 2ï¼ˆ2024ï¼‰**[^4] ã¯ã“ã‚Œã‚’è§£æ±º:
- **Repetition Aware Sampling**: ãƒ‡ã‚³ãƒ¼ãƒ‰å±¥æ­´ã®ãƒˆãƒ¼ã‚¯ãƒ³ç¹°ã‚Šè¿”ã—ã‚’è€ƒæ…®
- **Grouped Code Modeling**: Codec codes ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ– â†’ ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·çŸ­ç¸® â†’ æ¨è«–é«˜é€ŸåŒ–
- **çµæœ**: LibriSpeech/VCTK ã§ **human parity é”æˆ** â€” äººé–“ä¸¦ã¿éŸ³å£°å“è³ª

#### Flow Matching TTS ã®åˆ©ç‚¹

**F5-TTS / E2-TTSï¼ˆ2024ï¼‰**[^2]:
- Flow Matching: $\frac{dx}{dt} = v(x, t, \text{text})$ â€” é€£ç¶šçš„ãªå¤‰æ›
- **åˆ©ç‚¹1: å˜ç´”ãªè¨“ç·´** â€” ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ•ã‚£ãƒ©ãƒ¼ãƒˆãƒ¼ã‚¯ãƒ³ã§ãƒ‘ãƒ‡ã‚£ãƒ³ã‚° â†’ åŒã˜é•·ã•ã«ã—ã¦ denoising
- **åˆ©ç‚¹2: é«˜é€Ÿæ¨è«–** â€” 10-32ã‚¹ãƒ†ãƒƒãƒ—ã§ç”Ÿæˆï¼ˆvs Autoregressive ã®150ã‚¹ãƒ†ãƒƒãƒ—ï¼‰
- **åˆ©ç‚¹3: åˆ¶å¾¡æ€§** â€” Sway Sampling ã§æ¨è«–æ™‚ã«å“è³ª-é€Ÿåº¦ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•èª¿æ•´å¯èƒ½

```julia
# Autoregressive vs Flow Matching ã®æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—æ¯”è¼ƒ
function compare_inference_steps()
    duration_sec = 5.0
    sample_rate = 24000

    # Autoregressive (VALL-E): 1ãƒˆãƒ¼ã‚¯ãƒ³ãšã¤ç”Ÿæˆ
    ar_tokens_per_sec = 150
    ar_total_tokens = Int(duration_sec * ar_tokens_per_sec)
    ar_steps = ar_total_tokens  # å„ãƒˆãƒ¼ã‚¯ãƒ³ = 1 forward pass

    # Flow Matching (F5-TTS): ODEç©åˆ†
    fm_steps = 10  # F5-TTS default

    println("ã€Autoregressive vs Flow Matchingã€‘")
    println("ç”Ÿæˆæ™‚é–“: $(duration_sec) ç§’\n")
    println("Autoregressive (VALL-E):")
    println("  Steps: $ar_steps (1ãƒˆãƒ¼ã‚¯ãƒ³/step)")
    println("  Time: é€æ¬¡ç”Ÿæˆ â†’ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ä»¥ä¸‹")
    println()
    println("Flow Matching (F5-TTS):")
    println("  Steps: $fm_steps (ä¸¦åˆ—ç©åˆ†)")
    println("  Time: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã® 10x é«˜é€Ÿ")
    println()
    println("é€Ÿåº¦æ¯”: $(div(ar_steps, fm_steps))x faster (Flow Matching)")
end

compare_inference_steps()
```

**çµè«–**: Flow Matching ã¯ Autoregressive ã®é€Ÿåº¦å•é¡Œã‚’è§£æ±ºã—ã€VALL-E 2 ã¨åŒç­‰ã®å“è³ªã‚’å®Ÿç¾ã€‚2025å¹´ä»¥é™ã® TTS ã¯ Flow Matching ãŒä¸»æµã«ãªã‚‹ã€‚

### 2.4 Music Generation ã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ  â€” LM + Codec

éŸ³æ¥½ç”Ÿæˆã¯ TTS ã¨ç•°ãªã‚Šã€**é•·æ™‚é–“ãƒ»è¤‡é›‘ãªæ§‹é€ **ã‚’æ‰±ã†ã€‚

**MusicGenï¼ˆMeta, 2023ï¼‰**[^3]:
- **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: LMï¼ˆTransformerï¼‰+ EnCodec
- **è¨“ç·´ãƒ‡ãƒ¼ã‚¿**: 20K hours licensed musicï¼ˆMeta internal 10K hours + ShutterStock 25K + Pond5 365K tracksï¼‰
- **ç”Ÿæˆæ–¹å¼**: Text/Melody-conditioned autoregressive generation
- **åˆ©ç‚¹**: ã‚·ãƒ³ãƒ—ãƒ«ãƒ»é«˜å“è³ªãƒ»åˆ¶å¾¡å¯èƒ½ï¼ˆãƒ†ã‚­ã‚¹ãƒˆ or ãƒ¡ãƒ­ãƒ‡ã‚£æ¡ä»¶ä»˜ãï¼‰

**Stable Audioï¼ˆ2024ï¼‰**[^9]:
- **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: DiTï¼ˆDiffusion Transformerï¼‰+ Timing embeddings
- **ç”Ÿæˆé•·**: æœ€å¤§ **4åˆ†45ç§’** ã®é•·æ™‚é–“ç”Ÿæˆï¼ˆMusicGen ã¯ 30ç§’ï¼‰
- **ç‰¹å¾´**: Text + Timing controlï¼ˆ"0:00-0:30: intro, 0:30-2:00: verse, ..."ï¼‰

```mermaid
graph TD
    A[Text Prompt<br/>'upbeat pop with drums'] --> B[Text Encoder<br/>T5/CLAP]
    B --> C[LM / DiT<br/>Token Generation]
    C --> D[EnCodec Tokens<br/>150/sec x 4 quantizers]
    D --> E[Decoder<br/>EnCodec/VAE]
    E --> F[Audio Waveform<br/>44.1kHz stereo]

    style C fill:#ffeb3b
```

**å•†ç”¨ã‚µãƒ¼ãƒ“ã‚¹**: Suno v4.5 / Udio
- **å“è³ª**: ãƒ—ãƒ­ãƒ¬ãƒ™ãƒ«ã®ä½œæ›²ï¼ˆæ­Œè©ãƒ»ãƒœãƒ¼ã‚«ãƒ«ãƒ»æ¥½å™¨ãƒ»ãƒŸãƒƒã‚¯ã‚¹ï¼‰
- **é€Ÿåº¦**: æ•°ç§’ã§3åˆ†ã®æ¥½æ›²ç”Ÿæˆ
- **è«–äº‰**: è‘—ä½œæ¨©ãƒ»ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆæ¨©åˆ©ãƒ»è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®åˆæ³•æ€§

### 2.5 æ¾å°¾ãƒ»å²©æ¾¤ç ”ã¨ã®å·®åˆ¥åŒ– â€” Course V ã®ç‹¬è‡ªæ€§

| è¦³ç‚¹ | æ¾å°¾ãƒ»å²©æ¾¤ç ” (2026Spring) | æœ¬ã‚·ãƒªãƒ¼ã‚º Course V |
|:-----|:--------------------------|:-------------------|
| **éŸ³å£°ã®æ‰±ã„** | ãªã—ï¼ˆç”»åƒç”Ÿæˆã®ã¿ï¼‰ | **éŸ³å£°å°‚ç”¨è¬›ç¾©** (ç¬¬44å›) |
| **æ‰±ã†æ‰‹æ³•** | ãªã— | Codec (EnCodec/WavTokenizer) + TTS (F5/VALL-E 2) + Music (MusicGen/Stable Audio) |
| **ç†è«–** | ãªã— | **Flow Matching for Audio** ã®å®Œå…¨å°å‡º |
| **å®Ÿè£…** | ãªã— | **Julia (Flow Matching TTS) + Rust (ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–) + Elixir (é…ä¿¡)** |
| **æœ€æ–°æ€§** | 2023å¹´ã¾ã§ | **2025-2026**: WavTokenizer / F5-TTS / Stable Audio / KAD metric |

**æœ¬è¬›ç¾©ã®ç‹¬è‡ªæ€§**:
1. **Neural Audio Codec é€²åŒ–å²** ã‚’å®Œå…¨æ•´ç†ï¼ˆSoundStream â†’ WavTokenizerï¼‰
2. **Flow Matching for Audio** ã®æ•°å¼å°å‡º + Juliaå®Ÿè£…
3. **Zero-shot TTS** ã®åŸç†ã¨å®Ÿè£…ï¼ˆVALL-E 2 / F5-TTSï¼‰
4. **Music Generation** ã®æœ€æ–°æ‰‹æ³•ï¼ˆMusicGen / Stable Audioï¼‰
5. **è©•ä¾¡æŒ‡æ¨™** ã®æœ€æ–°å‹•å‘ï¼ˆFAD â†’ KAD[^10]ï¼‰

:::details ãƒˆãƒ­ã‚¤ã®æœ¨é¦¬æŒ¯ã‚Šè¿”ã‚Š: ç¬¬17å›ã§ Julia/Rust/Elixir ãŒå½“ãŸã‚Šå‰ã«
ç¬¬17å›ã§ Julia/Rust/Elixir ã®3è¨€èªãŒæƒã„ã€ã‚‚ã† Python ã«æˆ»ã‚‹ã“ã¨ã¯ãªã‹ã£ãŸã€‚

**Before (ç¬¬16å›ã¾ã§)**:
- Python 100% â€” NumPy/PyTorch ã§å®Ÿè£…
- ã€Œé…ã„ã‘ã©ä»•æ–¹ãªã„ã€

**After (ç¬¬44å›)**:
- **Julia**: Audio Flow Matching è¨“ç·´ï¼ˆæ•°å¼â†’ã‚³ãƒ¼ãƒ‰ãŒ1:1ï¼‰
- **Rust**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°æ¨è«–ï¼ˆã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ãƒ»ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ï¼‰
- **Elixir**: åˆ†æ•£éŸ³å£°é…ä¿¡ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ»è€éšœå®³æ€§ï¼‰
- **Python**: æŸ»èª­è€…ç”¨ï¼ˆèª­ã‚€ã ã‘ï¼‰

3è¨€èªãŒå½“ãŸã‚Šå‰ã®æ­¦å™¨ã«ãªã£ãŸã€‚ã“ã‚ŒãŒã€Œãƒˆãƒ­ã‚¤ã®æœ¨é¦¬ã€ã®æˆæœã ã€‚
:::

### 2.6 æœ¬è¬›ç¾©ã®æ§‹æˆ

æœ¬è¬›ç¾©ã¯ä»¥ä¸‹ã®æ§‹æˆã§é€²ã‚€:

**Part A: Neural Audio Codec ç†è«–** (Zone 3.1-3.3, ~600è¡Œ)
- VQ-VAE for Audio (SoundStream)
- RVQ vs Single VQ (EnCodec vs WavTokenizer)
- Semantic tokens (Supervised vs Unsupervised)

**Part B: Flow Matching for TTS** (Zone 3.4-3.6, ~600è¡Œ)
- E2-TTS / F5-TTS å®Œå…¨å°å‡º
- Sway Sampling æˆ¦ç•¥
- ConvNeXt text refinement

**Part C: Codec Language Models** (Zone 3.7-3.8, ~600è¡Œ)
- VALL-E 2ï¼ˆRepetition Aware Sampling + Grouped Code Modelingï¼‰
- NaturalSpeech 3ï¼ˆFACodec + Diffusionï¼‰
- CosyVoiceï¼ˆSupervised semantic tokensï¼‰

**Part D: Music Generation** (Zone 3.9-3.10, ~400è¡Œ)
- MusicGenï¼ˆLM + EnCodecï¼‰
- Stable Audioï¼ˆDiT + Timing controlï¼‰
- è©•ä¾¡æŒ‡æ¨™ï¼ˆFAD â†’ KADï¼‰

```julia
println("\nã€Course V ç¬¬44å›ã®æ—…è·¯ãƒãƒƒãƒ—ã€‘")
println("Zone 3.1-3.3: Neural Audio Codec (SoundStream â†’ WavTokenizer)")
println("Zone 3.4-3.6: Flow Matching TTS (F5-TTS å®Œå…¨å°å‡º)")
println("Zone 3.7-3.8: Codec LM (VALL-E 2 / NaturalSpeech 3)")
println("Zone 3.9-3.10: Music Generation (MusicGen / Stable Audio)")
println("\nâ†’ Zone 3 ã§ã€ã“ã‚Œã‚‰å…¨ã¦ã‚’æ•°å¼ã§ç†è§£ã™ã‚‹")
```

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®20%å®Œäº†ï¼** ç›´æ„Ÿçš„ç†è§£ãŒã§ããŸã€‚æ¬¡ã¯æ•°å­¦ã®æœ¬ä¸¸ â€” Zone 3 ã€Œæ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ã€ã§ã€Audio Codec ã¨ Flow Matching ã‚’å®Œå…¨ã«å°å‡ºã™ã‚‹ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” Audio Codec ã¨ Flow Matching ã®ç†è«–

**ã‚´ãƒ¼ãƒ«**: Neural Audio Codecï¼ˆVQ-VAE/RVQ/WavTokenizerï¼‰ã¨ Flow Matching for TTSï¼ˆF5-TTS/E2-TTSï¼‰ã®æ•°å­¦çš„åŸºç›¤ã‚’ã€å®Œå…¨ã«å°å‡ºã™ã‚‹ã€‚

ã“ã®ã‚¾ãƒ¼ãƒ³ã¯æœ¬è¬›ç¾©ã®å¿ƒè‡“éƒ¨ã ã€‚**ãƒšãƒ³ã¨ç´™ã‚’ç”¨æ„ã—ã¦**ã€å„å°å‡ºã‚’è‡ªåˆ†ã®æ‰‹ã§è¿½ã†ã“ã¨ã€‚

---

### 3.1 Neural Audio Codec ã®åŸºç¤ â€” VQ-VAE for Audio

#### 3.1.1 éŸ³å£°ã®é›¢æ•£åŒ–å•é¡Œ

**å•é¡Œè¨­å®š**: é€£ç¶šéŸ³å£°æ³¢å½¢ $x \in \mathbb{R}^T$ï¼ˆ$T$ = ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼‰ã‚’ã€é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³åˆ— $z \in \{1, ..., K\}^L$ï¼ˆ$L \ll T$, $K$ = Codebook sizeï¼‰ã«åœ§ç¸®ã—ãŸã„ã€‚

**è¦æ±‚**:
1. **é«˜åœ§ç¸®ç‡**: $L / T \ll 1$ï¼ˆä¾‹: 24,000ã‚µãƒ³ãƒ—ãƒ« â†’ 75ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
2. **é«˜å“è³ªå†æ§‹æˆ**: $\hat{x} \approx x$ï¼ˆçŸ¥è¦šçš„å“è³ªï¼‰
3. **æ„å‘³ä¿å­˜**: ãƒˆãƒ¼ã‚¯ãƒ³ $z$ ã«éŸ³ç´ ãƒ»éŸ»å¾‹ãƒ»è©±è€…æƒ…å ±ãŒä¿å­˜ã•ã‚Œã‚‹

**VQ-VAE ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**[^11]:
1. Encoder $E: \mathbb{R}^T \to \mathbb{R}^{L \times D}$ â€” é€£ç¶šæ½œåœ¨è¡¨ç¾ $z_e = E(x)$
2. Vector Quantization $Q: \mathbb{R}^D \to \{e_1, ..., e_K\}$ â€” æœ€è¿‘å‚ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã‚¨ãƒ³ãƒˆãƒªã«ç½®ãæ›ãˆ
3. Decoder $D: \mathbb{R}^{L \times D} \to \mathbb{R}^T$ â€” æ³¢å½¢å†æ§‹æˆ $\hat{x} = D(z_q)$

#### 3.1.2 Vector Quantization ã®å®šå¼åŒ–

**Encoder å‡ºåŠ›**: $z_e = E(x) \in \mathbb{R}^{L \times D}$ï¼ˆ$L$ timesteps, $D$ dimensionsï¼‰

**Codebook**: $\mathcal{C} = \{e_k\}_{k=1}^K \subset \mathbb{R}^D$ï¼ˆ$K$ ã‚¨ãƒ³ãƒˆãƒªã€å„ $e_k \in \mathbb{R}^D$ï¼‰

**Quantization**: å„ $z_e^{(i)} \in \mathbb{R}^D$ï¼ˆ$i = 1, ..., L$ï¼‰ã‚’æœ€è¿‘å‚ $e_k$ ã«ç½®ãæ›ãˆ:

$$
z_q^{(i)} = e_{k^*}, \quad k^* = \arg\min_{k \in \{1,...,K\}} \| z_e^{(i)} - e_k \|_2
$$

**é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³**: $z^{(i)} = k^*$ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨˜éŒ²ï¼‰

```julia
# VQ-VAE ã® Vector Quantization
function vector_quantization(z_e::Matrix{Float32}, codebook::Matrix{Float32})
    # z_e: (L, D) â€” encoder output
    # codebook: (K, D) â€” K codebook entries
    L, D = size(z_e)
    K = size(codebook, 1)

    tokens = zeros(Int, L)
    z_q = zeros(Float32, L, D)

    for i in 1:L
        # Find nearest codebook entry
        distances = [norm(z_e[i, :] - codebook[k, :]) for k in 1:K]
        k_star = argmin(distances)

        tokens[i] = k_star
        z_q[i, :] = codebook[k_star, :]
    end

    return tokens, z_q
end

# Example
L, D, K = 75, 128, 1024
z_e = randn(Float32, L, D)
codebook = randn(Float32, K, D)
tokens, z_q = vector_quantization(z_e, codebook)

println("ã€Vector Quantizationã€‘")
println("Encoder output z_e: $(size(z_e)) (L=$L timesteps, D=$D dims)")
println("Codebook: $(size(codebook)) (K=$K entries)")
println("Quantized z_q: $(size(z_q))")
println("Discrete tokens: $(size(tokens)) âˆˆ {1,...,$K}")
println("\nå„ timestep ã§æœ€è¿‘å‚ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã‚¨ãƒ³ãƒˆãƒªã‚’é¸æŠ")
```

**Quantization ã®æ€§è³ª**:
- **ä¸é€£ç¶š**: $z_q$ ã¯ $z_e$ ã®ä¸é€£ç¶šé–¢æ•°ï¼ˆæœ€è¿‘å‚ã§é›¢æ•£çš„ã«é£›ã¶ï¼‰
- **å‹¾é…å•é¡Œ**: $\frac{\partial z_q}{\partial z_e}$ ãŒå®šç¾©ã§ããªã„ï¼ˆå¾®åˆ†ä¸å¯èƒ½ï¼‰

#### 3.1.3 Straight-Through Estimator

VQ ã¯å¾®åˆ†ä¸å¯èƒ½ã ãŒã€**Straight-Through Estimator**[^12] ã§å‹¾é…ã‚’è¿‘ä¼¼ã™ã‚‹:

**Forward pass**: $z_q = \text{quantize}(z_e)$ï¼ˆæœ€è¿‘å‚ï¼‰

**Backward pass**: $\frac{\partial \mathcal{L}}{\partial z_e} \approx \frac{\partial \mathcal{L}}{\partial z_q}$ï¼ˆå‹¾é…ã‚’ã‚³ãƒ”ãƒ¼ï¼‰

ã“ã‚Œã«ã‚ˆã‚Šã€End-to-End è¨“ç·´ãŒå¯èƒ½ã«ãªã‚‹ã€‚

**VQ-VAE æå¤±é–¢æ•°**:

$$
\mathcal{L} = \underbrace{\| x - \hat{x} \|^2}_{\text{Reconstruction}} + \underbrace{\| \text{sg}[z_e] - z_q \|^2}_{\text{Codebook loss}} + \beta \underbrace{\| z_e - \text{sg}[z_q] \|^2}_{\text{Commitment loss}}
$$

- **Reconstruction loss**: ãƒ‡ã‚³ãƒ¼ãƒ€è¨“ç·´ï¼ˆ$\hat{x} = D(z_q)$ ãŒ $x$ ã«è¿‘ã¥ãï¼‰
- **Codebook loss**: ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯æ›´æ–°ï¼ˆ$z_q$ ãŒ $z_e$ ã«è¿‘ã¥ãï¼‰
- **Commitment loss**: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€è¨“ç·´ï¼ˆ$z_e$ ãŒ $z_q$ ã«è¿‘ã¥ãã€$\beta = 0.25$ typicalï¼‰
- $\text{sg}[\cdot]$ = stop gradientï¼ˆå‹¾é…ã‚’æ­¢ã‚ã‚‹ï¼‰

```julia
# VQ-VAE æå¤±é–¢æ•°ã®è¨ˆç®—
function vqvae_loss(x::Vector{Float32}, x_hat::Vector{Float32},
                    z_e::Matrix{Float32}, z_q::Matrix{Float32}, Î²=0.25f0)
    # Reconstruction loss
    recon_loss = mean((x .- x_hat).^2)

    # Codebook loss: ||sg[z_e] - z_q||Â²
    # sg[z_e] means z_e without gradient
    codebook_loss = mean((z_e .- z_q).^2)  # In practice, detach z_e

    # Commitment loss: ||z_e - sg[z_q]||Â²
    commitment_loss = mean((z_e .- z_q).^2)  # In practice, detach z_q

    total_loss = recon_loss + codebook_loss + Î² * commitment_loss

    return total_loss, recon_loss, codebook_loss, commitment_loss
end

# Example
x = randn(Float32, 24000)
x_hat = randn(Float32, 24000)
z_e_sample = randn(Float32, 75, 128)
z_q_sample = randn(Float32, 75, 128)

total, recon, cb, commit = vqvae_loss(x, x_hat, z_e_sample, z_q_sample)
println("\nã€VQ-VAE æå¤±é–¢æ•°ã€‘")
println("Reconstruction loss: $recon")
println("Codebook loss:       $cb")
println("Commitment loss:     $commit (Î²=0.25)")
println("Total loss:          $total")
println("\nCodebook lossã§ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯æ›´æ–°ã€Commitment lossã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€è¨“ç·´")
```

### 3.2 Residual Vector Quantization (RVQ) â€” å¤šæ®µéšé‡å­åŒ–

#### 3.2.1 RVQ ã®å‹•æ©Ÿ

**å•é¡Œ**: å˜ä¸€ VQï¼ˆ1ã¤ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ï¼‰ã§ã¯ã€è¤‡é›‘ãªéŸ³å£°ã®å…¨æƒ…å ±ã‚’ $K$ ã‚¨ãƒ³ãƒˆãƒªã§è¡¨ç¾ã§ããªã„ã€‚

**è§£æ±º**: **éšå±¤çš„é‡å­åŒ–** â€” æ®‹å·®ã‚’è¤‡æ•°å›é‡å­åŒ–ã™ã‚‹ã€‚

**RVQ ã®ã‚¢ã‚¤ãƒ‡ã‚¢**[^5]:
1. ç¬¬1æ®µéš: $z_e^{(1)} = z_e$, $z_q^{(1)} = Q_1(z_e^{(1)})$
2. æ®‹å·®è¨ˆç®—: $r^{(1)} = z_e^{(1)} - z_q^{(1)}$
3. ç¬¬2æ®µéš: $z_q^{(2)} = Q_2(r^{(1)})$
4. æ®‹å·®è¨ˆç®—: $r^{(2)} = r^{(1)} - z_q^{(2)}$
5. ... $N_q$ æ®µéšã¾ã§åå¾©

**æœ€çµ‚é‡å­åŒ–è¡¨ç¾**:

$$
z_q = z_q^{(1)} + z_q^{(2)} + \cdots + z_q^{(N_q)} = \sum_{n=1}^{N_q} z_q^{(n)}
$$

**ãƒˆãƒ¼ã‚¯ãƒ³æ•°**: $N_q$ å€‹ã®ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå„æ®µéš1å€‹ï¼‰per timestep

#### 3.2.2 RVQ ã®å®Œå…¨å°å‡º

**Encoder å‡ºåŠ›**: $z_e \in \mathbb{R}^{L \times D}$

**Codebooks**: $\{\mathcal{C}_n\}_{n=1}^{N_q}$, each $\mathcal{C}_n = \{e_k^{(n)}\}_{k=1}^K \subset \mathbb{R}^D$

**Quantization process** (for each timestep $i$):

1. $z_e^{(1)} = z_e^{(i)}$
2. For $n = 1$ to $N_q$:
   - $k_n^* = \arg\min_{k} \| z_e^{(n)} - e_k^{(n)} \|_2$
   - $z_q^{(n)} = e_{k_n^*}^{(n)}$
   - $z_e^{(n+1)} = z_e^{(n)} - z_q^{(n)}$ (residual)
3. $z_q^{(i)} = \sum_{n=1}^{N_q} z_q^{(n)}$

**Discrete representation**: $(k_1^*, k_2^*, ..., k_{N_q}^*)$ â€” $N_q$ ãƒˆãƒ¼ã‚¯ãƒ³ per timestep

```julia
# Residual Vector Quantization
function residual_vector_quantization(z_e::Matrix{Float32}, codebooks::Vector{Matrix{Float32}})
    # z_e: (L, D)
    # codebooks: vector of N_q codebooks, each (K, D)
    L, D = size(z_e)
    N_q = length(codebooks)
    K = size(codebooks[1], 1)

    tokens = zeros(Int, L, N_q)
    z_q_total = zeros(Float32, L, D)

    for i in 1:L
        residual = z_e[i, :]

        for n in 1:N_q
            # Quantize residual with codebook n
            distances = [norm(residual - codebooks[n][k, :]) for k in 1:K]
            k_star = argmin(distances)

            tokens[i, n] = k_star
            z_q_n = codebooks[n][k_star, :]
            z_q_total[i, :] += z_q_n

            # Update residual
            residual = residual - z_q_n
        end
    end

    return tokens, z_q_total
end

# Example: EnCodec uses N_q = 4 quantizers
N_q = 4
K = 1024
codebooks_rvq = [randn(Float32, K, D) for _ in 1:N_q]

tokens_rvq, z_q_rvq = residual_vector_quantization(z_e, codebooks_rvq)

println("\nã€Residual Vector Quantization (RVQ)ã€‘")
println("Encoder output z_e: $(size(z_e))")
println("Codebooks: $N_q x (K=$K, D=$D)")
println("Tokens: $(size(tokens_rvq)) â€” $N_q tokens/timestep")
println("Quantized z_q: $(size(z_q_rvq))")
println("\nEnCodec: 4 quantizers, 150 tokens/sec â†’ 600 total tokens/sec")
println("WavTokenizer: 1 quantizer, 75 tokens/sec â†’ 75 total tokens/sec (5x compression)")
```

**RVQ ã®åˆ©ç‚¹**:
- **è¡¨ç¾åŠ›å‘ä¸Š**: $K^{N_q}$ å€‹ã®æœ‰åŠ¹ã‚¨ãƒ³ãƒˆãƒªï¼ˆEnCodec: $1024^4 \approx 10^{12}$ï¼‰
- **éšå±¤çš„**: ç²—ã„æƒ…å ±ï¼ˆ1æ®µç›®ï¼‰â†’ ç´°ã‹ã„æƒ…å ±ï¼ˆNæ®µç›®ï¼‰

**RVQ ã®å•é¡Œ**:
- **ãƒˆãƒ¼ã‚¯ãƒ³æ•°å¢—åŠ **: $N_q$ å€ã®ãƒˆãƒ¼ã‚¯ãƒ³ â†’ ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ï¼ˆLMï¼‰ã®è² æ‹…å¢—
- **æ¨è«–é…å»¶**: å„æ®µéšã‚’é€æ¬¡å‡¦ç† â†’ ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·

### 3.3 WavTokenizer â€” å˜ä¸€é‡å­åŒ–å™¨ã«ã‚ˆã‚‹æ¥µé™åœ§ç¸®

#### 3.3.1 WavTokenizer ã®è¨­è¨ˆå“²å­¦

**å•ã„**: RVQ ã‚’ä½¿ã‚ãšã«ã€å˜ä¸€ VQ ã§é«˜å“è³ªã‚’å®Ÿç¾ã§ãã‚‹ã‹ï¼Ÿ

**WavTokenizer ã®ç­”ãˆ**[^1]:
1. **Broader VQ space**: Codebook ã®æœ‰åŠ¹æ´»ç”¨ï¼ˆ1024ã‚¨ãƒ³ãƒˆãƒªã§ååˆ†ï¼‰
2. **Extended context**: æ™‚é–“æ–¹å‘ã® receptive field æ‹¡å¤§
3. **Improved attention**: Self-attention ã§é•·è·é›¢ä¾å­˜æ€§ã‚’æ•æ‰

**çµæœ**: $N_q = 1$, $L = 75$ tokens/sec ã§ SOTA å“è³ª

#### 3.3.2 WavTokenizer ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

**Encoder**: 1D Convolutional layers + Attention

$$
z_e = \text{Encoder}(x) = \text{Attention}(\text{Conv1D}^{(N)}(...\text{Conv1D}^{(1)}(x)))
$$

- Conv1D stride: éŸ³å£°ã‚’ downsamplingï¼ˆ24000 samples â†’ 75 timestepsï¼‰
- Attention: æ™‚é–“æ–¹å‘ã®é•·è·é›¢ä¾å­˜æ€§ï¼ˆéŸ»å¾‹ãƒ»è©±è€…ç‰¹æ€§ï¼‰

**VQ**: Single codebook $\mathcal{C} = \{e_k\}_{k=1}^{1024} \subset \mathbb{R}^{128}$

$$
z_q^{(i)} = e_{k^*}, \quad k^* = \arg\min_k \| z_e^{(i)} - e_k \|_2
$$

**Decoder**: Transposed Conv1D + Attention

$$
\hat{x} = \text{Decoder}(z_q) = \text{TransposedConv1D}^{(N)}(...\text{Attention}(z_q))
$$

**Loss**: VQ-VAE loss + Adversarial lossï¼ˆMulti-scale discriminatorï¼‰

$$
\mathcal{L} = \mathcal{L}_{\text{VQ-VAE}} + \lambda_{\text{adv}} \mathcal{L}_{\text{GAN}}
$$

```julia
# WavTokenizer ã®ç°¡æ˜“å®Ÿè£…ï¼ˆæ¦‚å¿µçš„ï¼‰
struct WavTokenizer
    encoder_convs::Vector{Any}  # 1D Conv layers
    attention::Any
    codebook::Matrix{Float32}  # (K=1024, D=128)
    decoder_convs::Vector{Any}
end

function wavtokenizer_encode_simplified(x::Vector{Float32}, wt::WavTokenizer)
    # 1. Conv downsampling: 24000 samples â†’ 75 timesteps
    # stride = 320 (24000 / 75)
    L = 75
    D = 128
    z_e = zeros(Float32, L, D)

    stride = div(length(x), L)
    for i in 1:L
        start_idx = (i-1) * stride + 1
        end_idx = min(start_idx + stride - 1, length(x))
        frame = x[start_idx:end_idx]

        # Simplified: mean pooling + FFT features
        z_e[i, :] = abs.(fft(vcat(frame, zeros(Float32, stride - length(frame))))[1:D])
    end

    # 2. Attention (simplified: skip for demo)
    # z_e = attention(z_e)

    # 3. VQ
    tokens, z_q = vector_quantization(z_e, wt.codebook)

    return tokens, z_q
end

# Create dummy WavTokenizer
wt = WavTokenizer([], nothing, randn(Float32, 1024, 128), [])

x_audio = randn(Float32, 24000)
tokens_wt, z_q_wt = wavtokenizer_encode_simplified(x_audio, wt)

println("\nã€WavTokenizer Encodingã€‘")
println("Input audio: $(length(x_audio)) samples")
println("Output tokens: $(length(tokens_wt)) (75 tokens/sec)")
println("Codebook: single VQ, 1024 entries")
println("Compression: $(div(length(x_audio), length(tokens_wt)))x")
println("\nKey: Extended context (large stride) + Attention (long-range deps)")
```

**WavTokenizer ã®æˆæœ**[^1]:
- **UTMOS score**: SOTAï¼ˆäººé–“è©•ä¾¡æŒ‡æ¨™ï¼‰
- **Semantic-rich**: éŸ³ç´ èªè­˜ç²¾åº¦ãŒé«˜ã„ï¼ˆvs EnCodecï¼‰
- **Efficiency**: æ¨è«–é€Ÿåº¦ãŒ RVQ ã® 4å€ï¼ˆ$N_q = 1$ vs $N_q = 4$ï¼‰

#### 3.3.3 Supervised Semantic Tokens â€” CosyVoice

**å•ã„**: VQ ã¯ unsupervisedï¼ˆãƒ©ãƒ™ãƒ«ãªã—è¨“ç·´ï¼‰ã ãŒã€éŸ³å£°èªè­˜ãƒ¢ãƒ‡ãƒ«ã®ä¸­é–“è¡¨ç¾ã‚’ä½¿ãˆã° semantic-rich ãªãƒˆãƒ¼ã‚¯ãƒ³ãŒå¾—ã‚‰ã‚Œã‚‹ã®ã§ã¯ï¼Ÿ

**CosyVoice ã®ææ¡ˆ**[^13]:
- **Supervised semantic tokens**: å¤šè¨€èªéŸ³å£°èªè­˜ãƒ¢ãƒ‡ãƒ«ï¼ˆASRï¼‰ã® encoder ã« VQ ã‚’æŒ¿å…¥
- ASR encoder ã¯éŸ³ç´ æƒ…å ±ã‚’å­¦ç¿’æ¸ˆã¿ â†’ VQ tokens ãŒè‡ªå‹•çš„ã«éŸ³ç´ ã«å¯¾å¿œ

**Architecture**:

```mermaid
graph LR
    A[Audio] --> B[ASR Encoder]
    B --> C[VQ Layer<br/>supervised]
    C --> D[ASR Decoder<br/>CTC/Attention]
    D --> E[Phonemes]
    C --> F[LM<br/>Textâ†’Tokens]
    F --> G[Flow Matching<br/>Tokensâ†’Speech]
```

**Result**: Supervised tokens ãŒ unsupervised tokensï¼ˆEnCodecï¼‰ã‚’ **content consistency** ã¨ **speaker similarity** ã§ä¸Šå›ã‚‹ã€‚

```julia
println("\nã€Supervised vs Unsupervised Tokensã€‘")
println("Unsupervised (EnCodec/WavTokenizer):")
println("  è¨“ç·´: Self-supervised reconstruction")
println("  ç‰¹å¾´: éŸ³ç´ æƒ…å ±ã¯ implicitï¼ˆå¿…ãšã—ã‚‚æ˜ç¤ºçš„ã§ãªã„ï¼‰")
println()
println("Supervised (CosyVoice):")
println("  è¨“ç·´: ASR task (éŸ³ç´ äºˆæ¸¬)")
println("  ç‰¹å¾´: éŸ³ç´ æƒ…å ± explicitï¼ˆVQ codeãŒéŸ³ç´ ã«å¯¾å¿œï¼‰")
println("  åˆ©ç‚¹: Content consistency å‘ä¸Š (éŸ³ç´ ã®æ­£ç¢ºã•)")
println()
println("â†’ TTS ã§ã¯ Supervised tokens ãŒæœ‰åˆ©")
```

### 3.4 Flow Matching for TTS â€” E2-TTS / F5-TTS

#### 3.4.1 TTS ã®èª²é¡Œã¨Flow Matchingã®åˆ©ç‚¹

**å¾“æ¥ã® TTSï¼ˆTacotron/FastSpeechï¼‰**:
- **2æ®µéš**: Acoustic Modelï¼ˆãƒ†ã‚­ã‚¹ãƒˆ â†’ ãƒ¡ãƒ«ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ï¼‰+ Vocoderï¼ˆãƒ¡ãƒ« â†’ æ³¢å½¢ï¼‰
- **å•é¡Œ**: è¤‡é›‘ãªè¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€alignmentï¼ˆãƒ†ã‚­ã‚¹ãƒˆã¨éŸ³å£°ã®å¯¾å¿œï¼‰ã®å¿…è¦æ€§

**E2-TTS / F5-TTS ã®é©å‘½**[^2]:
- **1æ®µéš**: ãƒ†ã‚­ã‚¹ãƒˆ â†’ éŸ³å£°ï¼ˆç›´æ¥ï¼‰
- **No alignment**: ãƒ†ã‚­ã‚¹ãƒˆã‚’ filler tokens ã§ãƒ‘ãƒ‡ã‚£ãƒ³ã‚° â†’ éŸ³å£°ã¨åŒã˜é•·ã•
- **Flow Matching**: Diffusion ã®è¨“ç·´ç°¡ç•¥åŒ–ç‰ˆï¼ˆsimulation-freeï¼‰

#### 3.4.2 E2-TTS ã®å®Œå…¨å°å‡º

**Problem setup**:
- Input: ãƒ†ã‚­ã‚¹ãƒˆ $\mathbf{t} = (t_1, ..., t_{N_t})$ï¼ˆ$N_t$ = ãƒ†ã‚­ã‚¹ãƒˆé•·ï¼‰
- Output: éŸ³å£° $\mathbf{x}_1 \in \mathbb{R}^{T \times D}$ï¼ˆ$T$ = éŸ³å£° timesteps, $D$ = feature dimï¼‰

**Key idea**: ãƒ†ã‚­ã‚¹ãƒˆã‚’ $T$ timesteps ã«æ‹¡å¼µ

$$
\tilde{\mathbf{t}} = (\underbrace{t_1, ..., t_1}_{r_1}, \underbrace{t_2, ..., t_2}_{r_2}, ..., \underbrace{t_{N_t}, ..., t_{N_t}}_{r_{N_t}}, \underbrace{<\text{filler}>}_{T - \sum r_i})
$$

where $r_i$ = duration of token $t_i$ï¼ˆè‡ªå‹•æ±ºå®š or uniformï¼‰

**Flow Matching objective**:

Given:
- $\mathbf{x}_0 \sim p_0 = \mathcal{N}(0, I)$ (noise prior)
- $\mathbf{x}_1 \sim p_1$ (data distribution, i.e., real speech)

Define **conditional probability path**:

$$
p_t(\mathbf{x} | \mathbf{x}_0, \mathbf{x}_1, \tilde{\mathbf{t}}) = \mathcal{N}(\mathbf{x} | \mu_t(\mathbf{x}_0, \mathbf{x}_1), \sigma_t^2 I)
$$

where $\mu_t = (1-t)\mathbf{x}_0 + t \mathbf{x}_1$ (linear interpolation), $\sigma_t = 0$ (deterministic).

**Target vector field** (conditional):

$$
\mathbf{u}_t(\mathbf{x} | \mathbf{x}_0, \mathbf{x}_1) = \frac{d \mu_t}{dt} = \mathbf{x}_1 - \mathbf{x}_0
$$

**Neural network prediction**: $\mathbf{v}_\theta(\mathbf{x}_t, t, \tilde{\mathbf{t}})$

**Loss function** (Conditional Flow Matching):

$$
\mathcal{L}_{\text{CFM}}(\theta) = \mathbb{E}_{t, \mathbf{x}_0, \mathbf{x}_1, \tilde{\mathbf{t}}} \left[ \| \mathbf{v}_\theta(\mathbf{x}_t, t, \tilde{\mathbf{t}}) - \mathbf{u}_t(\mathbf{x} | \mathbf{x}_0, \mathbf{x}_1) \|^2 \right]
$$

where $\mathbf{x}_t = (1-t)\mathbf{x}_0 + t \mathbf{x}_1$.

**Sampling** (ODE integration):

$$
\frac{d\mathbf{x}}{dt} = \mathbf{v}_\theta(\mathbf{x}, t, \tilde{\mathbf{t}}), \quad \mathbf{x}(0) = \mathbf{x}_0 \sim \mathcal{N}(0, I)
$$

Euler integration:

$$
\mathbf{x}_{t+\Delta t} = \mathbf{x}_t + \mathbf{v}_\theta(\mathbf{x}_t, t, \tilde{\mathbf{t}}) \cdot \Delta t
$$

```julia
# E2-TTS / F5-TTS ã® Flow Matching è¨“ç·´
function e2_tts_train_step(x0::Matrix{Float32}, x1::Matrix{Float32},
                           text_emb::Matrix{Float32}, v_Î¸)
    # x0: (T, D) noise
    # x1: (T, D) real speech
    # text_emb: (T, D_text) extended text embedding (same T as speech)

    T, D = size(x1)

    # Sample t ~ Uniform(0, 1)
    t = rand(Float32)

    # Interpolate: x_t = (1-t)*x0 + t*x1
    x_t = (1 - t) .* x0 .+ t .* x1

    # Target vector field: u_t = x1 - x0
    u_t = x1 .- x0

    # Predict velocity
    v_pred = v_Î¸(x_t, [t], text_emb)  # (T, D)

    # CFM loss
    loss = mean((v_pred .- u_t).^2)

    return loss
end

# Sampling
function e2_tts_sample(text_emb::Matrix{Float32}, v_Î¸, steps=10)
    T, D_text = size(text_emb)
    D = 128  # latent dim

    # x0 ~ N(0, I)
    x0 = randn(Float32, T, D)

    # ODE integration
    dt = 1.0f0 / steps
    x_t = copy(x0)

    for step in 1:steps
        t = step * dt
        v = v_Î¸(x_t, [t], text_emb)
        x_t = x_t .+ v .* dt
    end

    return x_t  # x1 (latent speech)
end

# Dummy velocity network
v_Î¸_dummy(x, t, text) = x .* (1 .- t[1]) .+ text .* t[1]

# Example
T_audio = 150  # 150 timesteps (1 sec @ 150 tokens/sec)
D = 128
x0_audio = randn(Float32, T_audio, D)
x1_audio = randn(Float32, T_audio, D)
text_emb_e2 = randn(Float32, T_audio, D)  # text extended to T_audio

loss_e2 = e2_tts_train_step(x0_audio, x1_audio, text_emb_e2, v_Î¸_dummy)
x1_sampled = e2_tts_sample(text_emb_e2, v_Î¸_dummy)

println("\nã€E2-TTS / F5-TTS Flow Matchingã€‘")
println("Training:")
println("  Input: x0 (noise), x1 (real speech), text_emb (extended)")
println("  Loss: ||v_Î¸(x_t, t, text) - (x1 - x0)||Â² = $loss_e2")
println()
println("Sampling:")
println("  Steps: 10 (vs DDPM 1000)")
println("  Speed: Real-time synthesis on GPU")
println("  x0 â†’ integrate v_Î¸ â†’ x1")
```

**E2-TTS ã®ç‰¹å¾´**:
- **Alignment-free**: ãƒ†ã‚­ã‚¹ãƒˆã‚’ filler tokens ã§ãƒ‘ãƒ‡ã‚£ãƒ³ã‚° â†’ éŸ³å£°é•·ã«åˆã‚ã›ã‚‹
- **Simulation-free**: Flow Matching ã¯ç¢ºç‡çš„ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä¸è¦ï¼ˆvs DDPM ã® ancestral samplingï¼‰
- **Fast**: 10-32 steps ã§é«˜å“è³ª

#### 3.4.3 F5-TTS ã®æ”¹å–„ â€” ConvNeXt + Sway Sampling

**E2-TTS ã®å•é¡Œ**:
- Convergence ãŒé…ã„ï¼ˆè¨“ç·´ãŒé•·æ™‚é–“ï¼‰
- Robustness ãŒä½ã„ï¼ˆãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ä»˜ã‘ãŒå¼±ã„ï¼‰

**F5-TTS ã®è§£æ±ºç­–**[^2]:

1. **ConvNeXt text refinement**: ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã‚’ ConvNeXt ã§ refinement

$$
\tilde{\mathbf{t}}_{\text{refined}} = \text{ConvNeXt}(\tilde{\mathbf{t}})
$$

ConvNeXt ã¯å±€æ‰€çš„ãªæ–‡è„ˆã‚’æ‰ãˆã€éŸ³å£°ã¨ã® alignment ã‚’å®¹æ˜“ã«ã™ã‚‹ã€‚

2. **Sway Sampling**: æ¨è«–æ™‚ã®ã‚¹ãƒ†ãƒƒãƒ—é…åˆ†ã‚’æœ€é©åŒ–

é€šå¸¸ã® Euler integration: $t_i = i / N$ (uniform)

Sway Sampling: $t_i$ ã‚’éä¸€æ§˜ã«é…åˆ†

$$
t_i = \left( \frac{i}{N} \right)^\alpha, \quad \alpha \in [0.5, 2.0]
$$

- $\alpha < 1$: åˆæœŸã‚¹ãƒ†ãƒƒãƒ—ã‚’ dense ã«ï¼ˆãƒã‚¤ã‚ºé™¤å»ã‚’å¼·åŒ–ï¼‰
- $\alpha > 1$: å¾ŒæœŸã‚¹ãƒ†ãƒƒãƒ—ã‚’ dense ã«ï¼ˆè©³ç´°ã‚’ refinedï¼‰

```julia
# F5-TTS ã® Sway Sampling
function f5_tts_sway_sampling(text_emb::Matrix{Float32}, v_Î¸, steps=10, Î±=1.0f0)
    T, D_text = size(text_emb)
    D = 128

    # ConvNeXt refinement (simplified: skip for demo)
    text_refined = text_emb

    # x0 ~ N(0, I)
    x0 = randn(Float32, T, D)
    x_t = copy(x0)

    # Sway Sampling: t_i = (i/N)^Î±
    for step in 1:steps
        t_prev = ((step - 1) / steps)^Î±
        t_curr = (step / steps)^Î±
        dt = t_curr - t_prev

        v = v_Î¸(x_t, [t_curr], text_refined)
        x_t = x_t .+ v .* dt
    end

    return x_t
end

# Compare: uniform vs sway (Î±=0.7)
x1_uniform = e2_tts_sample(text_emb_e2, v_Î¸_dummy, 10)
x1_sway = f5_tts_sway_sampling(text_emb_e2, v_Î¸_dummy, 10, 0.7f0)

println("\nã€F5-TTS Sway Samplingã€‘")
println("Uniform sampling: t_i = i/N")
println("  ã‚¹ãƒ†ãƒƒãƒ—: 0.1, 0.2, 0.3, ..., 1.0")
println()
println("Sway sampling (Î±=0.7): t_i = (i/N)^0.7")
t_sway = [(i / 10)^0.7 for i in 1:10]
println("  ã‚¹ãƒ†ãƒƒãƒ—: ", round.(t_sway, digits=2))
println("  â†’ åˆæœŸã‚¹ãƒ†ãƒƒãƒ—ã‚’ dense ã«ï¼ˆãƒã‚¤ã‚ºé™¤å»å¼·åŒ–ï¼‰")
println()
println("F5-TTS innovations:")
println("  1. ConvNeXt: ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¾ refinement")
println("  2. Sway Sampling: æ¨è«–æ™‚ã®ã‚¹ãƒ†ãƒƒãƒ—é…åˆ†æœ€é©åŒ–")
```

**F5-TTS ã®æˆæœ**[^2]:
- **Zero-shot ability**: 3ç§’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ä»»æ„è©±è€…ã‚’ã‚¯ãƒ­ãƒ¼ãƒ³
- **Code-switching**: å¤šè¨€èªã‚·ãƒ¼ãƒ ãƒ¬ã‚¹åˆ‡ã‚Šæ›¿ãˆï¼ˆ"Hello ã“ã‚“ã«ã¡ã¯"ï¼‰
- **Speed control**: Duration åˆ¶å¾¡ãŒå®¹æ˜“

### 3.5 Codec Language Models â€” VALL-E 2

#### 3.5.1 VALL-E 2 ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

**VALL-Eï¼ˆåˆä»£ï¼‰**[^8]:
- EnCodec tokens ã‚’ autoregressive LM ã§ç”Ÿæˆ
- **å•é¡Œ**: Phoneme repetitionï¼ˆ"hello" â†’ "hehehe-llo"ï¼‰

**VALL-E 2ï¼ˆ2024ï¼‰**[^4]:
1. **Repetition Aware Sampling**: ãƒ‡ã‚³ãƒ¼ãƒ‰å±¥æ­´ã®ãƒˆãƒ¼ã‚¯ãƒ³ç¹°ã‚Šè¿”ã—ã‚’è€ƒæ…®
2. **Grouped Code Modeling**: RVQ ã® 4 quantizers ã‚’2ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†å‰² â†’ ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·åŠæ¸›

#### 3.5.2 Repetition Aware Sampling ã®å®šå¼åŒ–

**å•é¡Œ**: Autoregressive sampling ã§åŒã˜ãƒˆãƒ¼ã‚¯ãƒ³ãŒé€£ç¶šå‡ºç¾

$$
p(x_t | x_{<t}) = \text{softmax}(\text{logits}_\theta(x_{<t}))
$$

Naive samplingï¼ˆtemperature $\tau$ï¼‰:

$$
\text{probs} = \text{softmax}(\text{logits} / \tau)
$$

**Repetition Aware Sampling**:

$$
\text{logits}'_k = \text{logits}_k - \lambda \cdot \text{count}(k, x_{<t})
$$

where $\text{count}(k, x_{<t})$ = $k$ ã®å‡ºç¾å›æ•°ï¼ˆç›´è¿‘ $W$ tokensï¼‰

```julia
# Repetition Aware Sampling
function repetition_aware_sampling(logits::Vector{Float32}, history::Vector{Int},
                                   Î»=1.0f0, W=50, Ï„=1.0f0)
    K = length(logits)

    # Count token occurrences in recent history (last W tokens)
    recent_history = history[max(1, length(history) - W + 1):end]
    counts = zeros(Float32, K)
    for token in recent_history
        counts[token] += 1.0f0
    end

    # Penalize repeated tokens
    logits_adjusted = logits .- Î» .* counts

    # Temperature scaling + softmax
    probs = softmax(logits_adjusted ./ Ï„)

    # Sample
    sampled_token = sample_categorical(probs)

    return sampled_token, probs
end

function softmax(x::Vector{Float32})
    exp_x = exp.(x .- maximum(x))
    return exp_x ./ sum(exp_x)
end

function sample_categorical(probs::Vector{Float32})
    cumsum_probs = cumsum(probs)
    r = rand(Float32)
    return findfirst(cumsum_probs .>= r)
end

# Example
K = 1024  # codebook size
logits_example = randn(Float32, K)
history_example = rand(1:K, 100)  # 100 tokens history

token_sampled, probs_sampled = repetition_aware_sampling(logits_example, history_example)

# Count repetition in history
token_counts = [count(==(k), history_example) for k in 1:K]
max_count_token = argmax(token_counts)

println("\nã€Repetition Aware Samplingã€‘")
println("Most repeated token in history: $max_count_token (count: $(token_counts[max_count_token]))")
println("Its probability:")
println("  Before penalty: $(softmax(logits_example ./ 1.0)[max_count_token])")
println("  After penalty:  $(probs_sampled[max_count_token])")
println("\nâ†’ ç¹°ã‚Šè¿”ã—ãƒˆãƒ¼ã‚¯ãƒ³ã®ç¢ºç‡ã‚’ down-weight â†’ phoneme repetition è§£æ±º")
```

#### 3.5.3 Grouped Code Modeling

**å•é¡Œ**: EnCodec ã® 4 quantizers â†’ 4å€ã®ãƒˆãƒ¼ã‚¯ãƒ³åˆ— â†’ LM ã®è² æ‹…

**Grouped Code Modeling**:
- Group 1: Quantizers 1-2 â†’ coarse tokens
- Group 2: Quantizers 3-4 â†’ fine tokens

**Autoregressive generation**:
1. Generate Group 1 tokens (coarse): $p(z_1, z_2 | \text{text})$
2. Generate Group 2 tokens (fine): $p(z_3, z_4 | z_1, z_2, \text{text})$

**åˆ©ç‚¹**: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ãŒåŠæ¸› â†’ æ¨è«–é€Ÿåº¦ 2å€

```julia
println("\nã€Grouped Code Modelingã€‘")
println("EnCodec: 4 quantizers, 150 tokens/sec")
println("  Naive: 4 x 150 = 600 tokens/sec â†’ LM sequence length")
println()
println("Grouped Code Modeling:")
println("  Group 1 (Q1-Q2): 2 x 150 = 300 tokens/sec (coarse)")
println("  Group 2 (Q3-Q4): 2 x 150 = 300 tokens/sec (fine)")
println("  Sequential generation: Group 1 â†’ Group 2")
println("  Effective sequence: 300 tokens/sec (50% reduction)")
println()
println("â†’ æ¨è«–é€Ÿåº¦ 2å€ + ãƒ¡ãƒ¢ãƒªå‰Šæ¸›")
```

**VALL-E 2 ã®æˆæœ**[^4]:
- **Human parity**: LibriSpeech/VCTK ã§ WERï¼ˆWord Error Rateï¼‰ãŒäººé–“ä¸¦ã¿
- **Robustness**: Complex sentences + Repetitive phrases ã§ã‚‚å®‰å®š
- **Naturalness**: CMOSï¼ˆComparative Mean Opinion Scoreï¼‰ã§é«˜è©•ä¾¡

### 3.6 NaturalSpeech 3 â€” FACodec + Diffusion

#### 3.6.1 Factorized Codec (FACodec)

**å‹•æ©Ÿ**: EnCodec ã¯ prosody / timbre / content ã‚’åŒã˜ latent space ã«æ··åœ¨ã•ã›ã‚‹ â†’ disentanglement ãŒä¸ååˆ†

**FACodec ã®ææ¡ˆ**[^14]:
- **Factorized VQ**: 4ã¤ã®ã‚µãƒ–ç©ºé–“ã«åˆ†è§£
  1. **Content**: éŸ³ç´ ãƒ»è¨€èªå†…å®¹
  2. **Prosody**: ã‚¤ãƒ³ãƒˆãƒãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ãƒªã‚ºãƒ 
  3. **Timbre**: è©±è€…ç‰¹æ€§ãƒ»å£°è³ª
  4. **Acoustic details**: ç´°ã‹ã„éŸ³éŸ¿ç‰¹å¾´

**Architecture**:

$$
z = [z_{\text{content}}, z_{\text{prosody}}, z_{\text{timbre}}, z_{\text{acoustic}}]
$$

Each subspace has its own VQ codebook.

**Disentanglement loss**:

$$
\mathcal{L}_{\text{disentangle}} = \text{MI}(z_{\text{content}}, z_{\text{prosody}}) + \text{MI}(z_{\text{content}}, z_{\text{timbre}}) + \cdots
$$

where MI = Mutual Informationï¼ˆæœ€å°åŒ–ï¼‰

```julia
println("\nã€FACodec: Factorized Audio Codecã€‘")
println("EnCodec: æ··åœ¨ã—ãŸ latent space")
println("  z = [all information mixed]")
println()
println("FACodec: å› æ•°åˆ†è§£ã•ã‚ŒãŸ latent space")
println("  z_content:  éŸ³ç´ ãƒ»è¨€èªå†…å®¹ (VQ1)")
println("  z_prosody:  ã‚¤ãƒ³ãƒˆãƒãƒ¼ã‚·ãƒ§ãƒ³ (VQ2)")
println("  z_timbre:   è©±è€…ç‰¹æ€§ (VQ3)")
println("  z_acoustic: éŸ³éŸ¿è©³ç´° (VQ4)")
println()
println("â†’ Zero-shot TTS ã§å±æ€§åˆ¶å¾¡ãŒå®¹æ˜“")
println("  Content from text, Timbre from prompt, Prosody from model")
```

#### 3.6.2 Factorized Diffusion Model

**NaturalSpeech 3 ã®ç”Ÿæˆæ–¹å¼**:
- å„ã‚µãƒ–ç©ºé–“ã”ã¨ã« **å€‹åˆ¥ã® diffusion model**

$$
\begin{align}
z_{\text{content}} &\sim p_{\theta_1}(z_c | \text{text}) \\
z_{\text{prosody}} &\sim p_{\theta_2}(z_p | z_c, \text{prompt}) \\
z_{\text{timbre}} &\sim p_{\theta_3}(z_t | z_c, z_p, \text{prompt}) \\
z_{\text{acoustic}} &\sim p_{\theta_4}(z_a | z_c, z_p, z_t)
\end{align}
$$

**è¨“ç·´**: å„ diffusion model ã‚’ç‹¬ç«‹ã«è¨“ç·´ â†’ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ€§

**æ¨è«–**: é †æ¬¡ç”Ÿæˆ â†’ æœ€çµ‚çš„ã« $z = [z_c, z_p, z_t, z_a]$ â†’ FACodec decoder â†’ éŸ³å£°

**NaturalSpeech 3 ã®æˆæœ**[^14]:
- **Quality**: LibriSpeech ã§ SOTAï¼ˆMOS scoreï¼‰
- **Similarity**: è©±è€…ã‚¯ãƒ­ãƒ¼ãƒ³ç²¾åº¦ãŒ VALL-E ã‚’ä¸Šå›ã‚‹
- **Intelligibility**: WERï¼ˆå˜èªèª¤ã‚Šç‡ï¼‰ãŒä½ã„
- **Scalability**: 1B params + 200K hours â†’ å“è³ªå‘ä¸Š

```julia
println("\nã€NaturalSpeech 3: Factorized Diffusionã€‘")
println("Step 1: Content generation (text â†’ z_content)")
println("Step 2: Prosody generation (z_content + prompt â†’ z_prosody)")
println("Step 3: Timbre generation (z_content + z_prosody + prompt â†’ z_timbre)")
println("Step 4: Acoustic generation (all â†’ z_acoustic)")
println("Step 5: FACodec decode â†’ waveform")
println()
println("â†’ å„å±æ€§ã‚’ç‹¬ç«‹ã«åˆ¶å¾¡å¯èƒ½")
println("  Example: åŒã˜ content, ç•°ãªã‚‹ timbre â†’ è©±è€…å¤‰æ›")
```

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®50%å®Œäº†ï¼** Zone 3 å‰åŠï¼ˆAudio Codec + Flow Matching TTSï¼‰ã‚’å®Œèµ°ã—ãŸã€‚ãƒšãƒ³ã¨ç´™ã§å°å‡ºã‚’è¿½ãˆãŸã‚ãªãŸã¯ã€éŸ³å£°ç”Ÿæˆã®ç†è«–çš„åŸºç›¤ã‚’å®Œå…¨ã«ç†è§£ã—ãŸã€‚æ¬¡ã¯ Zone 3 å¾ŒåŠ â€” Music Generation ã¨è©•ä¾¡æŒ‡æ¨™ã€‚
:::

### 3.7 Music Generation â€” MusicGen ã¨ Stable Audio

#### 3.7.1 MusicGen ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

**MusicGenï¼ˆMeta, 2023ï¼‰**[^3] ã¯ EnCodec + Language Model ã®çµ„ã¿åˆã‚ã›ã ã€‚

**Pipeline**:
1. **EnCodec tokenization**: éŸ³æ¥½ â†’ 4å±¤ RVQ tokensï¼ˆ150 tokens/sec x 4 = 600 tokens/secï¼‰
2. **LM generation**: Text/Melody-conditioned autoregressive generation
3. **Decoding**: Tokens â†’ waveform

**Text conditioning**:

$$
p(z | \text{text}) = \prod_{t=1}^T \prod_{q=1}^4 p(z_t^{(q)} | z_{<t}, \text{text\_emb})
$$

where $z_t^{(q)}$ = token at time $t$, quantizer $q$.

**Parallel vs Sequential generation**:

- **Parallel** (MusicGen default): 4 quantizers ä¸¦åˆ—ç”Ÿæˆ â†’ é«˜é€Ÿ
- **Sequential**: 1å±¤ãšã¤é€æ¬¡ç”Ÿæˆ â†’ é…ã„ãŒå“è³ªé«˜

**Token interleaving pattern**:

MusicGen uses **delay pattern**:

```
Q1: t1  t2  t3  t4  ...
Q2: -   t1  t2  t3  ...
Q3: -   -   t1  t2  ...
Q4: -   -   -   t1  ...
```

Each quantizer is delayed by 1 step â†’ causal dependency.

```julia
# MusicGen ã® Token Interleaving Pattern
function musicgen_delay_pattern(T::Int, N_q=4)
    # T: sequence length (timesteps)
    # N_q: number of quantizers

    # Create token sequence with delay pattern
    # Total sequence length = T + (N_q - 1)
    total_len = T + (N_q - 1)
    tokens = fill(-1, total_len, N_q)  # -1 = padding

    for q in 1:N_q
        delay = q - 1
        for t in 1:T
            tokens[t + delay, q] = t  # Token index (simplified)
        end
    end

    return tokens
end

T_music = 10
pattern = musicgen_delay_pattern(T_music, 4)

println("\nã€MusicGen Delay Patternã€‘")
println("Sequence length: $T_music timesteps, 4 quantizers")
println("Delay pattern (each quantizer delayed by 1 step):\n")
for q in 1:4
    println("Q$q: ", join([t == -1 ? "-" : string(t) for t in pattern[:, q]], "  "))
end
println("\nâ†’ Autoregressive generation with causal dependency across quantizers")
```

#### 3.7.2 MusicGen ã®è¨“ç·´

**Dataset**:
- Internal Meta dataset: 10K hours high-quality music
- ShutterStock: 25K instrument-only tracks
- Pond5: 365K instrument-only tracks
- **Total**: ~20K hours licensed music

**Training objective**:

$$
\mathcal{L} = -\sum_{t=1}^T \sum_{q=1}^4 \log p_\theta(z_t^{(q)} | z_{<t}, c)
$$

where $c$ = text or melody condition.

**Melody conditioning**: Input melody â†’ EnCodec â†’ condition LM

**Evaluation**:
- **Automatic**: FADï¼ˆFrÃ©chet Audio Distanceï¼‰, KL divergence
- **Human**: MOSï¼ˆMean Opinion Scoreï¼‰, MUSHRA

```julia
println("\nã€MusicGen è¨“ç·´ã€‘")
println("Dataset: 20K hours licensed music")
println("  Meta internal: 10K hours (high-quality)")
println("  ShutterStock: 25K tracks (instrument)")
println("  Pond5: 365K tracks (instrument)")
println()
println("Model sizes:")
println("  Small: 300M params")
println("  Medium: 1.5B params")
println("  Large: 3.3B params")
println()
println("Conditioning:")
println("  Text: 'upbeat pop with guitar' â†’ CLAP/T5 embedding")
println("  Melody: input audio â†’ EnCodec tokens â†’ condition LM")
println()
println("â†’ State-of-the-art text-to-music generation (2023)")
```

#### 3.7.3 Stable Audio â€” DiT for Long-form Music

**Stable Audioï¼ˆ2024ï¼‰**[^9] ã¯ Diffusion Transformerï¼ˆDiTï¼‰ã‚’éŸ³æ¥½ç”Ÿæˆã«é©ç”¨ã—ãŸã€‚

**Key innovations**:
1. **Long-form generation**: æœ€å¤§ **4åˆ†45ç§’** ï¼ˆMusicGen ã¯ 30ç§’ï¼‰
2. **Timing embeddings**: Temporal structure controlï¼ˆ"0:00-0:30 intro, 0:30-2:00 verse, ..."ï¼‰
3. **Latent diffusion**: VAE latent space ã§ diffusion â†’ è¨ˆç®—é‡å‰Šæ¸›

**Architecture**:

```mermaid
graph TD
    A[Text + Timing<br/>'pop, 0:00-3:00'] --> B[Text Encoder<br/>CLAP]
    B --> C[DiT<br/>Latent Diffusion]
    D[VAE Encoder] --> E[Latent z]
    E --> C
    C --> F[Denoised Latent]
    F --> G[VAE Decoder]
    G --> H[Audio Waveform<br/>44.1kHz stereo]

    style C fill:#ffeb3b
```

**Timing embeddings**:

Input: $(t_{\text{start}}, t_{\text{end}}, t_{\text{total}})$ â†’ sinusoidal embeddings

$$
\text{timing\_emb} = [\sin(2\pi f_k t), \cos(2\pi f_k t)]_{k=1}^{D/2}
$$

**VAE latent rate**: 21.5 Hzï¼ˆ44.1kHz â†’ 21.5Hz, ç´„2000å€åœ§ç¸®ï¼‰

**Long-context DiT**:
- Sequence length: 4åˆ†45ç§’ @ 21.5Hz = **6,127 tokens**
- DiT handles this via efficient attentionï¼ˆFlashAttention / sparse attentionï¼‰

```julia
# Stable Audio ã® Timing Embeddings
function timing_embeddings(t_start::Float32, t_end::Float32, t_total::Float32, D=256)
    # Sinusoidal position embeddings for timing
    freqs = [10.0^(k / (D/2)) for k in 0:(DÃ·2-1)]

    emb_start = vcat([sin(2Ï€ * f * t_start) for f in freqs],
                     [cos(2Ï€ * f * t_start) for f in freqs])
    emb_end = vcat([sin(2Ï€ * f * t_end) for f in freqs],
                   [cos(2Ï€ * f * t_end) for f in freqs])
    emb_total = vcat([sin(2Ï€ * f * t_total) for f in freqs],
                     [cos(2Ï€ * f * t_total) for f in freqs])

    # Concatenate
    timing_emb = vcat(emb_start, emb_end, emb_total)

    return timing_emb
end

t_start = 0.0f0
t_end = 180.0f0  # 3 minutes
t_total = 180.0f0
emb_timing = timing_embeddings(t_start, t_end, t_total)

println("\nã€Stable Audio Timing Embeddingsã€‘")
println("Input timing: start=$t_start, end=$t_end, total=$t_total sec")
println("Timing embedding dim: $(length(emb_timing))")
println()
println("Long-form generation:")
println("  Max duration: 4åˆ†45ç§’ (285 sec)")
println("  Latent rate: 21.5 Hz â†’ 6,127 tokens")
println("  DiT sequence: 6,127 timesteps (vs image DiT 256-1024)")
println()
println("â†’ Coherent long-form music with temporal structure control")
```

**Stable Audio ã®æˆæœ**[^9]:
- **Quality**: ãƒ—ãƒ­ãƒ¬ãƒ™ãƒ«éŸ³è³ªï¼ˆ44.1kHz stereoï¼‰
- **Coherence**: é•·æ™‚é–“ã§ã‚‚æ§‹é€ çš„ä¸€è²«æ€§ï¼ˆintro â†’ verse â†’ chorus â†’ outroï¼‰
- **Control**: Timing embeddings ã§ temporal structure åˆ¶å¾¡

#### 3.7.4 Commercial Music Generation â€” Suno / Udio

**Suno v4.5 / Udio**ï¼ˆ2024-2025ï¼‰:
- **èƒ½åŠ›**: æ­Œè©ç”Ÿæˆ + ãƒœãƒ¼ã‚«ãƒ«åˆæˆ + æ¥½å™¨ç·¨æˆ + ãƒŸãƒƒã‚¯ã‚¹/ãƒã‚¹ã‚¿ãƒªãƒ³ã‚°
- **é€Ÿåº¦**: 3åˆ†ã®æ¥½æ›²ã‚’æ•°ç§’ã§ç”Ÿæˆ
- **å“è³ª**: ãƒ—ãƒ­ãƒ¬ãƒ™ãƒ«ï¼ˆäººé–“ä½œæ›²å®¶ã¨åŒºåˆ¥å›°é›£ï¼‰

**æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯**ï¼ˆæ¨å®šï¼‰:
- Codec: EnCodec / WavTokenizer
- LM: Large-scale Transformerï¼ˆæ¨å®š10B+ paramsï¼‰
- Vocal synthesis: Zero-shot TTSï¼ˆVALL-Eç³»ï¼‰
- Mixing: Neural audio effects

**è«–äº‰ç‚¹**:
1. **è‘—ä½œæ¨©**: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®åˆæ³•æ€§ï¼ˆè¨±å¯ãªã—ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼Ÿï¼‰
2. **ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆæ¨©åˆ©**: ãƒ—ãƒ­éŸ³æ¥½å®¶ã®é›‡ç”¨ã¸ã®å½±éŸ¿
3. **æ–‡åŒ–çš„ä¾¡å€¤**: AIç”ŸæˆéŸ³æ¥½ã¯ã€Œæœ¬ç‰©ã€ã‹ï¼Ÿ

```julia
println("\nã€Commercial Music Generation: Suno / Udioã€‘")
println("èƒ½åŠ›:")
println("  Input: 'Create a sad ballad about lost love'")
println("  Output: 3åˆ†ã®å®Œå…¨æ¥½æ›²ï¼ˆæ­Œè© + ãƒœãƒ¼ã‚«ãƒ« + æ¥½å™¨ + ãƒŸãƒƒã‚¯ã‚¹ï¼‰")
println()
println("æŠ€è¡“:")
println("  æ¨å®š: 10B+ params LM + EnCodec + VALL-Eç³» vocal")
println("  è¨“ç·´ãƒ‡ãƒ¼ã‚¿: æ¨å®šæ•°ç™¾ä¸‡æ›²ï¼ˆè‘—ä½œæ¨©çŠ¶æ³ä¸æ˜ï¼‰")
println()
println("è«–äº‰:")
println("  è‘—ä½œæ¨©: Fair use? Or infringement?")
println("  é›‡ç”¨: ã‚¹ã‚¿ã‚¸ã‚ªãƒŸãƒ¥ãƒ¼ã‚¸ã‚·ãƒ£ãƒ³ãƒ»ä½œæ›²å®¶ã¸ã®å½±éŸ¿")
println("  æ–‡åŒ–: AIéŸ³æ¥½ã¯ã€Œå‰µé€ æ€§ã€ã‚’æŒã¤ã‹ï¼Ÿ")
println()
println("â†’ æŠ€è¡“çš„ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¹ãƒ«ãƒ¼ã¨å€«ç†çš„èª²é¡Œã®äº¤å·®ç‚¹")
```

### 3.8 Audio è©•ä¾¡æŒ‡æ¨™ â€” FAD ã‹ã‚‰ KAD ã¸

#### 3.8.1 FrÃ©chet Audio Distance (FAD) ã®é™ç•Œ

**FAD**[^15] ã¯ç”»åƒã® FIDï¼ˆFrÃ©chet Inception Distanceï¼‰ã®éŸ³å£°ç‰ˆã ã€‚

**å®šç¾©**:

Given:
- Real audio embeddings $\{e_r^{(i)}\}_{i=1}^{N_r}$
- Generated audio embeddings $\{e_g^{(i)}\}_{i=1}^{N_g}$

Assume Gaussian distributions:

$$
e_r \sim \mathcal{N}(\mu_r, \Sigma_r), \quad e_g \sim \mathcal{N}(\mu_g, \Sigma_g)
$$

**FAD**:

$$
\text{FAD} = \| \mu_r - \mu_g \|^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2})
$$

**Embedding**: VGGish / PANNsï¼ˆpre-trained audio neural networksï¼‰

**FAD ã®å•é¡Œ**[^10]:
1. **Gaussian assumption**: Real audio embeddings ã¯éã‚¬ã‚¦ã‚¹åˆ†å¸ƒ â†’ ãƒã‚¤ã‚¢ã‚¹
2. **Sample size sensitivity**: å°ã‚µãƒ³ãƒ—ãƒ«ã§ä¸å®‰å®š
3. **Computational cost**: Covariance matrix ã®å›ºæœ‰å€¤åˆ†è§£ãŒé‡ã„

```julia
# FAD è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
using LinearAlgebra

function fad_distance(embeddings_real::Matrix{Float32}, embeddings_gen::Matrix{Float32})
    # embeddings: (N, D) â€” N samples, D dimensions

    # Compute mean
    Î¼_r = mean(embeddings_real, dims=1)[1, :]
    Î¼_g = mean(embeddings_gen, dims=1)[1, :]

    # Compute covariance
    Î£_r = cov(embeddings_real)
    Î£_g = cov(embeddings_gen)

    # FAD formula
    mean_diff = norm(Î¼_r - Î¼_g)^2

    # Tr(Î£_r + Î£_g - 2(Î£_r Î£_g)^{1/2})
    # Simplified: assume diagonal covariance (not exact)
    trace_term = tr(Î£_r) + tr(Î£_g) - 2 * sqrt(tr(Î£_r * Î£_g))

    fad = mean_diff + trace_term

    return fad
end

# Example
N_r, N_g, D = 100, 100, 128
emb_real = randn(Float32, N_r, D)
emb_gen = randn(Float32, N_g, D) .+ 0.1f0  # Slightly shifted

fad_score = fad_distance(emb_real, emb_gen)

println("\nã€FrÃ©chet Audio Distance (FAD)ã€‘")
println("Real embeddings: $(size(emb_real))")
println("Generated embeddings: $(size(emb_gen))")
println("FAD score: $fad_score")
println()
println("FAD ã®å•é¡Œ:")
println("  1. ã‚¬ã‚¦ã‚¹ä»®å®šï¼ˆå®Ÿéš›ã¯éã‚¬ã‚¦ã‚¹ï¼‰")
println("  2. ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºä¾å­˜æ€§ï¼ˆå°ã‚µãƒ³ãƒ—ãƒ«ã§ä¸å®‰å®šï¼‰")
println("  3. è¨ˆç®—ã‚³ã‚¹ãƒˆï¼ˆcovariance ã®å›ºæœ‰å€¤åˆ†è§£ï¼‰")
```

#### 3.8.2 Kernel Audio Distance (KAD) â€” Distribution-free Metric

**KADï¼ˆ2025ï¼‰**[^10] ã¯ FAD ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ **distribution-free** æŒ‡æ¨™ã ã€‚

**Based on**: Maximum Mean Discrepancy (MMD)

**MMD definition**:

Given two distributions $P$ and $Q$, kernel $k$:

$$
\text{MMD}^2(P, Q) = \mathbb{E}_{x, x' \sim P}[k(x, x')] + \mathbb{E}_{y, y' \sim Q}[k(y, y')] - 2\mathbb{E}_{x \sim P, y \sim Q}[k(x, y)]
$$

**KAD uses**: Polynomial kernelï¼ˆsecond-order ä»¥ä¸Šã§ kurtosis ã‚’æ‰ãˆã‚‹ï¼‰

$$
k(x, y) = (1 + \langle x, y \rangle)^d, \quad d \geq 3
$$

**Unbiased estimator** (U-statistic):

$$
\widehat{\text{MMD}}^2 = \frac{1}{n(n-1)} \sum_{i \neq j} k(x_i, x_j) + \frac{1}{m(m-1)} \sum_{i \neq j} k(y_i, y_j) - \frac{2}{nm} \sum_{i, j} k(x_i, y_j)
$$

**KAD ã®åˆ©ç‚¹**[^10]:
1. **Distribution-free**: ã‚¬ã‚¦ã‚¹ä»®å®šä¸è¦
2. **Unbiased**: U-statistic ã§ä¸åæ¨å®š
3. **Fast convergence**: å°ã‚µãƒ³ãƒ—ãƒ«ã§å®‰å®š
4. **Computationally efficient**: GPU åŠ é€Ÿå¯èƒ½
5. **Perceptually aligned**: äººé–“è©•ä¾¡ã¨é«˜ç›¸é–¢

```julia
# KAD è¨ˆç®—ï¼ˆMMD with polynomial kernelï¼‰
function polynomial_kernel(x::Vector{Float32}, y::Vector{Float32}, degree=3)
    return (1 + dot(x, y))^degree
end

function kad_distance(embeddings_real::Matrix{Float32}, embeddings_gen::Matrix{Float32}, degree=3)
    # embeddings: (N, D)
    n = size(embeddings_real, 1)
    m = size(embeddings_gen, 1)

    # Compute kernel matrices (simplified: full computation)
    # K_rr: real-real
    K_rr = 0.0f0
    for i in 1:n, j in 1:n
        if i != j
            K_rr += polynomial_kernel(embeddings_real[i, :], embeddings_real[j, :], degree)
        end
    end
    K_rr /= (n * (n - 1))

    # K_gg: gen-gen
    K_gg = 0.0f0
    for i in 1:m, j in 1:m
        if i != j
            K_gg += polynomial_kernel(embeddings_gen[i, :], embeddings_gen[j, :], degree)
        end
    end
    K_gg /= (m * (m - 1))

    # K_rg: real-gen
    K_rg = 0.0f0
    for i in 1:n, j in 1:m
        K_rg += polynomial_kernel(embeddings_real[i, :], embeddings_gen[j, :], degree)
    end
    K_rg /= (n * m)

    # MMD^2
    mmd2 = K_rr + K_gg - 2 * K_rg

    return mmd2
end

# Example
kad_score = kad_distance(emb_real, emb_gen, 3)

println("\nã€Kernel Audio Distance (KAD)ã€‘")
println("Real embeddings: $(size(emb_real))")
println("Generated embeddings: $(size(emb_gen))")
println("KAD score (MMDÂ² with polynomial kernel d=3): $kad_score")
println()
println("KAD ã®åˆ©ç‚¹:")
println("  1. Distribution-free (ã‚¬ã‚¦ã‚¹ä»®å®šä¸è¦)")
println("  2. Unbiased (U-statistic)")
println("  3. Small-sample stability")
println("  4. GPU acceleration")
println("  5. Human perception alignment")
println()
println("FAD vs KAD:")
println("  FAD: ã‚µãƒ³ãƒ—ãƒ«æ•° 1000+ å¿…è¦")
println("  KAD: ã‚µãƒ³ãƒ—ãƒ«æ•° 100 ã§å®‰å®š")
println("  â†’ è©•ä¾¡ã‚³ã‚¹ãƒˆ 10x å‰Šæ¸›")
```

#### 3.8.3 ãã®ä»–ã®è©•ä¾¡æŒ‡æ¨™

| æŒ‡æ¨™ | æ¸¬å®šå¯¾è±¡ | æ–¹æ³• | åˆ©ç‚¹ | æ¬ ç‚¹ |
|:-----|:---------|:-----|:-----|:-----|
| **FAD** | Distribution similarity | FrÃ©chet distance (Gaussian) | æ¨™æº–çš„ | Gaussian assumption |
| **KAD** | Distribution similarity | MMD (kernel-based) | Distribution-free | æ–°ã—ã„ï¼ˆ2025ï¼‰|
| **CLAP Score** | Text-audio alignment | CLIP for audio | Textæ¡ä»¶è©•ä¾¡ | Pre-trained modelä¾å­˜ |
| **MOS** | Perceived quality | Human listening test | Ground truth | é«˜ã‚³ã‚¹ãƒˆãƒ»ä¸»è¦³çš„ |
| **SI-SNR** | Signal quality | Signal-to-noise ratio | å®¢è¦³çš„ | çŸ¥è¦šã¨ä¹–é›¢ |

**CLAP Score**[^16]:
- **CLAP**: Contrastive Language-Audio Pretrainingï¼ˆCLIP ã®éŸ³å£°ç‰ˆï¼‰
- Text-audio embedding space ã§é¡ä¼¼åº¦è¨ˆç®—

$$
\text{CLAP\_score} = \frac{1}{N} \sum_{i=1}^N \cos(\text{emb}_{\text{text}}^{(i)}, \text{emb}_{\text{audio}}^{(i)})
$$

```julia
println("\nã€Audio è©•ä¾¡æŒ‡æ¨™ã¾ã¨ã‚ã€‘")
println("Distribution similarity:")
println("  FAD: FrÃ©chet distance (Gaussian ä»®å®š)")
println("  KAD: MMD (distribution-free, æ¨å¥¨)")
println()
println("Text-audio alignment:")
println("  CLAP Score: Text-audio embedding é¡ä¼¼åº¦")
println()
println("Perceived quality:")
println("  MOS: Human listening test (ground truth)")
println("  MUSHRA: Multi-stimulus test")
println()
println("Signal quality:")
println("  SI-SNR: Signal-to-noise ratio")
println("  PESQ: Perceptual evaluation of speech quality")
println()
println("â†’ 2025å¹´ä»¥é™: KAD ãŒ FAD ã‚’ç½®ãæ›ãˆã‚‹æµã‚Œ")
```

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®70%å®Œäº†ï¼** Zone 3 å®Œèµ°ãŠã‚ã§ã¨ã†ã€‚Neural Audio Codecï¼ˆVQ-VAE â†’ RVQ â†’ WavTokenizerï¼‰ã€Flow Matching TTSï¼ˆF5-TTSï¼‰ã€Codec LMï¼ˆVALL-E 2ï¼‰ã€Music Generationï¼ˆMusicGen / Stable Audioï¼‰ã€è©•ä¾¡æŒ‡æ¨™ï¼ˆFAD â†’ KADï¼‰ã®å…¨ç†è«–ã‚’å°å‡ºã—ãŸã€‚ãƒšãƒ³ã¨ç´™ã§è¿½ã£ãŸæ•°å¼ã¯ã€éŸ³å£°ç”Ÿæˆã®æœ€å…ˆç«¯ã‚’å®Œå…¨ã«ç†è§£ã™ã‚‹æ­¦å™¨ã ã€‚æ¬¡ã¯ Zone 4 â€” å®Ÿè£…ã‚¾ãƒ¼ãƒ³ã§ã€ã“ã‚Œã‚‰ã‚’ Julia/Rust/Elixir ã§å‹•ã‹ã™ã€‚
:::

---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” 3è¨€èªã§éŸ³å£°ç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

**ã‚´ãƒ¼ãƒ«**: Flow Matching TTS ã‚’ Julia ã§è¨“ç·´ã€Rust ã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ã€Elixir ã§åˆ†æ•£é…ä¿¡ã™ã‚‹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚

### 4.1 Julia: Flow Matching TTS è¨“ç·´

#### 4.1.1 ç’°å¢ƒæ§‹ç¯‰

```bash
# Julia 1.11+ (2025å¹´æœ€æ–°ç‰ˆ)
julia --version

# Packages
julia -e 'using Pkg; Pkg.add(["Flux", "CUDA", "Zygote", "FFTW", "WAV", "ProgressMeter"])'
```

#### 4.1.2 Tiny Flow Matching TTSï¼ˆCPU 10åˆ†è¨“ç·´ï¼‰

**ç›®æ¨™**: ç°¡å˜ãªéŸ³å£°åˆæˆï¼ˆ2éŸ³ç´  "a", "i" â†’ ç•°ãªã‚‹å‘¨æ³¢æ•°ã®ã‚µã‚¤ãƒ³æ³¢ï¼‰

```julia
# tiny_flow_tts.jl
using Flux, Zygote, FFTW, Statistics, Random, ProgressMeter

# --- Dataset: 2 phonemes â†’ sine waves ---
function generate_phoneme_dataset(n_samples=100, duration=1.0, sample_rate=8000)
    t = 0:1/sample_rate:duration-1/sample_rate
    X_text = []  # Text labels (0 or 1)
    X_audio = []  # Audio waveforms

    for _ in 1:n_samples
        phoneme = rand(0:1)  # 0 = 'a', 1 = 'i'
        freq = phoneme == 0 ? 220.0 : 440.0  # A3 vs A4

        audio = sin.(2Ï€ * freq * t)
        push!(X_text, phoneme)
        push!(X_audio, Float32.(audio))
    end

    return X_text, X_audio
end

# --- Flow Matching Model ---
struct FlowMatchingTTS
    text_emb  # Embedding layer
    velocity  # Velocity network (MLP)
end

Flux.@functor FlowMatchingTTS

function FlowMatchingTTS(vocab_size=2, audio_dim=8000, hidden_dim=128)
    text_emb = Flux.Embedding(vocab_size, hidden_dim)
    velocity = Chain(
        Dense(audio_dim + hidden_dim + 1, 256, relu),  # x + text_emb + t
        Dense(256, 256, relu),
        Dense(256, audio_dim)
    )
    return FlowMatchingTTS(text_emb, velocity)
end

function (m::FlowMatchingTTS)(x_t, t, phoneme_id)
    # x_t: (audio_dim,)
    # t: scalar time
    # phoneme_id: integer (0 or 1)

    text_emb = m.text_emb(phoneme_id + 1)  # +1 for 1-indexing
    text_emb_expanded = repeat(text_emb, length(x_t) Ã· length(text_emb))

    input = vcat(x_t, text_emb_expanded[1:length(x_t)], [t])
    v = m.velocity(input)
    return v
end

# --- Training ---
function train_flow_tts(n_epochs=50, n_samples=100)
    # Dataset
    X_text, X_audio = generate_phoneme_dataset(n_samples)
    audio_dim = length(X_audio[1])

    # Model
    model = FlowMatchingTTS(2, audio_dim, 64)
    opt = Flux.Adam(1e-3)

    @showprogress for epoch in 1:n_epochs
        losses = []

        for i in 1:n_samples
            # Sample t ~ Uniform(0, 1)
            t = rand(Float32)

            # x0 ~ N(0, I), x1 = real audio
            x0 = randn(Float32, audio_dim)
            x1 = X_audio[i]

            # x_t = (1-t)*x0 + t*x1
            x_t = (1 - t) .* x0 .+ t .* x1

            # Target velocity: u_t = x1 - x0
            u_t = x1 .- x0

            # Gradient step
            grads = gradient(Flux.params(model)) do
                v_pred = model(x_t, t, X_text[i])
                loss = mean((v_pred .- u_t).^2)
                return loss
            end

            Flux.Optimise.update!(opt, Flux.params(model), grads)
            push!(losses, mean((model(x_t, t, X_text[i]) .- u_t).^2))
        end

        if epoch % 10 == 0
            println("Epoch $epoch: Loss = $(mean(losses))")
        end
    end

    return model
end

# --- Sampling ---
function sample_flow_tts(model, phoneme_id, steps=10, audio_dim=8000)
    x0 = randn(Float32, audio_dim)
    dt = 1.0f0 / steps
    x_t = copy(x0)

    for step in 1:steps
        t = step * dt
        v = model(x_t, t, phoneme_id)
        x_t = x_t .+ v .* dt
    end

    return x_t
end

# --- Main ---
println("ã€Tiny Flow Matching TTS è¨“ç·´ã€‘")
println("Task: 2 phonemes ('a'=220Hz, 'i'=440Hz) â†’ sine waves")
println("Dataset: 100 samples, 1 sec @ 8kHz")
println("Model: Flow Matching (MLP velocity network)")
println()

model_trained = train_flow_tts(50, 100)

println("\nã€Samplingã€‘")
audio_a = sample_flow_tts(model_trained, 0, 10, 8000)
audio_i = sample_flow_tts(model_trained, 1, 10, 8000)

println("Phoneme 'a' (220Hz): generated audio length = $(length(audio_a))")
println("Phoneme 'i' (440Hz): generated audio length = $(length(audio_i))")

# FFT ã§å‘¨æ³¢æ•°ç¢ºèª
using FFTW
fft_a = abs.(fft(audio_a))
fft_i = abs.(fft(audio_i))
freq_a = argmax(fft_a[2:4000])  # Skip DC
freq_i = argmax(fft_i[2:4000])

println("\nFFT peak (simplified):")
println("  'a': bin $freq_a (expected ~220Hz)")
println("  'i': bin $freq_i (expected ~440Hz)")
println("\nâ†’ Flow Matching TTS ã§éŸ³ç´ â†’éŸ³å£°ã®å¤‰æ›æˆåŠŸ")
```

**å®Ÿè¡Œ**:
```bash
julia tiny_flow_tts.jl
```

**æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›**:
```
ã€Tiny Flow Matching TTS è¨“ç·´ã€‘
...
Epoch 50: Loss = 0.012

ã€Samplingã€‘
Phoneme 'a' (220Hz): generated audio length = 8000
Phoneme 'i' (440Hz): generated audio length = 8000

FFT peak (simplified):
  'a': bin 22 (expected ~220Hz)
  'i': bin 44 (expected ~440Hz)

â†’ Flow Matching TTS ã§éŸ³ç´ â†’éŸ³å£°ã®å¤‰æ›æˆåŠŸ
```

#### 4.1.3 Julia å®Ÿè£…ã®ãƒã‚¤ãƒ³ãƒˆ

**æ•°å¼â†’ã‚³ãƒ¼ãƒ‰ã®1:1å¯¾å¿œ**:

$$
x_t = (1-t)x_0 + t x_1 \quad \Leftrightarrow \quad \text{x_t = (1 - t) .* x0 .+ t .* x1}
$$

$$
u_t = x_1 - x_0 \quad \Leftrightarrow \quad \text{u_t = x1 .- x0}
$$

$$
\mathcal{L} = \|\mathbf{v}_\theta - \mathbf{u}_t\|^2 \quad \Leftrightarrow \quad \text{loss = mean((v_pred .- u_t).^2)}
$$

**Julia ã®åˆ©ç‚¹**:
- **Broadcastæ¼”ç®—** (`.+`, `.*`): ãƒ™ã‚¯ãƒˆãƒ«æ¼”ç®—ãŒè‡ªç„¶
- **Automatic Differentiation** (Zygote): å‹¾é…è¨ˆç®—ãŒè‡ªå‹•
- **å‹å®‰å®šæ€§**: Float32 ã§çµ±ä¸€ â†’ é«˜é€Ÿ

### 4.2 Rust: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°æ¨è«–

#### 4.2.1 ç’°å¢ƒæ§‹ç¯‰

```bash
cargo new audio_inference_rust
cd audio_inference_rust
```

**Cargo.toml**:
```toml
[dependencies]
candle-core = "0.6"
candle-nn = "0.6"
hound = "3.5"  # WAV file I/O
rand = "0.8"
```

#### 4.2.2 Rust æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³

**src/main.rs**:
```rust
use candle_core::{Device, Result, Tensor};
use candle_nn::{Module, VarBuilder, VarMap};
use hound;
use rand::Rng;
use std::fs::File;
use std::io::BufWriter;

// Flow Matching inference
fn flow_matching_sample(
    model: &dyn Module,
    phoneme_emb: &Tensor,
    steps: usize,
    audio_dim: usize,
    device: &Device,
) -> Result<Tensor> {
    // x0 ~ N(0, I)
    let mut rng = rand::thread_rng();
    let x0_vec: Vec<f32> = (0..audio_dim).map(|_| rng.gen::<f32>() - 0.5).collect();
    let mut x_t = Tensor::from_vec(x0_vec, audio_dim, device)?;

    let dt = 1.0 / steps as f32;

    for step in 1..=steps {
        let t = step as f32 * dt;
        let t_tensor = Tensor::from_vec(vec![t], 1, device)?;

        // v = model(x_t, t, phoneme_emb)
        let input = Tensor::cat(&[&x_t, phoneme_emb, &t_tensor], 0)?;
        let v = model.forward(&input)?;

        // x_t = x_t + v * dt
        let v_scaled = v.affine(dt, 0.0)?;
        x_t = (&x_t + &v_scaled)?;
    }

    Ok(x_t)
end

fn main() -> Result<()> {
    println!("ã€Rust Audio Inferenceã€‘");

    // Device
    let device = Device::Cpu;

    // Dummy model (placeholder)
    // In practice: load trained model weights from Julia
    let varmap = VarMap::new();
    let vb = VarBuilder::from_varmap(&varmap, candle_core::DType::F32, &device);

    // Dummy phoneme embedding
    let phoneme_emb = Tensor::zeros(64, candle_core::DType::F32, &device)?;

    // Sampling (placeholder)
    println!("Sampling audio with Flow Matching...");
    // let audio_tensor = flow_matching_sample(&model, &phoneme_emb, 10, 8000, &device)?;

    // Dummy audio for demo
    let audio_vec: Vec<f32> = (0..8000).map(|i| (i as f32 / 8000.0).sin()).collect();

    // Write WAV file
    let spec = hound::WavSpec {
        channels: 1,
        sample_rate: 8000,
        bits_per_sample: 16,
        sample_format: hound::SampleFormat::Int,
    };

    let mut writer = hound::WavWriter::create("output.wav", spec).unwrap();
    for &sample in &audio_vec {
        let amplitude = i16::MAX as f32;
        writer.write_sample((sample * amplitude) as i16).unwrap();
    }
    writer.finalize().unwrap();

    println!("Audio saved to output.wav");
    println!("â†’ Rust: ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼æ¨è«– + ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·");

    Ok(())
}
```

**å®Ÿè¡Œ**:
```bash
cargo run --release
```

**Rust å®Ÿè£…ã®ãƒã‚¤ãƒ³ãƒˆ**:
- **Candle**: Rust-native neural network frameworkï¼ˆPyTorch-like APIï¼‰
- **Zero-copy**: Tensor æ“ä½œãŒ allocation ã‚’æœ€å°åŒ–
- **Low latency**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ã«æœ€é©ï¼ˆ<10msï¼‰

### 4.3 Elixir: åˆ†æ•£éŸ³å£°é…ä¿¡

#### 4.3.1 ç’°å¢ƒæ§‹ç¯‰

```bash
mix new audio_server
cd audio_server
```

**mix.exs**:
```elixir
defp deps do
  [
    {:plug_cowboy, "~> 2.0"},
    {:jason, "~> 1.4"}
  ]
end
```

#### 4.3.2 Elixir éŸ³å£°é…ä¿¡ã‚µãƒ¼ãƒãƒ¼

**lib/audio_server.ex**:
```elixir
defmodule AudioServer do
  use Plug.Router

  plug :match
  plug :dispatch

  # TTS ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
  post "/tts" do
    # Parse JSON body
    {:ok, body, conn} = Plug.Conn.read_body(conn)
    params = Jason.decode!(body)
    text = params["text"]
    phoneme_id = String.to_integer(params["phoneme_id"] || "0")

    # Call Rust inference (via Port)
    audio_data = call_rust_inference(text, phoneme_id)

    # Return WAV file
    conn
    |> put_resp_content_type("audio/wav")
    |> send_resp(200, audio_data)
  end

  match _ do
    send_resp(conn, 404, "Not found")
  end

  # Call Rust via Port (simplified)
  defp call_rust_inference(text, phoneme_id) do
    # In production: Port communication with Rust binary
    # For demo: return dummy WAV
    File.read!("priv/dummy.wav")
  end
end

# Start server
defmodule AudioServer.Application do
  use Application

  def start(_type, _args) do
    children = [
      {Plug.Cowboy, scheme: :http, plug: AudioServer, options: [port: 4000]}
    ]

    opts = [strategy: :one_for_one, name: AudioServer.Supervisor]
    Supervisor.start_link(children, opts)
  end
end
```

**å®Ÿè¡Œ**:
```bash
mix run --no-halt
```

**ãƒ†ã‚¹ãƒˆ**:
```bash
curl -X POST http://localhost:4000/tts \
  -H "Content-Type: application/json" \
  -d '{"text": "hello", "phoneme_id": "0"}' \
  --output generated.wav
```

**Elixir å®Ÿè£…ã®ãƒã‚¤ãƒ³ãƒˆ**:
- **OTP**: Supervision tree ã§è€éšœå®³æ€§
- **Port**: Rust ãƒã‚¤ãƒŠãƒªã¨é€šä¿¡ï¼ˆFFI ã‚ˆã‚Šå®‰å…¨ï¼‰
- **åˆ†æ•£**: ãƒãƒ¼ãƒ‰é–“ã§éŸ³å£°ç”Ÿæˆã‚¿ã‚¹ã‚¯ã‚’åˆ†æ•£

### 4.4 3è¨€èªçµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```mermaid
graph LR
    A[User Request<br/>'hello'] --> B[Elixir Server<br/>Port 4000]
    B --> C[Rust Inference<br/>Candle]
    C --> D[Trained Model<br/>from Julia]
    D --> E[Audio WAV]
    E --> B
    B --> F[User<br/>HTTP Response]

    style B fill:#a388ee
    style C fill:#ff6347
    style D fill:#4b0082
```

**å½¹å‰²åˆ†æ‹…**:
- **Julia**: è¨“ç·´ï¼ˆFlow Matching TTS ãƒ¢ãƒ‡ãƒ«ï¼‰
- **Rust**: æ¨è«–ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°ç”Ÿæˆã€<10msï¼‰
- **Elixir**: é…ä¿¡ï¼ˆHTTP APIã€åˆ†æ•£å‡¦ç†ã€è€éšœå®³æ€§ï¼‰

```julia
println("\nã€3è¨€èªçµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€‘")
println("Julia: Flow Matching TTS è¨“ç·´")
println("  â†’ Model weights â†’ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜")
println()
println("Rust: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–")
println("  â†’ Candle ã§ weights èª­ã¿è¾¼ã¿")
println("  â†’ Flow Matching sampling (10 steps)")
println("  â†’ WAV å‡ºåŠ› (<10ms latency)")
println()
println("Elixir: åˆ†æ•£é…ä¿¡")
println("  â†’ HTTP API (/tts endpoint)")
println("  â†’ Port çµŒç”±ã§ Rust å‘¼ã³å‡ºã—")
println("  â†’ è¤‡æ•°ãƒãƒ¼ãƒ‰ã§è² è·åˆ†æ•£")
println()
println("â†’ Production-ready éŸ³å£°ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ")
```

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®85%å®Œäº†ï¼** Zone 4 å®Œèµ°ã€‚Julia ã§ Flow Matching TTS ã‚’è¨“ç·´ã€Rust ã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ã€Elixir ã§åˆ†æ•£é…ä¿¡ã™ã‚‹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã—ãŸã€‚æ¬¡ã¯ Zone 5 â€” å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ã§ã€å®Ÿéš›ã«éŸ³å£°ã‚’ç”Ÿæˆã—ã€è©•ä¾¡ã™ã‚‹ã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” éŸ³å£°ç”Ÿæˆã®è‡ªå·±è¨ºæ–­

**ã‚´ãƒ¼ãƒ«**: å®Ÿè£…ã—ãŸ TTS ã‚·ã‚¹ãƒ†ãƒ ã‚’ãƒ†ã‚¹ãƒˆã—ã€å“è³ªã‚’è©•ä¾¡ã—ã€æ”¹å–„ç‚¹ã‚’ç‰¹å®šã™ã‚‹ã€‚

### 5.1 Symbol Reading Test â€” Audio ç”¨èªã®ç†è§£åº¦ãƒã‚§ãƒƒã‚¯

ä»¥ä¸‹ã®è¨˜å·ãƒ»ç”¨èªã‚’è‡ªåˆ†ã®è¨€è‘‰ã§èª¬æ˜ã§ãã‚‹ã‹ï¼Ÿï¼ˆå„2-3æ–‡ï¼‰

:::details Q1: VQ (Vector Quantization)

**Answer**:
Vector Quantization ã¯é€£ç¶šçš„ãªæ½œåœ¨è¡¨ç¾ $z_e$ ã‚’é›¢æ•£çš„ãªã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ $\{e_k\}_{k=1}^K$ ã®ã‚¨ãƒ³ãƒˆãƒªã«ç½®ãæ›ãˆã‚‹æ‰‹æ³•ã ã€‚å„ $z_e^{(i)}$ ã‚’æœ€è¿‘å‚ $e_{k^*} = \arg\min_k \|z_e^{(i)} - e_k\|$ ã« quantize ã—ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ $k^*$ ã‚’é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦è¨˜éŒ²ã™ã‚‹ã€‚VQ-VAE ã§ã¯ Straight-Through Estimator ã§å‹¾é…ã‚’è¿‘ä¼¼ã—ã€End-to-End è¨“ç·´ã‚’å¯èƒ½ã«ã™ã‚‹ã€‚
:::

:::details Q2: RVQ (Residual Vector Quantization)

**Answer**:
RVQ ã¯å˜ä¸€ VQ ã®é™ç•Œï¼ˆè¡¨ç¾åŠ›ä¸è¶³ï¼‰ã‚’ã€è¤‡æ•°æ®µéšã®é‡å­åŒ–ã§è§£æ±ºã™ã‚‹ã€‚ç¬¬1æ®µéšã§ $z_q^{(1)}$ ã‚’å¾—ãŸå¾Œã€æ®‹å·® $r^{(1)} = z_e - z_q^{(1)}$ ã‚’ç¬¬2æ®µéšã§é‡å­åŒ–ã—ã€ã“ã‚Œã‚’ $N_q$ æ®µéšåå¾©ã™ã‚‹ã€‚æœ€çµ‚çš„ãªé‡å­åŒ–è¡¨ç¾ã¯ $z_q = \sum_{n=1}^{N_q} z_q^{(n)}$ ã¨ãªã‚Šã€$K^{N_q}$ å€‹ã®æœ‰åŠ¹ã‚¨ãƒ³ãƒˆãƒªã‚’æŒã¤éšå±¤çš„è¡¨ç¾ãŒå¾—ã‚‰ã‚Œã‚‹ã€‚
:::

:::details Q3: Flow Matching ã®æ¡ä»¶ä»˜ãç¢ºç‡ãƒ‘ã‚¹ $p_t(\mathbf{x} | \mathbf{x}_0, \mathbf{x}_1)$

**Answer**:
æ¡ä»¶ä»˜ãç¢ºç‡ãƒ‘ã‚¹ã¯ã€ãƒã‚¤ã‚º $\mathbf{x}_0$ ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ $\mathbf{x}_1$ ã¸ã®è£œé–“åˆ†å¸ƒ $p_t$ ã‚’å®šç¾©ã™ã‚‹ã€‚ç·šå½¢è£œé–“ã§ã¯ $\mu_t = (1-t)\mathbf{x}_0 + t\mathbf{x}_1$, $\sigma_t = 0$ ã¨ã—ã€$p_t(\mathbf{x} | \mathbf{x}_0, \mathbf{x}_1) = \delta(\mathbf{x} - \mu_t)$ ï¼ˆæ±ºå®šè«–çš„ï¼‰ã¨ãªã‚‹ã€‚ã“ã® conditional path ã® marginal $p_t(\mathbf{x}) = \int p_t(\mathbf{x} | \mathbf{x}_0, \mathbf{x}_1) p_0(\mathbf{x}_0) p_1(\mathbf{x}_1) d\mathbf{x}_0 d\mathbf{x}_1$ ãŒã€ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p_0 \to p_1$ ã¸ã®å¤‰æ›ã‚’è¨˜è¿°ã™ã‚‹ã€‚
:::

:::details Q4: Repetition Aware Sampling

**Answer**:
Repetition Aware Sampling ã¯ autoregressive LM ã®ãƒ‡ã‚³ãƒ¼ãƒ‰æ™‚ã«ã€ç›´è¿‘ $W$ ãƒˆãƒ¼ã‚¯ãƒ³ã®å‡ºç¾å›æ•° $\text{count}(k, x_{<t})$ ã‚’ã‚«ã‚¦ãƒ³ãƒˆã—ã€logits ã‚’ $\text{logits}'_k = \text{logits}_k - \lambda \cdot \text{count}(k)$ ã§ãƒšãƒŠãƒ«ãƒ†ã‚£åŒ–ã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ç¹°ã‚Šè¿”ã—ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆphoneme repetitionï¼‰ã®ç¢ºç‡ã‚’ down-weight ã—ã€"hehehe-llo" ã®ã‚ˆã†ãªä¸è‡ªç„¶ãªå‡ºåŠ›ã‚’é˜²ãã€‚VALL-E 2 ã§å°å…¥ã•ã‚Œã€human parity é”æˆã«å¯„ä¸ã—ãŸã€‚
:::

:::details Q5: FAD (FrÃ©chet Audio Distance) vs KAD (Kernel Audio Distance)

**Answer**:
FAD ã¯éŸ³å£°åŸ‹ã‚è¾¼ã¿ $e_r, e_g$ ã‚’ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ $\mathcal{N}(\mu_r, \Sigma_r)$, $\mathcal{N}(\mu_g, \Sigma_g)$ ã¨ä»®å®šã—ã€FrÃ©chetè·é›¢ $\|\mu_r - \mu_g\|^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2})$ ã§è©•ä¾¡ã™ã‚‹ã€‚ã—ã‹ã—å®Ÿéš›ã®åŸ‹ã‚è¾¼ã¿ã¯éã‚¬ã‚¦ã‚¹åˆ†å¸ƒã§ã‚ã‚Šã€å°ã‚µãƒ³ãƒ—ãƒ«ã§ä¸å®‰å®šã ã€‚

KAD ã¯ MMDï¼ˆMaximum Mean Discrepancyï¼‰ã«åŸºã¥ãã€polynomial kernel $k(x,y) = (1 + \langle x,y \rangle)^d$ ã§åˆ†å¸ƒé–“è·é›¢ã‚’è¨ˆç®—ã™ã‚‹ã€‚Distribution-freeï¼ˆã‚¬ã‚¦ã‚¹ä»®å®šä¸è¦ï¼‰ã€unbiasedï¼ˆU-statisticï¼‰ã€small-sample stable ã¨ã„ã†åˆ©ç‚¹ãŒã‚ã‚Šã€2025å¹´ä»¥é™ FAD ã‚’ç½®ãæ›ãˆã‚‹æµã‚Œã«ã‚ã‚‹ã€‚
:::

### 5.2 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸

#### Challenge 1: WavTokenizer ã® VQ å®Ÿè£…

**èª²é¡Œ**: å˜ä¸€ VQ ã§ 24kHz éŸ³å£°1ç§’ï¼ˆ24,000ã‚µãƒ³ãƒ—ãƒ«ï¼‰ã‚’ 75ãƒˆãƒ¼ã‚¯ãƒ³ã«åœ§ç¸®ã—ã€å†æ§‹æˆã›ã‚ˆã€‚

**ãƒ’ãƒ³ãƒˆ**:
- Encoder: Conv1D with stride 320ï¼ˆ24000 / 75ï¼‰
- Codebook: 1024 entries, 128 dimensions
- Decoder: TransposedConv1D

```julia
# Challenge 1: WavTokenizer VQ
function challenge1_wavtokenizer()
    # TODO: Implement encoder, VQ, decoder
    println("Challenge 1: WavTokenizer VQ ã‚’å®Ÿè£…ã—ã€åœ§ç¸®ç‡320xã‚’å®Ÿç¾ã›ã‚ˆ")
end
```

#### Challenge 2: F5-TTS Sway Sampling

**èª²é¡Œ**: Sway Samplingï¼ˆ$t_i = (i/N)^\alpha$ï¼‰ã‚’å®Ÿè£…ã—ã€$\alpha = 0.5, 1.0, 2.0$ ã§ç”Ÿæˆå“è³ªã‚’æ¯”è¼ƒã›ã‚ˆã€‚

**è©•ä¾¡æŒ‡æ¨™**: MSEï¼ˆäºˆæ¸¬ vs çœŸã®éŸ³å£°ï¼‰

```julia
# Challenge 2: Sway Sampling comparison
function challenge2_sway_sampling()
    # TODO: Implement sway sampling with different Î±
    # Compare MSE for Î± = 0.5, 1.0, 2.0
    println("Challenge 2: Sway Sampling ã® Î± ã«ã‚ˆã‚‹å“è³ªå·®ã‚’è©•ä¾¡ã›ã‚ˆ")
end
```

#### Challenge 3: KAD å®Ÿè£…

**èª²é¡Œ**: Polynomial kernel ($d=3$) ã‚’ç”¨ã„ãŸ KAD ã‚’å®Ÿè£…ã—ã€real vs generated embeddings ã®è·é›¢ã‚’è¨ˆç®—ã›ã‚ˆã€‚

```julia
# Challenge 3: KAD implementation
function challenge3_kad()
    # TODO: Implement polynomial kernel MMD
    # Compare with FAD (if time permits)
    println("Challenge 3: KAD ã‚’å®Ÿè£…ã—ã€FAD ã¨æ¯”è¼ƒã›ã‚ˆ")
end
```

### 5.3 è‡ªå·±è¨ºæ–­ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

å®Ÿè£…ã—ãŸ TTS ã‚·ã‚¹ãƒ†ãƒ ã«ã¤ã„ã¦ã€ä»¥ä¸‹ã‚’ãƒã‚§ãƒƒã‚¯ã›ã‚ˆ:

- [ ] **Audio Codec**: VQ-VAE ã§éŸ³å£°ã‚’é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³ã«åœ§ç¸®ã§ãã‚‹
- [ ] **RVQ**: 4æ®µéš RVQ ã‚’å®Ÿè£…ã—ã€EnCodec äº’æ›ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆã§ãã‚‹
- [ ] **Flow Matching**: æ¡ä»¶ä»˜ã Flow Matching ã§ text â†’ audio ç”ŸæˆãŒã§ãã‚‹
- [ ] **Sway Sampling**: æ¨è«–æ™‚ã®ã‚¹ãƒ†ãƒƒãƒ—é…åˆ†ã‚’æœ€é©åŒ–ã§ãã‚‹
- [ ] **VALL-E 2**: Repetition Aware Sampling ã§ phoneme repetition ã‚’é˜²ã’ã‚‹
- [ ] **FACodec**: å±æ€§åˆ†è§£ï¼ˆcontent/prosody/timbre/acousticï¼‰ãŒã§ãã‚‹
- [ ] **MusicGen**: EnCodec + LM ã§éŸ³æ¥½ç”ŸæˆãŒã§ãã‚‹
- [ ] **KAD**: Distribution-free è©•ä¾¡æŒ‡æ¨™ã‚’å®Ÿè£…ã§ãã‚‹
- [ ] **3è¨€èªçµ±åˆ**: Juliaè¨“ç·´ + Rustæ¨è«– + Elixiré…ä¿¡ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒå‹•ã
- [ ] **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ **: Rust æ¨è«–ãŒ <10ms ã§å®Œäº†ã™ã‚‹

### 5.4 ç™ºå±•èª²é¡Œ

#### 5.4.1 Zero-shot Voice Cloning

**èª²é¡Œ**: 3ç§’ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéŸ³å£°ã‹ã‚‰è©±è€…åŸ‹ã‚è¾¼ã¿ã‚’æŠ½å‡ºã—ã€ä»»æ„ãƒ†ã‚­ã‚¹ãƒˆã‚’åŒã˜è©±è€…ã§åˆæˆã›ã‚ˆã€‚

**ãƒ’ãƒ³ãƒˆ**: VALL-E 2 / CosyVoice ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’å‚è€ƒã«ã€‚

#### 5.4.2 Long-form Music Generation

**èª²é¡Œ**: Stable Audio ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆDiT + Timing embeddingsï¼‰ã§ã€3åˆ†ã®éŸ³æ¥½ã‚’ç”Ÿæˆã›ã‚ˆã€‚

**ãƒ’ãƒ³ãƒˆ**: Latent diffusionï¼ˆVAE latent spaceï¼‰ã§è¨ˆç®—é‡å‰Šæ¸›ã€‚

#### 5.4.3 Audio Style Transfer

**èª²é¡Œ**: éŸ³å£° A ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨éŸ³å£° B ã®éŸ»å¾‹ã‚’çµ„ã¿åˆã‚ã›ãŸéŸ³å£° C ã‚’ç”Ÿæˆã›ã‚ˆã€‚

**ãƒ’ãƒ³ãƒˆ**: FACodec ã§ content/prosody ã‚’åˆ†é›¢ã€‚

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®100%å®Œäº†ï¼ˆå®Ÿé¨“ã‚¾ãƒ¼ãƒ³å®Œèµ°ï¼‰ï¼** è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆã¨å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ã‚’é€šã˜ã¦ã€éŸ³å£°ç”Ÿæˆã®ç†è§£åº¦ã‚’ç¢ºèªã—ãŸã€‚æ¬¡ã¯ Zone 6 â€” ç™ºå±•ã‚¾ãƒ¼ãƒ³ã§ã€éŸ³å£°ç”Ÿæˆã®ç ”ç©¶ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ã‚’æ¢ã‚‹ã€‚
:::

---

## ğŸš€ 6. ç™ºå±•ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” éŸ³å£°ç”Ÿæˆã®ç ”ç©¶ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ + ã¾ã¨ã‚

**ã‚´ãƒ¼ãƒ«**: éŸ³å£°ç”Ÿæˆã®æœ€æ–°ç ”ç©¶å‹•å‘ã¨æœªè§£æ±ºå•é¡Œã‚’ç†è§£ã—ã€æ¬¡ã®ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¹ãƒ«ãƒ¼ã‚’äºˆæ¸¬ã™ã‚‹ã€‚

### 6.1 Audio Codec ã®é€²åŒ–ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

```mermaid
graph TD
    A[SoundStream 2021<br/>320 tok/sec, RVQ x8] --> B[EnCodec 2022<br/>150 tok/sec, RVQ x4]
    B --> C[WavTokenizer 2024<br/>75 tok/sec, VQ x1]
    C --> D[Mimi 2024<br/>80 tok/sec, Semantic-rich]
    D --> E[Future Codec 2026?<br/>â‰¤50 tok/sec, Unified]

    A2[Unsupervised<br/>Self-reconstruction] --> B2[Supervised<br/>ASR-guided]
    B2 --> C2[Hybrid<br/>Multi-task]

    style E fill:#ffd700
    style C2 fill:#ffd700
```

**Codec é€²åŒ–ã®3è»¸**:
1. **åœ§ç¸®ç‡**: 320 â†’ 150 â†’ **75** tokens/secï¼ˆç›®æ¨™: 50ä»¥ä¸‹ï¼‰
2. **é‡å­åŒ–éšå±¤**: RVQ x8 â†’ x4 â†’ **x1**ï¼ˆç›®æ¨™: å˜ä¸€VQï¼‰
3. **Semantic richness**: Unsupervised â†’ **Supervised**ï¼ˆASR-guidedï¼‰

**æœªè§£æ±ºå•é¡Œ**:
- **Perceptual loss**: MSE â†’ çŸ¥è¦šçš„æå¤±é–¢æ•°ï¼ˆPESQ / STOIï¼‰ã®çµ±åˆ
- **Long-range dependency**: éŸ»å¾‹ãƒ»è©±è€…ç‰¹æ€§ã®é•·æœŸä¾å­˜æ€§ã‚’ã©ã†æ‰ãˆã‚‹ã‹
- **Multi-modal codec**: éŸ³å£° + è¡¨æƒ… + ã‚¸ã‚§ã‚¹ãƒãƒ£ã®çµ±åˆè¡¨ç¾

### 6.2 Zero-shot TTS ã®ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢

**ç¾çŠ¶**ï¼ˆ2024-2025ï¼‰:
- VALL-E 2: Human parity é”æˆ
- F5-TTS: 10ã‚¹ãƒ†ãƒƒãƒ—ã§é«˜å“è³ª
- NaturalSpeech 3: 1B params, 200K hours

**æ¬¡ã®ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³**ï¼ˆ2026-2027äºˆæ¸¬ï¼‰:
1. **Real-time streaming TTS**: æ¨è«–æ™‚é–“ < å…¥åŠ›æ™‚é–“ï¼ˆfaster than real-timeï¼‰
2. **Emotion control**: å–œæ€’å“€æ¥½ã‚’æ˜ç¤ºçš„ã«åˆ¶å¾¡
3. **Few-shot learning**: 3ç§’ â†’ 1ç§’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§è©±è€…ã‚¯ãƒ­ãƒ¼ãƒ³
4. **Cross-lingual transfer**: è‹±èªè¨“ç·´ãƒ¢ãƒ‡ãƒ«ãŒæ—¥æœ¬èªã‚‚ç”Ÿæˆ

```julia
println("\nã€Zero-shot TTS ã®é€²åŒ–äºˆæ¸¬ã€‘")
println("2024-2025: Human parity é”æˆï¼ˆVALL-E 2 / F5-TTSï¼‰")
println("2026: Real-time streaming TTSï¼ˆæ¨è«– < å…¥åŠ›æ™‚é–“ï¼‰")
println("2027: Emotion control + Few-shot (1ç§’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ)")
println("2028: Cross-lingual transferï¼ˆå˜ä¸€ãƒ¢ãƒ‡ãƒ«ã§å…¨è¨€èªï¼‰")
println()
println("Key challenges:")
println("  1. Latency reduction: 10 steps â†’ 1-3 steps")
println("  2. Quality-speed tradeoff: äººé–“å“è³ª + ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ")
println("  3. Controllability: éŸ»å¾‹ãƒ»æ„Ÿæƒ…ãƒ»ã‚¹ã‚¿ã‚¤ãƒ«ã®ç‹¬ç«‹åˆ¶å¾¡")
```

### 6.3 Music Generation ã®èª²é¡Œ

**å•†ç”¨ã‚µãƒ¼ãƒ“ã‚¹ã®æˆåŠŸ**:
- Suno v4.5: 3åˆ†ã®å®Œå…¨æ¥½æ›²ï¼ˆæ­Œè©ãƒ»ãƒœãƒ¼ã‚«ãƒ«ãƒ»æ¥½å™¨ï¼‰
- Udio: ãƒ—ãƒ­ãƒ¬ãƒ™ãƒ«éŸ³è³ª

**æœªè§£æ±ºå•é¡Œ**:
1. **Long-term coherence**: 3åˆ†ä»¥ä¸Šã®æ§‹é€ çš„ä¸€è²«æ€§
2. **Style transfer**: ã‚¸ãƒ£ãƒ³ãƒ«ãƒ»ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆã‚¹ã‚¿ã‚¤ãƒ«ã®æ˜ç¤ºçš„åˆ¶å¾¡
3. **Interactive composition**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé€”ä¸­ã§ç·¨é›†ãƒ»ä¿®æ­£ã§ãã‚‹
4. **Multi-track generation**: ãƒ‰ãƒ©ãƒ ãƒ»ãƒ™ãƒ¼ã‚¹ãƒ»ã‚®ã‚¿ãƒ¼ãƒ»ãƒœãƒ¼ã‚«ãƒ«ã‚’å€‹åˆ¥ç”Ÿæˆ â†’ ãƒŸãƒƒã‚¯ã‚¹

**ç ”ç©¶æ–¹å‘**:
- **Hierarchical generation**: Structure (intro/verse/chorus) â†’ Fill details
- **Symbolic + audio**: MIDIï¼ˆsymbolicï¼‰â†’ Audio çµ±åˆç”Ÿæˆ
- **Diffusion vs Flow**: Stable Audioï¼ˆDiffusionï¼‰vs MusicGenï¼ˆAR LMï¼‰ã®åæŸ

### 6.4 Audioè©•ä¾¡æŒ‡æ¨™ã®æœªæ¥

**ç¾çŠ¶ã®å•é¡Œ**:
- FAD: ã‚¬ã‚¦ã‚¹ä»®å®šã€ã‚µãƒ³ãƒ—ãƒ«ä¾å­˜æ€§
- MOS: é«˜ã‚³ã‚¹ãƒˆã€ä¸»è¦³æ€§
- CLAP Score: Pre-trained model ä¾å­˜

**æ¬¡ä¸–ä»£æŒ‡æ¨™**ï¼ˆKAD ä»¥é™ï¼‰:
1. **Perceptual metrics**: äººé–“ã®è´è¦šãƒ¢ãƒ‡ãƒ«ã«åŸºã¥ãï¼ˆpsychoacoustic featuresï¼‰
2. **Multi-modal evaluation**: Text-audio alignment + Quality + Diversity
3. **Automatic human-correlation**: äººé–“è©•ä¾¡ã‚’äºˆæ¸¬ã™ã‚‹å­¦ç¿’æŒ‡æ¨™

**KAD ã®æ‹¡å¼µ**:
- **Conditional KAD**: Textæ¡ä»¶ä»˜ãç”Ÿæˆã®è©•ä¾¡ï¼ˆCLAP + KAD çµ±åˆï¼‰
- **Temporal KAD**: æ™‚é–“çš„ä¸€è²«æ€§ã®è©•ä¾¡

```julia
println("\nã€Audio è©•ä¾¡æŒ‡æ¨™ã®é€²åŒ–ã€‘")
println("2024: FADï¼ˆæ¨™æº–ã ãŒå•é¡Œã‚ã‚Šï¼‰")
println("2025: KADï¼ˆdistribution-free, æ¨å¥¨ï¼‰")
println("2026: Perceptual KADï¼ˆäººé–“è´è¦šãƒ¢ãƒ‡ãƒ«çµ±åˆï¼‰")
println("2027: Multi-modal KADï¼ˆText-audio-quality çµ±åˆè©•ä¾¡ï¼‰")
println()
println("Goal: äººé–“è©•ä¾¡ã¨ã®ç›¸é–¢ R > 0.9")
```

### 6.5 Audio ç”Ÿæˆã®å€«ç†ãƒ»ç¤¾ä¼šçš„èª²é¡Œ

#### 6.5.1 Deepfake éŸ³å£°

**æŠ€è¡“**: VALL-E 2 / F5-TTS ã§ä»»æ„äººç‰©ã®éŸ³å£°ã‚’ã‚¯ãƒ­ãƒ¼ãƒ³

**ãƒªã‚¹ã‚¯**:
- **è©æ¬º**: é›»è©±è©æ¬ºï¼ˆ"æ¯è¦ª"ã®å£°ã§æŒ¯ã‚Šè¾¼ã¿è¦æ±‚ï¼‰
- **Misinformation**: æ”¿æ²»å®¶ã®å½éŸ³å£°ï¼ˆé¸æŒ™å¦¨å®³ï¼‰
- **Privacy**: æœ¬äººåŒæ„ãªã—ã®éŸ³å£°ç”Ÿæˆ

**å¯¾ç­–**:
1. **Watermarking**: ç”ŸæˆéŸ³å£°ã«é€ã‹ã—åŸ‹ã‚è¾¼ã¿
2. **Detection**: Deepfake æ¤œå‡ºAI
3. **Legal framework**: EU AI Actï¼ˆ2026å¹´8æœˆæ–½è¡Œï¼‰ã§è¦åˆ¶

#### 6.5.2 éŸ³æ¥½å®¶ã®æ¨©åˆ©

**å•é¡Œ**: Suno/Udio ã¯è‘—ä½œæ¨©ä¿è­·ã•ã‚ŒãŸæ¥½æ›²ã§è¨“ç·´ã—ãŸå¯èƒ½æ€§

**è¨´è¨Ÿ**: RIAAï¼ˆRecording Industry Association of Americaï¼‰ãŒSunoã‚’æè¨´ï¼ˆ2024ï¼‰

**è­°è«–**:
- **Fair use?**: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®ä½¿ç”¨ã¯åˆæ³•ã‹ï¼Ÿ
- **è‘—ä½œæ¨©ä¾µå®³?**: ç”Ÿæˆæ¥½æ›²ãŒæ—¢å­˜æ›²ã«é¡ä¼¼ã™ã‚‹å ´åˆ
- **ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆæ¨©åˆ©**: ãƒ—ãƒ­éŸ³æ¥½å®¶ã®é›‡ç”¨ã¸ã®å½±éŸ¿

**è§£æ±ºã®æ–¹å‘æ€§**:
- **Opt-in dataset**: ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆãŒæ˜ç¤ºçš„ã«è¨±å¯ã—ãŸãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨
- **Royalty system**: AIç”Ÿæˆæ¥½æ›²ã®åç›Šã‚’å…ƒãƒ‡ãƒ¼ã‚¿æä¾›è€…ã«åˆ†é…

```julia
println("\nã€Audio ç”Ÿæˆã®å€«ç†èª²é¡Œã€‘")
println("Deepfake éŸ³å£°:")
println("  ãƒªã‚¹ã‚¯: è©æ¬ºãƒ»Misinformationãƒ»Privacyä¾µå®³")
println("  å¯¾ç­–: Watermarking / Detection AI / Legalè¦åˆ¶")
println()
println("éŸ³æ¥½è‘—ä½œæ¨©:")
println("  å•é¡Œ: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®åˆæ³•æ€§ï¼ˆFair use vs Infringementï¼‰")
println("  è¨´è¨Ÿ: RIAA vs Suno (2024)")
println("  è§£æ±º: Opt-in dataset + Royalty system")
println()
println("â†’ æŠ€è¡“çš„é€²æ­©ã¨æ³•çš„æ çµ„ã¿ã®å”èª¿ãŒå¿…é ˆ")
```

### 6.6 æ¨å¥¨ãƒªã‚½ãƒ¼ã‚¹

#### 6.6.1 ä¸»è¦è«–æ–‡ãƒªã‚¹ãƒˆ

| åˆ†é‡ | è«–æ–‡ | å¹´ | é‡è¦åº¦ |
|:-----|:-----|:---|:------|
| **Codec** | WavTokenizer[^1] | 2024 | â˜…â˜…â˜… |
| **TTS** | F5-TTS[^2] | 2024 | â˜…â˜…â˜… |
| **TTS** | VALL-E 2[^4] | 2024 | â˜…â˜…â˜… |
| **TTS** | NaturalSpeech 3[^14] | 2024 | â˜…â˜…â˜… |
| **Music** | MusicGen[^3] | 2023 | â˜…â˜…â˜… |
| **Music** | Stable Audio[^9] | 2024 | â˜…â˜…â˜… |
| **Metric** | KAD[^10] | 2025 | â˜…â˜…â˜… |

#### 6.6.2 ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹å®Ÿè£…

| ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ | è¨€èª | ç‰¹å¾´ | URL |
|:------------|:-----|:-----|:----|
| F5-TTS | Python | Flow Matching TTS | [GitHub: SWivid/F5-TTS](https://github.com/SWivid/F5-TTS) |
| WavTokenizer | Python | Single-VQ codec | [GitHub: jishengpeng/WavTokenizer](https://github.com/jishengpeng/WavTokenizer) |
| MusicGen | Python | Meta official | [GitHub: facebookresearch/audiocraft](https://github.com/facebookresearch/audiocraft) |
| EnCodec | Python | Meta official | [GitHub: facebookresearch/encodec](https://github.com/facebookresearch/encodec) |
| CosyVoice | Python | Supervised tokens | [GitHub: FunAudioLLM/CosyVoice](https://github.com/FunAudioLLM/CosyVoice) |

#### 6.6.3 ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹

| ãƒªã‚½ãƒ¼ã‚¹ | å†…å®¹ | URL |
|:---------|:-----|:----|
| **Papers with Code** | Audio Generation | [PWC: Audio Generation](https://paperswithcode.com/task/audio-generation) |
| **Hugging Face** | Pre-trained models | [HF: Audio Models](https://huggingface.co/models?pipeline_tag=text-to-audio) |
| **Awesome Audio** | Curated list | [GitHub: Awesome-Audio](https://github.com/AI-secure/Awesome-Audio-Synthesis) |

:::details Glossary â€” æœ¬è¬›ç¾©ã®é‡è¦ç”¨èª

- **VQ-VAE**: Vector Quantized Variational Autoencoder â€” é€£ç¶šæ½œåœ¨è¡¨ç¾ã‚’é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³ã«é‡å­åŒ–
- **RVQ**: Residual Vector Quantization â€” æ®‹å·®ã‚’è¤‡æ•°å›é‡å­åŒ–ã™ã‚‹éšå±¤çš„æ‰‹æ³•
- **EnCodec**: Meta ã® Neural Audio Codecï¼ˆ150 tokens/sec, RVQ x4ï¼‰
- **WavTokenizer**: å˜ä¸€VQã§75 tokens/sec ã®æ¥µé™åœ§ç¸® Codec
- **Flow Matching**: é€£ç¶šçš„ãªç¢ºç‡ãƒ‘ã‚¹ã«æ²¿ã£ã¦ãƒ™ã‚¯ãƒˆãƒ«å ´ã‚’å­¦ç¿’ã™ã‚‹ç”Ÿæˆæ‰‹æ³•
- **F5-TTS**: Flow Matching ã«ã‚ˆã‚‹ non-autoregressive TTSï¼ˆ10ã‚¹ãƒ†ãƒƒãƒ—ç”Ÿæˆï¼‰
- **VALL-E 2**: Codec LM ã«ã‚ˆã‚‹ zero-shot TTSï¼ˆRepetition Aware Sampling + Grouped Code Modelingï¼‰
- **NaturalSpeech 3**: Factorized Codec + Diffusion ã«ã‚ˆã‚‹é«˜å“è³ª TTS
- **MusicGen**: EnCodec + LM ã«ã‚ˆã‚‹éŸ³æ¥½ç”Ÿæˆï¼ˆMeta, 2023ï¼‰
- **Stable Audio**: DiT ã«ã‚ˆã‚‹é•·æ™‚é–“éŸ³æ¥½ç”Ÿæˆï¼ˆæœ€å¤§4åˆ†45ç§’ï¼‰
- **FAD**: FrÃ©chet Audio Distance â€” ã‚¬ã‚¦ã‚¹ä»®å®šã®éŸ³å£°è©•ä¾¡æŒ‡æ¨™
- **KAD**: Kernel Audio Distance â€” distribution-free è©•ä¾¡æŒ‡æ¨™ï¼ˆMMD-basedï¼‰
- **CLAP**: Contrastive Language-Audio Pretraining â€” Text-audio alignment è©•ä¾¡
:::

```mermaid
graph TD
    A[Audio Generation<br/>Research Map] --> B[Codec]
    A --> C[TTS]
    A --> D[Music]
    A --> E[Evaluation]

    B --> B1[VQ-VAE]
    B --> B2[RVQ]
    B --> B3[WavTokenizer]

    C --> C1[Flow Matching<br/>F5-TTS]
    C --> C2[Codec LM<br/>VALL-E 2]
    C --> C3[Diffusion<br/>NaturalSpeech 3]

    D --> D1[LM-based<br/>MusicGen]
    D --> D2[DiT-based<br/>Stable Audio]

    E --> E1[FAD]
    E --> E2[KAD]
    E --> E3[CLAP]

    style A fill:#ffd700
```

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®95%å®Œäº†ï¼** Zone 6 å®Œèµ°ã€‚éŸ³å£°ç”Ÿæˆã®ç ”ç©¶ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ï¼ˆCodecé€²åŒ–ãƒ»TTSæ¬¡ä¸–ä»£ãƒ»Musicèª²é¡Œãƒ»è©•ä¾¡æŒ‡æ¨™ãƒ»å€«ç†å•é¡Œï¼‰ã‚’æŠŠæ¡ã—ãŸã€‚æ¬¡ã¯ Zone 7 â€” æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ã§ã€å…¨ä½“ã‚’ç·æ‹¬ã™ã‚‹ã€‚
:::

---


**ã‚´ãƒ¼ãƒ«**: ç¬¬44å›ã®å­¦ã³ã‚’æ•´ç†ã—ã€æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’æ˜ç¢ºã«ã™ã‚‹ã€‚

### 6.7 æœ¬è¬›ç¾©ã®æ ¸å¿ƒçš„æ´å¯Ÿ

#### æ´å¯Ÿ1: Neural Audio Codec = éŸ³å£°ã®é›¢æ•£åŒ–é©å‘½

**Before (2020)**:
- Mel-spectrogram â†’ Neural Vocoderï¼ˆWaveNet/HiFi-GANï¼‰
- é€£ç¶šè¡¨ç¾ â†’ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é…ã„

**After (2024)**:
- Audio â†’ **é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³**ï¼ˆEnCodec/WavTokenizerï¼‰
- LM/Flow ã§ç”Ÿæˆ â†’ é«˜é€Ÿãƒ»é«˜å“è³ª

**æœ¬è³ª**: ç”»åƒã® VQ-VAE/VQ-GAN ã¨åŒã˜ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ  â€” **é›¢æ•£åŒ–ãŒãƒ–ãƒ¬ã‚¤ã‚¯ã‚¹ãƒ«ãƒ¼**

#### æ´å¯Ÿ2: Flow Matching ãŒ TTS ã‚’æ”¯é…

**Autoregressive TTS (VALL-E åˆä»£)**:
- 150 tokens/sec ã‚’é€æ¬¡ç”Ÿæˆ â†’ é…ã„
- Phoneme repetition å•é¡Œ

**Flow Matching TTS (F5-TTS)**:
- 10ã‚¹ãƒ†ãƒƒãƒ—ã§ä¸¦åˆ—ç”Ÿæˆ â†’ 15xé«˜é€Ÿ
- Alignment-freeï¼ˆãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼‰

**æœ¬è³ª**: Diffusion ã®è¨“ç·´ç°¡ç•¥åŒ–ï¼ˆsimulation-freeï¼‰ãŒé€Ÿåº¦ã¨å“è³ªã‚’ä¸¡ç«‹

#### æ´å¯Ÿ3: Codec LM ã®é™ç•Œã¨ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰åŒ–

**VALL-E 2**: Repetition Aware Sampling + Grouped Code Modeling ã§ human parity

**ã—ã‹ã—**:
- Autoregressive â†’ æœ¬è³ªçš„ã«é…ã„
- RVQ ãƒˆãƒ¼ã‚¯ãƒ³æ•° â†’ LMè² æ‹…

**æ¬¡ä¸–ä»£**:
- **Hybrid**: Flow Matchingï¼ˆç²—ã„ç”Ÿæˆï¼‰+ Refinementï¼ˆè©³ç´°åŒ–ï¼‰
- **Unified**: Single model ã§ TTS + Music + Audio editing

#### æ´å¯Ÿ4: è©•ä¾¡æŒ‡æ¨™ã®é€²åŒ– â€” FAD â†’ KAD

**FAD ã®å•é¡Œ**: ã‚¬ã‚¦ã‚¹ä»®å®šãƒ»ã‚µãƒ³ãƒ—ãƒ«ä¾å­˜æ€§ãƒ»è¨ˆç®—ã‚³ã‚¹ãƒˆ

**KAD ã®é©å‘½**: Distribution-freeãƒ»Unbiasedãƒ»Fast convergence

**æœ¬è³ª**: æ©Ÿæ¢°å­¦ç¿’ã®è©•ä¾¡ã¯ã€Œä»®å®šã®å°‘ãªã•ã€ã¸å‘ã‹ã†ï¼ˆFID â†’ KID â†’ KADï¼‰

```julia
println("\nã€ç¬¬44å›ã®4å¤§æ´å¯Ÿã€‘")
println("1. Neural Audio Codec: éŸ³å£°ã®é›¢æ•£åŒ–é©å‘½")
println("   â†’ VQ-VAE/RVQ/WavTokenizerï¼ˆç”»åƒã¨åŒã˜ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ï¼‰")
println()
println("2. Flow Matching TTS: é€Ÿåº¦ã¨å“è³ªã®ä¸¡ç«‹")
println("   â†’ F5-TTSï¼ˆ10ã‚¹ãƒ†ãƒƒãƒ—ã€alignment-freeï¼‰")
println()
println("3. Codec LM ã®é™ç•Œã¨ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰åŒ–")
println("   â†’ VALL-E 2ï¼ˆhuman parityï¼‰â†’ æ¬¡ä¸–ä»£ã¯ Flow + Refinement")
println()
println("4. è©•ä¾¡æŒ‡æ¨™ã®é€²åŒ–: FAD â†’ KAD")
println("   â†’ Distribution-freeï¼ˆä»®å®šã®å°‘ãªã• = æ±ç”¨æ€§ï¼‰")
```

### 6.8 FAQ â€” éŸ³å£°ç”Ÿæˆã§ã‚ˆãã‚ã‚‹ç–‘å•

:::details Q1: WavTokenizer ã¨ EnCodecã€ã©ã¡ã‚‰ã‚’ä½¿ã†ã¹ãï¼Ÿ

**Answer**:
ç”¨é€”ã«ã‚ˆã‚‹ã€‚**EnCodec**ï¼ˆRVQ x4ï¼‰ã¯å“è³ªé‡è¦–ãƒ»MusicGenäº’æ›ãŒåˆ©ç‚¹ã€‚**WavTokenizer**ï¼ˆVQ x1ï¼‰ã¯æ¨è«–é€Ÿåº¦ãƒ»ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãŒåˆ©ç‚¹ã€‚2025å¹´ä»¥é™ã®æ–°è¦ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ WavTokenizer ãŒæ¨å¥¨ï¼ˆå˜ä¸€VQã®ç°¡æ½”ã• + SOTAå“è³ªï¼‰ã€‚
:::

:::details Q2: F5-TTS ã¨ VALL-E 2ã€ã©ã¡ã‚‰ãŒå„ªã‚Œã¦ã„ã‚‹ã‹ï¼Ÿ

**Answer**:
ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã€‚**F5-TTS** ã¯é€Ÿåº¦ï¼ˆ10ã‚¹ãƒ†ãƒƒãƒ—ï¼‰ã¨è¨“ç·´ã®å˜ç´”ã•ã§å„ªä½ã€‚**VALL-E 2** ã¯å“è³ªï¼ˆhuman parityï¼‰ã¨ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆèƒ½åŠ›ã§å„ªä½ã€‚ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ  TTS â†’ F5-TTSã€æœ€é«˜å“è³ª â†’ VALL-E 2ã€‚2026å¹´äºˆæ¸¬: ä¸¡è€…ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãŒç™»å ´ã€‚
:::

:::details Q3: Julia ã§éŸ³å£°å‡¦ç†ã¯ç¾å®Ÿçš„ã‹ï¼Ÿ

**Answer**:
**Yes**ã€‚FFTW.jlï¼ˆé«˜é€ŸFFTï¼‰ã€WAV.jlï¼ˆWAV I/Oï¼‰ã€Flux.jlï¼ˆNNè¨“ç·´ï¼‰ãŒæƒã„ã€æ•°å¼â†’ã‚³ãƒ¼ãƒ‰ã®1:1å¯¾å¿œãŒç ”ç©¶ã«æœ€é©ã€‚ãŸã ã—æœ¬ç•ªæ¨è«–ã¯ Rustï¼ˆCandleï¼‰ãŒä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã§å„ªä½ã€‚Julia = ç ”ç©¶ãƒ»ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã€Rust = æœ¬ç•ªæ¨è«–ã€ãŒç¾å®Ÿçš„ãªåˆ†æ¥­ã€‚
:::

:::details Q4: Suno/Udio ã®æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯ã¯å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã‹ï¼Ÿ

**Answer**:
**No**ã€‚å•†ç”¨ã‚µãƒ¼ãƒ“ã‚¹ã‚†ãˆè©³ç´°ã¯éå…¬é–‹ã€‚ãŸã ã—æ¨å®š: EnCodecç³» Codec + 10Bç´š LM + VALL-Eç³» vocal synthesis + Neural audio effectsã€‚è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®åˆæ³•æ€§ãŒè«–äº‰ä¸­ï¼ˆRIAAè¨´è¨Ÿï¼‰ã€‚ã‚ªãƒ¼ãƒ—ãƒ³ãªä»£æ›¿ã¯ MusicGen / Stable Audioã€‚
:::

:::details Q5: KAD ã¯ FAD ã‚’å®Œå…¨ã«ç½®ãæ›ãˆã‚‹ã‹ï¼Ÿ

**Answer**:
**2026å¹´ä»¥é™ã€Yes**ã€‚KAD ã¯ FAD ã®å…¨å•é¡Œï¼ˆã‚¬ã‚¦ã‚¹ä»®å®šãƒ»ã‚µãƒ³ãƒ—ãƒ«ä¾å­˜æ€§ãƒ»è¨ˆç®—ã‚³ã‚¹ãƒˆï¼‰ã‚’è§£æ±ºã—ã€äººé–“è©•ä¾¡ã¨ã®ç›¸é–¢ã‚‚é«˜ã„ã€‚2025å¹´ã®è«–æ–‡ã§ã¯æ—¢ã« KAD ãŒ standard ã«ãªã‚Šã¤ã¤ã‚ã‚‹ã€‚FAD ã¯æ­´å²çš„å‚ç…§ã¨ã—ã¦æ®‹ã‚‹ãŒã€æ–°è¦ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ KAD æ¨å¥¨ã€‚
:::

### 6.9 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« â€” 1é€±é–“ã§éŸ³å£°ç”Ÿæˆã‚’ãƒã‚¹ã‚¿ãƒ¼

| æ—¥ | ã‚¿ã‚¹ã‚¯ | æ™‚é–“ | æˆæœç‰© |
|:---|:------|:-----|:------|
| **Day 1** | Zone 0-2 èª­ç ´ + VQ-VAE å®Ÿè£… | 3h | VQ-VAE encoder/decoder (Julia) |
| **Day 2** | Zone 3.1-3.3 æ•°å¼å°å‡º + RVQ å®Ÿè£… | 4h | RVQ 4-layer quantizer (Julia) |
| **Day 3** | Zone 3.4-3.6 Flow Matching å°å‡º + å®Ÿè£… | 4h | F5-TTS (tiny version, Julia) |
| **Day 4** | Zone 3.7-3.8 Codec LM + FACodec | 3h | VALL-E 2 Repetition Aware Sampling |
| **Day 5** | Zone 4 å®Ÿè£… + Rust æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ | 4h | Rust inference server (Candle) |
| **Day 6** | Zone 5 å®Ÿé¨“ + KAD å®Ÿè£… | 3h | KAD metric (Julia) |
| **Day 7** | Zone 6-7 + ç·åˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ | 4h | 3è¨€èªçµ±åˆ TTS pipeline |

**Total**: 25æ™‚é–“ã§éŸ³å£°ç”Ÿæˆã®ç†è«–ãƒ»å®Ÿè£…ãƒ»å¿œç”¨ã‚’å®Œå…¨ç¿’å¾—ã€‚

### 6.10 Progress Tracker â€” è‡ªå·±è©•ä¾¡ãƒ„ãƒ¼ãƒ«

```julia
# progress_tracker_audio.jl
function audio_generation_progress()
    skills = [
        ("Neural Audio Codec (VQ-VAE/RVQ/WavTokenizer)", false),
        ("Flow Matching TTS (F5-TTS)", false),
        ("Codec LM (VALL-E 2)", false),
        ("Music Generation (MusicGen/Stable Audio)", false),
        ("Audio è©•ä¾¡æŒ‡æ¨™ (FAD/KAD)", false),
        ("Julia éŸ³å£°å‡¦ç† (FFTW/WAV/Flux)", false),
        ("Rust éŸ³å£°æ¨è«– (Candle)", false),
        ("Elixir éŸ³å£°é…ä¿¡ (OTP/Port)", false),
        ("3è¨€èªçµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³", false),
        ("Deepfake éŸ³å£°ã®å€«ç†ç†è§£", false)
    ]

    println("ã€Audio Generation ã‚¹ã‚­ãƒ«ãƒã‚§ãƒƒã‚¯ã€‘")
    println("å„é …ç›®ã‚’ç†è§£ãƒ»å®Ÿè£…ã§ããŸã‚‰ true ã«å¤‰æ›´:\n")
    for (i, (skill, done)) in enumerate(skills)
        status = done ? "âœ“" : "â˜"
        println("$i. $status $skill")
    end

    completed = count(s -> s[2], skills)
    total = length(skills)
    progress = div(completed * 100, total)

    println("\né€²æ—: $completed / $total ã‚¹ã‚­ãƒ«å®Œäº† ($progress%)")
    println("ç›®æ¨™: 10 / 10 ã‚¹ã‚­ãƒ«å®Œäº†ã§éŸ³å£°ç”Ÿæˆãƒã‚¹ã‚¿ãƒ¼èªå®š")
end

audio_generation_progress()
```

**å®Ÿè¡Œã—ã¦é€²æ—ã‚’ç¢ºèªã›ã‚ˆ**ã€‚å…¨ã‚¹ã‚­ãƒ«å®Œäº† = éŸ³å£°ç”Ÿæˆãƒã‚¹ã‚¿ãƒ¼ã€‚

### 6.11 æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ— â€” ç¬¬45å›äºˆå‘Š

**ç¬¬45å›: Videoç”Ÿæˆ**ï¼ˆæ™‚ç©ºé–“ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã¸ï¼‰

**å‰å›ã®åˆ°é”ç‚¹**: ControlNet/IP-Adapterã§ç²¾å¯†åˆ¶å¾¡ã‚’å®Ÿç¾ã€‚é™æ­¢ç”»ç”Ÿæˆã‚’å®Œå…¨ç¿’å¾—ã€‚**éŸ³å£°ã‚‚ç¿’å¾—ã—ãŸ**ã€‚æ¬¡ã¯æ™‚é–“è»¸ã®è¿½åŠ ã¸ã€‚

**å­¦ã¶å†…å®¹**:
1. **Video Diffusion** (CogVideoX / Sora 2 / Open-Sora 2.0)
2. **Temporal Coherence** (æ™‚é–“çš„ä¸€è²«æ€§ã®æ•°ç†)
3. **3D VAE** (Video tokenization)
4. **SmolVLM2 & LTX-Video** (å‹•ç”»ç†è§£ & ç”Ÿæˆãƒ‡ãƒ¢)
5. **Julia/Rust/Elixir ã§å‹•ç”»ç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**

**éµã¨ãªã‚‹å•ã„**:
- ãªãœé™æ­¢ç”»ã®æˆåŠŸãŒå‹•ç”»ã«ç›´æ¥é©ç”¨ã§ããªã„ã®ã‹ï¼Ÿ
- Temporal Attention ã¯ã©ã†è¨­è¨ˆã™ã¹ãã‹ï¼Ÿ
- Sora 2 ã¯æœ¬å½“ã«ã€Œä¸–ç•Œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã€ã‹ï¼Ÿ

```julia
println("\nã€ç¬¬45å›äºˆå‘Š: Videoç”Ÿæˆã€‘")
println("é™æ­¢ç”»ï¼ˆDiT/FLUXï¼‰+ éŸ³å£°ï¼ˆF5-TTSï¼‰â†’ å‹•ç”»ï¼ˆæ™‚ç©ºé–“ï¼‰ã¸")
println()
println("Key topics:")
println("  1. Video Diffusion (CogVideoX / Sora 2 / Open-Sora)")
println("  2. Temporal Coherence (æ™‚é–“çš„ä¸€è²«æ€§)")
println("  3. 3D VAE (Video tokenization)")
println("  4. SmolVLM2 (å‹•ç”»ç†è§£) + LTX-Video (å‹•ç”»ç”Ÿæˆ)")
println()
println("â†’ æ™‚é–“è»¸ã‚’å¾æœã—ã€å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£åˆ¶è¦‡ã¸")
```

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®100%å®Œäº†ï¼** ç¬¬44å›ã€ŒéŸ³å£°ç”Ÿæˆã€ã‚’å®Œèµ°ã—ãŸã€‚Neural Audio Codecï¼ˆVQ-VAE â†’ RVQ â†’ WavTokenizerï¼‰ã€Flow Matching TTSï¼ˆF5-TTSï¼‰ã€Codec LMï¼ˆVALL-E 2ï¼‰ã€Music Generationï¼ˆMusicGen / Stable Audioï¼‰ã€è©•ä¾¡æŒ‡æ¨™ï¼ˆFAD â†’ KADï¼‰ã®å…¨ç†è«–ã‚’å°å‡ºã—ã€Julia/Rust/Elixir ã§å®Ÿè£…ã—ãŸã€‚éŸ³å£°ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’å®Œå…¨ã«ç¿’å¾—ã—ãŸã‚ãªãŸã¯ã€æ¬¡ã®æˆ¦å ´ â€” å‹•ç”»ç”Ÿæˆã¸å‘ã‹ã†æº–å‚™ãŒã§ããŸã€‚
:::

---

## ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **Suno/UdioãŒæ•°ç§’ã§ä½œæ›²ã™ã‚‹æ™‚ä»£ã€‚äººé–“ã®éŸ³æ¥½å®¶ã¯ä¸è¦ã«ãªã£ãŸã‹ï¼Ÿ**

### å•ã„ã®åˆ†è§£

#### 1. æŠ€è¡“çš„èƒ½åŠ›ã®ç¾çŠ¶

**Suno v4.5 / Udio ãŒã§ãã‚‹ã“ã¨**:
- 3åˆ†ã®å®Œå…¨æ¥½æ›²ï¼ˆæ­Œè©ãƒ»ãƒœãƒ¼ã‚«ãƒ«ãƒ»æ¥½å™¨ãƒ»ãƒŸãƒƒã‚¯ã‚¹ï¼‰
- ãƒ—ãƒ­ãƒ¬ãƒ™ãƒ«éŸ³è³ªï¼ˆäººé–“ã¨ã®åŒºåˆ¥å›°é›£ï¼‰
- æ•°ç§’ã§ç”Ÿæˆï¼ˆäººé–“ä½œæ›²å®¶ã®æ•°ç™¾æ™‚é–“åˆ†ã‚’æ•°ç§’ã§ï¼‰

**ã§ããªã„ã“ã¨**:
- æ„å›³çš„ãª"ãƒ«ãƒ¼ãƒ«ç ´ã‚Š"ï¼ˆã‚¸ãƒ£ã‚ºã®ä¸å”å’ŒéŸ³ã€ç¾ä»£éŸ³æ¥½ã®å®Ÿé¨“æ€§ï¼‰
- æ–‡åŒ–çš„æ–‡è„ˆã®æ·±ã„ç†è§£ï¼ˆç‰¹å®šæ™‚ä»£ãƒ»åœ°åŸŸã®éŸ³æ¥½æ§˜å¼ï¼‰
- ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªå…±åŒä½œæ¥­ï¼ˆãƒãƒ³ãƒ‰ãƒ¡ãƒ³ãƒãƒ¼é–“ã®å³èˆˆï¼‰

#### 2. å‰µé€ æ€§ã®æœ¬è³ª

**2ã¤ã®å‰µé€ æ€§**:
1. **çµ„ã¿åˆã‚ã›å‹**: æ—¢å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ–°ã—ã„çµ„ã¿åˆã‚ã›ï¼ˆAIå¾—æ„ï¼‰
2. **ç™ºè¦‹å‹**: å…¨ãæ–°ã—ã„ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã®å‰µé€ ï¼ˆäººé–“å„ªä½ï¼Ÿï¼‰

**AIéŸ³æ¥½ã¯ã€Œå‰µé€ çš„ã€ã‹ï¼Ÿ**
- è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ç¯„å›²å†… â†’ çµ„ã¿åˆã‚ã›å‹
- è¨“ç·´ãƒ‡ãƒ¼ã‚¿å¤– â†’ æœªæ¤œè¨¼ï¼ˆãƒ“ãƒ¼ãƒˆãƒ«ã‚ºç´šã®é©å‘½ã‚’èµ·ã“ã›ã‚‹ã‹ï¼Ÿï¼‰

#### 3. é›‡ç”¨ã¸ã®å½±éŸ¿

**ç½®ãæ›ãˆã‚‰ã‚Œã‚‹è·ç¨®**:
- BGMä½œæ›²ï¼ˆåºƒå‘Šãƒ»ã‚²ãƒ¼ãƒ ãƒ»å‹•ç”»ï¼‰
- ã‚¹ãƒˆãƒƒã‚¯ãƒŸãƒ¥ãƒ¼ã‚¸ãƒƒã‚¯åˆ¶ä½œ
- å˜ç´”ãªç·¨æ›²ãƒ»ãƒªãƒŸãƒƒã‚¯ã‚¹

**ç”Ÿãæ®‹ã‚‹è·ç¨®**:
- ãƒ©ã‚¤ãƒ–ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆè¦³å®¢ã¨ã®ç›¸äº’ä½œç”¨ï¼‰
- ãƒ—ãƒ­ãƒ‡ãƒ¥ãƒ¼ã‚µãƒ¼ï¼ˆAIå‡ºåŠ›ã®é¸åˆ¥ãƒ»ç·¨é›†ãƒ»æ–¹å‘æ€§æ±ºå®šï¼‰
- å®Ÿé¨“çš„ä½œæ›²å®¶ï¼ˆAI ãŒçœŸä¼¼ã§ããªã„å‰è¡›æ€§ï¼‰

#### 4. æ–‡åŒ–çš„ä¾¡å€¤

**AIéŸ³æ¥½ vs äººé–“éŸ³æ¥½**:
- **æŠ€è¡“çš„å“è³ª**: AI ãŒäººé–“ã‚’ä¸Šå›ã‚‹å¯èƒ½æ€§
- **æ„Ÿæƒ…çš„å…±é³´**: è´ãæ‰‹ãŒã€Œèª°ãŒä½œã£ãŸã‹ã€ã‚’çŸ¥ã‚‹ã¨è©•ä¾¡ãŒå¤‰ã‚ã‚‹ï¼ˆTuring Test ã®é€†ï¼‰
- **ç‰©èªæ€§**: ãƒ™ãƒ¼ãƒˆãƒ¼ãƒ´ã‚§ãƒ³ã®ç¬¬ä¹ã¯ã€Œè€³ãŒèã“ãˆãªã„ä½œæ›²å®¶ã®è‹¦é—˜ã€ã¨ã„ã†ç‰©èªè¾¼ã¿ã§ä¾¡å€¤ãŒã‚ã‚‹

**æ–°ã—ã„èŠ¸è¡“å½¢æ…‹**:
- **AI-human collaboration**: äººé–“ãŒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ»ç·¨é›†ã€AI ãŒç”Ÿæˆ
- **AI as instrument**: AI ã‚’ã€Œæ–°ã—ã„æ¥½å™¨ã€ã¨ã—ã¦æ‰±ã†ï¼ˆã‚®ã‚¿ãƒ¼ãƒ»ãƒ”ã‚¢ãƒã¨åŒåˆ—ï¼‰

### ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ãƒã‚¤ãƒ³ãƒˆ

1. **AIç”ŸæˆéŸ³æ¥½ã«è‘—ä½œæ¨©ã¯èªã‚ã‚‰ã‚Œã‚‹ã¹ãã‹ï¼Ÿ** ç¾è¡Œæ³•ã§ã¯ã€Œäººé–“ã®å‰µä½œã€ãŒè¦ä»¶ã€‚AIå˜ç‹¬ã®å‡ºåŠ›ã¯ä¿è­·ã•ã‚Œãªã„å¯èƒ½æ€§ã€‚

2. **è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®åˆæ³•æ€§**: Suno ã¯è¨±å¯ãªã—ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‹ï¼ŸFair use ã‹ï¼ŸRIAAè¨´è¨Ÿã®è¡Œæ–¹ã€‚

3. **éŸ³æ¥½æ•™è‚²ã®æœªæ¥**: å­ä¾›ã«æ¥½å™¨ã‚’ç¿’ã‚ã›ã‚‹æ„å‘³ã¯ï¼ŸAIæ™‚ä»£ã®éŸ³æ¥½æ•™è‚²ã¯ã€Œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã€ã«ãªã‚‹ã®ã‹ï¼Ÿ

4. **ãƒ©ã‚¤ãƒ–éŸ³æ¥½ã®ä¾¡å€¤**: AI ãŒå®Œç’§ãªéŒ²éŸ³ã‚’ä½œã‚Œã‚‹æ™‚ä»£ã€ãƒ©ã‚¤ãƒ–ã®ã€Œä¸å®Œå…¨ã•ã€ãŒé€†ã«ä¾¡å€¤ã‚’æŒã¤ã‹ï¼Ÿ

:::details æ­´å²çš„é¡ä¼¼: å†™çœŸã®ç™»å ´ã¨çµµç”»

19ä¸–ç´€ã€å†™çœŸã®ç™»å ´ã§ã€Œçµµç”»ã¯ä¸è¦ã«ãªã‚‹ã€ã¨è¨€ã‚ã‚ŒãŸã€‚

**çµæœ**:
- å†™å®Ÿçš„çµµç”»ã¯æ¸›å°‘ï¼ˆå†™çœŸãŒä»£æ›¿ï¼‰
- å°è±¡æ´¾ãƒ»æŠ½è±¡ç”»ãŒå°é ­ï¼ˆå†™çœŸã«ã§ããªã„è¡¨ç¾ï¼‰
- çµµç”»ã¯ã€Œè¨˜éŒ²ã€ã‹ã‚‰ã€Œè¡¨ç¾ã€ã¸ã‚·ãƒ•ãƒˆ

**éŸ³æ¥½ã‚‚åŒã˜é“ã‚’è¾¿ã‚‹ã‹ï¼Ÿ**
- AI ã¯ã€Œè¨˜éŒ²çš„éŸ³æ¥½ã€ï¼ˆBGMãƒ»ã‚¹ãƒˆãƒƒã‚¯ï¼‰ã‚’æ‹…å½“
- äººé–“ã¯ã€Œè¡¨ç¾çš„éŸ³æ¥½ã€ï¼ˆãƒ©ã‚¤ãƒ–ãƒ»å®Ÿé¨“ï¼‰ã¸ã‚·ãƒ•ãƒˆ
:::

### ã‚ãªãŸã®è€ƒãˆã¯ï¼Ÿ

ã“ã®å•ã„ã«ã€Œæ­£è§£ã€ã¯ãªã„ã€‚æŠ€è¡“ãƒ»çµŒæ¸ˆãƒ»æ–‡åŒ–ãƒ»å“²å­¦ãŒäº¤å·®ã™ã‚‹å ´æ‰€ã ã€‚

è‡ªåˆ†ãªã‚Šã®ç­”ãˆã‚’è€ƒãˆã€è­°è«–ã›ã‚ˆã€‚ãã‚ŒãŒã€AIæ™‚ä»£ã‚’ç”Ÿãã‚‹æˆ‘ã€…ã®è²¬ä»»ã ã€‚

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Ji, S., et al. (2024). "WavTokenizer: an Efficient Acoustic Discrete Codec Tokenizer for Audio Language Modeling". *arXiv:2408.16532*. ICLR 2025.
@[card](https://arxiv.org/abs/2408.16532)

[^2]: Chen, Y., et al. (2024). "F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching". *arXiv:2410.06885*.
@[card](https://arxiv.org/abs/2410.06885)

[^3]: Copet, J., et al. (2023). "Simple and Controllable Music Generation". *arXiv:2306.05284*. NeurIPS 2023.
@[card](https://arxiv.org/abs/2306.05284)

[^4]: Wang, Z., et al. (2024). "VALL-E 2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers". *arXiv:2406.05370*.
@[card](https://arxiv.org/abs/2406.05370)

[^5]: Zeghidour, N., et al. (2021). "SoundStream: An End-to-End Neural Audio Codec". *IEEE/ACM Transactions on Audio, Speech, and Language Processing*.

[^6]: DÃ©fossez, A., et al. (2022). "High Fidelity Neural Audio Compression". *arXiv:2210.13438*.
@[card](https://arxiv.org/abs/2210.13438)

[^7]: Kyutai Research (2024). "Mimi: A Semantic-rich Neural Audio Codec".

[^9]: Evans, Z., et al. (2024). "Stable Audio Open". *arXiv:2407.14358*.
@[card](https://arxiv.org/abs/2407.14358)

Evans, Z., et al. (2024). "Long-form Music Generation with Latent Diffusion". *arXiv:2404.10301*.
@[card](https://arxiv.org/abs/2404.10301)

[^10]: Yoon, J., et al. (2025). "KAD: No More FAD! An Effective and Efficient Evaluation Metric for Audio Generation". *arXiv:2502.15602*. ICML 2025.
@[card](https://arxiv.org/abs/2502.15602)

[^12]: Bengio, Y., et al. (2013). "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation". *arXiv:1308.3432*.

[^14]: Ju, Z., et al. (2024). "NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models". *arXiv:2403.03100*. ICML 2024.
@[card](https://arxiv.org/abs/2403.03100)

[^15]: Kilgour, K., et al. (2019). "FrÃ©chet Audio Distance: A Metric for Evaluating Music Enhancement Algorithms". *arXiv:1812.08466*.

### æ•™ç§‘æ›¸

- Bengio, Y., et al. (2016). *Deep Learning*. MIT Press. [Free online](http://www.deeplearningbook.org/)
- Murphy, K. P. (2022). *Probabilistic Machine Learning: Advanced Topics*. MIT Press.
- Goodfellow, I., et al. (2014). *Generative Adversarial Nets*. NeurIPS 2014. (åŸºç¤è«–æ–‡ã ãŒæ•™ç§‘æ›¸çš„ä¾¡å€¤)

---

## è¨˜æ³•è¦ç´„

æœ¬è¬›ç¾©ã§ä½¿ç”¨ã—ãŸæ•°å­¦è¨˜æ³•ã®çµ±ä¸€è¦å‰‡:

| è¨˜å· | æ„å‘³ | ä¾‹ |
|:-----|:-----|:---|
| $x$ | ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ï¼ˆéŸ³å£°æ³¢å½¢ï¼‰ | $x \in \mathbb{R}^T$ |
| $z$ | æ½œåœ¨è¡¨ç¾ï¼ˆé€£ç¶šï¼‰ | $z_e \in \mathbb{R}^{L \times D}$ |
| $z_q$ | é‡å­åŒ–å¾Œã®æ½œåœ¨è¡¨ç¾ | $z_q = e_{k^*}$ |
| $k$ | ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ | $k \in \{1, ..., K\}$ |
| $e_k$ | ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã‚¨ãƒ³ãƒˆãƒª | $e_k \in \mathbb{R}^D$ |
| $t$ | æ™‚åˆ»ï¼ˆFlow Matchingï¼‰ | $t \in [0, 1]$ |
| $\mathbf{x}_t$ | æ™‚åˆ» $t$ ã®çŠ¶æ…‹ | $\mathbf{x}_t = (1-t)\mathbf{x}_0 + t\mathbf{x}_1$ |
| $\mathbf{v}_\theta$ | ãƒ™ã‚¯ãƒˆãƒ«å ´ï¼ˆFlow Matchingï¼‰ | $\mathbf{v}_\theta(\mathbf{x}, t, c)$ |
| $\mathbf{u}_t$ | ç›®æ¨™ãƒ™ã‚¯ãƒˆãƒ«å ´ | $\mathbf{u}_t = \mathbf{x}_1 - \mathbf{x}_0$ |
| $p_t$ | æ™‚åˆ» $t$ ã®åˆ†å¸ƒ | $p_t(\mathbf{x})$ |
| $\mathcal{L}$ | æå¤±é–¢æ•° | $\mathcal{L}_{\text{CFM}}$ |
| $\theta$ | ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | $\theta \in \mathbb{R}^n$ |
| $K$ | ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã‚µã‚¤ã‚º | $K = 1024$ (typical) |
| $N_q$ | é‡å­åŒ–éšå±¤æ•°ï¼ˆRVQï¼‰ | $N_q = 4$ (EnCodec) |
| $\text{sg}[\cdot]$ | Stop gradient æ¼”ç®—å­ | $\text{sg}[z_e]$ |

**Notation conventions**:
- Bold lowercase $\mathbf{x}$: vectors
- Uppercase $X$: matrices or sets
- Calligraphic $\mathcal{L}$: loss functions, distributions
- Subscript $_t$: time index
- Superscript $^{(i)}$: sample index or quantizer layer index

---

**[ç¬¬44å› å®Œ]**
---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
