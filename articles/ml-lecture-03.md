---
title: "ç¬¬3å›: ç·šå½¢ä»£æ•° II: SVDãƒ»è¡Œåˆ—å¾®åˆ†ãƒ»ãƒ†ãƒ³ã‚½ãƒ« â€” ä¸‡èƒ½ãƒŠã‚¤ãƒ•SVDã¨é€†ä¼æ’­ã®æ•°å­¦"
emoji: "ğŸ”¬"
type: "tech"
topics: ["machinelearning", "deeplearning", "linearalgebra", "python"]
published: true
---

# ç¬¬3å›: ç·šå½¢ä»£æ•° II â€” SVDãƒ»è¡Œåˆ—å¾®åˆ†ãƒ»ãƒ†ãƒ³ã‚½ãƒ«

> **SVDã¯ä¸‡èƒ½ãƒŠã‚¤ãƒ•ã ã€‚ç”»åƒåœ§ç¸®ã‚‚PCAã‚‚æ¨è–¦ã‚‚ã€å…¨ã¦ã€ŒåŒã˜è¨ˆç®—ã€ã«å¸°ç€ã™ã‚‹ã€‚**

ç¬¬2å›ã§ç·šå½¢ä»£æ•°ã®åŸºç›¤ã‚’ç¯‰ã„ãŸã€‚ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã®å…¬ç†ã€è¡Œåˆ—æ¼”ç®—ã€å›ºæœ‰å€¤åˆ†è§£ã€æ­£å®šå€¤è¡Œåˆ—ã€å°„å½± â€” ã“ã‚Œã‚‰ã¯å…¨ã¦ã€Œæ­£æ–¹è¡Œåˆ—ã€ã®ä¸–ç•Œã®è©±ã ã£ãŸã€‚

ã ãŒã€ç¾å®Ÿã®ãƒ‡ãƒ¼ã‚¿ã¯æ­£æ–¹è¡Œåˆ—ã§ã¯ãªã„ã€‚ç”»åƒã¯ $3 \times 224 \times 224$ ã®ãƒ†ãƒ³ã‚½ãƒ«ã ã€‚è¨€èªãƒ¢ãƒ‡ãƒ«ã®é‡ã¿è¡Œåˆ—ã¯ $d_{\text{model}} \times d_{\text{ff}}$ ã®é•·æ–¹å½¢è¡Œåˆ—ã ã€‚ãƒãƒƒãƒå‡¦ç†ã•ã‚ŒãŸAttentionã‚¹ã‚³ã‚¢ã¯ $B \times H \times T \times T$ ã®4éšãƒ†ãƒ³ã‚½ãƒ«ã ã€‚

**æ­£æ–¹è¡Œåˆ—ã®å¤–ã®ä¸–ç•Œ**ã‚’æ‰±ã†ãŸã‚ã«ã€3ã¤ã®é“å…·ãŒå¿…è¦ã«ãªã‚‹:

1. **SVD**ï¼ˆç‰¹ç•°å€¤åˆ†è§£ï¼‰â€” ä»»æ„ã®è¡Œåˆ—ã‚’åˆ†è§£ã™ã‚‹ã€Œä¸‡èƒ½ãƒŠã‚¤ãƒ•ã€
2. **è¡Œåˆ—å¾®åˆ†** â€” ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å­¦ç¿’ã®æ•°å­¦çš„åŸºç›¤
3. **ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—** â€” å¤šæ¬¡å…ƒé…åˆ—ã‚’æ•°å­¦çš„ã«æ‰±ã†è¨€èª

ã“ã®3ã¤ã‚’æœ¬è¬›ç¾©ã§å®Œå…¨æ­¦è£…ã™ã‚‹ã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2025-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚
:::

```mermaid
graph LR
    A["ğŸ”¬ SVD<br/>ç‰¹ç•°å€¤åˆ†è§£"] --> B["ğŸ“‰ ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼<br/>Eckart-Young"]
    B --> C["ğŸ“Š PCA (SVDç‰ˆ)<br/>åˆ†æ•£æœ€å¤§åŒ–"]
    A --> D["ğŸ“ æ“¬ä¼¼é€†è¡Œåˆ—<br/>Moore-Penrose"]
    E["âœï¸ è¡Œåˆ—å¾®åˆ†<br/>ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ãƒ»ãƒ˜ã‚·ã‚¢ãƒ³"] --> F["â›“ï¸ é€£é–å¾‹<br/>è¨ˆç®—ã‚°ãƒ©ãƒ•"]
    F --> G["ğŸ”„ è‡ªå‹•å¾®åˆ†<br/>Forward/Reverse"]
    style A fill:#e1f5fe
    style G fill:#c8e6c9
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 6 | æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” SVDã§ç”»åƒã‚’åœ§ç¸®ã™ã‚‹

**ã‚´ãƒ¼ãƒ«**: SVDãŒã€Œãƒ‡ãƒ¼ã‚¿ã®æœ¬è³ªçš„ãªæ§‹é€ ã‚’æŠ½å‡ºã™ã‚‹é“å…·ã€ã§ã‚ã‚‹ã“ã¨ã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ã€‚

```python
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

# grayscale image as matrix
np.random.seed(42)
A = np.random.randn(100, 80)  # 100Ã—80 matrix (like a small grayscale image)

# SVD
U, s, Vt = np.linalg.svd(A, full_matrices=False)

# Rank-5 approximation
k = 5
A_approx = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]

# Compression ratio
original_params = A.shape[0] * A.shape[1]  # 8000
compressed_params = k * (A.shape[0] + A.shape[1] + 1)  # 5 * 181 = 905
print(f"Original:    {original_params} parameters")
print(f"Compressed:  {compressed_params} parameters (rank-{k})")
print(f"Compression: {compressed_params/original_params:.1%}")
print(f"Error:       {np.linalg.norm(A - A_approx, 'fro') / np.linalg.norm(A, 'fro'):.4f}")
```

å‡ºåŠ›:
```
Original:    8000 parameters
Compressed:  905 parameters (rank-5)
Compression: 11.3%
Error:       0.8716
```

**ã“ã®5è¡Œã®è£ã«ã‚ã‚‹æ•°å­¦**:

$$
A = U \Sigma V^\top = \sum_{i=1}^{r} \sigma_i \mathbf{u}_i \mathbf{v}_i^\top
$$

ä»»æ„ã®è¡Œåˆ— $A \in \mathbb{R}^{m \times n}$ ã‚’ã€ç›´äº¤è¡Œåˆ— $U$ã€å¯¾è§’è¡Œåˆ— $\Sigma$ã€ç›´äº¤è¡Œåˆ— $V^\top$ ã®ç©ã«åˆ†è§£ã™ã‚‹ã€‚ä¸Šä½ $k$ å€‹ã®ç‰¹ç•°å€¤ã ã‘ã‚’æ®‹ã›ã°ã€**æœ€é©ãª** rank-$k$ è¿‘ä¼¼ãŒå¾—ã‚‰ã‚Œã‚‹[^3]ã€‚ã€Œæœ€é©ã€ã®æ„å‘³ã¯Eckart-Youngå®šç†ãŒä¿è¨¼ã™ã‚‹ã€‚

:::message
**é€²æ—: 3% å®Œäº†** SVDã§è¡Œåˆ—ã‚’åœ§ç¸®ã§ãã‚‹ã“ã¨ã‚’ä½“æ„Ÿã—ãŸã€‚æ®‹ã‚Š7ã‚¾ãƒ¼ãƒ³ã®å†’é™ºãŒå¾…ã£ã¦ã„ã‚‹ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” SVDã¨è¡Œåˆ—å¾®åˆ†ã‚’ã€Œè§¦ã£ã¦ã€ç†è§£ã™ã‚‹

### 1.1 SVDã®å¹¾ä½•å­¦ â€” è¡Œåˆ—ã¯ã€Œå›è»¢â†’æ‹¡å¤§â†’å›è»¢ã€

ç¬¬2å›ã§ã€Œè¡Œåˆ—ã¯ç·šå½¢å¤‰æ›ã€ã¨è¨€ã£ãŸã€‚SVDã¯ã€ãã®å¤‰æ›ã‚’3ã¤ã®åŸºæœ¬æ“ä½œã«åˆ†è§£ã™ã‚‹ã€‚

$$
A = U \Sigma V^\top
$$

| æˆåˆ† | å¹¾ä½•å­¦çš„æ„å‘³ | è¡Œåˆ—ã®å‹ |
|:-----|:-----------|:---------|
| $V^\top$ | å…¥åŠ›ç©ºé–“ã§ã®å›è»¢ï¼ˆç›´äº¤å¤‰æ›ï¼‰ | $n \times n$ ç›´äº¤è¡Œåˆ— |
| $\Sigma$ | å„è»¸æ–¹å‘ã®æ‹¡å¤§ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼‰ | $m \times n$ å¯¾è§’è¡Œåˆ— |
| $U$ | å‡ºåŠ›ç©ºé–“ã§ã®å›è»¢ï¼ˆç›´äº¤å¤‰æ›ï¼‰ | $m \times m$ ç›´äº¤è¡Œåˆ— |

```python
import numpy as np
import matplotlib.pyplot as plt

# 2D example: matrix transforms a unit circle
A = np.array([[3, 1],
              [1, 2]])

# SVD
U, s, Vt = np.linalg.svd(A)
print(f"U = \n{np.round(U, 4)}")
print(f"Singular values = {np.round(s, 4)}")
print(f"Vt = \n{np.round(Vt, 4)}")

# Unit circle
theta = np.linspace(0, 2 * np.pi, 100)
circle = np.array([np.cos(theta), np.sin(theta)])

# Apply each SVD step
step1 = Vt @ circle         # V^T: rotate in input space
step2 = np.diag(s) @ step1  # Sigma: scale
step3 = U @ step2           # U: rotate in output space

# Verify: A @ circle == U @ Sigma @ Vt @ circle
direct = A @ circle
print(f"\nSVD reconstruction matches: {np.allclose(step3, direct)}")
```

**æ ¸å¿ƒ**: ã©ã‚“ãªè¡Œåˆ—ã«ã‚ˆã‚‹å¤‰æ›ã‚‚ã€Œå›è»¢ â†’ æ‹¡å¤§ â†’ å›è»¢ã€ã«åˆ†è§£ã§ãã‚‹ã€‚ç‰¹ç•°å€¤ $\sigma_1, \sigma_2, \ldots$ ã¯æ‹¡å¤§ç‡ã‚’è¡¨ã—ã€é™é †ã«ã‚½ãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã€‚

```mermaid
graph LR
    IN["ğŸ”µ å˜ä½å††<br/>(å…¥åŠ›)"] -->|"V^T<br/>å›è»¢"| R1["ğŸ”µ å›è»¢ã•ã‚ŒãŸå††"]
    R1 -->|"Î£<br/>æ‹¡å¤§"| EL["ğŸ”´ æ¥•å††"]
    EL -->|"U<br/>å›è»¢"| OUT["ğŸ”´ å›è»¢ã•ã‚ŒãŸæ¥•å††<br/>(å‡ºåŠ›)"]

    style IN fill:#e3f2fd
    style OUT fill:#ffcdd2
```

### 1.2 ç‰¹ç•°å€¤ã®æ¸›è¡° â€” ãªãœä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ãŒæœ‰åŠ¹ãªã®ã‹

å®Ÿãƒ‡ãƒ¼ã‚¿ã®è¡Œåˆ—ã¯ã€ç‰¹ç•°å€¤ãŒæ€¥é€Ÿã«æ¸›è¡°ã™ã‚‹ã€‚ã“ã‚ŒãŒä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ã‚„PCA[^5][^6]ãŒæœ‰åŠ¹ãªç†ç”±ã ã€‚

```python
import numpy as np
import matplotlib.pyplot as plt

# Example: create a matrix with rapid singular value decay
np.random.seed(42)
# Low-rank structure + noise
rank_true = 5
m, n = 100, 80
U_true = np.linalg.qr(np.random.randn(m, rank_true))[0]
V_true = np.linalg.qr(np.random.randn(n, rank_true))[0]
s_true = np.array([10, 5, 2, 1, 0.5])
A_clean = U_true @ np.diag(s_true) @ V_true.T
A_noisy = A_clean + 0.1 * np.random.randn(m, n)

# SVD of noisy matrix
U, s, Vt = np.linalg.svd(A_noisy, full_matrices=False)

print("Top 10 singular values:")
for i, sv in enumerate(s[:10]):
    bar = "â–ˆ" * int(sv * 3)
    print(f"  Ïƒ_{i+1:2d} = {sv:8.4f}  {bar}")

# Cumulative energy
energy = np.cumsum(s**2) / np.sum(s**2)
print(f"\nCumulative energy:")
for k in [1, 2, 3, 5, 10, 20]:
    if k <= len(energy):
        print(f"  rank-{k:2d}: {energy[k-1]:.4f} ({energy[k-1]*100:.1f}%)")
```

**é‡è¦ãªæ´å¯Ÿ**: ä¸Šä½5å€‹ã®ç‰¹ç•°å€¤ã ã‘ã§å…ƒã®è¡Œåˆ—ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼ˆFrobenius ãƒãƒ«ãƒ ã®äºŒä¹—ï¼‰ã®99%ä»¥ä¸Šã‚’æ•æ‰ã§ãã‚‹ã€‚ã“ã‚Œã¯å…ƒã®è¡Œåˆ—ãŒã€Œæœ¬è³ªçš„ã« rank-5ã€ã§ã‚ã‚‹ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã€‚

### 1.3 å‹¾é…ã‚’ã€Œè¦‹ã‚‹ã€â€” æå¤±é–¢æ•°ã®åœ°å½¢

Backpropagation[^2]ã®æ ¸å¿ƒã¯å‹¾é…ã®è¨ˆç®—ã ã€‚å‹¾é…ã¨ã¯ã€Œæå¤±é–¢æ•°ãŒã©ã®æ–¹å‘ã«ã©ã‚Œã ã‘å¤‰åŒ–ã™ã‚‹ã‹ã€ã‚’è¡¨ã™ãƒ™ã‚¯ãƒˆãƒ«ã€‚

```python
import numpy as np

# Simple loss: L(w) = (y - w^T x)^2
x = np.array([1.0, 2.0, 3.0])
y = 10.0
w = np.array([1.0, 1.0, 1.0])

# Forward pass
y_pred = w @ x  # w^T x = 6
loss = (y - y_pred) ** 2  # (10 - 6)^2 = 16
print(f"y_pred = {y_pred}, loss = {loss}")

# Gradient: dL/dw = -2(y - w^T x) * x
grad = -2 * (y - y_pred) * x
print(f"gradient = {grad}")

# Gradient descent step
lr = 0.01
w_new = w - lr * grad
y_pred_new = w_new @ x
loss_new = (y - y_pred_new) ** 2
print(f"After update: y_pred = {y_pred_new:.4f}, loss = {loss_new:.4f}")
print(f"Loss decreased: {loss:.4f} â†’ {loss_new:.4f}")
```

**å‹¾é… $\nabla_{\mathbf{w}} L$ ã¯ã€Œæå¤±ã‚’æœ€ã‚‚é€Ÿãæ¸›å°‘ã•ã›ã‚‹æ–¹å‘ã€**ã®é€†æ–¹å‘ã ã€‚$-\nabla L$ ã®æ–¹å‘ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‹•ã‹ã™ã®ãŒå‹¾é…é™ä¸‹æ³•ã€‚

### 1.4 ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã‚’ã€Œè¦‹ã‚‹ã€â€” ãƒ™ã‚¯ãƒˆãƒ«â†’ãƒ™ã‚¯ãƒˆãƒ«é–¢æ•°ã®å¾®åˆ†

ã‚¹ã‚«ãƒ©ãƒ¼é–¢æ•°ã®å‹¾é…ã¯ã€Œãƒ™ã‚¯ãƒˆãƒ«ã€ã ã£ãŸã€‚ã§ã¯ã€ãƒ™ã‚¯ãƒˆãƒ«ã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«ã¸ã®é–¢æ•°ã®å¾®åˆ†ã¯ï¼Ÿ â€” ãã‚ŒãŒ**ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³**ï¼ˆJacobian matrixï¼‰ã€‚

$$
\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m, \quad J = \frac{\partial \mathbf{f}}{\partial \mathbf{x}} = \begin{pmatrix} \frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\ \vdots & \ddots & \vdots \\ \frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n} \end{pmatrix}
$$

```python
import numpy as np

# f: R^2 -> R^2, f(x) = [x1^2 + x2, x1 * x2]
def f(x):
    return np.array([x[0]**2 + x[1], x[0] * x[1]])

# Analytical Jacobian
def jacobian(x):
    return np.array([
        [2 * x[0], 1],        # df1/dx1, df1/dx2
        [x[1],     x[0]]      # df2/dx1, df2/dx2
    ])

# Numerical Jacobian (finite differences)
def numerical_jacobian(f, x, eps=1e-7):
    n = len(x)
    m = len(f(x))
    J = np.zeros((m, n))
    for j in range(n):
        x_plus = x.copy()
        x_plus[j] += eps
        x_minus = x.copy()
        x_minus[j] -= eps
        J[:, j] = (f(x_plus) - f(x_minus)) / (2 * eps)
    return J

x = np.array([2.0, 3.0])
J_analytical = jacobian(x)
J_numerical = numerical_jacobian(f, x)

print(f"Analytical Jacobian:\n{J_analytical}")
print(f"Numerical Jacobian:\n{np.round(J_numerical, 6)}")
print(f"Match: {np.allclose(J_analytical, J_numerical, atol=1e-5)}")
```

**ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®å„è¡Œã¯ã€å‡ºåŠ›ã®å„æˆåˆ†ã®å‹¾é…**ã€‚ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®è¡Œåˆ—å¼ $\det(J)$ ã¯ã€Œå¤‰æ›ã«ã‚ˆã‚‹ä½“ç©ã®å¤‰åŒ–ç‡ã€ã‚’è¡¨ã—ã€Normalizing Flow[^13]ã®æ ¸å¿ƒçš„ãªè¨ˆç®—é‡ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã«ãªã‚‹ã€‚

### 1.5 è‡ªå‹•å¾®åˆ†ã®å¨åŠ› â€” PyTorchã® `backward()` ãŒå†…éƒ¨ã§ã‚„ã£ã¦ã„ã‚‹ã“ã¨

```python
# PyTorch-style automatic differentiation (manual implementation)
import numpy as np

class Var:
    """Simple autograd variable for demonstration"""
    def __init__(self, data, _children=(), _op=''):
        self.data = data
        self.grad = 0.0
        self._backward = lambda: None
        self._children = set(_children)
        self._op = _op

    def __mul__(self, other):
        other = other if isinstance(other, Var) else Var(other)
        out = Var(self.data * other.data, (self, other), '*')
        def _backward():
            self.grad += other.data * out.grad
            other.grad += self.data * out.grad
        out._backward = _backward
        return out

    def __add__(self, other):
        other = other if isinstance(other, Var) else Var(other)
        out = Var(self.data + other.data, (self, other), '+')
        def _backward():
            self.grad += out.grad
            other.grad += out.grad
        out._backward = _backward
        return out

    def backward(self):
        # topological sort
        topo = []
        visited = set()
        def build(v):
            if v not in visited:
                visited.add(v)
                for child in v._children:
                    build(child)
                topo.append(v)
        build(self)
        self.grad = 1.0
        for v in reversed(topo):
            v._backward()

# Demo: f(a, b) = a*b + b
a = Var(2.0)
b = Var(3.0)
c = a * b      # c = 6
d = c + b      # d = 9
d.backward()

print(f"d = {d.data}")        # 9.0
print(f"dd/da = {a.grad}")    # b = 3.0 (correct: d(ab+b)/da = b)
print(f"dd/db = {b.grad}")    # a+1 = 3.0 (correct: d(ab+b)/db = a+1)
```

ã“ã®ãŸã£ãŸ50è¡Œã®ã‚³ãƒ¼ãƒ‰ãŒã€PyTorchã® `loss.backward()` ã®æœ¬è³ªã [^7][^8]ã€‚è¨ˆç®—ã®ã€Œè¨˜éŒ²ã€ã‚’é€†é †ã«è¾¿ã£ã¦å‹¾é…ã‚’ä¼æ’­ã™ã‚‹ â€” ã“ã‚ŒãŒ**Reverse Mode è‡ªå‹•å¾®åˆ†**ã§ã‚ã‚Šã€Backpropagation[^2]ã®æ­£ä½“ã ã€‚

:::message
**é€²æ—: 15% å®Œäº†** SVDã®å¹¾ä½•å­¦ã€ç‰¹ç•°å€¤ã®æ¸›è¡°ã€å‹¾é…ã€ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã€è‡ªå‹•å¾®åˆ†ã®åŸºæœ¬ã‚’ä½“é¨“ã—ãŸã€‚ã“ã“ã‹ã‚‰ç›´æ„Ÿã‚’æ·±ã‚ã¦Zone 3ã®æ•°å¼ä¿®è¡Œã«å‚™ãˆã‚‹ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” SVDã¨è‡ªå‹•å¾®åˆ†ãŒAIã‚’æ”¯ãˆã‚‹ç†ç”±

### 2.1 ç¬¬3å›ã®ã€Œåœ°å›³ã€

ç¬¬2å›ã§ç·šå½¢ä»£æ•°ã®ã€Œæ–‡æ³•ã€ã‚’å­¦ã‚“ã ã€‚ç¬¬3å›ã§ã¯ã€Œä¿®è¾æ³•ã€ã‚’å­¦ã¶ã€‚

| é“å…· | æ¯”å–© | æ©Ÿæ¢°å­¦ç¿’ã§ã®å½¹å‰² |
|:-----|:-----|:--------------|
| **SVD** | ä¸‡èƒ½ãƒŠã‚¤ãƒ• | ãƒ‡ãƒ¼ã‚¿ã®æœ¬è³ªçš„æ§‹é€ ã‚’æŠ½å‡ºï¼ˆPCA, LoRA[^10], æ¨è–¦ï¼‰ |
| **è¡Œåˆ—å¾®åˆ†** | ç¾…é‡ç›¤ | æå¤±é–¢æ•°ã®å‹¾é…æ–¹å‘ã‚’ç¤ºã™ |
| **é€£é–å¾‹** | é€£é–åå¿œ | å¤šå±¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹¾é…ã‚’ä¸€æ‹¬è¨ˆç®— |
| **è‡ªå‹•å¾®åˆ†** | è‡ªå‹•ç¿»è¨³æ©Ÿ | æ•°å¼â†’å‹¾é…è¨ˆç®—ã‚³ãƒ¼ãƒ‰ã®è‡ªå‹•å¤‰æ› |
| **ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—** | å¤šæ¬¡å…ƒã®æ–‡æ³• | ãƒãƒƒãƒãƒ»ãƒ˜ãƒƒãƒ‰ãƒ»ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ä¸€æ‹¬å‡¦ç† |

### 2.2 Course I ã®ä¸­ã§ã®ä½ç½®ã¥ã‘

```mermaid
graph TD
    L1["ç¬¬1å›: æ¦‚è«–<br/>æ•°å¼ã¨è«–æ–‡ã®èª­ã¿æ–¹"]
    L2["ç¬¬2å›: ç·šå½¢ä»£æ•° I<br/>ãƒ™ã‚¯ãƒˆãƒ«ãƒ»è¡Œåˆ—ãƒ»å›ºæœ‰å€¤"]
    L3["ç¬¬3å›: ç·šå½¢ä»£æ•° II<br/>SVDãƒ»è¡Œåˆ—å¾®åˆ†ãƒ»ãƒ†ãƒ³ã‚½ãƒ«<br/>ğŸ¯ Backpropå®Œå…¨å°å‡º"]
    L4["ç¬¬4å›: ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦<br/>åˆ†å¸ƒãƒ»ãƒ™ã‚¤ã‚ºæ¨è«–"]

    L1 -->|"æ•°å¼ãŒèª­ã‚ãŸ"| L2
    L2 -->|"è¡Œåˆ—ã‚’æ‰±ãˆãŸ"| L3
    L3 -->|"å¾®åˆ†ã‚‚ã§ããŸ"| L4

    style L3 fill:#ffeb3b
```

| å› | ãƒ†ãƒ¼ãƒ | LLM/Transformerã¨ã®æ¥ç‚¹ |
|:---|:------|:----------------------|
| ç¬¬2å› | ç·šå½¢ä»£æ•° I | $QK^\top$ ã®å†…ç©ã€å›ºæœ‰å€¤â†’PCAâ†’åŸ‹ã‚è¾¼ã¿ |
| **ç¬¬3å›** | **ç·šå½¢ä»£æ•° II** | **ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³â†’Flow Modelã€å‹¾é…â†’Backpropã€é€£é–å¾‹â†’Transformerå„å±¤** |
| ç¬¬4å› | ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦ | $p(x_t \mid x_{<t})$ è‡ªå·±å›å¸°ã€Softmaxåˆ†å¸ƒ |

**ç¬¬2å›â†’ç¬¬3å›ã®æ¥ç¶š**: ç¬¬2å›ã§å›ºæœ‰å€¤åˆ†è§£ã‚’å­¦ã‚“ã ã€‚ã ãŒå›ºæœ‰å€¤åˆ†è§£ã¯æ­£æ–¹è¡Œåˆ—ã«ã—ã‹ä½¿ãˆãªã„ã€‚SVDã¯ãã®åˆ¶ç´„ã‚’å–ã‚Šæ‰•ã„ã€**ä»»æ„ã®é•·æ–¹å½¢è¡Œåˆ—**ã‚’åˆ†è§£ã§ãã‚‹ä¸‡èƒ½ãƒ„ãƒ¼ãƒ«ã ã€‚

### 2.3 æ¾å°¾ç ”ã¨ã®å·®åˆ¥åŒ–

| æ¾å°¾ç ”ã®å‰æ | å®Ÿéš›ã®å£ | æœ¬è¬›ç¾©ã®å¯¾ç­– |
|:------------|:--------|:-----------|
| ã€ŒSVDã¯çŸ¥ã£ã¦ã‚‹ã‚ˆã­ã€ | Eckart-Youngå®šç†[^3]ã®æ„å‘³ãŒèª¬æ˜ã§ããªã„ | å­˜åœ¨å®šç†â†’å¹¾ä½•å­¦â†’æœ€é©æ€§ã‚’å…¨å°å‡º |
| ã€ŒBackpropã¯ç†è§£ã—ã¦ã‚‹ã‚ˆã­ã€ | è¡Œåˆ—å¾®åˆ†ã®é€£é–å¾‹ãŒæ›¸ã‘ãªã„ | ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³â†’é€£é–å¾‹â†’Backpropã‚’ä¸€ã‹ã‚‰å°å‡º |
| ã€Œè‡ªå‹•å¾®åˆ†ã¯ PyTorch ã«ä»»ã›ã¦ã€ | Forward/Reverse ã®è¨ˆç®—é‡ã®å·®ãŒã‚ã‹ã‚‰ãªã„ | Wengert list ã‹ã‚‰ Forward/Reverse ã‚’æ‰‹å‹•å®Ÿè£… |
| ã€Œãƒ†ãƒ³ã‚½ãƒ«ã¯NumPyã®é…åˆ—ã€ | æ·»å­—ã®ç¸®ç´„è¦å‰‡ãŒèª­ã‚ãªã„ | Einsteinè¨˜æ³•â†’einsumå®Œå…¨ç‰ˆ |

### 2.4 LLMã®ä¸­ã®SVDã¨è¡Œåˆ—å¾®åˆ†

LLMã®å­¦ç¿’ã¨æ¨è«–ã®ä¸¡æ–¹ã§ã€SVDã¨è¡Œåˆ—å¾®åˆ†ãŒä½¿ã‚ã‚Œã¦ã„ã‚‹ã€‚

```mermaid
graph TD
    subgraph "ğŸ¯ æ¨è«–ï¼ˆForward Passï¼‰"
        EMB["åŸ‹ã‚è¾¼ã¿<br/>E[x_t]"] --> ATTN["Attention<br/>QK^T/âˆšd"]
        ATTN --> FFN["FFN<br/>W_2 Ïƒ(W_1 h)"]
        FFN --> OUT["å‡ºåŠ›<br/>logits"]
    end

    subgraph "ğŸ”„ å­¦ç¿’ï¼ˆBackward Passï¼‰"
        LOSS["Loss = -log p(x_t)"] --> GRAD_OUT["âˆ‚L/âˆ‚logits<br/>ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³"]
        GRAD_OUT --> GRAD_FFN["âˆ‚L/âˆ‚W_1, âˆ‚L/âˆ‚W_2<br/>é€£é–å¾‹"]
        GRAD_FFN --> GRAD_ATTN["âˆ‚L/âˆ‚W_Q, âˆ‚L/âˆ‚W_K, âˆ‚L/âˆ‚W_V<br/>é€£é–å¾‹"]
        GRAD_ATTN --> UPDATE["Adamæ›´æ–°<br/>W â† W - lrÂ·mÌ‚/(âˆšvÌ‚+Îµ)"]
    end

    subgraph "ğŸ”§ åŠ¹ç‡åŒ–"
        LORA["LoRA: Î”W = BA<br/>ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ (SVDçš„ç™ºæƒ³)"]
        PRUNE["æ§‹é€ åŒ–æåˆˆã‚Š<br/>SVDã§é‡è¦åº¦åˆ¤å®š"]
    end

    OUT -.->|"Cross-Entropy"| LOSS
    UPDATE -.->|"é‡ã¿æ›´æ–°"| EMB
    LORA -.->|"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡åŒ–"| UPDATE
    PRUNE -.->|"ãƒ¢ãƒ‡ãƒ«åœ§ç¸®"| FFN

    style LOSS fill:#ffcdd2
    style LORA fill:#c8e6c9
```

| LLMã®æ“ä½œ | ç¬¬3å›ã®å¯¾å¿œã‚»ã‚¯ã‚·ãƒ§ãƒ³ | ãªãœå¿…è¦ã‹ |
|:----------|:-------------------|:---------|
| Forward pass | 3.7 é€£é–å¾‹ | å„å±¤ã®å‡ºåŠ›ã‚’é †ã«è¨ˆç®— |
| Backward pass | 3.7 é€£é–å¾‹ + 3.8 Backprop | å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹¾é…ã‚’é€†é †ã«è¨ˆç®— |
| LoRA | 3.3 ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ | é‡ã¿æ›´æ–°ã‚’ rank-$r$ ã§è¿‘ä¼¼ |
| Adam optimizer | 3.6 å‹¾é… | ä¸€æ¬¡ãƒ»äºŒæ¬¡ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆã®æ¨å®š |
| å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚° | 3.6 ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ | å‹¾é…çˆ†ç™ºã®é˜²æ­¢ |

### 2.5 3ã¤ã®æ¯”å–©ã§æ‰ãˆã‚‹æœ¬è¬›ç¾©ã®æœ¬è³ª

**æ¯”å–©1: SVDã¯ã€Œé¡•å¾®é¡ã€**

è¡Œåˆ—ã®ã€Œå¾®ç´°æ§‹é€ ã€ã‚’ç‰¹ç•°å€¤ã¨ã„ã†æ•°å€¤ã§èª­ã¿å–ã‚‹ã€‚å¤§ãã„ç‰¹ç•°å€¤ = é‡è¦ãªæ§‹é€ ã€å°ã•ã„ç‰¹ç•°å€¤ = ãƒã‚¤ã‚ºã€‚é¡•å¾®é¡ã®å€ç‡ã‚’å¤‰ãˆã‚‹ã‚ˆã†ã«ã€æ®‹ã™ç‰¹ç•°å€¤ã®æ•°ï¼ˆãƒ©ãƒ³ã‚¯ $k$ï¼‰ã‚’å¤‰ãˆã‚‹ã“ã¨ã§ã€ç²—ã„æ§‹é€ ã‹ã‚‰ç²¾å¯†ãªæ§‹é€ ã¾ã§è¦‹ãˆã‚‹ã€‚

**æ¯”å–©2: è¡Œåˆ—å¾®åˆ†ã¯ã€Œé«˜æ¬¡å…ƒã®å‚¾ãã€**

2æ¬¡å…ƒã§ $y = f(x)$ ã®å‚¾ããŒ $f'(x)$ ã ã£ãŸã‚ˆã†ã«ã€é«˜æ¬¡å…ƒã§ $\mathbf{y} = \mathbf{f}(\mathbf{x})$ ã®ã€Œå‚¾ãã€ãŒãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ $J$ ã ã€‚ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã¯ã€Œå…¥åŠ›ã®å¾®å°å¤‰åŒ–ãŒå‡ºåŠ›ã«ã©ã†ä¼æ’­ã™ã‚‹ã‹ã€ã‚’è¡Œåˆ—ã¨ã—ã¦è¡¨ç¾ã™ã‚‹ã€‚

**æ¯”å–©3: è‡ªå‹•å¾®åˆ†ã¯ã€Œè¨ˆç®—ã®éŒ²ç”»ã¨å·»ãæˆ»ã—ã€**

Forward passã§è¨ˆç®—ã‚’ã€ŒéŒ²ç”»ã€ã—ã€Backward passã§ã€Œå·»ãæˆ»ã—ã€ãªãŒã‚‰å‹¾é…ã‚’è¨ˆç®—ã™ã‚‹ã€‚VHSãƒ†ãƒ¼ãƒ—ã®å·»ãæˆ»ã—ã¨åŒã˜ã§ã€æœ€å¾Œã«è¨ˆç®—ã—ãŸéƒ¨åˆ†ã‹ã‚‰é †ã«å‹¾é…ãŒæ±‚ã¾ã‚‹ã€‚

### 2.6 å­¦ç¿’æˆ¦ç•¥

ã“ã®è¬›ç¾©ã¯ç¬¬2å›ã‚ˆã‚Šã‚‚ã•ã‚‰ã«æ•°å¼ãŒå¤šã„ã€‚å¿ƒæ§‹ãˆ:

1. **Zone 3 ãŒæœ€é‡è¦**ã€‚90åˆ†ã‚’æƒœã—ã¾ãªã„
2. **SVD â†’ è¡Œåˆ—å¾®åˆ† â†’ è‡ªå‹•å¾®åˆ†** ã®é †ã§å­¦ã¶ï¼ˆå„ãƒˆãƒ”ãƒƒã‚¯ãŒå‰ã®ãƒˆãƒ”ãƒƒã‚¯ã«ä¾å­˜ã™ã‚‹ï¼‰
3. **æ•°å€¤æ¤œè¨¼ã‚’æ€ ã‚‰ãªã„**: è§£æçš„ãªçµæœã¯å¿…ãšã‚³ãƒ¼ãƒ‰ã§ç¢ºèªã™ã‚‹
4. **ç´™ã«æ›¸ã**: 2Ã—2è¡Œåˆ—ã®SVDã‚’æ‰‹è¨ˆç®—ã§1å›ã‚„ã‚‹ã¨ç†è§£ãŒæ®µé•ã„ã«æ·±ã¾ã‚‹
5. **Zone 5 ã§è…•è©¦ã—**: SVDç”»åƒåœ§ç¸®ã¨è‡ªå‹•å¾®åˆ†ã®æ‰‹å‹•å®Ÿè£…ãŒã€ç†è§£åº¦ã®æœ€è‰¯ã®ãƒ†ã‚¹ãƒˆ

### 2.7 SVDãƒ»è¡Œåˆ—å¾®åˆ†ã®æ©Ÿæ¢°å­¦ç¿’ã«ãŠã‘ã‚‹ä½ç½®ã¥ã‘

```mermaid
graph TD
    SVD["SVD<br/>ç‰¹ç•°å€¤åˆ†è§£"]
    AD["è‡ªå‹•å¾®åˆ†<br/>Forward/Reverse"]
    CALC["è¡Œåˆ—å¾®åˆ†<br/>ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ãƒ»ãƒ˜ã‚·ã‚¢ãƒ³"]

    SVD --> PCA_["PCA<br/>æ¬¡å…ƒå‰Šæ¸›"]
    SVD --> LORA["LoRA<br/>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡åŒ–"]
    SVD --> REC["æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ <br/>å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"]
    SVD --> COMPRESS["ãƒ¢ãƒ‡ãƒ«åœ§ç¸®<br/>ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼"]
    SVD --> NMF["NMF<br/>éè² è¡Œåˆ—åˆ†è§£"]

    AD --> BACKPROP["Backpropagation<br/>ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆå­¦ç¿’"]
    AD --> JAX_["JAX<br/>é–¢æ•°å¤‰æ›"]
    AD --> PHYSICS["Physics-Informed NN<br/>å¾®åˆ†æ–¹ç¨‹å¼"]

    CALC --> NF["Normalizing Flow<br/>ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—å¼"]
    CALC --> NATURAL["Natural Gradient<br/>Fisheræƒ…å ±è¡Œåˆ—"]
    CALC --> HESSIAN["äºŒæ¬¡æœ€é©åŒ–<br/>Newtonæ³•"]

    style SVD fill:#e3f2fd
    style AD fill:#c8e6c9
    style CALC fill:#fff9c4
```

| æŠ€è¡“ | é–¢é€£ã™ã‚‹æ•°å­¦ | å¿œç”¨ | è¬›ç¾© |
|:-----|:-----------|:-----|:-----|
| LoRA[^10] | SVD + ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ | LLMã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° | æœ¬è¬›ç¾© |
| FlashAttention[^12] | è¡Œåˆ—ã®ãƒ–ãƒ­ãƒƒã‚¯åˆ†å‰² | Attentioné«˜é€ŸåŒ– | ç¬¬2å› |
| Normalizing Flow[^13] | ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—å¼ | ç¢ºç‡å¯†åº¦å¤‰æ› | ç¬¬25å› |
| Natural Gradient | Fisheræƒ…å ±è¡Œåˆ— | æœ€é©åŒ–ã®å¹¾ä½•å­¦ | ç¬¬27å› |
| Neural ODE | è‡ªå‹•å¾®åˆ† + ODE | é€£ç¶šæ·±åº¦ãƒ¢ãƒ‡ãƒ« | ç¬¬26å› |
| Spectral Normalization | SVDã®æœ€å¤§ç‰¹ç•°å€¤ | GANå®‰å®šåŒ– | ç¬¬14å› |

### 2.8 è‡ªå‹•å¾®åˆ†ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®é€²åŒ–

```mermaid
timeline
    title è‡ªå‹•å¾®åˆ†ã®æ­´å²
    1964 : Wengert ãŒ AD ã®åŸºæœ¬ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ç™ºè¡¨
    1970 : Linnainmaa ãŒ Reverse Mode ã‚’å®šå¼åŒ–
    1986 : Rumelhart-Hinton ãŒ Backpropagation ã¨ã—ã¦å†ç™ºè¦‹
    2007 : Theano â€” è¨˜å·å¾®åˆ† + ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
    2015 : TensorFlow 1.x â€” é™çš„è¨ˆç®—ã‚°ãƒ©ãƒ•
    2016 : PyTorch â€” å‹•çš„è¨ˆç®—ã‚°ãƒ©ãƒ• (Define-by-Run)
    2018 : JAX â€” é–¢æ•°å¤‰æ› (grad, jit, vmap, pmap)
    2020 : PyTorch 2.0 â€” torch.compile (å‹•çš„+é™çš„ã®èåˆ)
    2024 : Reactant.jl â€” Julia + XLA ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
```

| ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ | ADæ–¹å¼ | ç‰¹å¾´ | é•·æ‰€ |
|:-------------|:------|:-----|:-----|
| PyTorch | Reverse (tape-based) | Define-by-Run | æŸ”è»Ÿã€ãƒ‡ãƒãƒƒã‚°ã—ã‚„ã™ã„ |
| JAX | Forward + Reverse (tracing) | é–¢æ•°å¤‰æ› | `grad`, `vmap`, `jit` ã®åˆæˆ |
| TensorFlow | Reverse (graph-based) | é™çš„æœ€é©åŒ– | ãƒ‡ãƒ—ãƒ­ã‚¤ã«å¼·ã„ |
| Zygote.jl | Source-to-source | Julia ASTå¤‰æ› | ä»»æ„ã®Juliaã‚³ãƒ¼ãƒ‰ã«é©ç”¨å¯èƒ½ |
| Enzyme | LLVM IR ãƒ¬ãƒ™ãƒ« | ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©çµ±åˆ | è¨€èªéä¾å­˜ |

:::details JAX ã®é–¢æ•°å¤‰æ›: grad, jit, vmap
JAXã®é©æ–°ã¯ã€è‡ªå‹•å¾®åˆ†ã‚’ã€Œé–¢æ•°å¤‰æ›ã€ã¨ã—ã¦æ‰±ã†ã“ã¨ã€‚

```python
# JAX-style function transforms (conceptual)
# grad: f â†’ âˆ‡f
# jit: f â†’ compiled f
# vmap: f â†’ batched f

# Real JAX code would look like:
# import jax
# import jax.numpy as jnp
#
# def loss(params, x, y):
#     pred = params @ x
#     return jnp.sum((pred - y)**2)
#
# grad_fn = jax.grad(loss)        # returns gradient function
# fast_grad = jax.jit(grad_fn)    # compile for speed
# batch_grad = jax.vmap(grad_fn)  # vectorize over batch
```

`grad` ãŒè¿”ã™ã®ã¯**é–¢æ•°**ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€Œå‹¾é…ã®å‹¾é…ã€ï¼ˆãƒ˜ã‚·ã‚¢ãƒ³ï¼‰ã‚‚ç°¡å˜ã«è¨ˆç®—ã§ãã‚‹:

```python
# hessian = jax.hessian(loss)  # âˆ‡Â²f
# jvp = jax.jvp(f, primals, tangents)  # Forward Mode
# vjp = jax.vjp(f, primals)  # Reverse Mode
```
:::

:::message
**é€²æ—: 20% å®Œäº†** SVDãƒ»è¡Œåˆ—å¾®åˆ†ãƒ»è‡ªå‹•å¾®åˆ†ã®å…¨ä½“åƒã‚’æ´ã‚“ã ã€‚ã“ã“ã‹ã‚‰Zone 3ã€Œæ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ã€â€” æœ¬è¬›ç¾©æœ€å¤§ã®å±±å ´ã ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” SVDã‹ã‚‰è‡ªå‹•å¾®åˆ†ã¾ã§

> **ç›®æ¨™**: SVDã®å­˜åœ¨å®šç†ã¨æœ€é©æ€§ã€è¡Œåˆ—å¾®åˆ†ã®ä½“ç³»ã€é€£é–å¾‹ã€è‡ªå‹•å¾®åˆ†ã®ç†è«–ã‚’å°å‡ºã—ã€Backpropagationã®æ•°å­¦çš„åŸºç›¤ã‚’å®Œå…¨ç†è§£ã™ã‚‹ã€‚

æœ¬ã‚·ãƒªãƒ¼ã‚ºã§æœ€ã‚‚æ•°å¼å¯†åº¦ãŒé«˜ã„ã‚¾ãƒ¼ãƒ³ã ã€‚ã ãŒã€ã“ã“ã§å­¦ã¶å…¨ã¦ã®æ¦‚å¿µã¯ã€ç¬¬9å›ä»¥é™ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã§ç¹°ã‚Šè¿”ã—ç™»å ´ã™ã‚‹ã€‚ä¸€ã¤ãšã¤ã€ç¢ºå®Ÿã«ç†è§£ã—ã¦ã„ã“ã†ã€‚

### 3.1 SVDï¼ˆç‰¹ç•°å€¤åˆ†è§£ï¼‰ã®å®šç¾©ã¨å­˜åœ¨å®šç†

#### å®šç¾©

**å®šç†** (ç‰¹ç•°å€¤åˆ†è§£): ä»»æ„ã®è¡Œåˆ— $A \in \mathbb{R}^{m \times n}$ ã«å¯¾ã—ã¦ã€ä»¥ä¸‹ã®åˆ†è§£ãŒå­˜åœ¨ã™ã‚‹:

$$
A = U \Sigma V^\top
$$

ã“ã“ã§:
- $U \in \mathbb{R}^{m \times m}$: ç›´äº¤è¡Œåˆ—ï¼ˆ$U^\top U = I_m$ï¼‰â€” **å·¦ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ«**
- $\Sigma \in \mathbb{R}^{m \times n}$: å¯¾è§’è¡Œåˆ—ï¼ˆ$\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0$ï¼‰â€” **ç‰¹ç•°å€¤**
- $V \in \mathbb{R}^{n \times n}$: ç›´äº¤è¡Œåˆ—ï¼ˆ$V^\top V = I_n$ï¼‰â€” **å³ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ«**
- $r = \text{rank}(A)$

#### å›ºæœ‰å€¤åˆ†è§£ã¨ã®é–¢ä¿‚

SVDã®å­˜åœ¨ã¯ã€å›ºæœ‰å€¤åˆ†è§£ã‹ã‚‰å°ã‘ã‚‹ã€‚

$A^\top A$ ã¯ $n \times n$ ã®åŠæ­£å®šå€¤å¯¾ç§°è¡Œåˆ—ãªã®ã§ã€ã‚¹ãƒšã‚¯ãƒˆãƒ«å®šç†ã‚ˆã‚Šç›´äº¤å¯¾è§’åŒ–å¯èƒ½:

$$
A^\top A = V \Lambda V^\top, \quad \Lambda = \text{diag}(\lambda_1, \ldots, \lambda_n), \quad \lambda_1 \geq \cdots \geq \lambda_n \geq 0
$$

ç‰¹ç•°å€¤ã‚’ $\sigma_i = \sqrt{\lambda_i}$ ã¨å®šç¾©ã™ã‚‹ã€‚$\sigma_i > 0$ ã®å€‹æ•°ãŒ $r = \text{rank}(A)$ã€‚

å·¦ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ«ã¯:

$$
\mathbf{u}_i = \frac{A \mathbf{v}_i}{\sigma_i} \quad (i = 1, \ldots, r)
$$

**æ¤œè¨¼**:

$$
A = U \Sigma V^\top \implies A^\top A = V \Sigma^\top U^\top U \Sigma V^\top = V \Sigma^\top \Sigma V^\top = V \Lambda V^\top \quad \checkmark
$$

åŒæ§˜ã« $AA^\top = U \Lambda' U^\top$ï¼ˆ$\Lambda'$ ã®éã‚¼ãƒ­å¯¾è§’è¦ç´ ã¯ $\Lambda$ ã¨åŒã˜ï¼‰ã€‚

```python
import numpy as np

# Verify SVD via eigendecomposition
A = np.array([[3, 2, 2],
              [2, 3, -2]])

# Method 1: np.linalg.svd
U, s, Vt = np.linalg.svd(A)
print("SVD:")
print(f"  Singular values: {np.round(s, 4)}")

# Method 2: eigendecomposition of A^T A
AtA = A.T @ A
eigenvalues, V_eig = np.linalg.eigh(AtA)
# eigh returns ascending order, reverse for descending
idx = np.argsort(eigenvalues)[::-1]
eigenvalues = eigenvalues[idx]
V_eig = V_eig[:, idx]

print(f"\nEigenvalues of A^T A: {np.round(eigenvalues, 4)}")
print(f"Singular values (sqrt): {np.round(np.sqrt(np.maximum(eigenvalues, 0)), 4)}")
print(f"Match: {np.allclose(s, np.sqrt(np.maximum(eigenvalues, 0))[:len(s)])}")
```

#### Compact SVD ã¨ Economy SVD

Full SVDã¯è¨ˆç®—é‡ãŒç„¡é§„ã«ãªã‚‹ã“ã¨ãŒå¤šã„ã€‚å®Ÿç”¨ä¸Šã¯ä»¥ä¸‹ã‚’ä½¿ã†:

| åç§° | å®šç¾© | ã‚µã‚¤ã‚º | ç”¨é€” |
|:-----|:-----|:------|:-----|
| Full SVD | $A = U \Sigma V^\top$ | $U: m \times m, \Sigma: m \times n, V: n \times n$ | ç†è«– |
| Compact SVD | $A = U_r \Sigma_r V_r^\top$ | $U_r: m \times r, \Sigma_r: r \times r, V_r: r \times n$ | $\text{rank}(A) = r \ll \min(m,n)$ |
| Truncated SVD | $A_k = U_k \Sigma_k V_k^\top$ | $U_k: m \times k, \Sigma_k: k \times k, V_k: k \times n$ | ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ |

```python
import numpy as np

A = np.random.randn(100, 50)

# Full SVD
U_full, s_full, Vt_full = np.linalg.svd(A, full_matrices=True)
print(f"Full SVD: U={U_full.shape}, s={s_full.shape}, Vt={Vt_full.shape}")

# Economy SVD (full_matrices=False)
U_econ, s_econ, Vt_econ = np.linalg.svd(A, full_matrices=False)
print(f"Economy SVD: U={U_econ.shape}, s={s_econ.shape}, Vt={Vt_econ.shape}")

# Truncated SVD (rank-k)
k = 10
U_k = U_econ[:, :k]
s_k = s_econ[:k]
Vt_k = Vt_econ[:k, :]
A_k = U_k @ np.diag(s_k) @ Vt_k
print(f"Truncated SVD (k={k}): error = {np.linalg.norm(A - A_k, 'fro'):.4f}")
```

### 3.2 Eckart-Youngå®šç† â€” ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ã®æœ€é©æ€§

#### å®šç†

**å®šç†** (Eckart-Young-Mirsky[^3]): $A \in \mathbb{R}^{m \times n}$ ã® SVD ã‚’ $A = U \Sigma V^\top$ ã¨ã—ã€$\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0$ ã‚’ç‰¹ç•°å€¤ã¨ã™ã‚‹ã€‚ä»»æ„ã® rank-$k$ è¡Œåˆ— $B$ ã«å¯¾ã—ã¦:

$$
\min_{\text{rank}(B) \leq k} \|A - B\|_F = \sqrt{\sum_{i=k+1}^{r} \sigma_i^2}
$$

ã“ã®æœ€å°å€¤ã‚’é”æˆã™ã‚‹ $B$ ã¯:

$$
A_k = \sum_{i=1}^{k} \sigma_i \mathbf{u}_i \mathbf{v}_i^\top = U_k \Sigma_k V_k^\top
$$

ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒãƒ«ãƒ ã«ã¤ã„ã¦ã‚‚:

$$
\min_{\text{rank}(B) \leq k} \|A - B\|_2 = \sigma_{k+1}
$$

#### è¨¼æ˜ã®ã‚¹ã‚±ãƒƒãƒ

$B$ ã‚’ä»»æ„ã® rank-$k$ è¡Œåˆ—ã¨ã™ã‚‹ã€‚$\ker(B)$ ã®æ¬¡å…ƒã¯ $n - k$ ä»¥ä¸Šã€‚ä¸€æ–¹ã€$V_1, \ldots, V_{k+1}$ ãŒå¼µã‚‹éƒ¨åˆ†ç©ºé–“ã¯ $k+1$ æ¬¡å…ƒã€‚æ¬¡å…ƒã®å¼•æ•°ï¼ˆdimension argumentï¼‰ã‚ˆã‚Šã€$\ker(B)$ ã¨ $\text{span}\{V_1, \ldots, V_{k+1}\}$ ã¯éè‡ªæ˜ãªäº¤ã‚ã‚Šã‚’æŒã¤ã€‚

$\mathbf{w} \neq \mathbf{0}$ ã‚’ã“ã®äº¤ã‚ã‚Šã®è¦ç´ ã¨ã™ã‚‹ã¨:

$$
\|A - B\|_F^2 \geq \|(A-B)\mathbf{w}\|^2 / \|\mathbf{w}\|^2 = \|A\mathbf{w}\|^2 / \|\mathbf{w}\|^2
$$

$\mathbf{w} \in \text{span}\{V_1, \ldots, V_{k+1}\}$ ã‚ˆã‚Šã€$\mathbf{w} = \sum_{i=1}^{k+1} c_i \mathbf{v}_i$ ã¨æ›¸ã‘ã‚‹ã€‚

$$
\|A\mathbf{w}\|^2 = \sum_{i=1}^{k+1} c_i^2 \sigma_i^2 \geq \sigma_{k+1}^2 \sum_{i=1}^{k+1} c_i^2 = \sigma_{k+1}^2 \|\mathbf{w}\|^2
$$

ã—ãŸãŒã£ã¦ $\|A - B\|_2 \geq \sigma_{k+1}$ã€‚$A_k$ ãŒã“ã®ä¸‹ç•Œã‚’é”æˆã™ã‚‹ã“ã¨ã¯ç›´æ¥è¨ˆç®—ã§ç¢ºèªã§ãã‚‹ã€‚$\square$

:::message alert
ä¸Šè¨˜ã®è¨¼æ˜ã‚¹ã‚±ãƒƒãƒã¯ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒãƒ«ãƒ ç‰ˆã§ã™ã€‚ãƒ•ãƒ­ãƒ™ãƒ‹ã‚¦ã‚¹ãƒãƒ«ãƒ ç‰ˆã®æœ€é©æ€§ã¯ $\|A - A_k\|_F^2 = \sum_{i=k+1}^{r} \sigma_i^2$ ã®ç›´æ¥è¨ˆç®—ã§ç¤ºã•ã‚Œã¾ã™ï¼ˆFan-Hoffmanä¸ç­‰å¼ï¼‰ã€‚
:::

```python
import numpy as np

# Verify Eckart-Young theorem
A = np.random.randn(50, 30)
U, s, Vt = np.linalg.svd(A, full_matrices=False)

for k in [1, 3, 5, 10, 20]:
    A_k = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]
    error_F = np.linalg.norm(A - A_k, 'fro')
    theoretical = np.sqrt(np.sum(s[k:]**2))
    print(f"rank-{k:2d}: ||A-A_k||_F = {error_F:.6f}, "
          f"theoretical = {theoretical:.6f}, "
          f"match = {np.isclose(error_F, theoretical)}")
```

:::message
**LoRAã¸ã®æ¥ç¶š**: LoRA[^10]ã¯ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚ã®é‡ã¿æ›´æ–° $\Delta W$ ã‚’ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ— $BA$ ã§è¿‘ä¼¼ã™ã‚‹ã€‚$B \in \mathbb{R}^{d \times r}, A \in \mathbb{R}^{r \times d}$ ã§ $r \ll d$ã€‚Eckart-Youngå®šç†ã¯ã€Œä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ã¯æœ€é©ã€ã‚’ä¿è¨¼ã™ã‚‹ãŒã€LoRAã®å ´åˆã¯å­¦ç¿’ã§ $B, A$ ã‚’æœ€é©åŒ–ã™ã‚‹ãŸã‚ã€SVDã¨ã¯ç•°ãªã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã€‚ã ãŒã€å­¦ç¿’å¾Œã® $\Delta W = BA$ ã‚’SVDã§åˆ†æã™ã‚‹ã¨ã€ç¢ºã‹ã«å°‘æ•°ã®ç‰¹ç•°å€¤ãŒæ”¯é…çš„ã§ã‚ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚Œã‚‹ã€‚
:::

### 3.3 ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ã®å¿œç”¨ â€” ç”»åƒåœ§ç¸®ãƒ»æ¨è–¦ãƒ»LoRA

#### ç”»åƒåœ§ç¸®

```python
import numpy as np

# Create a test image-like matrix (smooth gradients + structure)
m, n = 200, 150
x = np.linspace(0, 4*np.pi, m)
y = np.linspace(0, 3*np.pi, n)
X, Y = np.meshgrid(y, x)
A = np.sin(X) * np.cos(Y) + 0.5 * np.sin(2*X + Y)  # structured image

U, s, Vt = np.linalg.svd(A, full_matrices=False)

print("Singular value decay:")
for k in [1, 5, 10, 20, 50]:
    A_k = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]
    rel_error = np.linalg.norm(A - A_k, 'fro') / np.linalg.norm(A, 'fro')
    storage_original = m * n
    storage_compressed = k * (m + n + 1)
    ratio = storage_compressed / storage_original
    print(f"  rank-{k:2d}: error={rel_error:.6f}, "
          f"storage={ratio:.1%} ({storage_compressed}/{storage_original})")
```

#### æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰

ãƒ¦ãƒ¼ã‚¶ãƒ¼Ã—ã‚¢ã‚¤ãƒ†ãƒ ã®è©•ä¾¡è¡Œåˆ— $R$ ã¯å¤§éƒ¨åˆ†ãŒæ¬ æï¼ˆæœªè©•ä¾¡ï¼‰ã€‚ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ $R \approx U_k \Sigma_k V_k^\top$ ã§æ¬ æå€¤ã‚’äºˆæ¸¬ã§ãã‚‹ã€‚

$$
\hat{r}_{ij} = \sum_{l=1}^{k} \sigma_l u_{il} v_{jl}
$$

```python
import numpy as np

# Toy recommendation: 5 users Ã— 4 items
R = np.array([
    [5, 3, 0, 1],
    [4, 0, 0, 1],
    [1, 1, 0, 5],
    [1, 0, 0, 4],
    [0, 1, 5, 4],
], dtype=float)

# Replace 0 (unknown) with mean for SVD
mask = R > 0
R_filled = R.copy()
R_filled[~mask] = np.mean(R[mask])

U, s, Vt = np.linalg.svd(R_filled, full_matrices=False)

# Rank-2 approximation
k = 2
R_approx = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]

print("Original (0 = unknown):")
print(R.astype(int))
print(f"\nRank-{k} approximation (predictions for unknowns):")
print(np.round(R_approx, 1))
print(f"\nPredicted ratings for unknown entries:")
for i, j in zip(*np.where(~mask)):
    print(f"  User {i+1}, Item {j+1}: {R_approx[i,j]:.1f}")
```

### 3.4 æ“¬ä¼¼é€†è¡Œåˆ—ï¼ˆMoore-Penroseï¼‰

#### å®šç¾©

$A \in \mathbb{R}^{m \times n}$ ã® **Moore-Penrose æ“¬ä¼¼é€†è¡Œåˆ—** $A^+ \in \mathbb{R}^{n \times m}$ ã¯ä»¥ä¸‹ã®4æ¡ä»¶ã‚’æº€ãŸã™å”¯ä¸€ã®è¡Œåˆ—:

1. $A A^+ A = A$
2. $A^+ A A^+ = A^+$
3. $(A A^+)^\top = A A^+$
4. $(A^+ A)^\top = A^+ A$

#### SVDã«ã‚ˆã‚‹æ§‹æˆ

$A = U \Sigma V^\top$ ãªã‚‰ã°:

$$
A^+ = V \Sigma^+ U^\top
$$

ã“ã“ã§ $\Sigma^+ = \text{diag}(1/\sigma_1, \ldots, 1/\sigma_r, 0, \ldots, 0)$ã€‚

**ç›´æ„Ÿ**: ç‰¹ç•°å€¤ã®é€†æ•°ã‚’å–ã‚‹ã€‚ãŸã ã— $\sigma_i = 0$ ã®æˆåˆ†ã¯ç„¡è¦–ã™ã‚‹ã€‚

```python
import numpy as np

# Pseudoinverse via SVD
A = np.array([[1, 2],
              [3, 4],
              [5, 6]])

# Method 1: np.linalg.pinv
A_pinv = np.linalg.pinv(A)

# Method 2: manual SVD construction
U, s, Vt = np.linalg.svd(A, full_matrices=False)
S_pinv = np.diag(1.0 / s)
A_pinv_manual = Vt.T @ S_pinv @ U.T

print(f"A (shape {A.shape}):")
print(A)
print(f"\nA+ (shape {A_pinv.shape}):")
print(np.round(A_pinv, 4))
print(f"\nManual matches: {np.allclose(A_pinv, A_pinv_manual)}")

# Verify Moore-Penrose conditions
print(f"\nMoore-Penrose conditions:")
print(f"  A A+ A = A: {np.allclose(A @ A_pinv @ A, A)}")
print(f"  A+ A A+ = A+: {np.allclose(A_pinv @ A @ A_pinv, A_pinv)}")
print(f"  (A A+)^T = A A+: {np.allclose((A @ A_pinv).T, A @ A_pinv)}")
print(f"  (A+ A)^T = A+ A: {np.allclose((A_pinv @ A).T, A_pinv @ A)}")
```

#### æœ€å°äºŒä¹—æ³•ã¨ã®é–¢ä¿‚

éå‰°æ±ºå®šç³» $A\mathbf{x} = \mathbf{b}$ï¼ˆ$m > n$, è§£ãªã—ï¼‰ã®æœ€å°äºŒä¹—è§£ã¯:

$$
\hat{\mathbf{x}} = A^+ \mathbf{b} = V \Sigma^+ U^\top \mathbf{b}
$$

ç¬¬2å›ã®æ­£è¦æ–¹ç¨‹å¼ $A^\top A \hat{\mathbf{x}} = A^\top \mathbf{b}$ ã¨åŒã˜è§£ã‚’ä¸ãˆã‚‹ãŒã€SVDç‰ˆã¯ $A^\top A$ ãŒç‰¹ç•°ãªå ´åˆã§ã‚‚æ•°å€¤çš„ã«å®‰å®šã€‚

#### Tikhonovæ­£å‰‡åŒ–ï¼ˆRidgeå›å¸°ï¼‰

æ¡ä»¶æ•°ãŒå¤§ãã„å ´åˆã€æ“¬ä¼¼é€†è¡Œåˆ—ã¯æ•°å€¤çš„ã«ä¸å®‰å®šã€‚æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\lambda > 0$ ã‚’åŠ ãˆã‚‹:

$$
\hat{\mathbf{x}}_\lambda = (A^\top A + \lambda I)^{-1} A^\top \mathbf{b} = \sum_{i=1}^{r} \frac{\sigma_i}{\sigma_i^2 + \lambda} \mathbf{v}_i (\mathbf{u}_i^\top \mathbf{b})
$$

$\lambda$ ãŒå¤§ãã„ã»ã©ã€å°ã•ãªç‰¹ç•°å€¤ã®å½±éŸ¿ãŒæŠ‘åˆ¶ã•ã‚Œã‚‹ã€‚ã“ã‚Œã¯**Ridgeå›å¸°**ã¨ç­‰ä¾¡ã€‚

```python
import numpy as np

# Ill-conditioned system
np.random.seed(42)
A = np.random.randn(20, 10)
A[:, -1] = A[:, 0] + 1e-8 * np.random.randn(20)  # nearly collinear
b = np.random.randn(20)

print(f"Condition number: {np.linalg.cond(A):.2e}")

# Pseudoinverse (unstable)
x_pinv = np.linalg.pinv(A) @ b
print(f"||x_pinv|| = {np.linalg.norm(x_pinv):.4f}")

# Tikhonov regularization
for lam in [0.001, 0.01, 0.1, 1.0]:
    x_ridge = np.linalg.solve(A.T @ A + lam * np.eye(10), A.T @ b)
    residual = np.linalg.norm(A @ x_ridge - b)
    print(f"Î»={lam:.3f}: ||x||={np.linalg.norm(x_ridge):.4f}, "
          f"residual={residual:.4f}")
```

### 3.5 PCA ã® SVD ã«ã‚ˆã‚‹å°å‡º

ç¬¬2å›ã§ã¯å›ºæœ‰å€¤åˆ†è§£ã«ã‚ˆã‚‹PCA[^5][^6]ã‚’å°å‡ºã—ãŸã€‚ã“ã“ã§ã¯SVDã«ã‚ˆã‚‹PCAã‚’å°å‡ºã—ã€ä¸¡è€…ã®ç­‰ä¾¡æ€§ã‚’ç¤ºã™ã€‚

#### ãƒ‡ãƒ¼ã‚¿è¡Œåˆ—ã‹ã‚‰ã®å°å‡º

ãƒ‡ãƒ¼ã‚¿è¡Œåˆ— $X \in \mathbb{R}^{n \times d}$ï¼ˆ$n$ ã‚µãƒ³ãƒ—ãƒ«ã€$d$ æ¬¡å…ƒï¼‰ã‚’ä¸­å¿ƒåŒ–ï¼ˆå„åˆ—ã®å¹³å‡ã‚’å¼•ãï¼‰ã—ãŸã‚‚ã®ã‚’ $\tilde{X}$ ã¨ã™ã‚‹ã€‚

å…±åˆ†æ•£è¡Œåˆ—:

$$
C = \frac{1}{n-1} \tilde{X}^\top \tilde{X}
$$

$\tilde{X}$ ã® SVD ã‚’ $\tilde{X} = U \Sigma V^\top$ ã¨ã™ã‚‹ã¨:

$$
C = \frac{1}{n-1} V \Sigma^\top U^\top U \Sigma V^\top = \frac{1}{n-1} V \Sigma^2 V^\top
$$

ã“ã‚Œã¯ $C$ ã®å›ºæœ‰å€¤åˆ†è§£ãã®ã‚‚ã®ã ã€‚ã¤ã¾ã‚Š:
- **PCAã®ä¸»æˆåˆ†æ–¹å‘** = $\tilde{X}$ ã®å³ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ« $V$ ã®åˆ—
- **PCAã®ä¸»æˆåˆ†ã®åˆ†æ•£** = $\sigma_i^2 / (n-1)$

#### åˆ†æ•£æœ€å¤§åŒ– â†” å†æ§‹æˆèª¤å·®æœ€å°åŒ–ã®ç­‰ä¾¡æ€§

**åˆ†æ•£æœ€å¤§åŒ–**: ç¬¬1ä¸»æˆåˆ†ã¯ $\mathbf{w}_1 = \arg\max_{\|\mathbf{w}\|=1} \text{Var}(\tilde{X}\mathbf{w})$

**å†æ§‹æˆèª¤å·®æœ€å°åŒ–**: rank-$k$ è¿‘ä¼¼ $\hat{X} = \tilde{X} V_k V_k^\top$ ãŒ $\|\tilde{X} - \hat{X}\|_F^2$ ã‚’æœ€å°åŒ–

ã“ã®2ã¤ã¯**ç­‰ä¾¡**:

$$
\|\tilde{X} - \hat{X}\|_F^2 = \|\tilde{X}\|_F^2 - \|\tilde{X} V_k\|_F^2 = \sum_{i=1}^{r} \sigma_i^2 - \sum_{i=1}^{k} \sigma_i^2 = \sum_{i=k+1}^{r} \sigma_i^2
$$

å†æ§‹æˆèª¤å·®ã‚’æœ€å°åŒ–ã™ã‚‹ã«ã¯ $\sum_{i=1}^{k} \sigma_i^2$ï¼ˆ= å°„å½±å¾Œã®åˆ†æ•£ã®åˆè¨ˆï¼‰ã‚’æœ€å¤§åŒ–ã™ã‚Œã°ã‚ˆã„ã€‚ã“ã‚Œã¯Eckart-Youngå®šç†[^3]ã®ç›´æ¥çš„ãªå¸°çµã€‚

```python
import numpy as np

# PCA via SVD vs eigendecomposition
np.random.seed(42)
n, d = 200, 5
X = np.random.randn(n, d) @ np.diag([5, 3, 1, 0.5, 0.1])  # structured data

# Center the data
X_centered = X - X.mean(axis=0)

# Method 1: PCA via eigendecomposition of covariance
C = X_centered.T @ X_centered / (n - 1)
eigvals, eigvecs = np.linalg.eigh(C)
idx = np.argsort(eigvals)[::-1]
eigvals = eigvals[idx]
eigvecs = eigvecs[:, idx]

# Method 2: PCA via SVD
U, s, Vt = np.linalg.svd(X_centered, full_matrices=False)
pca_variance = s**2 / (n - 1)

print("PCA via Eigendecomposition vs SVD:")
print(f"  Eigenvalues: {np.round(eigvals, 4)}")
print(f"  s^2/(n-1):   {np.round(pca_variance, 4)}")
print(f"  Match: {np.allclose(eigvals, pca_variance)}")

# Principal components
PC_eig = X_centered @ eigvecs[:, :2]  # project onto top-2
PC_svd = U[:, :2] * s[:2]             # equivalent via SVD
print(f"\nPrincipal components match: {np.allclose(np.abs(PC_eig), np.abs(PC_svd))}")

# Explained variance ratio
total_var = np.sum(pca_variance)
for k in range(1, 6):
    ratio = np.sum(pca_variance[:k]) / total_var
    print(f"  Top-{k}: {ratio:.4f} ({ratio*100:.1f}%)")
```

### 3.6 ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—ã¨ Einsteinè¨˜æ³•

#### ãƒ†ãƒ³ã‚½ãƒ«ã¨ã¯

ãƒ†ãƒ³ã‚½ãƒ«ã¯å¤šæ¬¡å…ƒé…åˆ—ã®æ•°å­¦çš„ãªä¸€èˆ¬åŒ–ã€‚æ©Ÿæ¢°å­¦ç¿’ã§ã¯ã€Œå¤šæ¬¡å…ƒé…åˆ—ã€ã¨åŒç¾©ã§ä½¿ã†ã“ã¨ãŒå¤šã„ã€‚

| éšæ•° | æ•°å­¦çš„åç§° | ä¾‹ | NumPy |
|:-----|:---------|:---|:------|
| 0 | ã‚¹ã‚«ãƒ©ãƒ¼ | æå¤±å€¤ $L$ | `np.float64` |
| 1 | ãƒ™ã‚¯ãƒˆãƒ« | åŸ‹ã‚è¾¼ã¿ $\mathbf{e} \in \mathbb{R}^d$ | `shape=(d,)` |
| 2 | è¡Œåˆ— | é‡ã¿ $W \in \mathbb{R}^{m \times n}$ | `shape=(m, n)` |
| 3 | 3éšãƒ†ãƒ³ã‚½ãƒ« | ãƒãƒƒãƒå…¥åŠ› $X \in \mathbb{R}^{B \times T \times d}$ | `shape=(B, T, d)` |
| 4 | 4éšãƒ†ãƒ³ã‚½ãƒ« | Multi-Head Attention $\in \mathbb{R}^{B \times H \times T \times T}$ | `shape=(B, H, T, T)` |

#### Kroneckerç©

è¡Œåˆ—å¾®åˆ†ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹éš›ã«ä¸å¯æ¬ ãªé“å…·ã¨ã—ã¦ã€Kroneckerç©ã‚’å°å…¥ã—ã¾ã™ã€‚

è¡Œåˆ— $A \in \mathbb{R}^{m \times n}$, $B \in \mathbb{R}^{p \times q}$ ã® **Kroneckerç©**:

$$
A \otimes B = \begin{pmatrix} a_{11}B & \cdots & a_{1n}B \\ \vdots & \ddots & \vdots \\ a_{m1}B & \cdots & a_{mn}B \end{pmatrix} \in \mathbb{R}^{mp \times nq}
$$

é‡è¦ãªæ€§è³ª:
- $(A \otimes B)(C \otimes D) = (AC) \otimes (BD)$
- $(A \otimes B)^{-1} = A^{-1} \otimes B^{-1}$
- $\text{vec}(AXB) = (B^\top \otimes A) \text{vec}(X)$

æœ€å¾Œã®æ€§è³ªã¯è¡Œåˆ—æ–¹ç¨‹å¼ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã«ä¸å¯æ¬ :

```python
import numpy as np

A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

# Kronecker product
K = np.kron(A, B)
print(f"A âŠ— B (shape {K.shape}):")
print(K)

# vec(AXB) = (B^T âŠ— A) vec(X)
X = np.array([[1, 0], [0, 1]])
AXB = A @ X @ B
vec_AXB = AXB.flatten('F')  # column-major vectorization
kron_vec = np.kron(B.T, A) @ X.flatten('F')
print(f"\nvec(AXB) = {vec_AXB}")
print(f"(B^T âŠ— A)vec(X) = {kron_vec}")
print(f"Match: {np.allclose(vec_AXB, kron_vec)}")
```

#### Einsteinè¨˜æ³•ï¼ˆå®Œå…¨ç‰ˆï¼‰

Einsteinè¨˜æ³•ã¯ã€ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—ã‚’æ·»å­—ã®è¦å‰‡ã ã‘ã§è¨˜è¿°ã™ã‚‹å¼·åŠ›ãªè¨˜æ³•ã€‚NumPyã® `einsum` ã¯ã“ã®è¨˜æ³•ã‚’ç›´æ¥å®Ÿè£…ã—ã¦ã„ã‚‹ã€‚

**è¦å‰‡**: ç¹°ã‚Šè¿”ã•ã‚Œã‚‹æ·»å­—ã¯**æš—é»™ã«ç·å’Œ**ã•ã‚Œã‚‹ï¼ˆç¸®ç´„ï¼‰ã€‚

| æ¼”ç®— | æ•°å¼ | einsum | èª¬æ˜ |
|:-----|:-----|:-------|:-----|
| å†…ç© | $c = \sum_i a_i b_i$ | `'i,i->'` | ãƒ™ã‚¯ãƒˆãƒ«å†…ç© |
| å¤–ç© | $C_{ij} = a_i b_j$ | `'i,j->ij'` | ãƒ©ãƒ³ã‚¯1è¡Œåˆ— |
| è¡Œåˆ—ç© | $C_{ij} = \sum_k A_{ik} B_{kj}$ | `'ik,kj->ij'` | æ¨™æº–çš„ãªè¡Œåˆ—ç© |
| è¡Œåˆ—ã®ãƒˆãƒ¬ãƒ¼ã‚¹ | $t = \sum_i A_{ii}$ | `'ii->'` | å¯¾è§’è¦ç´ ã®å’Œ |
| è»¢ç½® | $B_{ji} = A_{ij}$ | `'ij->ji'` | è¡Œåˆ—ã®è»¢ç½® |
| å¯¾è§’æŠ½å‡º | $d_i = A_{ii}$ | `'ii->i'` | å¯¾è§’æˆåˆ† |
| ãƒãƒƒãƒè¡Œåˆ—ç© | $C_{bij} = \sum_k A_{bik} B_{bkj}$ | `'bik,bkj->bij'` | ãƒãƒƒãƒå‡¦ç† |
| Multi-Head Attention | $S_{bhij} = \sum_k Q_{bhik} K_{bhjk}$ | `'bhik,bhjk->bhij'` | $QK^\top$ per head |
| äºŒé‡ç¸®ç´„ | $s = \sum_{ij} A_{ij} B_{ij}$ | `'ij,ij->'` | Frobeniuså†…ç© |
| ãƒ†ãƒ³ã‚½ãƒ«ç¸®ç´„ | $C_{ik} = \sum_j A_{ij} B_{jk}$ | `'ij,jk->ik'` | ä¸€èˆ¬ç¸®ç´„ |

```python
import numpy as np

# einsum examples
A = np.random.randn(3, 4)
B = np.random.randn(4, 5)
v = np.random.randn(4)

# Matrix multiplication
C1 = A @ B
C2 = np.einsum('ik,kj->ij', A, B)
print(f"Matrix mul match: {np.allclose(C1, C2)}")

# Trace
t1 = np.trace(A[:3, :3])
# need square submatrix for trace
A_sq = np.random.randn(4, 4)
t1 = np.trace(A_sq)
t2 = np.einsum('ii->', A_sq)
print(f"Trace match: {np.allclose(t1, t2)}")

# Batch matrix multiplication (Attention-style)
B_size, H, T, d = 2, 4, 8, 16
Q = np.random.randn(B_size, H, T, d)
K = np.random.randn(B_size, H, T, d)

# QK^T per head
scores1 = Q @ K.transpose(0, 1, 3, 2)  # using @ and transpose
scores2 = np.einsum('bhik,bhjk->bhij', Q, K)  # using einsum
print(f"Batch attention match: {np.allclose(scores1, scores2)}")
print(f"Scores shape: {scores1.shape}")  # (2, 4, 8, 8)
```

:::details einsum ã®è¨ˆç®—ã‚°ãƒ©ãƒ•ã¨æœ€é©åŒ–
`np.einsum` ã¯æ·»å­—ã®ç¸®ç´„é †åºã‚’æœ€é©åŒ–ã§ãã‚‹ã€‚`optimize=True` ã‚’æŒ‡å®šã™ã‚‹ã¨ã€ä¸­é–“ãƒ†ãƒ³ã‚½ãƒ«ã®ã‚µã‚¤ã‚ºã‚’æœ€å°åŒ–ã™ã‚‹ç¸®ç´„é †åºã‚’è‡ªå‹•çš„ã«é¸æŠã™ã‚‹ã€‚

```python
import numpy as np

# Three-tensor contraction: different orders have different costs
A = np.random.randn(100, 50)
B = np.random.randn(50, 200)
C = np.random.randn(200, 100)

# Without optimization: may choose suboptimal contraction order
result1 = np.einsum('ij,jk,kl->il', A, B, C, optimize=False)

# With optimization: chooses optimal contraction order
result2 = np.einsum('ij,jk,kl->il', A, B, C, optimize=True)
print(f"Results match: {np.allclose(result1, result2)}")

# Check optimal contraction path
path, info = np.einsum_path('ij,jk,kl->il', A, B, C, optimize='optimal')
print(f"Optimal path: {path}")
print(info)
```
:::

### 3.7 å¤šå¤‰æ•°å¾®åˆ† â€” å‹¾é…ãƒ»ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ãƒ»ãƒ˜ã‚·ã‚¢ãƒ³

#### å‹¾é…ï¼ˆGradientï¼‰

ã‚¹ã‚«ãƒ©ãƒ¼é–¢æ•° $f: \mathbb{R}^n \to \mathbb{R}$ ã®**å‹¾é…**:

$$
\nabla f(\mathbf{x}) = \begin{pmatrix} \frac{\partial f}{\partial x_1} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{pmatrix} \in \mathbb{R}^n
$$

å‹¾é…ã¯ $f$ ãŒæœ€ã‚‚æ€¥ã«å¢—åŠ ã™ã‚‹æ–¹å‘ã‚’æŒ‡ã™ã€‚$-\nabla f$ ãŒæœ€æ€¥é™ä¸‹æ–¹å‘ã€‚

#### ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ï¼ˆJacobianï¼‰

ãƒ™ã‚¯ãƒˆãƒ«é–¢æ•° $\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m$ ã®**ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³**:

$$
J = \frac{\partial \mathbf{f}}{\partial \mathbf{x}} \in \mathbb{R}^{m \times n}, \quad J_{ij} = \frac{\partial f_i}{\partial x_j}
$$

ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®å„è¡Œã¯ $f_i$ ã®å‹¾é… $\nabla f_i^\top$ã€‚$m = 1$ ã®ã¨ãã€ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã¯å‹¾é…ã®è»¢ç½® $\nabla f^\top$ã€‚

**å¹¾ä½•å­¦çš„æ„å‘³**: $\mathbf{x}$ ã®è¿‘å‚ã§ã€$\mathbf{f}(\mathbf{x} + \boldsymbol{\delta}) \approx \mathbf{f}(\mathbf{x}) + J \boldsymbol{\delta}$ï¼ˆç·šå½¢è¿‘ä¼¼ï¼‰ã€‚

**ä½“ç©å¤‰åŒ–**: $\det(J)$ ã¯å¤‰æ› $\mathbf{f}$ ã«ã‚ˆã‚‹å±€æ‰€çš„ãªä½“ç©ã®æ‹¡å¤§ç‡ã€‚Normalizing Flow[^13]ã§ã¯:

$$
p_Y(\mathbf{y}) = p_X(\mathbf{f}^{-1}(\mathbf{y})) \cdot |\det(J_{\mathbf{f}^{-1}}(\mathbf{y}))|
$$

#### ãƒ˜ã‚·ã‚¢ãƒ³ï¼ˆHessianï¼‰

ã‚¹ã‚«ãƒ©ãƒ¼é–¢æ•° $f: \mathbb{R}^n \to \mathbb{R}$ ã®**ãƒ˜ã‚·ã‚¢ãƒ³**:

$$
H = \nabla^2 f(\mathbf{x}) \in \mathbb{R}^{n \times n}, \quad H_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}
$$

ãƒ˜ã‚·ã‚¢ãƒ³ã¯å¯¾ç§°è¡Œåˆ—ï¼ˆ$\frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_i}$ã€Schwarzã®å®šç†ï¼‰ã€‚

| ãƒ˜ã‚·ã‚¢ãƒ³ã®æ€§è³ª | æ„å‘³ |
|:-------------|:-----|
| $H \succ 0$ï¼ˆæ­£å®šå€¤ï¼‰ | $\mathbf{x}$ ã¯æ¥µå°ç‚¹ |
| $H \prec 0$ï¼ˆè² å®šå€¤ï¼‰ | $\mathbf{x}$ ã¯æ¥µå¤§ç‚¹ |
| $H$ ãŒä¸å®š | $\mathbf{x}$ ã¯éç‚¹ï¼ˆsaddle pointï¼‰ |

```python
import numpy as np

# Example: f(x, y) = x^2 + 3*y^2 + 2*x*y
# Gradient: [2x + 2y, 6y + 2x]
# Hessian: [[2, 2], [2, 6]]

def f(xy):
    x, y = xy
    return x**2 + 3*y**2 + 2*x*y

def grad_f(xy):
    x, y = xy
    return np.array([2*x + 2*y, 6*y + 2*x])

H = np.array([[2, 2], [2, 6]])  # constant Hessian

# Check positive definiteness
eigvals = np.linalg.eigvalsh(H)
print(f"Hessian eigenvalues: {eigvals}")
print(f"Positive definite: {np.all(eigvals > 0)}")  # True â†’ minimum exists

# Find minimum: grad = 0 â†’ x=0, y=0
x_min = np.array([0.0, 0.0])
print(f"Minimum at: {x_min}, f = {f(x_min)}")

# Newton's method: x_new = x - H^{-1} grad(x)
x = np.array([5.0, 3.0])
for i in range(5):
    g = grad_f(x)
    x = x - np.linalg.solve(H, g)
    print(f"Step {i+1}: x = {np.round(x, 6)}, f = {f(x):.6f}")
```

### 3.8 è¡Œåˆ—å¾®åˆ†ï¼ˆMatrix Calculusï¼‰

#### åŸºæœ¬çš„ãªå¾®åˆ†å…¬å¼

ã‚¹ã‚«ãƒ©ãƒ¼é–¢æ•° $L$ ã®è¡Œåˆ— $W \in \mathbb{R}^{m \times n}$ ã«é–¢ã™ã‚‹å¾®åˆ†:

$$
\frac{\partial L}{\partial W} \in \mathbb{R}^{m \times n}, \quad \left(\frac{\partial L}{\partial W}\right)_{ij} = \frac{\partial L}{\partial W_{ij}}
$$

**Matrix Cookbook[^9] ä¸»è¦å…¬å¼15é¸**:

| # | å…¬å¼ | æ¡ä»¶ |
|:--|:-----|:-----|
| 1 | $\frac{\partial}{\partial \mathbf{x}} (\mathbf{a}^\top \mathbf{x}) = \mathbf{a}$ | |
| 2 | $\frac{\partial}{\partial \mathbf{x}} (\mathbf{x}^\top A \mathbf{x}) = (A + A^\top) \mathbf{x}$ | |
| 3 | $\frac{\partial}{\partial \mathbf{x}} (\mathbf{x}^\top A \mathbf{x}) = 2A\mathbf{x}$ | $A$ å¯¾ç§° |
| 4 | $\frac{\partial}{\partial X} \text{tr}(AX) = A^\top$ | |
| 5 | $\frac{\partial}{\partial X} \text{tr}(X^\top A) = A$ | |
| 6 | $\frac{\partial}{\partial X} \text{tr}(AXB) = A^\top B^\top$ | |
| 7 | $\frac{\partial}{\partial X} \text{tr}(X^\top AX) = (A + A^\top)X$ | |
| 8 | $\frac{\partial}{\partial X} \|X\|_F^2 = 2X$ | |
| 9 | $\frac{\partial}{\partial X} \ln \det(X) = X^{-\top}$ | $X$ æ­£å‰‡ |
| 10 | $\frac{\partial}{\partial X} \det(X) = \det(X) X^{-\top}$ | $X$ æ­£å‰‡ |
| 11 | $\frac{\partial}{\partial \mathbf{x}} \|\mathbf{x}\|^2 = 2\mathbf{x}$ | |
| 12 | $\frac{\partial}{\partial \mathbf{x}} (A\mathbf{x} - \mathbf{b})^\top (A\mathbf{x} - \mathbf{b}) = 2A^\top(A\mathbf{x} - \mathbf{b})$ | |
| 13 | $\frac{\partial}{\partial A} \text{tr}(A^{-1}B) = -(A^{-1}BA^{-1})^\top$ | $A$ æ­£å‰‡ |
| 14 | $\frac{\partial}{\partial \mathbf{x}} \sigma(\mathbf{x}) = \sigma(\mathbf{x}) \odot (1 - \sigma(\mathbf{x}))$ | $\sigma$ = sigmoid |
| 15 | $\frac{\partial}{\partial \mathbf{x}} \text{softmax}(\mathbf{x})_i = s_i(\delta_{ij} - s_j)$ | $s = \text{softmax}(\mathbf{x})$ |

```python
import numpy as np

# Verify formula 3: d/dx (x^T A x) = 2Ax for symmetric A
def verify_matrix_derivative(A, x, eps=1e-7):
    n = len(x)
    # Analytical gradient
    grad_analytical = 2 * A @ x

    # Numerical gradient
    grad_numerical = np.zeros(n)
    for i in range(n):
        x_plus = x.copy()
        x_plus[i] += eps
        x_minus = x.copy()
        x_minus[i] -= eps
        grad_numerical[i] = (x_plus @ A @ x_plus - x_minus @ A @ x_minus) / (2 * eps)

    return grad_analytical, grad_numerical

A = np.array([[2, 1], [1, 3]], dtype=float)  # symmetric
x = np.array([1.0, 2.0])

grad_a, grad_n = verify_matrix_derivative(A, x)
print(f"Analytical: {grad_a}")
print(f"Numerical:  {np.round(grad_n, 6)}")
print(f"Match: {np.allclose(grad_a, grad_n)}")

# Verify formula 9: d/dX ln det(X) = X^{-T}
X = np.array([[2.0, 0.5], [0.5, 3.0]])
grad_analytical = np.linalg.inv(X).T
print(f"\nd/dX ln det(X) = X^{{-T}}:")
print(f"  Analytical:\n{np.round(grad_analytical, 4)}")

# Numerical verification
eps = 1e-7
grad_numerical = np.zeros_like(X)
for i in range(2):
    for j in range(2):
        X_plus = X.copy()
        X_plus[i, j] += eps
        X_minus = X.copy()
        X_minus[i, j] -= eps
        grad_numerical[i, j] = (np.log(np.linalg.det(X_plus)) -
                                  np.log(np.linalg.det(X_minus))) / (2 * eps)
print(f"  Numerical:\n{np.round(grad_numerical, 4)}")
print(f"  Match: {np.allclose(grad_analytical, grad_numerical)}")
```

### 3.9 é€£é–å¾‹ â€” Backpropagationã®æ•°å­¦çš„åŸºç›¤

#### ã‚¹ã‚«ãƒ©ãƒ¼ã®é€£é–å¾‹

$y = f(g(x))$ ã®ã¨ã:

$$
\frac{dy}{dx} = \frac{dy}{dg} \cdot \frac{dg}{dx}
$$

#### ãƒ™ã‚¯ãƒˆãƒ«ã®é€£é–å¾‹

$\mathbf{y} = \mathbf{f}(\mathbf{g}(\mathbf{x}))$ã€$\mathbf{g}: \mathbb{R}^n \to \mathbb{R}^p$ã€$\mathbf{f}: \mathbb{R}^p \to \mathbb{R}^m$ ã®ã¨ã:

$$
\frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \frac{\partial \mathbf{f}}{\partial \mathbf{g}} \cdot \frac{\partial \mathbf{g}}{\partial \mathbf{x}} = J_{\mathbf{f}} J_{\mathbf{g}} \in \mathbb{R}^{m \times n}
$$

**ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®ç©**ã€‚ã“ã‚ŒãŒé€£é–å¾‹ã®è¡Œåˆ—ç‰ˆã€‚

#### å¤šå±¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¸ã®é©ç”¨

$L$ å±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯:

$$
\mathbf{h}_0 = \mathbf{x}, \quad \mathbf{h}_l = f_l(W_l \mathbf{h}_{l-1} + \mathbf{b}_l), \quad L = \ell(\mathbf{h}_L, \mathbf{y})
$$

æå¤± $L$ ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $W_l$ ã«é–¢ã™ã‚‹å‹¾é…:

$$
\frac{\partial L}{\partial W_l} = \frac{\partial L}{\partial \mathbf{h}_L} \cdot \frac{\partial \mathbf{h}_L}{\partial \mathbf{h}_{L-1}} \cdots \frac{\partial \mathbf{h}_{l+1}}{\partial \mathbf{h}_l} \cdot \frac{\partial \mathbf{h}_l}{\partial W_l}
$$

```mermaid
graph LR
    X["x"] --> H1["h_1 = f(W_1 x + b_1)"]
    H1 --> H2["h_2 = f(W_2 h_1 + b_2)"]
    H2 --> HL["h_L = f(W_L h_{L-1} + b_L)"]
    HL --> LOSS["L = â„“(h_L, y)"]

    LOSS -->|"âˆ‚L/âˆ‚h_L"| HL
    HL -->|"âˆ‚h_L/âˆ‚h_{L-1}"| H2
    H2 -->|"âˆ‚h_2/âˆ‚h_1"| H1
    H1 -->|"âˆ‚h_1/âˆ‚x"| X

    style LOSS fill:#ffcdd2
    style X fill:#e3f2fd
```

**Forward pass**: $\mathbf{x} \to \mathbf{h}_1 \to \cdots \to \mathbf{h}_L \to L$ï¼ˆå·¦â†’å³ï¼‰

**Backward pass**: $\frac{\partial L}{\partial \mathbf{h}_L} \to \frac{\partial L}{\partial \mathbf{h}_{L-1}} \to \cdots \to \frac{\partial L}{\partial W_l}$ï¼ˆå³â†’å·¦ï¼‰

#### Backpropagation ã®å®Œå…¨å°å‡º

1å±¤ã®ç·šå½¢å¤‰æ› + æ´»æ€§åŒ–: $\mathbf{h}_l = \sigma(\mathbf{z}_l)$, $\mathbf{z}_l = W_l \mathbf{h}_{l-1} + \mathbf{b}_l$

**èª¤å·®ä¿¡å·** $\boldsymbol{\delta}_l = \frac{\partial L}{\partial \mathbf{z}_l}$ ã‚’å®šç¾©ã™ã‚‹ã€‚

å‡ºåŠ›å±¤ ($l = L$):

$$
\boldsymbol{\delta}_L = \frac{\partial L}{\partial \mathbf{z}_L} = \frac{\partial L}{\partial \mathbf{h}_L} \odot \sigma'(\mathbf{z}_L)
$$

éš ã‚Œå±¤ ($l < L$ã€é€†ä¼æ’­ã®æœ¬ä½“ï¼‰:

$$
\boldsymbol{\delta}_l = (W_{l+1}^\top \boldsymbol{\delta}_{l+1}) \odot \sigma'(\mathbf{z}_l)
$$

ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹¾é…:

$$
\frac{\partial L}{\partial W_l} = \boldsymbol{\delta}_l \mathbf{h}_{l-1}^\top, \quad \frac{\partial L}{\partial \mathbf{b}_l} = \boldsymbol{\delta}_l
$$

```python
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))

def sigmoid_deriv(x):
    s = sigmoid(x)
    return s * (1 - s)

# Simple 3-layer network: 3 -> 4 -> 2 -> 1
np.random.seed(42)
W1 = np.random.randn(4, 3) * 0.5
b1 = np.zeros(4)
W2 = np.random.randn(2, 4) * 0.5
b2 = np.zeros(2)
W3 = np.random.randn(1, 2) * 0.5
b3 = np.zeros(1)

# Input and target
x = np.array([1.0, 0.5, -0.5])
y = np.array([1.0])

# === Forward pass ===
z1 = W1 @ x + b1
h1 = sigmoid(z1)
z2 = W2 @ h1 + b2
h2 = sigmoid(z2)
z3 = W3 @ h2 + b3
h3 = sigmoid(z3)
loss = 0.5 * np.sum((h3 - y)**2)
print(f"Forward: loss = {loss:.6f}")

# === Backward pass (manual backpropagation) ===
# Output layer
dL_dh3 = h3 - y                       # dL/dh3
delta3 = dL_dh3 * sigmoid_deriv(z3)   # delta_3

# Hidden layer 2
delta2 = (W3.T @ delta3) * sigmoid_deriv(z2)

# Hidden layer 1
delta1 = (W2.T @ delta2) * sigmoid_deriv(z1)

# Parameter gradients
dL_dW3 = np.outer(delta3, h2)
dL_db3 = delta3
dL_dW2 = np.outer(delta2, h1)
dL_db2 = delta2
dL_dW1 = np.outer(delta1, x)
dL_db1 = delta1

print(f"\nGradients:")
print(f"  dL/dW3 shape: {dL_dW3.shape}, norm: {np.linalg.norm(dL_dW3):.6f}")
print(f"  dL/dW2 shape: {dL_dW2.shape}, norm: {np.linalg.norm(dL_dW2):.6f}")
print(f"  dL/dW1 shape: {dL_dW1.shape}, norm: {np.linalg.norm(dL_dW1):.6f}")

# === Numerical verification ===
def compute_loss(W1, b1, W2, b2, W3, b3, x, y):
    h1 = sigmoid(W1 @ x + b1)
    h2 = sigmoid(W2 @ h1 + b2)
    h3 = sigmoid(W3 @ h2 + b3)
    return 0.5 * np.sum((h3 - y)**2)

# Verify dL/dW1[0,0]
eps = 1e-7
W1_plus = W1.copy()
W1_plus[0, 0] += eps
W1_minus = W1.copy()
W1_minus[0, 0] -= eps
numerical = (compute_loss(W1_plus, b1, W2, b2, W3, b3, x, y) -
             compute_loss(W1_minus, b1, W2, b2, W3, b3, x, y)) / (2 * eps)
print(f"\nNumerical check dL/dW1[0,0]:")
print(f"  Analytical: {dL_dW1[0,0]:.8f}")
print(f"  Numerical:  {numerical:.8f}")
print(f"  Match: {np.isclose(dL_dW1[0,0], numerical, rtol=1e-4)}")
```

:::message
**ã“ã‚ŒãŒBackpropagation[^2]ã®å…¨ã¦ã ã€‚** ã€Œé€£é–å¾‹ã§ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã‚’é€†é †ã«æ›ã‘ã¦ã€å„å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‹¾é…ã‚’è¨ˆç®—ã™ã‚‹ã€â€” ã“ã®ä¸€æ–‡ã«å…¨ã¦ãŒå‡ç¸®ã•ã‚Œã¦ã„ã‚‹ã€‚1986å¹´ã«Rumelhart, Hinton, WilliamsãŒç™ºè¡¨ã—ãŸã“ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒã€æ·±å±¤å­¦ç¿’ã®è¨ˆç®—çš„åŸºç›¤ã‚’ç¯‰ã„ãŸã€‚
:::

### 3.10 è‡ªå‹•å¾®åˆ†ã®ç†è«– â€” Forward Mode ã¨ Reverse Mode

è‡ªå‹•å¾®åˆ†ï¼ˆAutomatic Differentiation, ADï¼‰[^7][^8]ã¯ã€æ•°å€¤å¾®åˆ†ã§ã‚‚è¨˜å·å¾®åˆ†ã§ã‚‚ãªã„ã€ç¬¬3ã®å¾®åˆ†æ³•ã ã€‚

#### 3ã¤ã®å¾®åˆ†æ³•ã®æ¯”è¼ƒ

| æ–¹æ³• | ç²¾åº¦ | è¨ˆç®—é‡ | é•·æ‰€ | çŸ­æ‰€ |
|:-----|:-----|:------|:-----|:-----|
| æ•°å€¤å¾®åˆ† | $O(\epsilon)$ èª¤å·® | $O(n)$ å›ã®é–¢æ•°è©•ä¾¡ | å®Ÿè£…ãŒç°¡å˜ | é…ã„ã€ä¸æ­£ç¢º |
| è¨˜å·å¾®åˆ† | å³å¯† | å¼è†¨å¼µï¼ˆexpression swellï¼‰ | æ•°å­¦çš„ã«æ­£ç¢º | å¼ãŒå·¨å¤§ã« |
| è‡ªå‹•å¾®åˆ† | æ©Ÿæ¢°ç²¾åº¦ | $O(1)$ å€ï¼ˆreverse modeï¼‰ | é€Ÿã„ã€æ­£ç¢º | å®Ÿè£…ãŒè¤‡é›‘ |

#### Wengert Listï¼ˆè¨ˆç®—ãƒˆãƒ¬ãƒ¼ã‚¹ï¼‰

è‡ªå‹•å¾®åˆ†ã®æ ¸å¿ƒã¯ã€è¨ˆç®—ã‚’ãƒ—ãƒªãƒŸãƒ†ã‚£ãƒ–æ“ä½œã®åˆ—ï¼ˆWengert listï¼‰ã¨ã—ã¦è¨˜éŒ²ã™ã‚‹ã“ã¨ã€‚

ä¾‹: $f(x_1, x_2) = x_1 x_2 + \sin(x_1)$

| Step | æ¼”ç®— | å€¤ ($x_1=2, x_2=3$) |
|:-----|:-----|:---------------------|
| $v_1 = x_1$ | å…¥åŠ› | $2$ |
| $v_2 = x_2$ | å…¥åŠ› | $3$ |
| $v_3 = v_1 \cdot v_2$ | ä¹—ç®— | $6$ |
| $v_4 = \sin(v_1)$ | sin | $0.9093$ |
| $v_5 = v_3 + v_4$ | åŠ ç®— | $6.9093$ |

#### Forward Mode AD

å…¥åŠ›ã«å¯¾ã™ã‚‹å¾®åˆ† $\dot{v}_i = \frac{\partial v_i}{\partial x_j}$ ã‚’**å‰å‘ã**ã«ä¼æ’­:

| Step | å€¤ | $\dot{v}_i = \partial v_i / \partial x_1$ |
|:-----|:---|:----------------------------------------|
| $v_1 = x_1$ | $2$ | $\dot{v}_1 = 1$ |
| $v_2 = x_2$ | $3$ | $\dot{v}_2 = 0$ |
| $v_3 = v_1 v_2$ | $6$ | $\dot{v}_3 = \dot{v}_1 v_2 + v_1 \dot{v}_2 = 3$ |
| $v_4 = \sin(v_1)$ | $0.909$ | $\dot{v}_4 = \cos(v_1) \dot{v}_1 = -0.416$ |
| $v_5 = v_3 + v_4$ | $6.909$ | $\dot{v}_5 = \dot{v}_3 + \dot{v}_4 = 2.584$ |

$\frac{\partial f}{\partial x_1} = 2.584$ã€‚æ­£ã—ã„ï¼ˆ$\frac{\partial}{\partial x_1}(x_1 x_2 + \sin x_1) = x_2 + \cos x_1 = 3 + \cos 2 = 2.584$ï¼‰ã€‚

**è¨ˆç®—é‡**: 1å›ã® Forward Mode ã§ã€1ã¤ã®å…¥åŠ›å¤‰æ•°ã«å¯¾ã™ã‚‹å¾®åˆ†ãŒå¾—ã‚‰ã‚Œã‚‹ã€‚$n$ å€‹ã®å…¥åŠ›å¤‰æ•°ã®å‹¾é…ã‚’æ±‚ã‚ã‚‹ã«ã¯ $n$ å›ã® Forward pass ãŒå¿…è¦ã€‚

#### Reverse Mode ADï¼ˆ= Backpropagationï¼‰

å‡ºåŠ›ã«å¯¾ã™ã‚‹å¾®åˆ† $\bar{v}_i = \frac{\partial f}{\partial v_i}$ ã‚’**é€†å‘ã**ã«ä¼æ’­:

| Step (é€†é †) | $\bar{v}_i = \partial f / \partial v_i$ |
|:-----------|:---------------------------------------|
| $\bar{v}_5 = 1$ | å‡ºåŠ›ã«å¯¾ã™ã‚‹å¾®åˆ†ã¯1 |
| $\bar{v}_3 = \bar{v}_5 \cdot 1 = 1$ | $v_5 = v_3 + v_4$ ã® $v_3$ ã«å¯¾ã™ã‚‹åå¾®åˆ† |
| $\bar{v}_4 = \bar{v}_5 \cdot 1 = 1$ | $v_5 = v_3 + v_4$ ã® $v_4$ ã«å¯¾ã™ã‚‹åå¾®åˆ† |
| $\bar{v}_1 = \bar{v}_3 \cdot v_2 + \bar{v}_4 \cdot \cos(v_1) = 2.584$ | ç©ã®è¦å‰‡ + sinå¾®åˆ† |
| $\bar{v}_2 = \bar{v}_3 \cdot v_1 = 2$ | |

$\frac{\partial f}{\partial x_1} = 2.584$, $\frac{\partial f}{\partial x_2} = 2.0$ã€‚**1å›ã® Reverse pass ã§å…¨å…¥åŠ›å¤‰æ•°ã®å‹¾é…ãŒå¾—ã‚‰ã‚Œã‚‹**ã€‚

#### Forward vs Reverse: è¨ˆç®—é‡ã®æ¯”è¼ƒ

| | Forward Mode | Reverse Mode |
|:--|:------------|:------------|
| 1å›ã®passã§å¾—ã‚‰ã‚Œã‚‹ | 1ã¤ã®å…¥åŠ›ã«å¯¾ã™ã‚‹å‹¾é… | 1ã¤ã®å‡ºåŠ›ã«å¯¾ã™ã‚‹å…¨å…¥åŠ›ã®å‹¾é… |
| $n$ å…¥åŠ›, $m$ å‡ºåŠ›ã®å‹¾é… | $n$ å›ã®pass | $m$ å›ã®pass |
| æœ€é©ãªå ´åˆ | $n \ll m$ï¼ˆãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ãŒã€Œæ¨ªé•·ã€ï¼‰ | $m \ll n$ï¼ˆãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ãŒã€Œç¸¦é•·ã€ï¼‰ |
| æ©Ÿæ¢°å­¦ç¿’ã§ã®å…¸å‹ | â€” | $m = 1$ï¼ˆæå¤±ã¯ã‚¹ã‚«ãƒ©ãƒ¼ï¼‰â†’ **1å›ã®passã§å…¨å‹¾é…** |

**ã ã‹ã‚‰Backpropã¯Reverse Mode ADãªã®ã ã€‚** æå¤±é–¢æ•°ã¯ã‚¹ã‚«ãƒ©ãƒ¼å€¤ï¼ˆ$m = 1$ï¼‰ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯æ•°åå„„ï¼ˆ$n \sim 10^9$ï¼‰ã€‚Reverse modeãªã‚‰1å›ã®backward passã§å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹¾é…ãŒå¾—ã‚‰ã‚Œã‚‹ã€‚Forward modeã§ã¯ $10^9$ å›ã®forward passãŒå¿…è¦ã€‚

```python
import numpy as np

# Implementing Forward Mode AD with dual numbers
class Dual:
    """Dual number: a + bÎµ where Îµ^2 = 0"""
    def __init__(self, val, deriv=0.0):
        self.val = val      # primal value
        self.deriv = deriv   # tangent (derivative)

    def __add__(self, other):
        other = other if isinstance(other, Dual) else Dual(other)
        return Dual(self.val + other.val, self.deriv + other.deriv)

    def __radd__(self, other):
        return self.__add__(other)

    def __mul__(self, other):
        other = other if isinstance(other, Dual) else Dual(other)
        return Dual(self.val * other.val,
                    self.val * other.deriv + self.deriv * other.val)

    def __rmul__(self, other):
        return self.__mul__(other)

    def __repr__(self):
        return f"Dual({self.val:.4f}, {self.deriv:.4f})"

def sin_dual(x):
    return Dual(np.sin(x.val), np.cos(x.val) * x.deriv)

# f(x1, x2) = x1*x2 + sin(x1)
def f_dual(x1, x2):
    return x1 * x2 + sin_dual(x1)

# df/dx1 at (2, 3): set x1.deriv = 1
x1 = Dual(2.0, 1.0)  # seed: dx1/dx1 = 1
x2 = Dual(3.0, 0.0)  # seed: dx2/dx1 = 0
result = f_dual(x1, x2)
print(f"f(2, 3) = {result.val:.4f}")
print(f"df/dx1  = {result.deriv:.4f}")
print(f"Expected: {3 + np.cos(2):.4f}")

# df/dx2 at (2, 3): set x2.deriv = 1
x1 = Dual(2.0, 0.0)
x2 = Dual(3.0, 1.0)
result = f_dual(x1, x2)
print(f"df/dx2  = {result.deriv:.4f}")
print(f"Expected: {2.0:.4f}")
```

### 3.11 ãƒ†ã‚¤ãƒ©ãƒ¼å±•é–‹ã¨äºŒæ¬¡è¿‘ä¼¼

å¤šå¤‰æ•°ã®ãƒ†ã‚¤ãƒ©ãƒ¼å±•é–‹ã¯ã€æœ€é©åŒ–ç†è«–ã®åŸºç›¤:

$$
f(\mathbf{x} + \boldsymbol{\delta}) \approx f(\mathbf{x}) + \nabla f(\mathbf{x})^\top \boldsymbol{\delta} + \frac{1}{2} \boldsymbol{\delta}^\top H(\mathbf{x}) \boldsymbol{\delta}
$$

**Newtonæ³•**: äºŒæ¬¡è¿‘ä¼¼ã‚’æœ€å°åŒ–ã™ã‚‹ $\boldsymbol{\delta}$ ã‚’æ±‚ã‚ã‚‹:

$$
\nabla f + H \boldsymbol{\delta} = 0 \implies \boldsymbol{\delta}^* = -H^{-1} \nabla f
$$

```python
import numpy as np

# Rosenbrock function (classic optimization test)
def rosenbrock(xy):
    x, y = xy
    return (1 - x)**2 + 100 * (y - x**2)**2

def rosenbrock_grad(xy):
    x, y = xy
    dx = -2*(1-x) + 100*2*(y-x**2)*(-2*x)
    dy = 100*2*(y-x**2)
    return np.array([dx, dy])

def rosenbrock_hessian(xy):
    x, y = xy
    dxx = 2 - 400*(y - x**2) + 800*x**2
    dxy = -400*x
    dyy = 200.0
    return np.array([[dxx, dxy], [dxy, dyy]])

# Newton's method
x = np.array([-1.0, 1.0])
print(f"Newton's method on Rosenbrock:")
for i in range(10):
    g = rosenbrock_grad(x)
    H = rosenbrock_hessian(x)
    delta = -np.linalg.solve(H, g)
    x = x + delta
    f_val = rosenbrock(x)
    print(f"  Step {i+1}: x={np.round(x, 6)}, f={f_val:.8f}")
    if f_val < 1e-14:
        print(f"  Converged in {i+1} steps!")
        break
```

### 3.12 Softmaxã®å¾®åˆ† â€” Attentionå­¦ç¿’ã®éµ

Softmaxã®å¾®åˆ†ã¯Transformerã®å­¦ç¿’ã§æœ€ã‚‚é »ç¹ã«ç¾ã‚Œã‚‹è¨ˆç®—ã®ä¸€ã¤ã€‚

#### Softmaxã®å®šç¾©ã¨æ€§è³ª

$$
s_i = \text{softmax}(\mathbf{z})_i = \frac{e^{z_i}}{\sum_j e^{z_j}}
$$

æ€§è³ª:
- $s_i > 0$ ã‹ã¤ $\sum_i s_i = 1$ï¼ˆç¢ºç‡åˆ†å¸ƒï¼‰
- $\frac{\partial s_i}{\partial z_j} = s_i(\delta_{ij} - s_j)$

#### ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®å°å‡º

$i = j$ ã®ã¨ã:

$$
\frac{\partial s_i}{\partial z_i} = s_i(1 - s_i)
$$

$i \neq j$ ã®ã¨ã:

$$
\frac{\partial s_i}{\partial z_j} = -s_i s_j
$$

ã¾ã¨ã‚ã‚‹ã¨:

$$
\frac{\partial \mathbf{s}}{\partial \mathbf{z}} = \text{diag}(\mathbf{s}) - \mathbf{s}\mathbf{s}^\top
$$

```python
import numpy as np

def softmax(z):
    e = np.exp(z - np.max(z))
    return e / np.sum(e)

def softmax_jacobian(z):
    """Analytical Jacobian of softmax"""
    s = softmax(z)
    return np.diag(s) - np.outer(s, s)

# Verify with numerical differentiation
z = np.array([2.0, 1.0, 0.1])
J_analytical = softmax_jacobian(z)

eps = 1e-7
n = len(z)
J_numerical = np.zeros((n, n))
for j in range(n):
    z_plus = z.copy(); z_plus[j] += eps
    z_minus = z.copy(); z_minus[j] -= eps
    J_numerical[:, j] = (softmax(z_plus) - softmax(z_minus)) / (2 * eps)

print("Softmax Jacobian (analytical):")
print(np.round(J_analytical, 6))
print(f"\nMatch numerical: {np.allclose(J_analytical, J_numerical)}")

# Key property: each row sums to 0
print(f"Row sums: {np.round(J_analytical.sum(axis=1), 10)}")
```

#### Cross-Entropyæå¤±ã®Softmaxå¾®åˆ†

Cross-Entropyæå¤± $L = -\sum_i y_i \log s_i$ ã®Softmaxå…¥åŠ› $\mathbf{z}$ ã«é–¢ã™ã‚‹å‹¾é…:

$$
\frac{\partial L}{\partial \mathbf{z}} = \mathbf{s} - \mathbf{y}
$$

ã“ã®çµæœã¯é©šãã»ã©ã‚·ãƒ³ãƒ—ãƒ«ã€‚å°å‡º:

$$
\frac{\partial L}{\partial z_j} = -\sum_i y_i \frac{1}{s_i} \frac{\partial s_i}{\partial z_j} = -\sum_i y_i \frac{s_i(\delta_{ij} - s_j)}{s_i} = -y_j + s_j \sum_i y_i = s_j - y_j
$$

ï¼ˆ$\sum_i y_i = 1$ ã‚’ä½¿ã£ãŸï¼‰

```python
import numpy as np

z = np.array([2.0, 1.0, 0.1])
s = softmax(z)
y = np.array([1.0, 0.0, 0.0])  # one-hot target

# Analytical gradient: s - y
grad_analytical = s - y

# Numerical gradient
def cross_entropy_loss(z, y):
    s = softmax(z)
    return -np.sum(y * np.log(s + 1e-12))

eps = 1e-7
grad_numerical = np.zeros(len(z))
for j in range(len(z)):
    z_plus = z.copy(); z_plus[j] += eps
    z_minus = z.copy(); z_minus[j] -= eps
    grad_numerical[j] = (cross_entropy_loss(z_plus, y) - cross_entropy_loss(z_minus, y)) / (2 * eps)

print(f"Analytical: {np.round(grad_analytical, 6)}")
print(f"Numerical:  {np.round(grad_numerical, 6)}")
print(f"Match: {np.allclose(grad_analytical, grad_numerical)}")
```

:::message
**LLMã¸ã®æ¥ç¶š**: GPTç³»ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã§ã¯ã€å„ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®ã§ Softmax + Cross-Entropy ã®å‹¾é… $\mathbf{s} - \mathbf{y}$ ã‚’è¨ˆç®—ã™ã‚‹ã€‚èªå½™ã‚µã‚¤ã‚ºãŒ50,000ä»¥ä¸Šã®ã¨ãã€ã“ã®è¨ˆç®—ãŒå­¦ç¿’ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®ä¸€ã¤ã«ãªã‚‹ã€‚
:::

### 3.13 å¤‰åˆ†æ³•å…¥é–€ â€” å¤‰åˆ†æ¨è«–ã¸ã®äºˆå‘Š

å¤‰åˆ†æ³•ã¯ã€Œé–¢æ•°ã®é–¢æ•°ã€ï¼ˆæ±é–¢æ•°ï¼‰ã‚’æœ€é©åŒ–ã™ã‚‹ã€‚VAEï¼ˆç¬¬15å›ï¼‰ã§ä½¿ã†å¤‰åˆ†æ¨è«–ã®æ•°å­¦çš„åŸºç›¤ã€‚

#### æ±é–¢æ•°ã¨å¤‰åˆ†

**æ±é–¢æ•°**: é–¢æ•°ã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚Šã€ã‚¹ã‚«ãƒ©ãƒ¼ã‚’è¿”ã™å†™åƒã€‚

$$
F[f] = \int_a^b L(x, f(x), f'(x)) \, dx
$$

ä¾‹: æ›²ç·šã®é•·ã• $F[f] = \int_a^b \sqrt{1 + f'(x)^2} \, dx$

#### Euler-Lagrangeæ–¹ç¨‹å¼

$F[f]$ ã‚’æœ€å°åŒ–ã™ã‚‹ $f$ ã¯ä»¥ä¸‹ã‚’æº€ãŸã™:

$$
\frac{\partial L}{\partial f} - \frac{d}{dx} \frac{\partial L}{\partial f'} = 0
$$

#### å¤‰åˆ†æ¨è«–ã¨ã®æ¥ç¶šï¼ˆäºˆå‘Šï¼‰

VAEã§ã¯ã€çœŸã®äº‹å¾Œåˆ†å¸ƒ $p(\mathbf{z} \mid \mathbf{x})$ ã‚’è¿‘ä¼¼ã™ã‚‹åˆ†å¸ƒ $q(\mathbf{z} \mid \mathbf{x})$ ã‚’è¦‹ã¤ã‘ãŸã„ã€‚ã“ã‚Œã¯ã€ŒKLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¨ã„ã†æ±é–¢æ•°ã‚’ã€åˆ†å¸ƒã®ç©ºé–“ä¸Šã§æœ€å°åŒ–ã™ã‚‹ã€å•é¡Œ:

$$
q^* = \arg\min_q \text{KL}(q(\mathbf{z} \mid \mathbf{x}) \| p(\mathbf{z} \mid \mathbf{x}))
$$

ã“ã®æœ€é©åŒ–å•é¡Œã‚’è§£ãã®ãŒå¤‰åˆ†æ¨è«–ã€‚ãã®ç†è«–çš„åŸºç›¤ãŒå¤‰åˆ†æ³•ã ã€‚è©³ç´°ã¯ç¬¬15å›ï¼ˆVAEï¼‰ã§æ‰±ã†ã€‚

### 3.14 Boss Battle: Transformer 1å±¤ã®å®Œå…¨å¾®åˆ†

Transformer[^1]ã®1å±¤ã«ãŠã‘ã‚‹ Forward + Backward ã‚’è¡Œåˆ—å¾®åˆ†ã§å®Œå…¨ã«è¨˜è¿°ã™ã‚‹ã€‚

#### Forward Pass

å…¥åŠ› $H \in \mathbb{R}^{T \times d}$ï¼ˆ$T$ ãƒˆãƒ¼ã‚¯ãƒ³ã€$d$ æ¬¡å…ƒï¼‰ã«å¯¾ã—ã¦:

$$
Q = HW_Q, \quad K = HW_K, \quad V = HW_V
$$
$$
S = \frac{QK^\top}{\sqrt{d_k}}, \quad A = \text{softmax}(S), \quad O = AV
$$
$$
\text{output} = OW_O + H \quad \text{(residual connection)}
$$

#### Backward Passï¼ˆ$\frac{\partial L}{\partial W_Q}$ ã®å°å‡ºï¼‰

$L$ ã‚’ã‚¹ã‚«ãƒ©ãƒ¼æå¤±ã¨ã—ã€$\frac{\partial L}{\partial O}$ ãŒæ—¢çŸ¥ã¨ã™ã‚‹ã€‚

$$
\frac{\partial L}{\partial W_Q} = H^\top \frac{\partial L}{\partial Q}
$$

ã“ã“ã§ $\frac{\partial L}{\partial Q}$ ã¯é€£é–å¾‹ã§:

$$
\frac{\partial L}{\partial Q} = \frac{\partial L}{\partial S} \cdot \frac{\partial S}{\partial Q}
$$

$S = QK^\top / \sqrt{d_k}$ ã‚ˆã‚Š $\frac{\partial S}{\partial Q} = K / \sqrt{d_k}$ã€ã¤ã¾ã‚Š:

$$
\frac{\partial L}{\partial Q} = \frac{1}{\sqrt{d_k}} \frac{\partial L}{\partial S} K
$$

Softmax ã®å¾®åˆ†ã¯:

$$
\frac{\partial L}{\partial S_{ij}} = \sum_k \frac{\partial L}{\partial A_{ik}} A_{ik} (\delta_{jk} - A_{ij})
$$

```python
import numpy as np

def softmax(x, axis=-1):
    e = np.exp(x - np.max(x, axis=axis, keepdims=True))
    return e / np.sum(e, axis=axis, keepdims=True)

# Transformer single layer forward + backward
np.random.seed(42)
T, d, dk = 4, 8, 8  # 4 tokens, 8 dims
H = np.random.randn(T, d)
W_Q = np.random.randn(d, dk) * 0.1
W_K = np.random.randn(d, dk) * 0.1
W_V = np.random.randn(d, dk) * 0.1
W_O = np.random.randn(dk, d) * 0.1

# Forward
Q = H @ W_Q
K = H @ W_K
V = H @ W_V
S = Q @ K.T / np.sqrt(dk)
A = softmax(S)
O = A @ V
output = O @ W_O + H  # residual

# Backward (assume dL/doutput = random for demo)
dL_doutput = np.random.randn(T, d)

# dL/dO
dL_dO = dL_doutput @ W_O.T

# dL/dA (from O = AV)
dL_dA = dL_dO @ V.T

# dL/dS (softmax backward)
dL_dS = np.zeros_like(S)
for i in range(T):
    a = A[i, :]  # (T,)
    dL_da = dL_dA[i, :]  # (T,)
    # Jacobian of softmax: diag(a) - a a^T
    J_softmax = np.diag(a) - np.outer(a, a)
    dL_dS[i, :] = J_softmax @ dL_da

# dL/dQ, dL/dK
dL_dQ = dL_dS @ K / np.sqrt(dk)
dL_dK = dL_dS.T @ Q / np.sqrt(dk)

# dL/dW_Q, dL/dW_K, dL/dW_V
dL_dW_Q = H.T @ dL_dQ
dL_dW_K = H.T @ dL_dK
dL_dW_V = H.T @ (A.T @ dL_dO)
dL_dW_O = O.T @ dL_doutput

print("Gradient norms:")
print(f"  dL/dW_Q: {np.linalg.norm(dL_dW_Q):.6f}")
print(f"  dL/dW_K: {np.linalg.norm(dL_dW_K):.6f}")
print(f"  dL/dW_V: {np.linalg.norm(dL_dW_V):.6f}")
print(f"  dL/dW_O: {np.linalg.norm(dL_dW_O):.6f}")

# Numerical verification for dL/dW_Q[0,0]
eps = 1e-5
def forward_loss(W_Q_):
    Q_ = H @ W_Q_
    S_ = Q_ @ K.T / np.sqrt(dk)
    A_ = softmax(S_)
    O_ = A_ @ V
    out_ = O_ @ W_O + H
    return np.sum(out_ * dL_doutput)  # proxy loss

W_Q_plus = W_Q.copy(); W_Q_plus[0, 0] += eps
W_Q_minus = W_Q.copy(); W_Q_minus[0, 0] -= eps
numerical = (forward_loss(W_Q_plus) - forward_loss(W_Q_minus)) / (2 * eps)
print(f"\nNumerical check dL/dW_Q[0,0]:")
print(f"  Analytical: {dL_dW_Q[0,0]:.8f}")
print(f"  Numerical:  {numerical:.8f}")
print(f"  Match: {np.isclose(dL_dW_Q[0,0], numerical, rtol=1e-3)}")
```

:::message
**é€²æ—: 70% å®Œäº†** SVDã®ç†è«–ï¼ˆå­˜åœ¨å®šç†ãƒ»Eckart-Youngãƒ»æ“¬ä¼¼é€†è¡Œåˆ—ãƒ»PCAï¼‰ã€ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—ãƒ»Einsteinè¨˜æ³•ã€è¡Œåˆ—å¾®åˆ†ã€é€£é–å¾‹ã€Backpropagationã€è‡ªå‹•å¾®åˆ†ã€Transformer 1å±¤ã®å®Œå…¨å¾®åˆ†ã‚’å°å‡ºã—ãŸã€‚
:::

---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” SVDã¨è‡ªå‹•å¾®åˆ†ã‚’ã‚³ãƒ¼ãƒ‰ã§æ“ã‚‹

### 4.1 SVDç”»åƒåœ§ç¸®ã®å®Œå…¨å®Ÿè£…

```python
import numpy as np

def svd_compress(A, k):
    """Compress matrix A to rank-k using SVD"""
    U, s, Vt = np.linalg.svd(A, full_matrices=False)
    return U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :], s

def compression_stats(m, n, k):
    """Calculate compression statistics"""
    original = m * n
    compressed = k * (m + n + 1)
    return compressed / original

# Demo with synthetic image
np.random.seed(42)
m, n = 256, 192
# Create structured image: smooth gradients + edges
x = np.linspace(0, 8*np.pi, m)
y = np.linspace(0, 6*np.pi, n)
X, Y = np.meshgrid(y, x)
image = (np.sin(X) * np.cos(Y) + 0.3 * np.sin(3*X + 2*Y) +
         0.5 * np.sign(np.sin(X/2)))
image += 0.05 * np.random.randn(m, n)  # small noise

_, s = np.linalg.svd(image, full_matrices=False)

print("SVD Image Compression Results:")
print(f"Image size: {m}x{n} = {m*n:,} values")
print(f"{'rank':>6} {'ratio':>10} {'error':>10} {'PSNR(dB)':>10}")
print("-" * 42)

for k in [1, 5, 10, 20, 50, 100]:
    A_k, _ = svd_compress(image, k)
    ratio = compression_stats(m, n, k)
    mse = np.mean((image - A_k)**2)
    max_val = np.max(np.abs(image))
    psnr = 10 * np.log10(max_val**2 / mse) if mse > 0 else float('inf')
    rel_error = np.linalg.norm(image - A_k, 'fro') / np.linalg.norm(image, 'fro')
    print(f"{k:6d} {ratio:9.1%} {rel_error:10.6f} {psnr:10.2f}")
```

### 4.2 Randomized SVD â€” å¤§è¦æ¨¡è¡Œåˆ—ã®åŠ¹ç‡çš„ãªSVD

é€šå¸¸ã®SVDã¯ $O(\min(mn^2, m^2n))$ã€‚æ•°ä¸‡Ã—æ•°ä¸‡ã®è¡Œåˆ—ã«ã¯é…ã™ãã‚‹ã€‚**Randomized SVD** ã¯ãƒ©ãƒ³ãƒ€ãƒ å°„å½±ã§æ¬¡å…ƒã‚’è½ã¨ã—ã¦ã‹ã‚‰SVDã‚’è¨ˆç®—ã™ã‚‹ã€‚

```python
import numpy as np

def randomized_svd(A, k, n_oversamples=10, n_iter=2):
    """
    Randomized SVD (Halko, Martinsson, Tropp 2011)

    Parameters:
        A: (m, n) matrix
        k: target rank
        n_oversamples: oversampling parameter (default 10)
        n_iter: power iterations for accuracy (default 2)

    Returns:
        U, s, Vt: truncated SVD components
    """
    m, n = A.shape
    p = k + n_oversamples

    # Step 1: Random projection
    Omega = np.random.randn(n, p)
    Y = A @ Omega  # (m, p)

    # Step 2: Power iteration (improves accuracy for slow singular value decay)
    for _ in range(n_iter):
        Y = A @ (A.T @ Y)

    # Step 3: QR factorization of Y
    Q, _ = np.linalg.qr(Y)  # (m, p) orthonormal

    # Step 4: Form small matrix and compute its SVD
    B = Q.T @ A  # (p, n) â€” much smaller!
    U_hat, s, Vt = np.linalg.svd(B, full_matrices=False)

    # Step 5: Recover left singular vectors
    U = Q @ U_hat

    return U[:, :k], s[:k], Vt[:k, :]

# Benchmark
np.random.seed(42)
m, n, true_rank = 5000, 3000, 20
U_true = np.linalg.qr(np.random.randn(m, true_rank))[0]
V_true = np.linalg.qr(np.random.randn(n, true_rank))[0]
s_true = np.logspace(1, -1, true_rank)
A = U_true @ np.diag(s_true) @ V_true.T + 0.01 * np.random.randn(m, n)

import time

# Full SVD
t0 = time.time()
U_f, s_f, Vt_f = np.linalg.svd(A, full_matrices=False)
t_full = time.time() - t0

# Randomized SVD
k = 20
t0 = time.time()
U_r, s_r, Vt_r = randomized_svd(A, k)
t_rand = time.time() - t0

A_full = U_f[:, :k] @ np.diag(s_f[:k]) @ Vt_f[:k, :]
A_rand = U_r @ np.diag(s_r) @ Vt_r

print(f"Matrix size: {m}x{n}")
print(f"Full SVD:       {t_full:.3f}s, error = {np.linalg.norm(A - A_full, 'fro'):.6f}")
print(f"Randomized SVD: {t_rand:.3f}s, error = {np.linalg.norm(A - A_rand, 'fro'):.6f}")
print(f"Speedup: {t_full/t_rand:.1f}x")
```

### 4.3 Reverse Mode è‡ªå‹•å¾®åˆ†ã®å®Œå…¨å®Ÿè£…

Zone 1.5 ã§ç°¡æ˜“ç‰ˆã‚’ç¤ºã—ãŸã€‚ã“ã“ã§ã¯ã‚ˆã‚Šæœ¬æ ¼çš„ãªå®Ÿè£…ã‚’ç¤ºã™ã€‚

```python
import numpy as np

class Value:
    """Scalar autograd engine (Reverse Mode AD)"""
    def __init__(self, data, _children=(), _op='', label=''):
        self.data = float(data)
        self.grad = 0.0
        self._backward = lambda: None
        self._prev = set(_children)
        self._op = _op
        self.label = label

    def __repr__(self):
        return f"Value({self.data:.4f}, grad={self.grad:.4f})"

    def __add__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        out = Value(self.data + other.data, (self, other), '+')
        def _backward():
            self.grad += out.grad
            other.grad += out.grad
        out._backward = _backward
        return out

    def __mul__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        out = Value(self.data * other.data, (self, other), '*')
        def _backward():
            self.grad += other.data * out.grad
            other.grad += self.data * out.grad
        out._backward = _backward
        return out

    def __pow__(self, other):
        assert isinstance(other, (int, float))
        out = Value(self.data ** other, (self,), f'**{other}')
        def _backward():
            self.grad += other * (self.data ** (other - 1)) * out.grad
        out._backward = _backward
        return out

    def __neg__(self):
        return self * -1

    def __sub__(self, other):
        return self + (-other)

    def __truediv__(self, other):
        return self * other**-1

    def __radd__(self, other):
        return self + other

    def __rmul__(self, other):
        return self * other

    def exp(self):
        out = Value(np.exp(self.data), (self,), 'exp')
        def _backward():
            self.grad += out.data * out.grad
        out._backward = _backward
        return out

    def log(self):
        out = Value(np.log(self.data), (self,), 'log')
        def _backward():
            self.grad += (1.0 / self.data) * out.grad
        out._backward = _backward
        return out

    def tanh(self):
        t = np.tanh(self.data)
        out = Value(t, (self,), 'tanh')
        def _backward():
            self.grad += (1 - t**2) * out.grad
        out._backward = _backward
        return out

    def backward(self):
        topo = []
        visited = set()
        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for child in v._prev:
                    build_topo(child)
                topo.append(v)
        build_topo(self)
        self.grad = 1.0
        for node in reversed(topo):
            node._backward()

# Demo: simple neural network loss
x1 = Value(2.0, label='x1')
x2 = Value(0.0, label='x2')
w1 = Value(-3.0, label='w1')
w2 = Value(1.0, label='w2')
b = Value(6.8813735870195432, label='b')

# Forward: neuron
n = x1*w1 + x2*w2 + b
o = n.tanh()

# Backward
o.backward()

print("Forward:  o =", o)
print("Gradients:")
print(f"  do/dx1 = {x1.grad:.4f}")
print(f"  do/dx2 = {x2.grad:.4f}")
print(f"  do/dw1 = {w1.grad:.4f}")
print(f"  do/dw2 = {w2.grad:.4f}")
print(f"  do/db  = {b.grad:.4f}")
```

### 4.4 æ¡ä»¶æ•°ã¨æ•°å€¤å®‰å®šæ€§

#### IEEE 754 æµ®å‹•å°æ•°ç‚¹

| å‹ | ãƒ“ãƒƒãƒˆæ•° | ä»®æ•°éƒ¨ | æŒ‡æ•°éƒ¨ | æœ‰åŠ¹æ¡ | ç¯„å›² |
|:---|:--------|:------|:------|:------|:-----|
| float16 (half) | 16 | 10 | 5 | ~3.3æ¡ | $\pm 6.5 \times 10^4$ |
| bfloat16 | 16 | 7 | 8 | ~2.4æ¡ | $\pm 3.4 \times 10^{38}$ |
| float32 (single) | 32 | 23 | 8 | ~7.2æ¡ | $\pm 3.4 \times 10^{38}$ |
| float64 (double) | 64 | 52 | 11 | ~15.9æ¡ | $\pm 1.8 \times 10^{308}$ |

**æ©Ÿæ¢°å­¦ç¿’ã§ã®ä½¿ã„åˆ†ã‘**: æ¨è«–=float16/bfloat16ã€å­¦ç¿’=float32(ãƒã‚¹ã‚¿ãƒ¼ã‚¦ã‚§ã‚¤ãƒˆ)+bfloat16(forward/backward)ã€ç²¾å¯†è¨ˆç®—=float64ã€‚

#### Log-Sum-Exp trick

Softmaxã®è¨ˆç®—ã§å¿…é ˆã®æ•°å€¤å®‰å®šåŒ–æŠ€æ³•:

$$
\log \sum_i e^{x_i} = c + \log \sum_i e^{x_i - c}, \quad c = \max_i x_i
$$

```python
import numpy as np

# Naive vs numerically stable log-sum-exp
x = np.array([1000.0, 1001.0, 1002.0])

# Naive: overflow!
try:
    naive = np.log(np.sum(np.exp(x)))
    print(f"Naive: {naive}")
except:
    print("Naive: OVERFLOW")

# Stable: subtract max
c = np.max(x)
stable = c + np.log(np.sum(np.exp(x - c)))
print(f"Stable: {stable:.4f}")
print(f"Expected: {1002 + np.log(np.exp(-2) + np.exp(-1) + 1):.4f}")
```

### 4.5 ã‚¹ãƒ‘ãƒ¼ã‚¹è¡Œåˆ—ã¨GPUä¸Šã®è¡Œåˆ—æ¼”ç®—

#### ã‚¹ãƒ‘ãƒ¼ã‚¹è¡Œåˆ—

å¤§è¦æ¨¡ãªè¡Œåˆ—ã®å¤šãã¯ã‚¹ãƒ‘ãƒ¼ã‚¹ï¼ˆã»ã¨ã‚“ã©ã®è¦ç´ ãŒ0ï¼‰ã€‚SciPyã®ç–è¡Œåˆ—è¡¨ç¾ã‚’ä½¿ã†ã¨ãƒ¡ãƒ¢ãƒªã¨è¨ˆç®—é‡ã‚’å¤§å¹…ã«å‰Šæ¸›ã§ãã‚‹ã€‚

```python
import numpy as np
from scipy import sparse
from scipy.sparse.linalg import svds

# Dense vs Sparse comparison
n = 10000
density = 0.001  # 0.1% non-zero

# Create sparse matrix
A_sparse = sparse.random(n, n, density=density, format='csr')
print(f"Matrix size: {n}x{n} = {n**2:,} elements")
print(f"Non-zero: {A_sparse.nnz:,} ({density:.1%})")
print(f"Dense memory: {n**2 * 8 / 1e6:.1f} MB")
print(f"Sparse memory: {(A_sparse.data.nbytes + A_sparse.indices.nbytes + A_sparse.indptr.nbytes) / 1e6:.1f} MB")

# Sparse SVD (top-k singular values only)
import time
k = 10
t0 = time.time()
U, s, Vt = svds(A_sparse, k=k)
t_sparse = time.time() - t0
print(f"\nSparse SVD (top-{k}): {t_sparse:.3f}s")
print(f"Top singular values: {np.round(s[::-1][:5], 4)}")
```

| å½¢å¼ | èª¬æ˜ | é•·æ‰€ | ç”¨é€” |
|:-----|:-----|:-----|:-----|
| CSR (Compressed Sparse Row) | è¡Œã”ã¨ã«éã‚¼ãƒ­è¦ç´ ã‚’æ ¼ç´ | è¡Œã‚¹ãƒ©ã‚¤ã‚¹ãŒé«˜é€Ÿ | è¡Œåˆ—Ã—ãƒ™ã‚¯ãƒˆãƒ« |
| CSC (Compressed Sparse Column) | åˆ—ã”ã¨ã«éã‚¼ãƒ­è¦ç´ ã‚’æ ¼ç´ | åˆ—ã‚¹ãƒ©ã‚¤ã‚¹ãŒé«˜é€Ÿ | è»¢ç½®æ“ä½œ |
| COO (Coordinate) | (è¡Œ, åˆ—, å€¤) ã®ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆ | æ§‹ç¯‰ãŒç°¡å˜ | åˆæœŸæ§‹ç¯‰ |

#### GPUä¸Šã®è¡Œåˆ—æ¼”ç®—

| ãƒ©ã‚¤ãƒ–ãƒ©ãƒª | ç”¨é€” | ç‰¹å¾´ |
|:----------|:-----|:-----|
| cuBLAS | å¯†è¡Œåˆ—æ¼”ç®— | NVIDIA GPUä¸Šã®BLAS |
| cuSPARSE | ç–è¡Œåˆ—æ¼”ç®— | GPUä¸Šã®ã‚¹ãƒ‘ãƒ¼ã‚¹è¡Œåˆ—ç© |
| cuSOLVER | å›ºæœ‰å€¤åˆ†è§£ãƒ»SVD | GPUä¸Šã®LAPACK |
| Tensor Core | æ··åˆç²¾åº¦è¡Œåˆ—ç© | FP16/BF16ã§é«˜é€ŸåŒ– |

```python
# PyTorch GPU example (conceptual)
# import torch
#
# A = torch.randn(4096, 4096, device='cuda', dtype=torch.float16)
# B = torch.randn(4096, 4096, device='cuda', dtype=torch.float16)
#
# # Tensor Core accelerated matrix multiply
# C = torch.mm(A, B)  # uses Tensor Core if available
#
# # GPU SVD
# U, s, Vt = torch.linalg.svd(A.float())
```

### 4.6 è¡Œåˆ—å¾®åˆ†ã®æ•°å€¤æ¤œè¨¼ãƒ‘ã‚¿ãƒ¼ãƒ³

è¡Œåˆ—å¾®åˆ†ã‚’æ‰‹ã§å°å‡ºã—ãŸã‚‰ã€å¿…ãšæ•°å€¤å¾®åˆ†ã§æ¤œè¨¼ã™ã‚‹ã€‚ã“ã‚Œã¯ç ”ç©¶ã§ã‚‚å®Ÿå‹™ã§ã‚‚ä¸å¯æ¬ ãªãƒ‡ãƒãƒƒã‚°æ‰‹æ³•ã€‚

```python
import numpy as np

def numerical_gradient_matrix(f, X, eps=1e-7):
    """Compute numerical gradient df/dX for scalar-valued f"""
    grad = np.zeros_like(X)
    m, n = X.shape
    for i in range(m):
        for j in range(n):
            X_plus = X.copy(); X_plus[i, j] += eps
            X_minus = X.copy(); X_minus[i, j] -= eps
            grad[i, j] = (f(X_plus) - f(X_minus)) / (2 * eps)
    return grad

# Example: verify d/dX tr(AXB) = A^T B^T
A = np.random.randn(3, 4)
B = np.random.randn(5, 3)
X = np.random.randn(4, 5)

def f_trace(X_):
    return np.trace(A @ X_ @ B)

grad_analytical = A.T @ B.T
grad_numerical = numerical_gradient_matrix(f_trace, X)

print(f"d/dX tr(AXB) = A^T B^T")
print(f"Match: {np.allclose(grad_analytical, grad_numerical)}")
print(f"Max error: {np.max(np.abs(grad_analytical - grad_numerical)):.2e}")

# Example: verify d/dX ||X||_F^2 = 2X
def f_frob(X_):
    return np.sum(X_**2)

grad_analytical_2 = 2 * X
grad_numerical_2 = numerical_gradient_matrix(f_frob, X)
print(f"\nd/dX ||X||_F^2 = 2X")
print(f"Match: {np.allclose(grad_analytical_2, grad_numerical_2)}")

# Example: verify d/dX ln det(X) = X^{-T}
X_sq = np.random.randn(4, 4)
X_sq = X_sq @ X_sq.T + 2 * np.eye(4)  # positive definite

def f_logdet(X_):
    return np.log(np.linalg.det(X_))

grad_analytical_3 = np.linalg.inv(X_sq).T
grad_numerical_3 = numerical_gradient_matrix(f_logdet, X_sq)
print(f"\nd/dX ln det(X) = X^{{-T}}")
print(f"Match: {np.allclose(grad_analytical_3, grad_numerical_3)}")
```

:::message
**å®Ÿè·µã®ãƒ«ãƒ¼ãƒ«**: è¡Œåˆ—å¾®åˆ†ã‚’å°å‡ºã—ãŸã‚‰ã€**å¿…ãš** `numerical_gradient_matrix` ã§æ¤œè¨¼ã™ã‚‹ã€‚ä¸€è‡´ã—ãªã‘ã‚Œã°å°å‡ºã«é–“é•ã„ãŒã‚ã‚‹ã€‚ã“ã®ç¿’æ…£ãŒã€Backpropã®å®Ÿè£…ãƒã‚°ã‚’é˜²ãã€‚
:::

:::message
**é€²æ—: 85% å®Œäº†** SVDç”»åƒåœ§ç¸®ã€Randomized SVDã€Reverse Mode AD ã®å®Œå…¨å®Ÿè£…ã€æ¡ä»¶æ•°ã¨æ•°å€¤å®‰å®šæ€§ã€ã‚¹ãƒ‘ãƒ¼ã‚¹è¡Œåˆ—ã€è¡Œåˆ—å¾®åˆ†ã®æ•°å€¤æ¤œè¨¼ã‚’ç¿’å¾—ã—ãŸã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

### 5.1 è¨˜å·èª­è§£ãƒ†ã‚¹ãƒˆ

ä»¥ä¸‹ã®æ•°å¼ã‚’å£°ã«å‡ºã—ã¦èª­ã¿ã€æ„å‘³ã‚’èª¬æ˜ã›ã‚ˆã€‚

:::details Q1: $A = U \Sigma V^\top$
**èª­ã¿**: ã€Œ$A$ ã‚¤ã‚³ãƒ¼ãƒ« $U$ ã‚·ã‚°ãƒ $V$ ãƒˆãƒ©ãƒ³ã‚¹ãƒãƒ¼ã‚ºã€

**æ„å‘³**: è¡Œåˆ— $A$ ã®ç‰¹ç•°å€¤åˆ†è§£ã€‚$U$ ã¨ $V$ ã¯ç›´äº¤è¡Œåˆ—ã€$\Sigma$ ã¯ç‰¹ç•°å€¤ã‚’å¯¾è§’ã«æŒã¤è¡Œåˆ—ã€‚ä»»æ„ã®é•·æ–¹å½¢è¡Œåˆ—ã«é©ç”¨å¯èƒ½ã€‚
:::

:::details Q2: $A_k = \sum_{i=1}^{k} \sigma_i \mathbf{u}_i \mathbf{v}_i^\top$
**èª­ã¿**: ã€Œ$A$ ã‚µãƒ– $k$ ã¯ã€$i$ ã‚¤ã‚³ãƒ¼ãƒ« 1 ã‹ã‚‰ $k$ ã¾ã§ã€ã‚·ã‚°ãƒ $i$ ãƒ¦ãƒ¼ $i$ ãƒ–ã‚¤ $i$ ãƒˆãƒ©ãƒ³ã‚¹ãƒãƒ¼ã‚ºã®å’Œã€

**æ„å‘³**: rank-$k$ ã®æˆªæ–­SVDã€‚ä¸Šä½ $k$ å€‹ã®ç‰¹ç•°å€¤æˆåˆ†ã®å’Œã€‚Eckart-Youngå®šç†ã«ã‚ˆã‚Šã€ã“ã‚Œã¯Frobeniusãƒãƒ«ãƒ ã§æœ€é©ãª rank-$k$ è¿‘ä¼¼ã€‚
:::

:::details Q3: $J = \frac{\partial \mathbf{f}}{\partial \mathbf{x}} \in \mathbb{R}^{m \times n}$
**èª­ã¿**: ã€Œ$J$ ã‚¤ã‚³ãƒ¼ãƒ« ãƒ‘ãƒ¼ã‚·ãƒ£ãƒ« $\mathbf{f}$ ãƒ‘ãƒ¼ã‚·ãƒ£ãƒ« $\mathbf{x}$ã€$m$ ã‹ã‘ã‚‹ $n$ ã®å®Ÿæ•°è¡Œåˆ—ã€

**æ„å‘³**: ãƒ™ã‚¯ãƒˆãƒ«é–¢æ•° $\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m$ ã®ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã€‚$J_{ij} = \partial f_i / \partial x_j$ã€‚å…¥åŠ›ã®å¾®å°å¤‰åŒ–ãŒå‡ºåŠ›ã«ã©ã†å½±éŸ¿ã™ã‚‹ã‹ã‚’ç·šå½¢è¿‘ä¼¼ã™ã‚‹è¡Œåˆ—ã€‚
:::

:::details Q4: $\boldsymbol{\delta}_l = (W_{l+1}^\top \boldsymbol{\delta}_{l+1}) \odot \sigma'(\mathbf{z}_l)$
**èª­ã¿**: ã€Œãƒ‡ãƒ«ã‚¿ $l$ ã‚¤ã‚³ãƒ¼ãƒ«ã€$W$ $l$ãƒ—ãƒ©ã‚¹1 ãƒˆãƒ©ãƒ³ã‚¹ãƒãƒ¼ã‚º ãƒ‡ãƒ«ã‚¿ $l$ãƒ—ãƒ©ã‚¹1ã€ãƒãƒ€ãƒãƒ¼ãƒ‰ç© ã‚·ã‚°ãƒãƒ—ãƒ©ã‚¤ãƒ  $\mathbf{z}_l$ã€

**æ„å‘³**: Backpropagationã®å†å¸°å¼ã€‚ç¬¬ $l$ å±¤ã®èª¤å·®ä¿¡å·ã¯ã€ç¬¬ $l+1$ å±¤ã®èª¤å·®ã‚’é‡ã¿è¡Œåˆ—ã§é€†ä¼æ’­ã—ã€æ´»æ€§åŒ–é–¢æ•°ã®å¾®åˆ†ã¨è¦ç´ ã”ã¨ã«æ›ã‘ã‚‹ã€‚
:::

:::details Q5: $A^+ = V \Sigma^+ U^\top$
**èª­ã¿**: ã€Œ$A$ ãƒ—ãƒ©ã‚¹ ã‚¤ã‚³ãƒ¼ãƒ« $V$ ã‚·ã‚°ãƒãƒ—ãƒ©ã‚¹ $U$ ãƒˆãƒ©ãƒ³ã‚¹ãƒãƒ¼ã‚ºã€

**æ„å‘³**: Moore-Penroseæ“¬ä¼¼é€†è¡Œåˆ—ã®SVDã«ã‚ˆã‚‹æ§‹æˆã€‚$\Sigma^+$ ã¯éã‚¼ãƒ­ç‰¹ç•°å€¤ã®é€†æ•°ã‚’å¯¾è§’ã«æŒã¤ã€‚æ­£å‰‡ã§ãªã„è¡Œåˆ—ã‚„é•·æ–¹å½¢è¡Œåˆ—ã«å¯¾ã™ã‚‹ã€Œé€†è¡Œåˆ—ã€ã®ä¸€èˆ¬åŒ–ã€‚
:::

### 5.2 ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ†ã‚¹ãƒˆ

ä»¥ä¸‹ã®æ•°å¼ã‚’NumPyã‚³ãƒ¼ãƒ‰ã«ç¿»è¨³ã›ã‚ˆã€‚

:::details Q1: $\hat{\mathbf{x}} = V \Sigma^+ U^\top \mathbf{b}$ï¼ˆæ“¬ä¼¼é€†è¡Œåˆ—ã«ã‚ˆã‚‹æœ€å°äºŒä¹—è§£ï¼‰
```python
U, s, Vt = np.linalg.svd(A, full_matrices=False)
S_pinv = np.diag(1.0 / s)  # assumes all singular values > 0
x_hat = Vt.T @ S_pinv @ U.T @ b
# or simply: x_hat = np.linalg.pinv(A) @ b
```
:::

:::details Q2: $\text{tr}(A^\top B) = \sum_{ij} A_{ij} B_{ij}$ï¼ˆFrobeniuså†…ç©ï¼‰
```python
# Method 1: trace
result1 = np.trace(A.T @ B)
# Method 2: element-wise (faster, no matrix multiply)
result2 = np.sum(A * B)
# Method 3: einsum
result3 = np.einsum('ij,ij->', A, B)
```
:::

:::details Q3: $S_{bhij} = \frac{1}{\sqrt{d_k}} \sum_k Q_{bhik} K_{bhjk}$ï¼ˆMulti-Head Attention ã‚¹ã‚³ã‚¢ï¼‰
```python
scores = np.einsum('bhik,bhjk->bhij', Q, K) / np.sqrt(dk)
# or: scores = Q @ K.transpose(0, 1, 3, 2) / np.sqrt(dk)
```
:::

### 5.3 ãƒŸãƒ‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: SVDã§ç”»åƒã®ãƒã‚¤ã‚ºé™¤å»

```python
import numpy as np

# Create image with noise
np.random.seed(42)
m, n = 128, 128
x = np.linspace(0, 4*np.pi, m)
y = np.linspace(0, 4*np.pi, n)
X, Y = np.meshgrid(y, x)
clean = np.sin(X) * np.cos(Y) + 0.5 * np.sin(2*X + Y)

# Add noise
noise_level = 0.5
noisy = clean + noise_level * np.random.randn(m, n)

# SVD denoising: try different ranks
U, s, Vt = np.linalg.svd(noisy, full_matrices=False)

print(f"Original signal energy: {np.linalg.norm(clean, 'fro'):.4f}")
print(f"Noise energy: {noise_level * np.sqrt(m * n):.4f}")
print(f"{'rank':>6} {'error_vs_clean':>15} {'error_vs_noisy':>15} {'SNR(dB)':>10}")

best_k, best_error = 0, float('inf')
for k in [1, 3, 5, 10, 20, 50, 100]:
    denoised = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]
    err_clean = np.linalg.norm(denoised - clean, 'fro') / np.linalg.norm(clean, 'fro')
    err_noisy = np.linalg.norm(denoised - noisy, 'fro') / np.linalg.norm(noisy, 'fro')
    signal_power = np.mean(denoised**2)
    noise_power = np.mean((denoised - clean)**2)
    snr = 10 * np.log10(signal_power / noise_power) if noise_power > 0 else float('inf')
    print(f"{k:6d} {err_clean:15.6f} {err_noisy:15.6f} {snr:10.2f}")
    if err_clean < best_error:
        best_error = err_clean
        best_k = k

print(f"\nBest rank for denoising: {best_k} (error = {best_error:.6f})")
```

### 5.4 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸: Forward Mode AD with Dual Numbers

```python
import numpy as np

class Dual:
    """Dual number for Forward Mode AD"""
    def __init__(self, val, deriv=0.0):
        self.val = float(val)
        self.deriv = float(deriv)

    def __repr__(self):
        return f"Dual({self.val:.6f}, d={self.deriv:.6f})"

    def __add__(self, other):
        o = other if isinstance(other, Dual) else Dual(other)
        return Dual(self.val + o.val, self.deriv + o.deriv)

    def __radd__(self, other):
        return self + other

    def __sub__(self, other):
        o = other if isinstance(other, Dual) else Dual(other)
        return Dual(self.val - o.val, self.deriv - o.deriv)

    def __rsub__(self, other):
        return Dual(other) - self

    def __mul__(self, other):
        o = other if isinstance(other, Dual) else Dual(other)
        return Dual(self.val * o.val, self.val * o.deriv + self.deriv * o.val)

    def __rmul__(self, other):
        return self * other

    def __truediv__(self, other):
        o = other if isinstance(other, Dual) else Dual(other)
        return Dual(self.val / o.val,
                    (self.deriv * o.val - self.val * o.deriv) / o.val**2)

    def __pow__(self, n):
        return Dual(self.val**n, n * self.val**(n-1) * self.deriv)

def sin_d(x):
    return Dual(np.sin(x.val), np.cos(x.val) * x.deriv)

def cos_d(x):
    return Dual(np.cos(x.val), -np.sin(x.val) * x.deriv)

def exp_d(x):
    e = np.exp(x.val)
    return Dual(e, e * x.deriv)

def log_d(x):
    return Dual(np.log(x.val), x.deriv / x.val)

# Test: f(x) = sin(x^2) * exp(-x) + log(x)
def f(x):
    return sin_d(x**2) * exp_d(-1 * x) + log_d(x)

# Compute derivative at x = 1.5
x = Dual(1.5, 1.0)  # seed: dx/dx = 1
result = f(x)
print(f"f(1.5)  = {result.val:.8f}")
print(f"f'(1.5) = {result.deriv:.8f}")

# Numerical verification
h = 1e-8
def f_float(x):
    return np.sin(x**2) * np.exp(-x) + np.log(x)
numerical = (f_float(1.5 + h) - f_float(1.5 - h)) / (2 * h)
print(f"Numerical: {numerical:.8f}")
print(f"Match: {abs(result.deriv - numerical) < 1e-6}")
```

### 5.5 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸: 2å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®å­¦ç¿’

Backpropagation ã‚’ä½¿ã£ã¦ã€2å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ã‚¼ãƒ­ã‹ã‚‰å­¦ç¿’ã•ã›ã‚‹ã€‚

```python
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))

def sigmoid_deriv(x):
    s = sigmoid(x)
    return s * (1 - s)

# XOR problem
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=float)
y = np.array([[0], [1], [1], [0]], dtype=float)

# Initialize weights
np.random.seed(42)
W1 = np.random.randn(2, 8) * 0.5  # 2 inputs -> 8 hidden
b1 = np.zeros((1, 8))
W2 = np.random.randn(8, 1) * 0.5  # 8 hidden -> 1 output
b2 = np.zeros((1, 1))

lr = 1.0
losses = []

for epoch in range(5000):
    # Forward pass
    z1 = X @ W1 + b1
    h1 = sigmoid(z1)
    z2 = h1 @ W2 + b2
    h2 = sigmoid(z2)

    # Loss (MSE)
    loss = np.mean((h2 - y)**2)
    losses.append(loss)

    # Backward pass
    dL_dh2 = 2 * (h2 - y) / len(X)
    delta2 = dL_dh2 * sigmoid_deriv(z2)

    dL_dW2 = h1.T @ delta2
    dL_db2 = np.sum(delta2, axis=0, keepdims=True)

    delta1 = (delta2 @ W2.T) * sigmoid_deriv(z1)
    dL_dW1 = X.T @ delta1
    dL_db1 = np.sum(delta1, axis=0, keepdims=True)

    # Update
    W2 -= lr * dL_dW2
    b2 -= lr * dL_db2
    W1 -= lr * dL_dW1
    b1 -= lr * dL_db1

    if epoch % 1000 == 0:
        print(f"Epoch {epoch:5d}: loss = {loss:.6f}")

# Final predictions
z1 = X @ W1 + b1
h1 = sigmoid(z1)
z2 = h1 @ W2 + b2
predictions = sigmoid(z2)

print(f"\nFinal predictions:")
for i in range(len(X)):
    print(f"  {X[i]} -> {predictions[i, 0]:.4f} (target: {y[i, 0]:.0f})")
print(f"\nLoss: {losses[0]:.6f} -> {losses[-1]:.6f}")
```

### 5.6 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸: è¡Œåˆ—å¾®åˆ†ã®å°å‡ºã¨æ¤œè¨¼

ä»¥ä¸‹ã®è¡Œåˆ—å¾®åˆ†ã‚’æ‰‹ã§å°å‡ºã—ã€æ•°å€¤æ¤œè¨¼ã›ã‚ˆã€‚

:::details Challenge 1: $\frac{\partial}{\partial W} \|XW - Y\|_F^2$
**å°å‡º**:

$$
L = \text{tr}((XW-Y)^\top(XW-Y)) = \text{tr}(W^\top X^\top XW - 2Y^\top XW + Y^\top Y)
$$

$$
\frac{\partial L}{\partial W} = 2X^\top XW - 2X^\top Y = 2X^\top(XW - Y)
$$

**æ¤œè¨¼**:
```python
import numpy as np

X = np.random.randn(10, 5)
W = np.random.randn(5, 3)
Y = np.random.randn(10, 3)

# Analytical
grad_analytical = 2 * X.T @ (X @ W - Y)

# Numerical
eps = 1e-7
grad_numerical = np.zeros_like(W)
for i in range(W.shape[0]):
    for j in range(W.shape[1]):
        W_plus = W.copy(); W_plus[i, j] += eps
        W_minus = W.copy(); W_minus[i, j] -= eps
        f_plus = np.sum((X @ W_plus - Y)**2)
        f_minus = np.sum((X @ W_minus - Y)**2)
        grad_numerical[i, j] = (f_plus - f_minus) / (2 * eps)

print(f"Match: {np.allclose(grad_analytical, grad_numerical)}")
```
:::

:::details Challenge 2: $\frac{\partial}{\partial \mathbf{x}} \text{softmax}(\mathbf{x})^\top \mathbf{a}$
**å°å‡º**:

$L = \mathbf{s}^\top \mathbf{a} = \sum_i s_i a_i$ ã¨ã™ã‚‹ã€‚

$$
\frac{\partial L}{\partial x_j} = \sum_i a_i \frac{\partial s_i}{\partial x_j} = \sum_i a_i s_i(\delta_{ij} - s_j) = a_j s_j - s_j \sum_i a_i s_i
$$

$$
\frac{\partial L}{\partial \mathbf{x}} = \mathbf{s} \odot \mathbf{a} - (\mathbf{s}^\top \mathbf{a}) \mathbf{s} = \mathbf{s} \odot (\mathbf{a} - (\mathbf{s}^\top \mathbf{a}) \mathbf{1})
$$

**æ¤œè¨¼**:
```python
import numpy as np

def softmax(z):
    e = np.exp(z - np.max(z))
    return e / np.sum(e)

x = np.array([1.0, 2.0, 0.5])
a = np.array([3.0, 1.0, 2.0])
s = softmax(x)

# Analytical
grad_analytical = s * (a - np.dot(s, a))

# Numerical
eps = 1e-7
grad_numerical = np.zeros(len(x))
for j in range(len(x)):
    x_plus = x.copy(); x_plus[j] += eps
    x_minus = x.copy(); x_minus[j] -= eps
    grad_numerical[j] = (softmax(x_plus) @ a - softmax(x_minus) @ a) / (2 * eps)

print(f"Analytical: {np.round(grad_analytical, 6)}")
print(f"Numerical:  {np.round(grad_numerical, 6)}")
print(f"Match: {np.allclose(grad_analytical, grad_numerical)}")
```
:::

### 5.7 LaTeX è¨˜è¿°ãƒ†ã‚¹ãƒˆ

ä»¥ä¸‹ã®æ•°å¼ã‚’LaTeX ã§è¨˜è¿°ã›ã‚ˆã€‚

:::details Q1: SVDã®å®šç¾©
```latex
A = U \Sigma V^\top = \sum_{i=1}^{r} \sigma_i \mathbf{u}_i \mathbf{v}_i^\top
```
:::

:::details Q2: Backpropagation ã®å†å¸°å¼
```latex
\boldsymbol{\delta}_l = (W_{l+1}^\top \boldsymbol{\delta}_{l+1}) \odot \sigma'(\mathbf{z}_l)
```
:::

:::details Q3: Eckart-Young å®šç†
```latex
\min_{\text{rank}(B) \leq k} \|A - B\|_F = \sqrt{\sum_{i=k+1}^{r} \sigma_i^2}
```
:::

:::details Q4: Normalizing Flow ã®å¤‰æ•°å¤‰æ›å…¬å¼
```latex
p_Y(\mathbf{y}) = p_X(\mathbf{f}^{-1}(\mathbf{y})) \cdot |\det(J_{\mathbf{f}^{-1}}(\mathbf{y}))|
```
:::

:::details Q5: Cross-Entropy + Softmax ã®å‹¾é…
```latex
\frac{\partial}{\partial \mathbf{z}} \left[ -\sum_i y_i \log \text{softmax}(\mathbf{z})_i \right] = \text{softmax}(\mathbf{z}) - \mathbf{y}
```
:::

### 5.8 è‡ªå·±ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

| # | ãƒã‚§ãƒƒã‚¯é …ç›® | é”æˆ |
|:--|:-----------|:-----|
| 1 | SVDã®å®šç¾©ï¼ˆ$A = U\Sigma V^\top$ï¼‰ã‚’ç™½ç´™ã«æ›¸ã‘ã‚‹ | [ ] |
| 2 | å›ºæœ‰å€¤åˆ†è§£ã¨ã®é–¢ä¿‚ã‚’èª¬æ˜ã§ãã‚‹ | [ ] |
| 3 | Eckart-Youngå®šç†ã®æ„å‘³ã‚’èª¬æ˜ã§ãã‚‹ | [ ] |
| 4 | æ“¬ä¼¼é€†è¡Œåˆ—ã®SVDã«ã‚ˆã‚‹æ§‹æˆã‚’æ›¸ã‘ã‚‹ | [ ] |
| 5 | PCA ã‚’SVDã§å°å‡ºã§ãã‚‹ | [ ] |
| 6 | Kroneckerç©ã®æ€§è³ªã‚’3ã¤æŒ™ã’ã‚‰ã‚Œã‚‹ | [ ] |
| 7 | einsum ã§è¡Œåˆ—ç©ãƒ»ãƒãƒƒãƒè¡Œåˆ—ç©ã‚’æ›¸ã‘ã‚‹ | [ ] |
| 8 | ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®å®šç¾©ã¨å¹¾ä½•å­¦çš„æ„å‘³ã‚’èª¬æ˜ã§ãã‚‹ | [ ] |
| 9 | ãƒ˜ã‚·ã‚¢ãƒ³ã®æ­£å®šå€¤æ€§ã¨æœ€é©åŒ–ã®é–¢ä¿‚ã‚’èª¬æ˜ã§ãã‚‹ | [ ] |
| 10 | Matrix Cookbook ã®ä¸»è¦å…¬å¼ã‚’5ã¤æ›¸ã‘ã‚‹ | [ ] |
| 11 | é€£é–å¾‹ã®ãƒ™ã‚¯ãƒˆãƒ«ç‰ˆã‚’æ›¸ã‘ã‚‹ | [ ] |
| 12 | Backpropã®å†å¸°å¼ $\boldsymbol{\delta}_l$ ã‚’å°å‡ºã§ãã‚‹ | [ ] |
| 13 | Forward vs Reverse Mode AD ã®è¨ˆç®—é‡ã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹ | [ ] |
| 14 | Log-Sum-Exp trick ã®å¿…è¦æ€§ã‚’èª¬æ˜ã§ãã‚‹ | [ ] |

:::message
**é€²æ—: 90% å®Œäº†** è¨˜å·èª­è§£ãƒ†ã‚¹ãƒˆã€ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ†ã‚¹ãƒˆã€SVDãƒã‚¤ã‚ºé™¤å»ã€Forward Mode ADå®Ÿè£…ã€è‡ªå·±ãƒã‚§ãƒƒã‚¯ã‚’å®Œäº†ã—ãŸã€‚
:::

---

## ğŸš€ 6. æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨æ¬¡å›äºˆå‘Š

### 6.1 NumPy / SciPy ã® SVDãƒ»å¾®åˆ†é–¢é€£ãƒãƒ¼ãƒˆã‚·ãƒ¼ãƒˆ

| ç›®çš„ | é–¢æ•° | æ³¨æ„ç‚¹ |
|:-----|:-----|:------|
| SVD (full) | `np.linalg.svd(A, full_matrices=True)` | $U: m \times m$ |
| SVD (economy) | `np.linalg.svd(A, full_matrices=False)` | $U: m \times \min(m,n)$ |
| æ“¬ä¼¼é€†è¡Œåˆ— | `np.linalg.pinv(A)` | å†…éƒ¨ã§SVDä½¿ç”¨ |
| Truncated SVD | `scipy.sparse.linalg.svds(A, k)` | ç–è¡Œåˆ—å‘ã‘ã€‚ä¸Šä½$k$å€‹ |
| æ•°å€¤å‹¾é… | `scipy.optimize.approx_fprime(x, f, eps)` | ãƒ‡ãƒãƒƒã‚°ç”¨ã€‚æœ¬ç•ªã§ã¯ä½¿ã‚ãªã„ |
| Kroneckerç© | `np.kron(A, B)` | çµæœã¯å¤§è¡Œåˆ— |
| einsum | `np.einsum(subscripts, *operands)` | `optimize=True` æ¨å¥¨ |

:::details ç”¨èªé›†
| è‹±èª | æ—¥æœ¬èª | è¨˜å· |
|:-----|:------|:-----|
| Singular Value Decomposition | ç‰¹ç•°å€¤åˆ†è§£ | $A = U\Sigma V^\top$ |
| Singular value | ç‰¹ç•°å€¤ | $\sigma_i$ |
| Left/Right singular vector | å·¦/å³ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ« | $\mathbf{u}_i, \mathbf{v}_i$ |
| Low-rank approximation | ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ | $A_k$ |
| Pseudoinverse | æ“¬ä¼¼é€†è¡Œåˆ— | $A^+$ |
| Jacobian | ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ | $J$ |
| Hessian | ãƒ˜ã‚·ã‚¢ãƒ³ | $H$ |
| Gradient | å‹¾é… | $\nabla f$ |
| Chain rule | é€£é–å¾‹ | |
| Automatic differentiation | è‡ªå‹•å¾®åˆ† | AD |
| Forward mode | å‰é€²ãƒ¢ãƒ¼ãƒ‰ | tangent propagation |
| Reverse mode | é€†ä¼æ’­ãƒ¢ãƒ¼ãƒ‰ | adjoint propagation |
| Backpropagation | èª¤å·®é€†ä¼æ’­æ³• | BP |
| Dual number | åŒå¯¾æ•° | $a + b\epsilon$ |
| Wengert list | Wengert ãƒªã‚¹ãƒˆ | è¨ˆç®—ãƒˆãƒ¬ãƒ¼ã‚¹ |
| Kronecker product | ã‚¯ãƒ­ãƒãƒƒã‚«ãƒ¼ç© | $A \otimes B$ |
| Einstein notation | ã‚¢ã‚¤ãƒ³ã‚·ãƒ¥ã‚¿ã‚¤ãƒ³è¨˜æ³• | æ·»å­—ã®æš—é»™çš„ç¸®ç´„ |
| Tikhonov regularization | ãƒãƒ›ãƒãƒ•æ­£å‰‡åŒ– | Ridgeå›å¸° |
| Condition number | æ¡ä»¶æ•° | $\kappa(A)$ |
:::

```mermaid
mindmap
  root((ç·šå½¢ä»£æ•° II))
    SVD
      å­˜åœ¨å®šç†
      Eckart-Young
      ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼
      æ“¬ä¼¼é€†è¡Œåˆ—
      PCA via SVD
    ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—
      Kroneckerç©
      Einsteinè¨˜æ³•
      einsum
    è¡Œåˆ—å¾®åˆ†
      å‹¾é…
      ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³
      ãƒ˜ã‚·ã‚¢ãƒ³
      Matrix Cookbook
    é€£é–å¾‹
      ã‚¹ã‚«ãƒ©ãƒ¼ç‰ˆ
      ãƒ™ã‚¯ãƒˆãƒ«ç‰ˆ
      Backpropagation
    è‡ªå‹•å¾®åˆ†
      Forward Mode
      Reverse Mode
      Dual Numbers
      Wengert List
```

:::message
**é€²æ—: 95% å®Œäº†** SVDãƒ»è¡Œåˆ—å¾®åˆ†ãƒ»è‡ªå‹•å¾®åˆ†ã®ç ”ç©¶æœ€å‰ç·šã€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®é€²åŒ–ã€æ¨è–¦ãƒªã‚½ãƒ¼ã‚¹ã‚’ç¢ºèªã—ãŸã€‚
:::

### 6.2 æœ¬è¬›ç¾©ã®3ã¤ã®ãƒã‚¤ãƒ³ãƒˆ

**1. SVDã¯è¡Œåˆ—ã®ä¸‡èƒ½ãƒŠã‚¤ãƒ•**

$$
A = U \Sigma V^\top = \sum_{i=1}^{r} \sigma_i \mathbf{u}_i \mathbf{v}_i^\top
$$

ä»»æ„ã®è¡Œåˆ—ã‚’åˆ†è§£ã§ãã€ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ã®æœ€é©æ€§ï¼ˆEckart-Youngå®šç†[^3]ï¼‰ãŒä¿è¨¼ã•ã‚Œã‚‹ã€‚PCAã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã€LoRA[^10]ã€ç”»åƒåœ§ç¸® â€” å…¨ã¦SVDã®å¿œç”¨ã€‚

**2. è¡Œåˆ—å¾®åˆ† + é€£é–å¾‹ = Backpropagation**

$$
\frac{\partial L}{\partial W_l} = \boldsymbol{\delta}_l \mathbf{h}_{l-1}^\top, \quad \boldsymbol{\delta}_l = (W_{l+1}^\top \boldsymbol{\delta}_{l+1}) \odot \sigma'(\mathbf{z}_l)
$$

ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®ç©ã¨ã—ã¦é€£é–å¾‹ã‚’æ›¸ãã€é€†é †ã«ä¼æ’­ã™ã‚‹ã“ã¨ã§å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹¾é…ã‚’1å›ã®backward passã§è¨ˆç®—[^2]ã€‚

**3. è‡ªå‹•å¾®åˆ†ã¯Reverse ModeãŒæ©Ÿæ¢°å­¦ç¿’ã®æ¨™æº–**

$$
\text{Forward: } O(n) \text{ passes for } n \text{ inputs} \quad \text{vs} \quad \text{Reverse: } O(1) \text{ pass for scalar output}
$$

æå¤±é–¢æ•°ã¯ã‚¹ã‚«ãƒ©ãƒ¼ï¼ˆ$m = 1$ï¼‰ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯è†¨å¤§ï¼ˆ$n \sim 10^9$ï¼‰ã€‚Reverse mode[^7][^8]ãŒå”¯ä¸€ã®ç¾å®Ÿçš„ãªé¸æŠè‚¢ã€‚

### 6.3 FAQ

:::details Q: SVDã¨å›ºæœ‰å€¤åˆ†è§£ã¯ã©ã†ä½¿ã„åˆ†ã‘ã‚‹ï¼Ÿ
- **æ­£æ–¹å¯¾ç§°è¡Œåˆ—** â†’ å›ºæœ‰å€¤åˆ†è§£ï¼ˆ`eigh`ï¼‰ãŒåŠ¹ç‡çš„
- **é•·æ–¹å½¢è¡Œåˆ—** â†’ SVDä¸€æŠï¼ˆå›ºæœ‰å€¤åˆ†è§£ã¯ä½¿ãˆãªã„ï¼‰
- **æ­£æ–¹éå¯¾ç§°è¡Œåˆ—** â†’ SVDæ¨å¥¨ï¼ˆå›ºæœ‰å€¤ãŒè¤‡ç´ æ•°ã«ãªã‚Šå¾—ã‚‹ï¼‰
- **PCA** â†’ ã©ã¡ã‚‰ã§ã‚‚ã‚ˆã„ãŒã€SVDã®æ–¹ãŒæ•°å€¤å®‰å®š

å®Ÿå‹™ã§ã¯SVDã‚’ä½¿ã£ã¦ãŠã‘ã°é–“é•ã„ãªã„ã€‚å›ºæœ‰å€¤åˆ†è§£ã¯SVDã®ç‰¹æ®Šã‚±ãƒ¼ã‚¹ã€‚
:::

:::details Q: Backpropagation ã®å‹¾é…ã¯å¸¸ã«æ­£ç¢ºã‹ï¼Ÿ
è‡ªå‹•å¾®åˆ†ã®å‹¾é…ã¯**æ©Ÿæ¢°ç²¾åº¦**ï¼ˆ$\sim 10^{-16}$ for float64ï¼‰ã¾ã§æ­£ç¢ºã€‚æ•°å€¤å¾®åˆ†ï¼ˆ$\sim 10^{-8}$ï¼‰ã‚ˆã‚Šã¯ã‚‹ã‹ã«ç²¾åº¦ãŒé«˜ã„ã€‚

ãŸã ã—ã€ä»¥ä¸‹ã®å ´åˆã«æ•°å€¤çš„å•é¡ŒãŒç”Ÿã˜ã‚‹:
1. **å‹¾é…æ¶ˆå¤±**: sigmoid/tanhã®é£½å’Œé ˜åŸŸã§å‹¾é…ãŒã»ã¼0 â†’ ReLUç³»æ´»æ€§åŒ–é–¢æ•°ã§è§£æ±º
2. **å‹¾é…çˆ†ç™º**: æ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®ç©ãŒã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ â†’ å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã§è§£æ±º
3. **éå¾®åˆ†ç‚¹**: ReLUã® $x = 0$ â†’ sub-gradient ã§ä»£ç”¨ï¼ˆå®Ÿç”¨ä¸Šã¯å•é¡Œã«ãªã‚‰ãªã„ï¼‰
:::

:::details Q: einsum ã¨ @ ã¯ã©ã¡ã‚‰ã‚’ä½¿ã†ã¹ãï¼Ÿ
- **2Dã®å˜ç´”ãªè¡Œåˆ—ç©** â†’ `@`ï¼ˆèª­ã¿ã‚„ã™ã„ï¼‰
- **ãƒãƒƒãƒå‡¦ç†ã€è¤‡é›‘ãªç¸®ç´„** â†’ `einsum`ï¼ˆè¡¨ç¾åŠ›ãŒé«˜ã„ï¼‰
- **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**: ã»ã¼åŒç­‰ã€‚`einsum` ã¯ `optimize=True` ã§æœ€é©ãªç¸®ç´„é †åºã‚’é¸æŠ

Transformerå®Ÿè£…ã§ã¯ `einsum` ãŒã‚ˆãä½¿ã‚ã‚Œã‚‹ï¼ˆç‰¹ã«Multi-Head Attentionã® $B \times H \times T \times d$ ãƒ†ãƒ³ã‚½ãƒ«æ“ä½œï¼‰ã€‚
:::

:::details Q: LoRAã¯ãªãœä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ã§å‹•ãã®ã‹ï¼Ÿ
çµŒé¨“çš„ã«ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚ã®é‡ã¿æ›´æ–° $\Delta W$ ãŒä½ãƒ©ãƒ³ã‚¯æ§‹é€ ã‚’æŒã¤ã“ã¨ãŒçŸ¥ã‚‰ã‚Œã¦ã„ã‚‹ã€‚ã¤ã¾ã‚Šã€$\Delta W$ ã®SVDã‚’å–ã‚‹ã¨ã€ä¸Šä½æ•°å€‹ã®ç‰¹ç•°å€¤ãŒæ”¯é…çš„ã§ã€æ®‹ã‚Šã¯ã»ã¼0ã€‚

ç†è«–çš„ãªèª¬æ˜ã¯å®Œå…¨ã§ã¯ãªã„ãŒã€ä¸€ã¤ã®ä»®èª¬ã¯: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã€Œäº‹å‰å­¦ç¿’ã§ç²å¾—ã—ãŸè¡¨ç¾ç©ºé–“ã®ä¸­ã®ã€ã‚¿ã‚¹ã‚¯å›ºæœ‰ã®ä½æ¬¡å…ƒéƒ¨åˆ†ç©ºé–“ã‚’èª¿æ•´ã™ã‚‹æ“ä½œã€ã§ã‚ã‚Šã€ãã®éƒ¨åˆ†ç©ºé–“ã®æ¬¡å…ƒãŒ $r \ll d$ ã§ã‚ã‚‹ã¨ã„ã†ã“ã¨ã€‚
:::

:::details Q: JAXã®gradã¨PyTorchã®backwardã®é•ã„ã¯ï¼Ÿ
**PyTorch**: ãƒ†ãƒ¼ãƒ—ãƒ™ãƒ¼ã‚¹ã€‚è¨ˆç®—ã‚’å®Ÿè¡Œã™ã‚‹ãŸã³ã«ã‚°ãƒ©ãƒ•ã‚’è¨˜éŒ²ã—ã€`.backward()` ã§é€†ä¼æ’­ã€‚

**JAX**: ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ™ãƒ¼ã‚¹ã€‚é–¢æ•°ã‚’ä¸€åº¦ãƒˆãƒ¬ãƒ¼ã‚¹ã—ã¦è¨ˆç®—ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰ã—ã€`jax.grad(f)` ã§å‹¾é…é–¢æ•°ã‚’è¿”ã™ã€‚

ä¸»ãªé•ã„:
1. JAXã® `grad` ã¯**é–¢æ•°**ã‚’è¿”ã™ï¼ˆé«˜éšé–¢æ•°ï¼‰ã€‚PyTorchã® `.backward()` ã¯**å‰¯ä½œç”¨**ã§å‹¾é…ã‚’è“„ç©
2. JAXã¯ `jit` ã§ XLA ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å¯èƒ½ã€‚PyTorchã¯ `torch.compile` ã§åŒç­‰ã®ã“ã¨ãŒå¯èƒ½
3. JAXã® `vmap` ã§ãƒãƒƒãƒå‡¦ç†ã‚’è‡ªå‹•åŒ–ã€‚PyTorchã¯ `torch.vmap` ã§åŒç­‰
:::

:::details Q: Truncated SVD ã® k ã¯ã©ã†é¸ã¶ã¹ãã‹ï¼Ÿ
ç›®çš„ã«ã‚ˆã£ã¦ç•°ãªã‚‹:

1. **ãƒ‡ãƒ¼ã‚¿åœ§ç¸®ãƒ»ãƒã‚¤ã‚ºé™¤å»**: ç´¯ç©ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼ˆ$\sum_{i=1}^k \sigma_i^2 / \sum_{i=1}^r \sigma_i^2$ï¼‰ãŒ 90-99% ã«ãªã‚‹ $k$
2. **PCA / å¯è¦–åŒ–**: $k = 2$ or $3$ï¼ˆäººé–“ãŒè¦‹ã‚‰ã‚Œã‚‹æ¬¡å…ƒï¼‰
3. **LoRA**: çµŒé¨“çš„ã« $r = 4, 8, 16, 64$ ç¨‹åº¦ã€‚ã‚¿ã‚¹ã‚¯ã¨å…ƒã®ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«ä¾å­˜
4. **æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ **: Cross-validation ã§æœ€é©ãª $k$ ã‚’é¸æŠ

ç‰¹ç•°å€¤ã®scree plotï¼ˆç‰¹ç•°å€¤ã‚’é™é †ã«ãƒ—ãƒ­ãƒƒãƒˆã—ãŸã‚°ãƒ©ãƒ•ï¼‰ã§ã€Œè‚˜ã€ï¼ˆæ€¥æ¿€ã«æ¸›è¡°ãŒç·©ã‚„ã‹ã«ãªã‚‹ç‚¹ï¼‰ã‚’è¦‹ã¤ã‘ã‚‹ã®ãŒä¸€èˆ¬çš„ãªçµŒé¨“å‰‡ã€‚
:::

:::details Q: ãƒ˜ã‚·ã‚¢ãƒ³ã‚’è¨ˆç®—ã™ã‚‹ã®ã¯å®Ÿç”¨çš„ã‹ï¼Ÿ
$n$ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ˜ã‚·ã‚¢ãƒ³ã¯ $n \times n$ è¡Œåˆ—ã€‚LLMã§ã¯ $n \sim 10^9$ ãªã®ã§ã€$10^{18}$ è¦ç´ ã®ãƒ˜ã‚·ã‚¢ãƒ³ã¯æ ¼ç´ä¸å¯èƒ½ã€‚

å®Ÿç”¨çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:
1. **ãƒ˜ã‚·ã‚¢ãƒ³-ãƒ™ã‚¯ãƒˆãƒ«ç©**: $H\mathbf{v}$ ã ã‘ãªã‚‰ $O(n)$ ã§è¨ˆç®—å¯èƒ½ï¼ˆForward-over-Reverse ADï¼‰
2. **å¯¾è§’è¿‘ä¼¼**: $H$ ã®å¯¾è§’è¦ç´ ã ã‘è¨ˆç®—ã€‚AdaGrad, Adam ãŒä½¿ã†
3. **ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼**: L-BFGS ãŒ $H^{-1}$ ã‚’ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼
4. **Fisheræƒ…å ±è¡Œåˆ—**: æœŸå¾…å€¤ãƒ˜ã‚·ã‚¢ãƒ³ã®è¿‘ä¼¼ã€‚Natural Gradient ã§ä½¿ç”¨

ãƒ˜ã‚·ã‚¢ãƒ³è‡ªä½“ã‚’é™½ã«è¨ˆç®—ã™ã‚‹ã®ã¯ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒæ•°åƒä»¥ä¸‹ã®å ´åˆã®ã¿ã€‚
:::

### 6.4 ã‚ˆãã‚ã‚‹é–“é•ã„

| é–“é•ã„ | æ­£ã—ã„ç†è§£ |
|:------|:---------|
| SVDã¯æ­£æ–¹è¡Œåˆ—ã«ã—ã‹ä½¿ãˆãªã„ | **ä»»æ„ã®** $m \times n$ è¡Œåˆ—ã«ä½¿ãˆã‚‹ |
| ç‰¹ç•°å€¤ã¯å›ºæœ‰å€¤ã¨åŒã˜ | ç‰¹ç•°å€¤ã¯ $A^\top A$ ã®å›ºæœ‰å€¤ã®**å¹³æ–¹æ ¹** |
| Backpropã¯è¿‘ä¼¼çš„ãªå‹¾é…è¨ˆç®— | Reverse Mode AD ã§ã‚ã‚Šã€**æ©Ÿæ¢°ç²¾åº¦ã§å³å¯†** |
| Forward Mode ãŒå¸¸ã«é…ã„ | $n \ll m$ ãªã‚‰ Forward Mode ã®æ–¹ãŒåŠ¹ç‡çš„ |
| `pinv` ã¯ `inv` ã®ä»£ã‚ã‚Šã«ä½¿ãˆã‚‹ | `pinv` ã¯æœ€å°äºŒä¹—è§£ã‚’è¿”ã™ã€‚æ­£å‰‡è¡Œåˆ—ãªã‚‰ `solve` ã‚’ä½¿ã† |
| å‹¾é…ã¯å¸¸ã«å‹¾é…é™ä¸‹ã®æ–¹å‘ã‚’ç¤ºã™ | å‹¾é…ã¯æœ€æ€¥**ä¸Šæ˜‡**æ–¹å‘ã€‚$-\nabla f$ ãŒé™ä¸‹æ–¹å‘ |
| Softmaxã®å‹¾é…ã¯è¤‡é›‘ | Cross-Entropyã¨çµ„ã¿åˆã‚ã›ã‚‹ã¨ $\mathbf{s} - \mathbf{y}$ ã¨ã‚·ãƒ³ãƒ—ãƒ«ã« |

### 6.5 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ1é€±é–“ãƒ—ãƒ©ãƒ³ï¼‰

| æ—¥ | å†…å®¹ | æ‰€è¦æ™‚é–“ |
|:---|:-----|:--------|
| Day 1 | Zone 0-2 é€šèª­ | 30åˆ† |
| Day 2 | Zone 3 å‰åŠï¼ˆ3.1-3.5: SVDï¼‰ | 45åˆ† |
| Day 3 | Zone 3 ä¸­ç›¤ï¼ˆ3.6-3.8: ãƒ†ãƒ³ã‚½ãƒ«ãƒ»è¡Œåˆ—å¾®åˆ†ï¼‰ | 45åˆ† |
| Day 4 | Zone 3 å¾ŒåŠï¼ˆ3.9-3.12: é€£é–å¾‹ãƒ»è‡ªå‹•å¾®åˆ†ãƒ»Boss Battleï¼‰ | 60åˆ† |
| Day 5 | Zone 4ï¼ˆå®Ÿè£…ï¼‰ | 45åˆ† |
| Day 6 | Zone 5ï¼ˆãƒ†ã‚¹ãƒˆï¼‰ | 30åˆ† |
| Day 7 | å¾©ç¿’: 2Ã—2è¡Œåˆ—ã®SVDã‚’æ‰‹è¨ˆç®— + è‡ªå·±ãƒã‚§ãƒƒã‚¯ | 30åˆ† |

### 6.6 é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼

```python
"""ç¬¬3å› ç·šå½¢ä»£æ•° II ã®å­¦ç¿’é€²æ—ãƒã‚§ãƒƒã‚«ãƒ¼"""

topics = {
    "SVDã®å®šç¾©ã¨å­˜åœ¨å®šç†": False,
    "Eckart-Youngå®šç†": False,
    "ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ã®å¿œç”¨": False,
    "æ“¬ä¼¼é€†è¡Œåˆ— (Moore-Penrose)": False,
    "PCA ã® SVD ã«ã‚ˆã‚‹å°å‡º": False,
    "Kroneckerç©": False,
    "Einsteinè¨˜æ³• / einsum": False,
    "å‹¾é…ãƒ»ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ãƒ»ãƒ˜ã‚·ã‚¢ãƒ³": False,
    "Matrix Cookbook ä¸»è¦å…¬å¼": False,
    "é€£é–å¾‹ (ãƒ™ã‚¯ãƒˆãƒ«ç‰ˆ)": False,
    "Backpropagation å®Œå…¨å°å‡º": False,
    "Forward Mode AD (Dual Numbers)": False,
    "Reverse Mode AD": False,
    "Transformer 1å±¤ã®å®Œå…¨å¾®åˆ†": False,
    "æ•°å€¤å®‰å®šæ€§ / Log-Sum-Exp": False,
}

completed = sum(topics.values())
total = len(topics)
print(f"=== ç¬¬3å› é€²æ—: {completed}/{total} ({100*completed/total:.0f}%) ===")
for topic, done in topics.items():
    mark = "âœ“" if done else " "
    print(f"  [{mark}] {topic}")

if completed == total:
    print("\nç¬¬3å› å®Œå…¨ã‚¯ãƒªã‚¢ï¼ ç¬¬4å›ï¼ˆç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦ï¼‰ã¸é€²ã‚‚ã†ã€‚")
elif completed >= total * 0.7:
    print("\nã‚ˆãã§ããŸã€‚æ®‹ã‚Šã¯ç¬¬4å›ã‚’èª­ã‚“ã å¾Œã«æˆ»ã£ã¦ç¢ºèªã—ã‚ˆã†ã€‚")
else:
    print("\nZone 3 ã‚’ä¸­å¿ƒã«ã‚‚ã†ä¸€åº¦å¾©ç¿’ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã™ã‚‹ã€‚")
```

### 6.7 æ¬¡å›äºˆå‘Š: ç¬¬4å›ã€Œç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦ã€

ç¬¬4å›ã§ã¯ã€ç¢ºç‡åˆ†å¸ƒã®è¨˜è¿°ãƒ»æ“ä½œãƒ»æ¨å®šã‚’å®Œå…¨ã«ç¿’å¾—ã™ã‚‹:

1. **ç¢ºç‡ç©ºé–“ã®å®šç¾©** â€” Kolmogorovã®å…¬ç†ã‹ã‚‰ç¢ºç‡åˆ†å¸ƒã¾ã§
2. **ãƒ™ã‚¤ã‚ºã®å®šç†** â€” äº‹å‰åˆ†å¸ƒ Ã— å°¤åº¦ â†’ äº‹å¾Œåˆ†å¸ƒ
3. **æŒ‡æ•°å‹åˆ†å¸ƒæ—** â€” æ­£è¦åˆ†å¸ƒã‚‚ãƒã‚¢ã‚½ãƒ³ã‚‚Bernoulliã‚‚çµ±ä¸€çš„ã«ç†è§£
4. **æœ€å°¤æ¨å®šï¼ˆMLEï¼‰** â€” æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ = ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«åˆ†å¸ƒã®MLE

**ã‚­ãƒ¼ã¨ãªã‚‹LLM/Transformeræ¥ç‚¹**:
- æ¡ä»¶ä»˜ãç¢ºç‡ â†’ è‡ªå·±å›å¸° $p(x_t \mid x_{<t})$
- Softmax â†’ ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«åˆ†å¸ƒ
- Cross-Entropy â†’ å¯¾æ•°å°¤åº¦ã®è² 

> **ç¬¬3å›ã®é™ç•Œ**: è¡Œåˆ—ã‚’ã€Œåˆ†è§£ã€ã—ã€Œå¾®åˆ†ã€ã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸã€‚ã ãŒã€ãƒ‡ãƒ¼ã‚¿ã®ã€Œä¸ç¢ºå®Ÿæ€§ã€ã‚’æ‰±ã†ã«ã¯ç¢ºç‡è«–ãŒå¿…è¦ã€‚ã€Œã“ã®äºˆæ¸¬ã¯ã©ã®ãã‚‰ã„ç¢ºã‹ã‚‰ã—ã„ã®ã‹ã€ã‚’æ•°å­¦çš„ã«è¨˜è¿°ã™ã‚‹é“å…·ãŒã€ç¬¬4å›ã§æ‰‹ã«å…¥ã‚‹ã€‚

:::message
**é€²æ—: 100% å®Œäº†!** ç¬¬3å›ã€Œç·šå½¢ä»£æ•° II: SVDãƒ»è¡Œåˆ—å¾®åˆ†ãƒ»ãƒ†ãƒ³ã‚½ãƒ«ã€ã‚’å®Œèµ°ã—ãŸã€‚SVDã®å­˜åœ¨å®šç†ãƒ»Eckart-Youngå®šç†ã‹ã‚‰å§‹ã¾ã‚Šã€ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—ãƒ»Einsteinè¨˜æ³•ã€è¡Œåˆ—å¾®åˆ†ã€é€£é–å¾‹ã€Backpropagationã€è‡ªå‹•å¾®åˆ†ã‚’çµŒã¦ã€Transformer 1å±¤ã®å®Œå…¨å¾®åˆ†ã‚’å°å‡ºã—ãŸã€‚ãŠç–²ã‚Œã•ã¾ã§ã—ãŸã€‚
:::

### 6.8 ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **SVDã¯ä¸‡èƒ½ãƒŠã‚¤ãƒ•ã€‚ç”»åƒåœ§ç¸®ã‚‚PCAã‚‚æ¨è–¦ã‚‚LoRAã‚‚ã€å…¨ã¦ã€ŒåŒã˜è¨ˆç®—ã€ã«å¸°ç€ã™ã‚‹ã®ã§ã¯ï¼Ÿ**

ã“ã®å•ã„ã®æ„å‘³ã‚’è€ƒãˆã¦ã¿ã¦ã»ã—ã„ã€‚

ä¸€è¦‹ã™ã‚‹ã¨å…¨ãç•°ãªã‚‹å•é¡Œ â€” ç”»åƒã‚’åœ§ç¸®ã™ã‚‹ã€ãƒ‡ãƒ¼ã‚¿ã®æ¬¡å…ƒã‚’å‰Šæ¸›ã™ã‚‹ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¥½ã¿ã‚’äºˆæ¸¬ã™ã‚‹ã€LLMã‚’åŠ¹ç‡çš„ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ â€” ãŒã€å…¨ã¦ã€Œè¡Œåˆ—ã‚’ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ã™ã‚‹ã€ã¨ã„ã†åŒä¸€ã®æ•°å­¦çš„æ“ä½œã«å¸°ç€ã™ã‚‹ã€‚

ã“ã‚Œã¯å¶ç„¶ã§ã¯ãªã„ã€‚**å®Ÿä¸–ç•Œã®ãƒ‡ãƒ¼ã‚¿ã®å¤šãã¯æœ¬è³ªçš„ã«ä½æ¬¡å…ƒ**ã ã‹ã‚‰ã ã€‚100ä¸‡ç”»ç´ ã®ç”»åƒã‚‚ã€æ•°ç™¾ã®ç‰¹ç•°å€¤ã§ååˆ†ã«è¡¨ç¾ã§ãã‚‹ã€‚10å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®LLMã®é‡ã¿æ›´æ–°ã‚‚ã€rank-16ç¨‹åº¦ã§ååˆ†ãªå ´åˆãŒå¤šã„ã€‚

Eckart-Youngå®šç†ã¯ã€ã“ã®ä½æ¬¡å…ƒæ§‹é€ ã‚’ã€Œæœ€é©ã«ã€æŠ½å‡ºã™ã‚‹æ–¹æ³•ã‚’ä¿è¨¼ã™ã‚‹ã€‚SVDã¯ã€ãƒ‡ãƒ¼ã‚¿ã®æœ¬è³ªçš„ãªæ¬¡å…ƒã‚’è¦‹æŠœã**æ™®éçš„ãªé“å…·**ã ã€‚

:::details è­°è«–ãƒã‚¤ãƒ³ãƒˆ
1. **SVDã®é™ç•Œã¯ã©ã“ã«ã‚ã‚‹ã‹ï¼Ÿ** â€” SVDã¯ç·šå½¢éƒ¨åˆ†ç©ºé–“ã‚’è¦‹ã¤ã‘ã‚‹ã€‚ãƒ‡ãƒ¼ã‚¿ãŒéç·šå½¢å¤šæ§˜ä½“ä¸Šã«ã‚ã‚‹å ´åˆï¼ˆä¾‹: æ‰‹æ›¸ãæ•°å­—ã®ç”»åƒï¼‰ã€ã‚«ãƒ¼ãƒãƒ«PCAã‚„ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãªã©éç·šå½¢æ‰‹æ³•ãŒå¿…è¦ã€‚
2. **è‡ªå‹•å¾®åˆ†ã¯"çŸ¥èƒ½"ã®ä¸€éƒ¨ã‹ï¼Ÿ** â€” å­¦ç¿’ã¨ã¯å‹¾é…ã«å¾“ã£ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã™ã‚‹ã“ã¨ã€‚è‡ªå‹•å¾®åˆ†ãŒãªã‘ã‚Œã°æ·±å±¤å­¦ç¿’ã¯å­˜åœ¨ã—ãªã„ã€‚è¨ˆç®—ã®ã€Œå·»ãæˆ»ã—ã€ãŒå­¦ç¿’ã‚’å¯èƒ½ã«ã™ã‚‹ã¨ã„ã†äº‹å®Ÿã¯ã€çŸ¥èƒ½ã®æœ¬è³ªã«ä½•ã‚’ç¤ºå”†ã™ã‚‹ã‹ã€‚
3. **è¡Œåˆ—å¾®åˆ†ã®è¨ˆç®—é‡ã¯AIã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®é™ç•Œã‚’æ±ºã‚ã‚‹ã‹ï¼Ÿ** â€” Transformerã®å­¦ç¿’ã¯Attentionã®å¾®åˆ†ãŒæ”¯é…çš„ã€‚FlashAttention[^12]ã¯ã“ã®è¨ˆç®—ã‚’å†æ§‹æˆã—ã¦é«˜é€ŸåŒ–ã—ãŸã€‚è¡Œåˆ—å¾®åˆ†ã®åŠ¹ç‡åŒ–ãŒã€æ¬¡ä¸–ä»£AIã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’æ±ºå®šã™ã‚‹ã€‚
:::

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Å., & Polosukhin, I. (2017). Attention Is All You Need. *NeurIPS 2017*.
@[card](https://arxiv.org/abs/1706.03762)

[^2]: Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by back-propagating errors. *Nature*, 323, 533-536.
@[card](https://doi.org/10.1038/323533a0)

[^3]: Eckart, C. & Young, G. (1936). The Approximation of One Matrix by Another of Lower Rank. *Psychometrika*, 1, 211-218.
@[card](https://doi.org/10.1007/BF02288367)

[^5]: Pearson, K. (1901). On Lines and Planes of Closest Fit to Systems of Points in Space. *Philosophical Magazine*, 2(11), 559-572.
@[card](https://doi.org/10.1080/14786440109462720)

[^6]: Hotelling, H. (1933). Analysis of a complex of statistical variables into principal components. *Journal of Educational Psychology*, 24(6), 417-441.
@[card](https://doi.org/10.1037/h0071325)

[^7]: Baydin, A. G., Pearlmutter, B. A., Radul, A. A., & Siskind, J. M. (2018). Automatic Differentiation in Machine Learning: a Survey. *JMLR*, 18(153), 1-43.
@[card](https://arxiv.org/abs/1502.05767)

[^10]: Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W. (2022). LoRA: Low-Rank Adaptation of Large Language Models. *ICLR 2022*.
@[card](https://arxiv.org/abs/2106.09685)

[^12]: Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & RÃ©, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. *NeurIPS 2022*.
@[card](https://arxiv.org/abs/2205.14135)

[^13]: Rezende, D. J. & Mohamed, S. (2015). Variational Inference with Normalizing Flows. *ICML 2015*.
@[card](https://arxiv.org/abs/1505.05770)

### æ•™ç§‘æ›¸

[^8]: Griewank, A. & Walther, A. (2008). *Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation* (2nd ed.). SIAM.

[^9]: Petersen, K. B. & Pedersen, M. S. (2012). *The Matrix Cookbook*. Technical Report, DTU.

---

## è¨˜æ³•è¦ç´„

| è¨˜å· | æ„å‘³ | åˆå‡º |
|:-----|:-----|:-----|
| $A, B, W$ | è¡Œåˆ—ï¼ˆå¤§æ–‡å­—ï¼‰ | 3.1 |
| $\mathbf{x}, \mathbf{v}$ | ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆå¤ªå­—å°æ–‡å­—ï¼‰ | 3.7 |
| $\sigma_i$ | ç‰¹ç•°å€¤ | 3.1 |
| $\mathbf{u}_i, \mathbf{v}_i$ | å·¦/å³ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ« | 3.1 |
| $U, \Sigma, V$ | SVDã®æ§‹æˆè¡Œåˆ— | 3.1 |
| $A^+$ | Moore-Penroseæ“¬ä¼¼é€†è¡Œåˆ— | 3.4 |
| $A_k$ | rank-$k$ æˆªæ–­SVD | 3.2 |
| $\nabla f$ | å‹¾é… | 3.7 |
| $J$ | ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ | 3.7 |
| $H$ | ãƒ˜ã‚·ã‚¢ãƒ³ | 3.7 |
| $\boldsymbol{\delta}_l$ | ç¬¬$l$å±¤ã®èª¤å·®ä¿¡å· | 3.9 |
| $\otimes$ | Kroneckerç© | 3.6 |
| $\odot$ | Hadamardç©ï¼ˆè¦ç´ ã”ã¨ã®ç©ï¼‰ | 3.9 |
| $\text{vec}(A)$ | è¡Œåˆ—ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ– | 3.6 |
| $\kappa(A)$ | æ¡ä»¶æ•° | 4.4 |

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
