---
title: "ç¬¬20å›: VAE/GAN/Transformerãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯å®Ÿè£… & åˆ†æ•£ã‚µãƒ¼ãƒ“ãƒ³ã‚°: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸ”¥"
type: "tech"
topics: ["machinelearning", "deeplearning", "julia", "rust", "elixir"]
published: true
---

:::message
**å‰æçŸ¥è­˜**: ç¬¬19å›ã§3è¨€èªç’°å¢ƒã¨FFIãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰æ¸ˆã¿ã€‚Course IIã§VAE/GAN/Transformerã®ç†è«–ã‚’ç¿’å¾—æ¸ˆã¿ã€‚
**ç›®æ¨™**: ç†è«–ã‚’3è¨€èªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆJuliaè¨“ç·´â†’Rustæ¨è«–â†’Elixiré…ä¿¡ï¼‰ã§å®Ÿè£…ã™ã‚‹ã€‚
**é€²æ—**: å…¨ä½“ã®80%å®Œäº†
:::

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” ç†è«–â†’å®Ÿè£…ã®1è¡Œå¯¾å¿œ

ç¬¬19å›ã§ç’°å¢ƒã‚’æ•´ãˆãŸã€‚ç¬¬10å›ã§VAEã€ç¬¬12å›ã§GANã€ç¬¬16å›ã§Transformerã®**ç†è«–**ã‚’å­¦ã‚“ã ã€‚ä»Šå›ã¯ãã‚Œã‚’**å‹•ã‹ã™**ã€‚

ç†è«–ã¨å®Ÿè£…ã®å¯¾å¿œã‚’ä½“æ„Ÿã—ã‚ˆã†ã€‚VAEã®ELBOã‚’1è¡Œã§ï¼š

```julia
using Lux, Optimisers, Random

# VAE ELBO = å†æ§‹æˆé … - KLæ­£å‰‡åŒ–é …
function elbo_loss(encoder, decoder, ps_enc, ps_dec, st_enc, st_dec, x)
    # Encoder: q_Ï†(z|x) â†’ (Î¼, log_ÏƒÂ²)
    (Î¼, logÏƒÂ²), st_enc = encoder(x, ps_enc, st_enc)

    # Reparameterization: z = Î¼ + ÏƒâŠ™Îµ
    Îµ = randn(Float32, size(Î¼)...)
    Ïƒ = exp.(logÏƒÂ² ./ 2)
    z = Î¼ .+ Ïƒ .* Îµ

    # Decoder: p_Î¸(x|z) â†’ xÌ‚
    xÌ‚, st_dec = decoder(z, ps_dec, st_dec)

    # ELBO = ğ”¼[log p(x|z)] - KL[q(z|x) || p(z)]
    recon = -sum((x .- xÌ‚).^2) / size(x, 2)  # å†æ§‹æˆé …ï¼ˆã‚¬ã‚¦ã‚¹å°¤åº¦ï¼‰
    kl = -0.5f0 * sum(1 .+ logÏƒÂ² .- Î¼.^2 .- exp.(logÏƒÂ²)) / size(x, 2)  # KLç™ºæ•£

    return -(recon - kl), (st_enc, st_dec)  # ELBOã‚’æœ€å¤§åŒ– = è² ã®ELBOã‚’æœ€å°åŒ–
end
```

**ã“ã®30è¡ŒãŒç¬¬10å›ã®æ•°å¼ã‚’ã™ã¹ã¦å«ã‚€**ï¼š

$$
\mathcal{L}_{\text{ELBO}} = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{\text{KL}}[q_\phi(z|x) \| p(z)]
$$

- å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãƒˆãƒªãƒƒã‚¯: $z = \mu + \sigma \odot \epsilon$ï¼ˆ23è¡Œç›®ï¼‰
- ã‚¬ã‚¦ã‚¹KLé–‰å½¢å¼: $-\frac{1}{2}\sum(1 + \log\sigma^2 - \mu^2 - \sigma^2)$ï¼ˆ28è¡Œç›®ï¼‰
- æ•°å¼ã®å„é …ãŒã‚³ãƒ¼ãƒ‰ã®å„è¡Œã«**1:1å¯¾å¿œ**

ã“ã‚ŒãŒJuliaã®å¨åŠ›ã€‚æ•°å¼â†”ã‚³ãƒ¼ãƒ‰ã®è·é›¢ãŒã‚¼ãƒ­ã€‚

:::message
**é€²æ—**: å…¨ä½“ã®3%å®Œäº†ã€‚ç†è«–ã‚’å®Ÿè£…ã«ç¿»è¨³ã™ã‚‹æº–å‚™ãŒã§ããŸã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” 3ãƒ¢ãƒ‡ãƒ«ã‚’è§¦ã‚‹

ç†è«–ã‚’å¾©ç¿’ã—ãªãŒã‚‰ã€3ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’å‹•ã‹ã™ã€‚æ•°å¼â†’ã‚³ãƒ¼ãƒ‰å¯¾å¿œã‚’ä½“æ„Ÿã™ã‚‹ã€‚

### 1.1 VAE â€” æ½œåœ¨ç©ºé–“ã§ç”»åƒã‚’åœ§ç¸®ãƒ»å†æ§‹æˆ

ç¬¬10å›ã§å­¦ã‚“ã VAEã®æ ¸å¿ƒï¼š**è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ $x$ ã‚’ä½æ¬¡å…ƒæ½œåœ¨å¤‰æ•° $z$ ã«åœ§ç¸®ã—ã€ãã“ã‹ã‚‰å†æ§‹æˆã™ã‚‹**ã€‚

```julia
using Lux, MLUtils, MLDatasets, Optimisers

# MNIST ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
train_data = MNIST(split=:train)
x_train = Float32.(train_data.features) |> flatten_images  # (784, 60000)

# VAE ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
encoder = Chain(
    Dense(784 => 400, tanh),
    Dense(400 => 200, tanh),
    Dense(200 => 40)  # â†’ [Î¼(20æ¬¡å…ƒ), log_ÏƒÂ²(20æ¬¡å…ƒ)]
)

decoder = Chain(
    Dense(20 => 200, tanh),
    Dense(200 => 400, tanh),
    Dense(400 => 784, sigmoid)  # sigmoid for pixel values [0,1]
)

# è¨“ç·´ãƒ«ãƒ¼ãƒ—ï¼ˆç°¡ç•¥ç‰ˆï¼‰
opt = Adam(0.001f0)
ps_enc, st_enc = Lux.setup(Random.default_rng(), encoder)
ps_dec, st_dec = Lux.setup(Random.default_rng(), decoder)

for epoch in 1:10
    for batch in DataLoader((x_train,), batchsize=128, shuffle=true)
        x = batch[1]
        loss, grads = Lux.Training.compute_gradients(
            AutoZygote(), elbo_loss, encoder, decoder, ps_enc, ps_dec, st_enc, st_dec, x
        )
        ps_enc, ps_dec = Optimisers.update!(opt, (ps_enc, ps_dec), grads)
    end
    println("Epoch $epoch: loss = $(loss)")
end

# æ½œåœ¨ç©ºé–“ã§ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
z_random = randn(Float32, 20, 10)  # 10å€‹ã®ãƒ©ãƒ³ãƒ€ãƒ æ½œåœ¨ãƒ™ã‚¯ãƒˆãƒ«
x_generated, _ = decoder(z_random, ps_dec, st_dec)
# â†’ æ–°ã—ã„æ•°å­—ç”»åƒãŒç”Ÿæˆã•ã‚Œã‚‹
```

| æ•°å¼ | ã‚³ãƒ¼ãƒ‰ | æ„å‘³ |
|:-----|:-------|:-----|
| $q_\phi(z\|x) = \mathcal{N}(z; \mu_\phi(x), \sigma^2_\phi(x)\mathbf{I})$ | `(Î¼, logÏƒÂ²) = encoder(x)` | EncoderãŒå¹³å‡ã¨åˆ†æ•£ã‚’å‡ºåŠ› |
| $z = \mu + \sigma \odot \epsilon, \epsilon \sim \mathcal{N}(0, \mathbf{I})$ | `z = Î¼ .+ Ïƒ .* randn(...)` | å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãƒˆãƒªãƒƒã‚¯ |
| $p_\theta(x\|z) = \mathcal{N}(x; \mu_\theta(z), \mathbf{I})$ | `xÌ‚ = decoder(z)` | DecoderãŒå†æ§‹æˆç”»åƒã‚’å‡ºåŠ› |
| $D_{\text{KL}}[q_\phi(z\|x) \| \mathcal{N}(0, \mathbf{I})]$ | `-0.5 * sum(1 + logÏƒÂ² - Î¼Â² - exp(logÏƒÂ²))` | ã‚¬ã‚¦ã‚¹KLé–‰å½¢å¼ |

**ä½“æ„Ÿ**ï¼šæ½œåœ¨ç©ºé–“ $z \in \mathbb{R}^{20}$ ã§784æ¬¡å…ƒç”»åƒã‚’è¡¨ç¾ã€‚ç¬¬10å›ã®æ•°å¼ãŒãã®ã¾ã¾å‹•ãã€‚

---

### 1.2 GAN â€” ç”Ÿæˆå™¨ã¨è­˜åˆ¥å™¨ã®å¯¾æ±º

ç¬¬12å›ã§å­¦ã‚“ã GANã®æ ¸å¿ƒï¼š**Generator $G$ ãŒãƒã‚¤ã‚º $z$ ã‹ã‚‰å½ç”»åƒã‚’ç”Ÿæˆã—ã€Criticï¼ˆè­˜åˆ¥å™¨ï¼‰ $D$ ãŒæœ¬ç‰©/å½ç‰©ã‚’è¦‹åˆ†ã‘ã‚‹ç«¶äº‰**ã€‚

WGANã®æå¤±é–¢æ•°ï¼ˆç¬¬13å›ã§å­¦ã‚“ã Wassersteinè·é›¢ãƒ™ãƒ¼ã‚¹ï¼‰ï¼š

$$
\mathcal{L}_D = \mathbb{E}_{x \sim p_r}[D(x)] - \mathbb{E}_{z \sim p_z}[D(G(z))] - \lambda \mathbb{E}_{\hat{x}}[(\|\nabla_{\hat{x}} D(\hat{x})\|_2 - 1)^2]
$$

$$
\mathcal{L}_G = -\mathbb{E}_{z \sim p_z}[D(G(z))]
$$

```julia
# Generator: z (100æ¬¡å…ƒãƒã‚¤ã‚º) â†’ ç”»åƒ (28Ã—28)
generator = Chain(
    Dense(100 => 256, relu),
    Dense(256 => 512, relu),
    Dense(512 => 784, tanh)  # tanh for [-1, 1] pixel range
)

# Critic (WGAN-GPã§ã¯è­˜åˆ¥å™¨ã‚’"Critic"ã¨å‘¼ã¶)
critic = Chain(
    Dense(784 => 512, leakyrelu),
    Dense(512 => 256, leakyrelu),
    Dense(256 => 1)  # ã‚¹ã‚³ã‚¢å‡ºåŠ›ï¼ˆç¢ºç‡ã§ã¯ãªã„ï¼‰
)

# WGAN-GPè¨“ç·´ãƒ«ãƒ¼ãƒ—ï¼ˆç°¡ç•¥ç‰ˆï¼‰
function train_wgan_gp!(generator, critic, real_data, epochs=100, Î»_gp=10.0f0)
    opt_g = Adam(0.0001f0, (0.5f0, 0.9f0))  # Generator optimizer
    opt_c = Adam(0.0001f0, (0.5f0, 0.9f0))  # Critic optimizer

    for epoch in 1:epochs
        for batch in DataLoader((real_data,), batchsize=64, shuffle=true)
            x_real = batch[1]
            batch_size = size(x_real, 2)

            # --- Criticã‚’5å›æ›´æ–° ---
            for _ in 1:5
                z = randn(Float32, 100, batch_size)
                x_fake = generator(z, ps_g, st_g)[1]

                # Gradient Penalty è¨ˆç®—
                Î± = rand(Float32, 1, batch_size)
                x_interp = Î± .* x_real .+ (1 .- Î±) .* x_fake
                grad_interp = gradient(x -> sum(critic(x, ps_c, st_c)[1]), x_interp)[1]
                gp = mean((sqrt.(sum(grad_interp.^2, dims=1)) .- 1).^2)

                # Critic loss
                loss_c = mean(critic(x_fake, ps_c, st_c)[1]) - mean(critic(x_real, ps_c, st_c)[1]) + Î»_gp * gp
                ps_c = update!(opt_c, ps_c, gradient(loss_c, ps_c)[1])
            end

            # --- Generatorã‚’1å›æ›´æ–° ---
            z = randn(Float32, 100, batch_size)
            loss_g = -mean(critic(generator(z, ps_g, st_g)[1], ps_c, st_c)[1])
            ps_g = update!(opt_g, ps_g, gradient(loss_g, ps_g)[1])
        end
        println("Epoch $epoch: D_loss=$(loss_c), G_loss=$(loss_g)")
    end
end
```

| æ•°å¼ | ã‚³ãƒ¼ãƒ‰ | æ„å‘³ |
|:-----|:-------|:-----|
| $G(z)$ | `generator(z)` | ãƒã‚¤ã‚ºâ†’å½ç”»åƒ |
| $D(x)$ | `critic(x)` | ç”»åƒâ†’ã‚¹ã‚³ã‚¢ |
| $\hat{x} = \alpha x + (1-\alpha)G(z)$ | `x_interp = Î± .* x_real .+ (1 .- Î±) .* x_fake` | æœ¬ç‰©ã¨å½ç‰©ã®è£œé–“ |
| $\|\nabla_{\hat{x}} D(\hat{x})\|_2$ | `sqrt(sum(grad_interp.^2, dims=1))` | å‹¾é…ãƒãƒ«ãƒ  |
| $(\|\nabla_{\hat{x}} D(\hat{x})\|_2 - 1)^2$ | `(sqrt(...) .- 1).^2` | Gradient Penalty |

**ä½“æ„Ÿ**ï¼šCriticã‚’5å›ã€Generatorã‚’1å›æ›´æ–°ï¼ˆWGAN-GPæ¨å¥¨æ¯”ç‡ï¼‰ã€‚ç¬¬12å›ãƒ»ç¬¬13å›ã®æ•°å¼ãŒãã®ã¾ã¾å‹•ãã€‚

---

### 1.3 Transformer â€” Attentionã§æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬

ç¬¬16å›ã§å­¦ã‚“ã Transformerã®æ ¸å¿ƒï¼š**Multi-Head Attentionã§æ–‡è„ˆã‚’ä¸¦åˆ—å‡¦ç†ã—ã€æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³ã‚’äºˆæ¸¬**ã€‚

Scaled Dot-Product Attentionã®æ•°å¼ï¼š

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
$$

```julia
using Lux, NNlib

# Multi-Head Attention ãƒ¬ã‚¤ãƒ¤ãƒ¼
struct MultiHeadAttention <: Lux.AbstractExplicitLayer
    num_heads::Int
    d_model::Int
    d_k::Int
    q_proj::Dense
    k_proj::Dense
    v_proj::Dense
    o_proj::Dense
end

function MultiHeadAttention(d_model::Int, num_heads::Int)
    d_k = d_model Ã· num_heads
    return MultiHeadAttention(
        num_heads, d_model, d_k,
        Dense(d_model => d_model),  # Q projection
        Dense(d_model => d_model),  # K projection
        Dense(d_model => d_model),  # V projection
        Dense(d_model => d_model)   # Output projection
    )
end

function (mha::MultiHeadAttention)(x, ps, st)
    batch_size, seq_len, _ = size(x)

    # Q, K, V projection
    Q, st_q = mha.q_proj(x, ps.q_proj, st.q_proj)
    K, st_k = mha.k_proj(x, ps.k_proj, st.k_proj)
    V, st_v = mha.v_proj(x, ps.v_proj, st.v_proj)

    # Reshape for multi-head: (batch, seq_len, d_model) â†’ (batch, num_heads, seq_len, d_k)
    Q = reshape(Q, batch_size, mha.num_heads, seq_len, mha.d_k) |> permutedims([1,2,4,3])
    K = reshape(K, batch_size, mha.num_heads, seq_len, mha.d_k) |> permutedims([1,2,4,3])
    V = reshape(V, batch_size, mha.num_heads, seq_len, mha.d_k) |> permutedims([1,2,4,3])

    # Scaled Dot-Product Attention: softmax(QK^T / âˆšd_k) V
    scores = batched_mul(Q, batched_transpose(K)) ./ sqrt(Float32(mha.d_k))  # (batch, heads, seq, seq)
    attn_weights = softmax(scores, dims=4)  # Softmax over key dimension
    out = batched_mul(attn_weights, V)  # (batch, heads, d_k, seq)

    # Concatenate heads and project
    out = permutedims(out, [1,4,2,3]) |> x -> reshape(x, batch_size, seq_len, mha.d_model)
    out, st_o = mha.o_proj(out, ps.o_proj, st.o_proj)

    return out, (st_q=st_q, st_k=st_k, st_v=st_v, st_o=st_o)
end

# Causal Maskï¼ˆæœªæ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¦‹ã›ãªã„ï¼‰
function causal_mask(seq_len)
    mask = triu(ones(Float32, seq_len, seq_len), 1)  # ä¸Šä¸‰è§’è¡Œåˆ—
    return mask .* -Inf32  # Softmaxå‰ã«åŠ ç®— â†’ æœªæ¥ãƒˆãƒ¼ã‚¯ãƒ³ã®é‡ã¿ã‚’0ã«
end

# ä½¿ç”¨ä¾‹
x = randn(Float32, 2, 10, 512)  # (batch=2, seq_len=10, d_model=512)
mha = MultiHeadAttention(512, 8)
ps, st = Lux.setup(Random.default_rng(), mha)
y, st = mha(x, ps, st)  # y: (2, 10, 512) â€” å„ãƒˆãƒ¼ã‚¯ãƒ³ã®æ–°ã—ã„è¡¨ç¾
```

| æ•°å¼ | ã‚³ãƒ¼ãƒ‰ | æ„å‘³ |
|:-----|:-------|:-----|
| $Q, K, V = xW_Q, xW_K, xW_V$ | `Q = mha.q_proj(x)` | ç·šå½¢å¤‰æ› |
| $\frac{QK^\top}{\sqrt{d_k}}$ | `scores = Q @ K.T / sqrt(d_k)` | ã‚¹ã‚³ã‚¢è¨ˆç®— |
| $\text{softmax}(\cdot)$ | `softmax(scores, dims=4)` | æ³¨æ„é‡ã¿æ­£è¦åŒ– |
| $\text{softmax}(\cdot)V$ | `attn_weights @ V` | åŠ é‡å’Œ |
| Causal Mask | `scores + causal_mask` | æœªæ¥ã‚’è¦‹ã›ãªã„ |

**ä½“æ„Ÿ**ï¼šMulti-Head AttentionãŒä¸¦åˆ—ã«è¤‡æ•°ã®è¦–ç‚¹ã§æ–‡è„ˆã‚’æ‰ãˆã‚‹ã€‚ç¬¬16å›ã®æ•°å¼ãŒãã®ã¾ã¾å‹•ãã€‚

---

### 1.4 æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œã®å®Œå…¨æ€§

3ãƒ¢ãƒ‡ãƒ«ã§å…±é€šã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼š

```julia
# æ•°å¼: ğ”¼[f(z)] where z ~ q(z)
# ã‚³ãƒ¼ãƒ‰: mean(f(z) for z in sample(q, n_samples))

# æ•°å¼: âˆ‡_Î¸ L(Î¸)
# ã‚³ãƒ¼ãƒ‰: gradient(Î¸ -> L(Î¸), Î¸)

# æ•°å¼: Î¸ â† Î¸ - Î·âˆ‡_Î¸ L
# ã‚³ãƒ¼ãƒ‰: Î¸ = update!(optimizer, Î¸, grads)
```

Juliaã®åˆ©ç‚¹ï¼š
- `.=` broadcastæ¼”ç®—å­ â†’ è¦ç´ ã”ã¨ã®æ¼”ç®—ã‚’1è¡Œã§
- `|>` pipeæ¼”ç®—å­ â†’ ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æ˜ç¤º
- å‹å®‰å®šæ€§ â†’ `@code_warntype`ã§å‹æ¨è«–ãƒã‚§ãƒƒã‚¯ â†’ è‡ªå‹•æœ€é©åŒ–

æ¬¡ã®Zone 2ã§ã€ãªãœã“ã®3ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè£…ã™ã‚‹ã®ã‹ã€å…¨ä½“åƒã‚’è¦‹ã‚‹ã€‚

:::message
**é€²æ—**: å…¨ä½“ã®10%å®Œäº†ã€‚3ãƒ¢ãƒ‡ãƒ«ã®å‹•ä½œã‚’ä½“æ„Ÿã—ãŸã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœã“ã®3ãƒ¢ãƒ‡ãƒ«ã‹

### 2.1 Course IIIã®ä½ç½®ã¥ã‘ â€” ç†è«–â†’å®Ÿè£…ã®æ©‹æ¸¡ã—

```mermaid
graph TD
    A[Course I: æ•°å­¦åŸºç¤<br>ç¬¬1-8å›] --> B[Course II: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«–<br>ç¬¬9-16å›]
    B --> C[Course III: å®Ÿè£…ç·¨<br>ç¬¬17-24å›]
    C --> D[Course IV: Diffusionç†è«–<br>ç¬¬25-32å›]
    D --> E[Course V: å¿œç”¨ãƒ»æœ€å‰ç·š<br>ç¬¬33-40å›]

    B -->|ç†è«–| F[VAE ç¬¬10å›]
    B -->|ç†è«–| G[GAN ç¬¬12å›]
    B -->|ç†è«–| H[Transformer ç¬¬16å›]

    F --> I[VAEå®Ÿè£… ç¬¬20å›<br>ä»Šã‚³ã‚³]
    G --> I
    H --> I

    style I fill:#ff6b6b,stroke:#333,stroke-width:4px
```

**Course IIã§å­¦ã‚“ã ã“ã¨**ï¼ˆç†è«–ï¼‰ï¼š
- ç¬¬10å›ï¼šVAEã®ELBOå°å‡ºã€å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãƒˆãƒªãƒƒã‚¯ã€ã‚¬ã‚¦ã‚¹KLé–‰å½¢å¼
- ç¬¬12å›ï¼šGANã®Minimaxæå¤±ã€JSDã€Mode Collapseå•é¡Œ
- ç¬¬13å›ï¼šOptimal Transportã¨Wassersteinè·é›¢ã€WGAN-GP
- ç¬¬16å›ï¼šTransformerã®Attentionæ©Ÿæ§‹ã€Positional Encodingã€Causal Mask

**Course IIIã§å­¦ã¶ã“ã¨**ï¼ˆå®Ÿè£…ï¼‰ï¼š
- ç¬¬19å›ï¼š3è¨€èªç’°å¢ƒæ§‹ç¯‰ï¼ˆJulia/Rust/Elixirï¼‰ã€FFIãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­è¨ˆ
- **ç¬¬20å›ï¼ˆä»Šå›ï¼‰**ï¼šVAE/GAN/Transformerã®å®Œå…¨å®Ÿè£…ã€æ•°å¼â†”ã‚³ãƒ¼ãƒ‰1:1å¯¾å¿œ
- ç¬¬21å›ï¼šãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹åŸºç¤ã€HuggingFace Datasetsçµ±åˆ
- ç¬¬22å›ï¼šè©•ä¾¡æŒ‡æ¨™å®Ÿè£…ï¼ˆFID/IS/Perplexityï¼‰ã€ãƒ¢ãƒ‡ãƒ«é¸æŠ

**ä»Šå›ã®å·®åˆ¥åŒ–**ï¼š
- **æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤ã¨ã®é•ã„**ï¼šå½¼ã‚‰ã¯ç†è«–è¬›ç¾©ã§å®Ÿè£…ã¯èª²é¡Œã€‚æœ¬ã‚·ãƒªãƒ¼ã‚ºã¯**ç†è«–ã¨å®Ÿè£…ã®ä¸¡æ–¹ã‚’ç¶²ç¾…**ã€‚
- **ä»–ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã¨ã®é•ã„**ï¼šPyTorch/TensorFlowã«é–‰ã˜ãªã„ã€‚**3è¨€èªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**ï¼ˆè¨“ç·´â†’æ¨è«–â†’é…ä¿¡ï¼‰ã§å®Ÿæˆ¦ã‚¹ã‚­ãƒ«ã‚’ç²å¾—ã€‚
- **è«–æ–‡å®Ÿè£…ã¨ã®é•ã„**ï¼šå˜ãªã‚‹å†™çµŒã§ã¯ãªã„ã€‚**æ•°å¼ã®å„é …ã¨ã‚³ãƒ¼ãƒ‰ã®å„è¡Œã‚’1:1å¯¾å¿œ**ã•ã›ã€ç†è§£ã‚’æ·±ã‚ã‚‹ã€‚

---

### 2.2 ãªãœVAE/GAN/Transformerã‹ â€” 3å¤§ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã®ä»£è¡¨

| ãƒ¢ãƒ‡ãƒ« | ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ  | è¨“ç·´æ–¹æ³• | å¼·ã¿ | å¼±ã¿ |
|:-------|:-----------|:---------|:-----|:-----|
| **VAE** | å°¤åº¦ãƒ™ãƒ¼ã‚¹ï¼ˆæ˜ç¤ºçš„å¯†åº¦ï¼‰ | ELBOæœ€å¤§åŒ– | å®‰å®šè¨“ç·´ã€æ½œåœ¨ç©ºé–“è§£é‡ˆå¯èƒ½ | ã¼ã‚„ã‘ãŸç”Ÿæˆã€è¡¨ç¾åŠ›åˆ¶ç´„ |
| **GAN** | æš—é»™çš„å¯†åº¦ï¼ˆAdversarialï¼‰ | Minimaxç«¶äº‰ | é®®æ˜ãªç”Ÿæˆã€é«˜å“è³ªç”»åƒ | è¨“ç·´ä¸å®‰å®šã€Mode Collapse |
| **Transformer** | è‡ªå·±å›å¸°ï¼ˆæ˜ç¤ºçš„å¯†åº¦ï¼‰ | æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬MLE | ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€æ±ç”¨æ€§ | é€æ¬¡ç”Ÿæˆã€è¨ˆç®—ã‚³ã‚¹ãƒˆ |

**3ã¤ã®ç”Ÿæˆãƒ‘ãƒ©ãƒ€ã‚¤ãƒ **ï¼ˆç¬¬7å›ã§å­¦ã‚“ã åˆ†é¡ï¼‰ï¼š

```mermaid
graph TD
    A[ç”Ÿæˆãƒ¢ãƒ‡ãƒ«] --> B[æ˜ç¤ºçš„å¯†åº¦ p_Î¸ x]
    A --> C[æš—é»™çš„å¯†åº¦]

    B --> D[Tractable<br>è¨ˆç®—å¯èƒ½]
    B --> E[Approximate<br>è¿‘ä¼¼æ¨è«–]

    D --> F[è‡ªå·±å›å¸°<br>Transformer]
    D --> G[Flow<br>RealNVP]

    E --> H[å¤‰åˆ†æ¨è«–<br>VAE]
    E --> I[ãƒãƒ«ã‚³ãƒ•é€£é–<br>Diffusion]

    C --> J[GANãƒ•ã‚¡ãƒŸãƒªãƒ¼<br>WGAN-GP/StyleGAN]
    C --> K[Implicit MLE<br>Noise Contrastive]

    style F fill:#4ecdc4
    style H fill:#ff6b6b
    style J fill:#ffe66d
```

**ãªãœã“ã®3ã¤ã‚’é¸ã‚“ã ã‹**ï¼š
1. **VAE**ï¼šå¤‰åˆ†æ¨è«–ã®å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¿’å¾—ã€‚ELBOã¯ä»–ã®å¤šãã®ãƒ¢ãƒ‡ãƒ«ï¼ˆDiffusion/Flowï¼‰ã®åŸºç¤ã€‚
2. **GAN**ï¼šAdversarialè¨“ç·´ã®å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¿’å¾—ã€‚å®‰å®šåŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ï¼ˆWGAN-GPï¼‰ã¯å¿…é ˆã‚¹ã‚­ãƒ«ã€‚
3. **Transformer**ï¼šè‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¿’å¾—ã€‚LLMæ™‚ä»£ã®å¿…é ˆçŸ¥è­˜ã€‚KV-Cacheã¯æ¨è«–åŠ¹ç‡åŒ–ã®éµã€‚

ã“ã‚Œã‚‰3ã¤ã‚’ãƒã‚¹ã‚¿ãƒ¼ã™ã‚Œã°ã€ä»–ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆDiffusion/Flow/VQ-VAEï¼‰ã®å®Ÿè£…ã‚‚ç†è§£ã§ãã‚‹ã€‚

---

### 2.3 3è¨€èªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å…¨ä½“åƒ

```mermaid
graph LR
    A[Julia<br>Lux.jl] -->|è¨“ç·´| B[ãƒ¢ãƒ‡ãƒ«<br>VAE/GAN/Trans]
    B -->|ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ| C[safetensors/<br>ONNX]
    C -->|ãƒ­ãƒ¼ãƒ‰| D[Rust<br>Candle]
    D -->|æ¨è«–| E[ãƒãƒƒãƒå‡¦ç†<br>ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼]
    E -->|FFI| F[Elixir<br>Broadway]
    F -->|é…ä¿¡| G[åˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ <br>è€éšœå®³æ€§]

    style A fill:#9b59b6
    style D fill:#e67e22
    style F fill:#3498db
```

**å„è¨€èªã®å½¹å‰²**ï¼ˆç¬¬19å›ã§è¨­è¨ˆï¼‰ï¼š

| æ®µéš | è¨€èª | ç†ç”± | ãƒ„ãƒ¼ãƒ« |
|:-----|:-----|:-----|:-------|
| è¨“ç·´ | âš¡ Julia | æ•°å¼â†”ã‚³ãƒ¼ãƒ‰1:1ã€JITé«˜é€ŸåŒ–ã€REPLãƒ«ãƒ¼ãƒ— | Lux.jl, Reactant |
| æ¨è«– | ğŸ¦€ Rust | ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ã€å‹å®‰å…¨ã€ä¸¦åˆ—å‡¦ç†ã€C-ABI FFI | Candle, ndarray |
| é…ä¿¡ | ğŸ”® Elixir | è€éšœå®³æ€§ã€ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼ã€ç›£è¦–ãƒ„ãƒªãƒ¼ | GenStage, Broadway |

**ãªãœ3è¨€èªã‹**ï¼š
- **Python 1è¨€èªã§ã¯ä¸å¯èƒ½**ï¼šGILãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã€ãƒ¡ãƒ¢ãƒªã‚³ãƒ”ãƒ¼ã€å‹å®‰å…¨æ€§æ¬ å¦‚ã€è€éšœå®³æ€§å¼±ã„
- **PyTorchã ã‘ã§ã¯ä¸ååˆ†**ï¼šè¨“ç·´ã¯å¾—æ„ã ãŒã€æ¨è«–æœ€é©åŒ–ãƒ»åˆ†æ•£é…ä¿¡ã¯è‹¦æ‰‹
- **å„è¨€èªãŒæœ€é©é ˜åŸŸã‚’æ‹…å½“**ï¼šJuliaï¼ˆè¨“ç·´ï¼‰ã€Rustï¼ˆæ¨è«–ï¼‰ã€Elixirï¼ˆé…ä¿¡ï¼‰ã®åˆ†æ¥­ã§ã€å„æ®µéšã§æœ€é«˜æ€§èƒ½ã‚’é”æˆ

**ä»Šå›ã®å®Ÿè£…ç¯„å›²**ï¼š
- Zone 3ï¼ˆæ•°å¼ä¿®è¡Œï¼‰ï¼šVAE/GAN/Transformerã®Juliaè¨“ç·´å®Ÿè£…ã€æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å®Œå…¨å¯¾å¿œ
- Zone 4ï¼ˆå®Ÿè£…ï¼‰ï¼šRustæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã€Candleã§ã®ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ãƒ»ãƒãƒƒãƒå‡¦ç†
- Zone 5ï¼ˆå®Ÿé¨“ï¼‰ï¼šElixiråˆ†æ•£ã‚µãƒ¼ãƒ“ãƒ³ã‚°ã€Broadwayéœ€è¦é§†å‹•ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€è€éšœå®³æ€§ãƒ‡ãƒ¢

---

### 2.4 Trojan Horseå®Œçµç·¨ â€” Pythonã‹ã‚‰ã®å®Œå…¨è„±å´

ç¬¬1-8å›ï¼ˆCourse Iï¼‰ã®Trojan Horseæˆ¦ç•¥ã‚’æŒ¯ã‚Šè¿”ã‚‹ï¼š

```mermaid
graph LR
    A[ç¬¬1-4å›<br>ğŸPython 100%] --> B[ç¬¬5-8å›<br>ğŸ+ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯<br>é…ã•ä½“æ„Ÿ]
    B --> C[ç¬¬9å›<br>ğŸğŸ”¥çµ¶æœ›<br>ğŸ¦€Rustç™»å ´]
    C --> D[ç¬¬10å›<br>ğŸ30%<br>âš¡Juliaç™»å ´]
    D --> E[ç¬¬11-16å›<br>âš¡Juliaä¸»ä½“<br>ğŸ¦€Rustä¸¦èµ°]
    E --> F[ç¬¬17-24å›<br>âš¡ğŸ¦€ğŸ”®<br>Pythonå®Œå…¨æ¶ˆæ»…]

    style A fill:#3498db
    style B fill:#f39c12
    style C fill:#e74c3c
    style D fill:#9b59b6
    style F fill:#2ecc71
```

**å„è¨€èªã®ç™»å ´ã‚¿ã‚¤ãƒŸãƒ³ã‚°**ï¼š
- **ç¬¬1-8å›ï¼ˆCourse Iï¼‰**ï¼šğŸ Python 100% â€” æ•°å­¦åŸºç¤ã‚’å­¦ã¶ãŸã‚ã®é“å…·
- **ç¬¬9å›ï¼ˆCourse IIé–‹å§‹ï¼‰**ï¼šğŸ¦€ Ruståˆç™»å ´ â€” Pythonã®GIL/ãƒ¡ãƒ¢ãƒªã‚³ãƒ”ãƒ¼é™ç•Œã‚’ä½“æ„Ÿâ†’Rustã§ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼å®Ÿç¾
- **ç¬¬10å›**ï¼šâš¡ Juliaåˆç™»å ´ â€” Rustã®å‹ãƒ‘ã‚ºãƒ«è‹¦ç—›â†’Juliaã®å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã§æ•°å¼â†”ã‚³ãƒ¼ãƒ‰1:1å¯¾å¿œ
- **ç¬¬15å›**ï¼šğŸ”® Elixiråˆç™»å ´ â€” LLMæ¨è«–ã®ä¸¦è¡Œå‡¦ç†ãƒ»è€éšœå®³æ€§ã‚’ç¿’å¾—
- **ç¬¬17-24å›ï¼ˆCourse IIIï¼‰**ï¼šâš¡ğŸ¦€ğŸ”® 3è¨€èªãƒ•ãƒ«æ´»ç”¨ â€” Pythonã¯å®Œå…¨ã«æ¶ˆãˆã‚‹

**ä»Šå›ï¼ˆç¬¬20å›ï¼‰ã®ä½ç½®ã¥ã‘**ï¼š
- Pythonã§æ›¸ã‹ã‚ŒãŸæ—¢å­˜å®Ÿè£…ï¼ˆPyTorch VAE/GAN Tutorialï¼‰ã‚’å‚ç…§ã™ã‚‹ãŒã€**å®Ÿè£…ã¯å…¨ã¦Julia/Rust/Elixir**
- PyTorchã¨ã®æ¯”è¼ƒã¯ã€Œå‚è€ƒã€ã§ã‚ã‚Šã€ã‚‚ã¯ã‚„ä¸»å½¹ã§ã¯ãªã„
- **Trojan Horseå®Œçµ**ï¼šå—è¬›ç”Ÿã¯3è¨€èªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒã€Œæ™®é€šã€ã«ãªã£ãŸ

---

### 2.5 å­¦ç¿’æˆ¦ç•¥ â€” æ•°å¼â†’ã‚³ãƒ¼ãƒ‰â†’ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆ

**3æ®µéšã®ç¿’å¾—**ï¼š

```mermaid
graph TD
    A[Zone 3: æ•°å¼ä¿®è¡Œ<br>60åˆ†] --> B[Zone 4: å®Ÿè£…ã‚¾ãƒ¼ãƒ³<br>45åˆ†]
    B --> C[Zone 5: å®Ÿé¨“ã‚¾ãƒ¼ãƒ³<br>30åˆ†]

    A -->|VAE| A1[ELBOå„é …ã®å°å‡º]
    A -->|GAN| A2[WGAN-GP losså°å‡º]
    A -->|Trans| A3[Attentionè¡Œåˆ—æ¼”ç®—]

    B -->|Julia| B1[è¨“ç·´ãƒ«ãƒ¼ãƒ—å®Ÿè£…]
    B -->|Rust| B2[æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³å®Ÿè£…]
    B -->|Elixir| B3[é…ä¿¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³]

    C -->|è©•ä¾¡| C1[æå¤±æ›²ç·šãƒ»ç”Ÿæˆå“è³ª]
    C -->|è€éšœå®³æ€§| C2[ãƒ—ãƒ­ã‚»ã‚¹killâ†’å¾©æ—§]
    C -->|ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯| C3[è¨“ç·´é€Ÿåº¦ãƒ»æ¨è«–ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·]
```

**æ¨å¥¨å­¦ç¿’é †åº**ï¼š
1. **Zone 0-2ï¼ˆä»Šã“ã“ã¾ã§ï¼‰**ï¼šå…¨ä½“åƒæŠŠæ¡ã€3ãƒ¢ãƒ‡ãƒ«ã‚’è§¦ã‚‹
2. **Zone 3**ï¼šæ•°å¼ã‚’1è¡Œãšã¤å°å‡ºã€ç´™ã¨ãƒšãƒ³å¿…é ˆã€Juliaã‚³ãƒ¼ãƒ‰ã¨å¯¾å¿œä»˜ã‘
3. **Zone 4**ï¼šJuliaè¨“ç·´å®Ÿè£…â†’Rustæ¨è«–å®Ÿè£…â†’Elixiré…ä¿¡å®Ÿè£…ã®é †
4. **Zone 5**ï¼šå®Ÿéš›ã«è¨“ç·´ãƒ»æ¨è«–ãƒ»é…ä¿¡ã‚’å‹•ã‹ã—ã€è€éšœå®³æ€§ã‚’ãƒ‡ãƒ¢
5. **Zone 6-7**ï¼šç ”ç©¶ç³»è­œæŠŠæ¡ã€FAQç¢ºèªã€æ¬¡å›ï¼ˆç¬¬21å›ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ï¼‰ã¸ã®æ¥ç¶š

**é‡è¦ãªå¿ƒæ§‹ãˆ**ï¼š
- **æ•°å¼ã‚’é£›ã°ã•ãªã„**ï¼šZone 3ã®å°å‡ºã¯å…¨ã¦è¿½ã†ã€‚ç†è§£ã›ãšã«å®Ÿè£…ã—ã¦ã‚‚ã€ãƒ‡ãƒãƒƒã‚°æ™‚ã«è©°ã‚€ã€‚
- **ã‚³ãƒ¼ãƒ‰ã‚’å‹•ã‹ã™**ï¼šå†™çµŒã§ã¯ãªãã€è‡ªåˆ†ã§æ‰“ã£ã¦å‹•ã‹ã™ã€‚ã‚¨ãƒ©ãƒ¼ã‚’èª­ã¿ã€ä¿®æ­£ã™ã‚‹ã€‚
- **ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã‚’è¦‹ã‚‹**ï¼šè¨“ç·´ã ã‘ã€æ¨è«–ã ã‘ã§ã¯ä¸ååˆ†ã€‚è¨“ç·´â†’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆâ†’æ¨è«–â†’é…ä¿¡ã®å…¨ä½“ãƒ•ãƒ­ãƒ¼ã‚’ç†è§£ã™ã‚‹ã€‚

**æœ¬è¬›ç¾©ã®ç›®æ¨™åˆ°é”ç‚¹**ï¼š
- [ ] VAE/GAN/Transformerã®ELBOã‚’**ç´™ã§å°å‡º**ã§ãã‚‹
- [ ] Juliaã§**ã‚¼ãƒ­ã‹ã‚‰è¨“ç·´ãƒ«ãƒ¼ãƒ—**ã‚’æ›¸ã‘ã‚‹
- [ ] Rustã§**safetensorsã‚’ãƒ­ãƒ¼ãƒ‰**ã—ã€æ¨è«–ã§ãã‚‹
- [ ] Elixirã§**Broadwayãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**ã‚’æ§‹ç¯‰ã§ãã‚‹
- [ ] ãƒ—ãƒ­ã‚»ã‚¹ã‚’killã—ã¦ã‚‚**è‡ªå‹•å¾©æ—§**ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã‚’è¨­è¨ˆã§ãã‚‹

æ¬¡ã®Zone 3ã§ã€æ•°å¼ä¿®è¡Œã«å…¥ã‚‹ã€‚

:::message
**é€²æ—**: å…¨ä½“ã®20%å®Œäº†ã€‚å…¨ä½“åƒã‚’æŠŠæ¡ã—ãŸã€‚æ•°å¼ä¿®è¡Œã®æº–å‚™ãŒã§ããŸã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” VAE/GAN/Transformerå®Œå…¨å°å‡º

ã“ã®ã‚¾ãƒ¼ãƒ³ã¯**æœ€ã‚‚é‡è¦**ã€‚ç†è«–ï¼ˆCourse IIï¼‰ã§å­¦ã‚“ã æ•°å¼ã‚’ã€å®Ÿè£…ã¨1:1å¯¾å¿œã•ã›ã‚‹ã€‚

### 3.1 VAE â€” ELBOå®Œå…¨åˆ†è§£ã¨å®Ÿè£…å¯¾å¿œ

**å¾©ç¿’ï¼šVAEã®ç›®çš„**ï¼ˆç¬¬10å›ã‚ˆã‚Šï¼‰

è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ $\mathbf{x}$ ã®å°¤åº¦ $p_\theta(\mathbf{x})$ ã‚’æœ€å¤§åŒ–ã—ãŸã„ãŒã€æ½œåœ¨å¤‰æ•° $\mathbf{z}$ ã‚’å‘¨è¾ºåŒ–ã™ã‚‹ç©åˆ†ãŒè¨ˆç®—ä¸èƒ½ï¼š

$$
p_\theta(\mathbf{x}) = \int p_\theta(\mathbf{x}|\mathbf{z})p(\mathbf{z})d\mathbf{z}
$$

ãã“ã§å¤‰åˆ†æ¨è«–ã§è¿‘ä¼¼äº‹å¾Œåˆ†å¸ƒ $q_\phi(\mathbf{z}|\mathbf{x})$ ã‚’å°å…¥ã—ã€ELBOã‚’å°å‡ºã—ãŸã€‚

---

#### 3.1.1 ELBOå°å‡ºï¼ˆå¾©ç¿’ï¼‰

ç¬¬8å›ãƒ»ç¬¬9å›ã§å­¦ã‚“ã ELBOå°å‡ºã‚’ã€å®Ÿè£…ã¨å¯¾å¿œä»˜ã‘ãªãŒã‚‰å†ç¢ºèªã€‚

**Step 1: å¯¾æ•°å°¤åº¦ã®åˆ†è§£**

$$
\begin{align}
\log p_\theta(\mathbf{x})
&= \log \int p_\theta(\mathbf{x}, \mathbf{z})d\mathbf{z} \\
&= \log \int p_\theta(\mathbf{x}, \mathbf{z}) \frac{q_\phi(\mathbf{z}|\mathbf{x})}{q_\phi(\mathbf{z}|\mathbf{x})} d\mathbf{z} \\
&= \log \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}\left[\frac{p_\theta(\mathbf{x}, \mathbf{z})}{q_\phi(\mathbf{z}|\mathbf{x})}\right]
\end{align}
$$

**Step 2: Jensenã®ä¸ç­‰å¼**ï¼ˆç¬¬6å›ã§è¨¼æ˜ï¼‰

$\log$ ã¯å‡¹é–¢æ•°ãªã®ã§ï¼š

$$
\log \mathbb{E}[f(\mathbf{z})] \geq \mathbb{E}[\log f(\mathbf{z})]
$$

é©ç”¨ã™ã‚‹ã¨ï¼š

$$
\log p_\theta(\mathbf{x}) \geq \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}\left[\log \frac{p_\theta(\mathbf{x}, \mathbf{z})}{q_\phi(\mathbf{z}|\mathbf{x})}\right] \equiv \mathcal{L}_{\text{ELBO}}(\theta, \phi; \mathbf{x})
$$

**Step 3: ELBOåˆ†è§£**

$$
\begin{align}
\mathcal{L}_{\text{ELBO}}
&= \mathbb{E}_{q_\phi}\left[\log p_\theta(\mathbf{x}, \mathbf{z}) - \log q_\phi(\mathbf{z}|\mathbf{x})\right] \\
&= \mathbb{E}_{q_\phi}\left[\log p_\theta(\mathbf{x}|\mathbf{z}) + \log p(\mathbf{z}) - \log q_\phi(\mathbf{z}|\mathbf{x})\right] \\
&= \mathbb{E}_{q_\phi}[\log p_\theta(\mathbf{x}|\mathbf{z})] - D_{\text{KL}}[q_\phi(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z})]
\end{align}
$$

ç¬¬1é …ï¼š**å†æ§‹æˆé …**ï¼ˆReconstruction termï¼‰
ç¬¬2é …ï¼š**KLæ­£å‰‡åŒ–é …**ï¼ˆKL Divergence regularizationï¼‰

---

#### 3.1.2 å†æ§‹æˆé …ã®å®Ÿè£…

**æ•°å¼**ï¼š

$$
\mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}[\log p_\theta(\mathbf{x}|\mathbf{z})]
$$

DecoderãŒå‡ºåŠ› $\hat{\mathbf{x}} = \mu_\theta(\mathbf{z})$ ã‚’ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®å¹³å‡ã¨ã™ã‚‹ã¨ï¼š

$$
p_\theta(\mathbf{x}|\mathbf{z}) = \mathcal{N}(\mathbf{x}; \mu_\theta(\mathbf{z}), \sigma^2\mathbf{I})
$$

å¯¾æ•°å°¤åº¦ï¼š

$$
\log p_\theta(\mathbf{x}|\mathbf{z}) = -\frac{1}{2\sigma^2}\|\mathbf{x} - \mu_\theta(\mathbf{z})\|^2 + \text{const}
$$

$\sigma^2 = 1$ ã¨å›ºå®šã™ã‚‹ã¨ï¼ˆå®Ÿè£…ä¸Šã®ç°¡ç•¥åŒ–ï¼‰ï¼š

$$
\log p_\theta(\mathbf{x}|\mathbf{z}) \propto -\|\mathbf{x} - \hat{\mathbf{x}}\|^2
$$

**Juliaã‚³ãƒ¼ãƒ‰**ï¼š

```julia
# Decoderå‡ºåŠ›: xÌ‚ = decoder(z)
xÌ‚, st_dec = decoder(z, ps_dec, st_dec)

# å†æ§‹æˆé …: -||x - xÌ‚||Â² / batch_size
recon_term = -sum((x .- xÌ‚).^2) / size(x, 2)
```

| æ•°å¼ | ã‚³ãƒ¼ãƒ‰ | å¯¾å¿œ |
|:-----|:-------|:-----|
| $\mu_\theta(\mathbf{z})$ | `decoder(z)` | Decoderã®å‡ºåŠ› |
| $\|\mathbf{x} - \mu_\theta(\mathbf{z})\|^2$ | `sum((x .- xÌ‚).^2)` | äºŒä¹—èª¤å·® |
| $\mathbb{E}_{q_\phi}[\cdot]$ | `/ size(x, 2)` | ãƒãƒƒãƒå¹³å‡ |

---

#### 3.1.3 KLæ­£å‰‡åŒ–é …ã®å®Ÿè£… â€” ã‚¬ã‚¦ã‚¹KLé–‰å½¢å¼

**æ•°å¼**ï¼š

$$
D_{\text{KL}}[q_\phi(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z})]
$$

ä»®å®šï¼š
- $q_\phi(\mathbf{z}|\mathbf{x}) = \mathcal{N}(\mathbf{z}; \boldsymbol{\mu}_\phi(\mathbf{x}), \text{diag}(\boldsymbol{\sigma}^2_\phi(\mathbf{x})))$
- $p(\mathbf{z}) = \mathcal{N}(\mathbf{0}, \mathbf{I})$

**ã‚¬ã‚¦ã‚¹KLé–‰å½¢å¼**ï¼ˆç¬¬4å›ã§å°å‡ºï¼‰ï¼š

$$
D_{\text{KL}}[\mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\sigma}^2\mathbf{I}) \| \mathcal{N}(\mathbf{0}, \mathbf{I})] = \frac{1}{2}\sum_{i=1}^d (\mu_i^2 + \sigma_i^2 - \log\sigma_i^2 - 1)
$$

Encoderã¯ $\log\sigma^2$ ã‚’å‡ºåŠ›ã™ã‚‹ã¨ä¾¿åˆ©ï¼ˆæ•°å€¤å®‰å®šæ€§ï¼‰ï¼š

$$
D_{\text{KL}} = -\frac{1}{2}\sum_{i=1}^d (1 + \log\sigma_i^2 - \mu_i^2 - \sigma_i^2)
$$

**Juliaã‚³ãƒ¼ãƒ‰**ï¼š

```julia
# Encoderå‡ºåŠ›: (Î¼, log_ÏƒÂ²)
output, st_enc = encoder(x, ps_enc, st_enc)
Î¼ = output[1:latent_dim, :]
logÏƒÂ² = output[latent_dim+1:end, :]

# KLç™ºæ•£: -0.5 * Î£(1 + log_ÏƒÂ² - Î¼Â² - ÏƒÂ²) / batch_size
kl_term = -0.5f0 * sum(1 .+ logÏƒÂ² .- Î¼.^2 .- exp.(logÏƒÂ²)) / size(x, 2)
```

| æ•°å¼ | ã‚³ãƒ¼ãƒ‰ | å¯¾å¿œ |
|:-----|:-------|:-----|
| $\boldsymbol{\mu}_\phi(\mathbf{x})$ | `Î¼ = output[1:d, :]` | Encoderã®å‰åŠå‡ºåŠ› |
| $\log\boldsymbol{\sigma}^2_\phi(\mathbf{x})$ | `logÏƒÂ² = output[d+1:end, :]` | Encoderã®å¾ŒåŠå‡ºåŠ› |
| $\mu_i^2$ | `Î¼.^2` | è¦ç´ ã”ã¨ã®äºŒä¹— |
| $\sigma_i^2 = \exp(\log\sigma_i^2)$ | `exp.(logÏƒÂ²)` | æŒ‡æ•°é–¢æ•° |
| $\sum_{i=1}^d$ | `sum(...)` | å…¨è¦ç´ ã®å’Œ |

:::message alert
**æ³¨æ„**: $\log\sigma^2$ ã‚’å‡ºåŠ›ã™ã‚‹ç†ç”±ã¯æ•°å€¤å®‰å®šæ€§ã€‚ç›´æ¥ $\sigma$ ã‚’å‡ºåŠ›ã™ã‚‹ã¨ã€å‹¾é…æ¶ˆå¤±ãƒ»çˆ†ç™ºã®ãƒªã‚¹ã‚¯ãŒã‚ã‚‹ã€‚
:::

---

#### 3.1.4 å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãƒˆãƒªãƒƒã‚¯ â€” å‹¾é…ã‚’é€šã™é­”æ³•

**å•é¡Œ**ï¼š$\mathbf{z} \sim q_\phi(\mathbf{z}|\mathbf{x})$ ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã¨ã€ç¢ºç‡çš„ãƒãƒ¼ãƒ‰ã§å‹¾é…ãŒæ­¢ã¾ã‚‹ã€‚

**è§£æ±º**ï¼šå†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãƒˆãƒªãƒƒã‚¯ï¼ˆReparameterization Trick, ç¬¬10å›ã§å­¦ã‚“ã ï¼‰

$$
\mathbf{z} = \boldsymbol{\mu}_\phi(\mathbf{x}) + \boldsymbol{\sigma}_\phi(\mathbf{x}) \odot \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})
$$

ã“ã‚Œã§ $\mathbf{z}$ ã¯ $\phi$ ã®æ±ºå®šçš„é–¢æ•°ã«ãªã‚Šã€å‹¾é…ãŒé€šã‚‹ã€‚

**Juliaã‚³ãƒ¼ãƒ‰**ï¼š

```julia
# Reparameterization: z = Î¼ + Ïƒ âŠ™ Îµ
Îµ = randn(Float32, size(Î¼)...)
Ïƒ = exp.(logÏƒÂ² ./ 2)  # Ïƒ = exp(log_ÏƒÂ² / 2) = âˆš(ÏƒÂ²)
z = Î¼ .+ Ïƒ .* Îµ
```

| æ•°å¼ | ã‚³ãƒ¼ãƒ‰ | å¯¾å¿œ |
|:-----|:-------|:-----|
| $\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ | `Îµ = randn(Float32, size(Î¼))` | æ¨™æº–æ­£è¦ãƒã‚¤ã‚º |
| $\boldsymbol{\sigma} = \exp(\log\boldsymbol{\sigma}^2 / 2)$ | `Ïƒ = exp.(logÏƒÂ² ./ 2)` | æ¨™æº–åå·®è¨ˆç®— |
| $\boldsymbol{\mu} + \boldsymbol{\sigma} \odot \boldsymbol{\epsilon}$ | `Î¼ .+ Ïƒ .* Îµ` | è¦ç´ ã”ã¨ã®ç©ã¨å’Œ |

**å‹¾é…ã®æµã‚Œ**ï¼š

```mermaid
graph LR
    A[x] -->|Encoder| B[Î¼, log_ÏƒÂ²]
    B -->|æ±ºå®šçš„å¤‰æ›| C[z = Î¼ + ÏƒâŠ™Îµ]
    C -->|Decoder| D[xÌ‚]
    D -->|Loss| E[ELBO]
    E -->|âˆ‡_Ï†| B
    E -->|âˆ‡_Î¸| D

    style C fill:#4ecdc4
```

å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã«ã‚ˆã‚Šã€$\nabla_\phi \mathcal{L}_{\text{ELBO}}$ ãŒè¨ˆç®—å¯èƒ½ã«ãªã‚‹ã€‚

---

#### 3.1.5 VAEå®Œå…¨å®Ÿè£… â€” å…¨ã¦ã‚’çµ±åˆ

```julia
using Lux, Optimisers, Zygote, Random

# === ãƒ¢ãƒ‡ãƒ«å®šç¾© ===
function create_vae(input_dim, latent_dim, hidden_dim)
    encoder = Chain(
        Dense(input_dim => hidden_dim, tanh),
        Dense(hidden_dim => hidden_dimÃ·2, tanh),
        Dense(hidden_dimÃ·2 => latent_dim*2)  # [Î¼, log_ÏƒÂ²]
    )

    decoder = Chain(
        Dense(latent_dim => hidden_dimÃ·2, tanh),
        Dense(hidden_dimÃ·2 => hidden_dim, tanh),
        Dense(hidden_dim => input_dim, sigmoid)  # [0, 1] pixel range
    )

    return encoder, decoder
end

# === ELBOæå¤±é–¢æ•° ===
function elbo_loss(encoder, decoder, ps_enc, ps_dec, st_enc, st_dec, x, latent_dim)
    # Encoder: q_Ï†(z|x) â†’ (Î¼, log_ÏƒÂ²)
    output, st_enc = encoder(x, ps_enc, st_enc)
    Î¼ = output[1:latent_dim, :]
    logÏƒÂ² = output[latent_dim+1:end, :]

    # Reparameterization: z = Î¼ + ÏƒâŠ™Îµ
    Îµ = randn(Float32, size(Î¼)...)
    Ïƒ = exp.(logÏƒÂ² ./ 2)
    z = Î¼ .+ Ïƒ .* Îµ

    # Decoder: p_Î¸(x|z) â†’ xÌ‚
    xÌ‚, st_dec = decoder(z, ps_dec, st_dec)

    # ELBO = å†æ§‹æˆé … - KLæ­£å‰‡åŒ–é …
    batch_size = size(x, 2)
    recon = -sum((x .- xÌ‚).^2) / batch_size  # ã‚¬ã‚¦ã‚¹å°¤åº¦
    kl = -0.5f0 * sum(1 .+ logÏƒÂ² .- Î¼.^2 .- exp.(logÏƒÂ²)) / batch_size

    elbo = recon - kl

    return -elbo, (st_enc, st_dec)  # æœ€å¤§åŒ– = è² ã®æœ€å°åŒ–
end

# === è¨“ç·´ãƒ«ãƒ¼ãƒ— ===
function train_vae!(encoder, decoder, train_data, latent_dim, epochs=100, lr=1e-3)
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆæœŸåŒ–
    rng = Random.default_rng()
    ps_enc, st_enc = Lux.setup(rng, encoder)
    ps_dec, st_dec = Lux.setup(rng, decoder)

    # Optimizer
    opt_state_enc = Optimisers.setup(Adam(lr), ps_enc)
    opt_state_dec = Optimisers.setup(Adam(lr), ps_dec)

    for epoch in 1:epochs
        total_loss = 0.0f0

        for batch in DataLoader((train_data,), batchsize=128, shuffle=true)
            x = batch[1]

            # å‹¾é…è¨ˆç®—
            (loss, (st_enc, st_dec)), back = Zygote.pullback(
                (pe, pd) -> elbo_loss(encoder, decoder, pe, pd, st_enc, st_dec, x, latent_dim),
                ps_enc, ps_dec
            )
            grads_enc, grads_dec = back((one(loss), nothing))

            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
            opt_state_enc, ps_enc = Optimisers.update(opt_state_enc, ps_enc, grads_enc)
            opt_state_dec, ps_dec = Optimisers.update(opt_state_dec, ps_dec, grads_dec)

            total_loss += loss
        end

        println("Epoch $epoch: ELBO loss = $(total_loss/length(train_data))")
    end

    return ps_enc, ps_dec, st_enc, st_dec
end

# === ä½¿ç”¨ä¾‹ ===
encoder, decoder = create_vae(784, 20, 400)
ps_enc, ps_dec, st_enc, st_dec = train_vae!(encoder, decoder, x_train, 20, epochs=50)
```

**å…¨ä½“ã®æµã‚Œ**ï¼š

```mermaid
sequenceDiagram
    participant Data as è¨“ç·´ãƒ‡ãƒ¼ã‚¿ x
    participant Enc as Encoder q_Ï†
    participant Reparam as Reparameterization
    participant Dec as Decoder p_Î¸
    participant Loss as ELBO Loss

    Data->>Enc: x (784æ¬¡å…ƒ)
    Enc->>Reparam: Î¼, log_ÏƒÂ² (å„20æ¬¡å…ƒ)
    Reparam->>Reparam: Îµ ~ N(0, I)
    Reparam->>Dec: z = Î¼ + ÏƒâŠ™Îµ (20æ¬¡å…ƒ)
    Dec->>Loss: xÌ‚ (784æ¬¡å…ƒ)
    Loss->>Loss: recon = -||x - xÌ‚||Â²
    Loss->>Loss: kl = -0.5Î£(1 + log_ÏƒÂ² - Î¼Â² - ÏƒÂ²)
    Loss->>Loss: ELBO = recon - kl
    Loss->>Enc: âˆ‡_Ï† ELBO
    Loss->>Dec: âˆ‡_Î¸ ELBO
```

**è¨“ç·´æ™‚ã®ãƒ‡ãƒãƒƒã‚°Tips**ï¼š

```julia
# æå¤±ãŒç™ºæ•£ã™ã‚‹å ´åˆã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ
function debug_vae_loss(encoder, decoder, ps_enc, ps_dec, st_enc, st_dec, x)
    # 1. Encoderå‡ºåŠ›ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯
    enc_out, _ = encoder(x, ps_enc, st_enc)
    Î¼ = enc_out[1:20, :]
    logÏƒÂ² = enc_out[21:end, :]

    println("Î¼ range: [$(minimum(Î¼)), $(maximum(Î¼))]")  # æœŸå¾…: [-3, 3]ç¨‹åº¦
    println("logÏƒÂ² range: [$(minimum(logÏƒÂ²)), $(maximum(logÏƒÂ²))]")  # æœŸå¾…: [-5, 5]ç¨‹åº¦

    # 2. ÏƒÂ²ãŒæ¥µç«¯ã«å°ã•ã„/å¤§ãã„å ´åˆã¯clip
    logÏƒÂ² = clamp.(logÏƒÂ², -10.0f0, 10.0f0)

    # 3. Decoderå‡ºåŠ›ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯
    z = Î¼ .+ exp.(logÏƒÂ² ./ 2) .* randn(Float32, size(Î¼)...)
    xÌ‚, _ = decoder(z, ps_dec, st_dec)

    println("Decoder output range: [$(minimum(xÌ‚)), $(maximum(xÌ‚))]")  # æœŸå¾…: [0, 1]

    # 4. KLé …ãŒè² ã«ãªã‚‰ãªã„ã“ã¨ã‚’ç¢ºèª
    kl = -0.5f0 * sum(1 .+ logÏƒÂ² .- Î¼.^2 .- exp.(logÏƒÂ²))
    println("KL term: $kl")  # æœŸå¾…: â‰¥0 (è² ãªã‚‰å®Ÿè£…ãƒã‚°)

    # 5. å„é …ã®ã‚¹ã‚±ãƒ¼ãƒ«ç¢ºèª
    recon = -sum((x .- xÌ‚).^2) / size(x, 2)
    println("Recon: $recon, KL: $kl")
    # æœŸå¾…: åŒã˜ã‚ªãƒ¼ãƒ€ãƒ¼ï¼ˆKLãŒæ¥µç«¯ã«å¤§ãã„ã¨Posterior Collapseï¼‰
end
```

**Posterior Collapseå¯¾ç­–**ï¼š

```julia
# KL Annealing: KLé …ã®é‡ã¿ã‚’å¾ã€…ã«å¢—åŠ 
function kl_annealing_schedule(epoch, total_epochs, anneal_start=10, anneal_end=50)
    if epoch < anneal_start
        return 0.0f0
    elseif epoch > anneal_end
        return 1.0f0
    else
        return Float32((epoch - anneal_start) / (anneal_end - anneal_start))
    end
end

# è¨“ç·´ãƒ«ãƒ¼ãƒ—ã§ä½¿ç”¨
for epoch in 1:epochs
    Î²_kl = kl_annealing_schedule(epoch, epochs)
    # loss = recon - Î²_kl * kl
end
```

---

### 3.2 GAN â€” WGAN-GPå®Œå…¨å°å‡ºã¨å®Ÿè£…å¯¾å¿œ

**å¾©ç¿’ï¼šGANã®ç›®çš„**ï¼ˆç¬¬12å›ã‚ˆã‚Šï¼‰

Generator $G$ ã¨ Discriminator $D$ ã®2ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚²ãƒ¼ãƒ ï¼š

$$
\min_G \max_D \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}}[\log D(\mathbf{x})] + \mathbb{E}_{\mathbf{z} \sim p_z}[\log(1 - D(G(\mathbf{z})))]
$$

å•é¡Œç‚¹ï¼š
- è¨“ç·´ä¸å®‰å®šï¼ˆæŒ¯å‹•ãƒ»ç™ºæ•£ï¼‰
- Mode Collapseï¼ˆå¤šæ§˜æ€§ã®æ¬ å¦‚ï¼‰
- å‹¾é…æ¶ˆå¤±ï¼ˆ$D$ ãŒå¼·ã™ãã‚‹ã¨ $G$ ã®å‹¾é…ãŒæ¶ˆãˆã‚‹ï¼‰

è§£æ±ºç­–ï¼š**WGAN-GP**ï¼ˆWasserstein GAN with Gradient Penalty, ç¬¬13å›ã§å­¦ã‚“ã ï¼‰

---

#### 3.2.1 Wassersteinè·é›¢ã®å°å‡ºï¼ˆå¾©ç¿’ï¼‰

ç¬¬13å›ã§å­¦ã‚“ã Wasserstein-1è·é›¢ï¼ˆEarth Mover's Distanceï¼‰ï¼š

$$
W_1(p_r, p_g) = \inf_{\gamma \in \Pi(p_r, p_g)} \mathbb{E}_{(\mathbf{x}, \mathbf{y}) \sim \gamma}[\|\mathbf{x} - \mathbf{y}\|]
$$

Kantorovich-RubinsteinåŒå¯¾æ€§ï¼ˆç¬¬13å›ã§è¨¼æ˜ï¼‰ï¼š

$$
W_1(p_r, p_g) = \sup_{\|f\|_L \leq 1} \mathbb{E}_{\mathbf{x} \sim p_r}[f(\mathbf{x})] - \mathbb{E}_{\mathbf{x} \sim p_g}[f(\mathbf{x})]
$$

ã“ã“ã§ $\|f\|_L \leq 1$ ã¯1-Lipschitzé€£ç¶šåˆ¶ç´„ã€‚

**WGANã®æå¤±é–¢æ•°**ï¼š

$$
\mathcal{L}_D = \mathbb{E}_{\mathbf{x} \sim p_r}[D(\mathbf{x})] - \mathbb{E}_{\mathbf{z} \sim p_z}[D(G(\mathbf{z}))]
$$

$$
\mathcal{L}_G = -\mathbb{E}_{\mathbf{z} \sim p_z}[D(G(\mathbf{z}))]
$$

$D$ ã¯"Critic"ï¼ˆè­˜åˆ¥å™¨ã§ã¯ãªãã€ã‚¹ã‚³ã‚¢é–¢æ•°ï¼‰ã€‚

---

#### 3.2.2 Gradient Penalty â€” Lipschitzåˆ¶ç´„ã®å¼·åˆ¶

**å•é¡Œ**ï¼šå…ƒã®WGANã¯weight clippingã§ $\|f\|_L \leq 1$ ã‚’å¼·åˆ¶ã—ãŸãŒã€å®¹é‡ä½ä¸‹ãƒ»å‹¾é…æ¶ˆå¤±ã‚’å¼•ãèµ·ã“ã™ã€‚

**è§£æ±º**ï¼šWGAN-GPï¼ˆGulrajani+ 2017 [^2]ï¼‰ã¯Gradient Penaltyã§åˆ¶ç´„ï¼š

$$
\|\nabla_{\hat{\mathbf{x}}} D(\hat{\mathbf{x}})\|_2 = 1
$$

ã‚’ $\hat{\mathbf{x}} = \alpha \mathbf{x} + (1 - \alpha)G(\mathbf{z})$ ï¼ˆæœ¬ç‰©ã¨å½ç‰©ã®è£œé–“ç‚¹ï¼‰ã§å¼·åˆ¶ã€‚

**WGAN-GPæå¤±é–¢æ•°**ï¼š

$$
\mathcal{L}_D = \mathbb{E}_{\mathbf{z} \sim p_z}[D(G(\mathbf{z}))] - \mathbb{E}_{\mathbf{x} \sim p_r}[D(\mathbf{x})] + \lambda \mathbb{E}_{\hat{\mathbf{x}}}[(\|\nabla_{\hat{\mathbf{x}}} D(\hat{\mathbf{x}})\|_2 - 1)^2]
$$

ç¬¬1é …ï¼šå½ç‰©ã®ã‚¹ã‚³ã‚¢ï¼ˆæœ€å°åŒ–ï¼‰
ç¬¬2é …ï¼šæœ¬ç‰©ã®ã‚¹ã‚³ã‚¢ï¼ˆæœ€å¤§åŒ–ï¼‰
ç¬¬3é …ï¼šGradient Penaltyï¼ˆå‹¾é…ãƒãƒ«ãƒ ã‚’1ã«è¿‘ã¥ã‘ã‚‹ï¼‰

---

#### 3.2.3 Gradient Penalty ã®å®Ÿè£…

**æ•°å¼**ï¼š

$$
\text{GP} = \mathbb{E}_{\hat{\mathbf{x}}}[(\|\nabla_{\hat{\mathbf{x}}} D(\hat{\mathbf{x}})\|_2 - 1)^2]
$$

**Step 1: è£œé–“ç‚¹ç”Ÿæˆ**

$$
\hat{\mathbf{x}} = \alpha \mathbf{x} + (1 - \alpha)G(\mathbf{z}), \quad \alpha \sim \text{Uniform}(0, 1)
$$

**Juliaã‚³ãƒ¼ãƒ‰**ï¼š

```julia
# æœ¬ç‰©ã¨å½ç‰©ã®è£œé–“
Î± = rand(Float32, 1, batch_size)
x_interp = Î± .* x_real .+ (1 .- Î±) .* x_fake
```

| æ•°å¼ | ã‚³ãƒ¼ãƒ‰ | å¯¾å¿œ |
|:-----|:-------|:-----|
| $\alpha \sim \text{Uniform}(0, 1)$ | `Î± = rand(Float32, 1, batch_size)` | è£œé–“ä¿‚æ•° |
| $\alpha \mathbf{x}$ | `Î± .* x_real` | broadcastä¹—ç®— |
| $(1 - \alpha)G(\mathbf{z})$ | `(1 .- Î±) .* x_fake` | broadcastæ¸›ç®—ãƒ»ä¹—ç®— |

**Step 2: å‹¾é…è¨ˆç®—**

$$
\nabla_{\hat{\mathbf{x}}} D(\hat{\mathbf{x}})
$$

Juliaã§ã¯`Zygote.gradient`ã‚’ä½¿ã†ï¼š

```julia
# è£œé–“ç‚¹ã§ã®å‹¾é…è¨ˆç®—
grad_interp = Zygote.gradient(x -> sum(critic(x, ps_c, st_c)[1]), x_interp)[1]
```

**Step 3: å‹¾é…ãƒãƒ«ãƒ è¨ˆç®—**

$$
\|\nabla_{\hat{\mathbf{x}}} D(\hat{\mathbf{x}})\|_2 = \sqrt{\sum_i (\partial D / \partial \hat{x}_i)^2}
$$

```julia
# å‹¾é…ãƒãƒ«ãƒ : âˆš(Î£ gradÂ²) for each sample
grad_norm = sqrt.(sum(grad_interp.^2, dims=1))  # (1, batch_size)

# Gradient Penalty: ğ”¼[(||âˆ‡D||â‚‚ - 1)Â²]
gp = mean((grad_norm .- 1).^2)
```

| æ•°å¼ | ã‚³ãƒ¼ãƒ‰ | å¯¾å¿œ |
|:-----|:-------|:-----|
| $(\partial D / \partial \hat{x}_i)^2$ | `grad_interp.^2` | å‹¾é…ã®äºŒä¹— |
| $\sum_i$ | `sum(..., dims=1)` | å„ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã®å’Œ |
| $\sqrt{\cdot}$ | `sqrt.(...)` | å¹³æ–¹æ ¹ï¼ˆbroadcastï¼‰ |
| $(\|\nabla D\|_2 - 1)^2$ | `(grad_norm .- 1).^2` | ãƒšãƒŠãƒ«ãƒ†ã‚£é … |
| $\mathbb{E}[\cdot]$ | `mean(...)` | ãƒãƒƒãƒå¹³å‡ |

---

#### 3.2.4 WGAN-GPå®Œå…¨å®Ÿè£…

```julia
using Lux, Optimisers, Zygote, Random

# === ãƒ¢ãƒ‡ãƒ«å®šç¾© ===
function create_wgan_gp(latent_dim, img_dim, hidden_dim)
    generator = Chain(
        Dense(latent_dim => hidden_dim, relu),
        Dense(hidden_dim => hidden_dim*2, relu),
        Dense(hidden_dim*2 => img_dim, tanh)  # [-1, 1] range
    )

    critic = Chain(
        Dense(img_dim => hidden_dim*2, x -> leakyrelu(x, 0.2f0)),
        Dense(hidden_dim*2 => hidden_dim, x -> leakyrelu(x, 0.2f0)),
        Dense(hidden_dim => 1)  # ã‚¹ã‚³ã‚¢å‡ºåŠ›
    )

    return generator, critic
end

# === Criticæå¤±ï¼ˆWGAN-GPï¼‰ ===
function critic_loss(generator, critic, ps_g, ps_c, st_g, st_c, x_real, Î»_gp=10.0f0)
    batch_size = size(x_real, 2)

    # å½ç”»åƒç”Ÿæˆ
    z = randn(Float32, size(ps_g)[1], batch_size)
    x_fake, st_g = generator(z, ps_g, st_g)

    # Criticã‚¹ã‚³ã‚¢
    score_real, st_c_real = critic(x_real, ps_c, st_c)
    score_fake, st_c_fake = critic(x_fake, ps_c, st_c)

    # Wassersteinè·é›¢: ğ”¼[D(fake)] - ğ”¼[D(real)]
    wasserstein = mean(score_fake) - mean(score_real)

    # Gradient Penalty
    Î± = rand(Float32, 1, batch_size)
    x_interp = Î± .* x_real .+ (1 .- Î±) .* x_fake

    grad_interp = Zygote.gradient(x -> sum(critic(x, ps_c, st_c)[1]), x_interp)[1]
    grad_norm = sqrt.(sum(grad_interp.^2, dims=1))
    gp = mean((grad_norm .- 1).^2)

    loss = wasserstein + Î»_gp * gp

    return loss, st_c
end

# === Generatoræå¤±ï¼ˆWGAN-GPï¼‰ ===
function generator_loss(generator, critic, ps_g, ps_c, st_g, st_c, batch_size)
    # å½ç”»åƒç”Ÿæˆ
    z = randn(Float32, size(ps_g)[1], batch_size)
    x_fake, st_g = generator(z, ps_g, st_g)

    # Generatorã®ç›®çš„: Criticã‚¹ã‚³ã‚¢ã‚’æœ€å¤§åŒ–
    score_fake, st_c = critic(x_fake, ps_c, st_c)
    loss = -mean(score_fake)

    return loss, st_g
end

# === è¨“ç·´ãƒ«ãƒ¼ãƒ— ===
function train_wgan_gp!(generator, critic, train_data, latent_dim, epochs=100, n_critic=5)
    rng = Random.default_rng()
    ps_g, st_g = Lux.setup(rng, generator)
    ps_c, st_c = Lux.setup(rng, critic)

    opt_g = Optimisers.setup(Adam(1e-4, (0.5, 0.9)), ps_g)
    opt_c = Optimisers.setup(Adam(1e-4, (0.5, 0.9)), ps_c)

    for epoch in 1:epochs
        for batch in DataLoader((train_data,), batchsize=64, shuffle=true)
            x_real = batch[1]

            # Criticã‚’ n_critic å›æ›´æ–°
            for _ in 1:n_critic
                (loss_c, st_c), back_c = Zygote.pullback(
                    pc -> critic_loss(generator, critic, ps_g, pc, st_g, st_c, x_real),
                    ps_c
                )
                grads_c = back_c((one(loss_c), nothing))[1]
                opt_c, ps_c = Optimisers.update(opt_c, ps_c, grads_c)
            end

            # Generatorã‚’ 1 å›æ›´æ–°
            (loss_g, st_g), back_g = Zygote.pullback(
                pg -> generator_loss(generator, critic, pg, ps_c, st_g, st_c, size(x_real, 2)),
                ps_g
            )
            grads_g = back_g((one(loss_g), nothing))[1]
            opt_g, ps_g = Optimisers.update(opt_g, ps_g, grads_g)
        end

        println("Epoch $epoch: C_loss=$(loss_c), G_loss=$(loss_g)")
    end

    return ps_g, ps_c, st_g, st_c
end
```

**è¨“ç·´ãƒ«ãƒ¼ãƒ—ã®æµã‚Œ**ï¼š

```mermaid
sequenceDiagram
    participant Data as è¨“ç·´ãƒ‡ãƒ¼ã‚¿ x_real
    participant G as Generator G
    participant C as Critic D
    participant GP as Gradient Penalty

    loop Criticã‚’5å›æ›´æ–°
        Data->>G: z ~ N(0, I)
        G->>C: x_fake = G(z)
        C->>C: score_real = D(x_real)
        C->>C: score_fake = D(x_fake)
        C->>GP: x_interp = Î±x_real + (1-Î±)x_fake
        GP->>GP: grad = âˆ‡_x_interp D(x_interp)
        GP->>GP: gp = ğ”¼[(||grad||â‚‚ - 1)Â²]
        GP->>C: loss_C = score_fake - score_real + Î»*gp
        C->>C: ps_c â† ps_c - Î·âˆ‡_c loss_C
    end

    loop Generatorã‚’1å›æ›´æ–°
        G->>C: x_fake = G(z)
        C->>G: score_fake = D(x_fake)
        G->>G: loss_G = -score_fake
        G->>G: ps_g â† ps_g - Î·âˆ‡_g loss_G
    end
```

**é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ**ï¼š
- Criticã‚’$n_{\text{critic}}=5$å›ã€Generatorã‚’1å›æ›´æ–°ï¼ˆWGAN-GPæ¨å¥¨æ¯”ç‡ï¼‰
- Gradient Penaltyã® $\lambda=10$ ã¯ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆè«–æ–‡æ¨å¥¨å€¤ï¼‰
- Adamã® $\beta_1=0.5$ ã¯GANè¨“ç·´ã®å®‰å®šåŒ–ã«æœ‰åŠ¹ï¼ˆé€šå¸¸ã¯0.9ï¼‰

:::message
**ã“ã“ãŒé‡è¦**: WGAN-GPã®æ ¸å¿ƒã¯ã€Œå‹¾é…ãƒãƒ«ãƒ ã‚’1ã«ä¿ã¤ã€ã“ã¨ã€‚ã“ã‚ŒãŒLipschitzåˆ¶ç´„ã®å®Ÿç”¨çš„å®Ÿè£…ã€‚
:::

---

### 3.3 Transformer â€” Multi-Head Attentionå®Œå…¨å°å‡º

**å¾©ç¿’ï¼šTransformerã®ç›®çš„**ï¼ˆç¬¬16å›ã‚ˆã‚Šï¼‰

è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ï¼š

$$
p(\mathbf{x}) = \prod_{t=1}^T p(x_t | x_{<t})
$$

RNN/LSTMã®é€æ¬¡å‡¦ç†ã‚’æ¨ã¦ã€Attentionã§ä¸¦åˆ—å‡¦ç†ã€‚

---

#### 3.3.1 Scaled Dot-Product Attentionå°å‡º

**Step 1: Attentionæ©Ÿæ§‹ã®ç›´æ„Ÿ**

Query $\mathbf{q}$ ã¨ Key $\mathbf{k}_i$ ã®é¡ä¼¼åº¦ã§Value $\mathbf{v}_i$ ã‚’é‡ã¿ä»˜ã‘ï¼š

$$
\text{Attention}(\mathbf{q}, \{\mathbf{k}_i, \mathbf{v}_i\}) = \sum_{i} \alpha_i \mathbf{v}_i
$$

ã“ã“ã§ $\alpha_i = \text{softmax}(\text{score}(\mathbf{q}, \mathbf{k}_i))$

**Step 2: ã‚¹ã‚³ã‚¢é–¢æ•°ã®é¸æŠ**

å†…ç©ã‚¹ã‚³ã‚¢ï¼š

$$
\text{score}(\mathbf{q}, \mathbf{k}) = \mathbf{q}^\top \mathbf{k}
$$

å•é¡Œï¼š$d_k$ ãŒå¤§ãã„ã¨ã‚¹ã‚³ã‚¢ã®åˆ†æ•£ãŒå¤§ãããªã‚Šã€softmaxãŒé£½å’Œï¼ˆå‹¾é…æ¶ˆå¤±ï¼‰ã€‚

è§£æ±ºï¼šã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°

$$
\text{score}(\mathbf{q}, \mathbf{k}) = \frac{\mathbf{q}^\top \mathbf{k}}{\sqrt{d_k}}
$$

**Step 3: è¡Œåˆ—å½¢å¼**

Queryè¡Œåˆ— $Q \in \mathbb{R}^{n \times d_k}$ã€Keyè¡Œåˆ— $K \in \mathbb{R}^{m \times d_k}$ã€Valueè¡Œåˆ— $V \in \mathbb{R}^{m \times d_v}$ ã‚’ä½¿ã†ã¨ï¼š

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
$$

ã“ã“ã§ï¼š
- $QK^\top \in \mathbb{R}^{n \times m}$ï¼šå„Queryã¨Keyã®é¡ä¼¼åº¦è¡Œåˆ—
- $\text{softmax}$ï¼šè¡Œã”ã¨ã«æ­£è¦åŒ–ï¼ˆå„QueryãŒå…¨Keyã®é‡ã¿ã‚’åˆè¨ˆ1ã«ï¼‰
- çµæœ $\in \mathbb{R}^{n \times d_v}$ï¼šå„Queryã«å¯¾ã™ã‚‹åŠ é‡Valueã®å’Œ

---

#### 3.3.2 Multi-Head Attentionå°å‡º

**å‹•æ©Ÿ**ï¼šå˜ä¸€ã®Attentionã§ã¯è¡¨ç¾åŠ›ä¸è¶³ã€‚è¤‡æ•°ã®"è¦–ç‚¹"ã§Attentionã‚’ä¸¦åˆ—è¨ˆç®—ã€‚

**Step 1: ãƒ˜ãƒƒãƒ‰ã®åˆ†å‰²**

$d_{\text{model}}$ æ¬¡å…ƒã‚’ $h$ å€‹ã®ãƒ˜ãƒƒãƒ‰ã«åˆ†å‰²ï¼š

$$
d_k = d_v = \frac{d_{\text{model}}}{h}
$$

**Step 2: å„ãƒ˜ãƒƒãƒ‰ã§ç‹¬ç«‹ã«Attention**

$$
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$

ã“ã“ã§ $W_i^Q, W_i^K, W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_k}$ ã¯å­¦ç¿’å¯èƒ½ãªå°„å½±è¡Œåˆ—ã€‚

**Step 3: Concatenate and Project**

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O
$$

ã“ã“ã§ $W^O \in \mathbb{R}^{d_{\text{model}} \times d_{\text{model}}}$

**å®Œå…¨ãªæ•°å¼**ï¼š

$$
\begin{align}
\text{MultiHead}(Q, K, V) &= \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O \\
\text{head}_i &= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \\
&= \text{softmax}\left(\frac{(QW_i^Q)(KW_i^K)^\top}{\sqrt{d_k}}\right)(VW_i^V)
\end{align}
$$

---

#### 3.3.3 Causal Mask â€” æœªæ¥ã‚’è¦‹ã›ãªã„

è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€æ™‚åˆ» $t$ ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¯ $t+1$ ä»¥é™ã‚’è¦‹ã¦ã¯ã„ã‘ãªã„ã€‚

**Maskè¡Œåˆ—**ï¼š

$$
M_{ij} = \begin{cases}
0 & \text{if } i \geq j \\
-\infty & \text{if } i < j
\end{cases}
$$

Softmaxå‰ã«ã‚¹ã‚³ã‚¢ã«åŠ ç®—ï¼š

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}} + M\right)V
$$

$M_{ij} = -\infty$ ã®éƒ¨åˆ†ã¯ $\exp(-\infty) = 0$ ã«ãªã‚Šã€æœªæ¥ãƒˆãƒ¼ã‚¯ãƒ³ã®é‡ã¿ãŒ0ã«ãªã‚‹ã€‚

**Juliaã‚³ãƒ¼ãƒ‰**ï¼š

```julia
# Causal Maskç”Ÿæˆ
function causal_mask(seq_len)
    mask = triu(ones(Float32, seq_len, seq_len), 1)  # ä¸Šä¸‰è§’è¡Œåˆ—ï¼ˆå¯¾è§’ã‚ˆã‚Šä¸Šï¼‰
    return mask .* -Inf32  # Softmaxå‰ã«åŠ ç®— â†’ exp(-âˆ) = 0
end

# Attentionã«ãƒã‚¹ã‚¯é©ç”¨
scores = Q @ K' ./ sqrt(Float32(d_k))  # (seq_len, seq_len)
scores = scores .+ causal_mask(seq_len)  # æœªæ¥ãƒˆãƒ¼ã‚¯ãƒ³ã‚’-âˆã«
attn_weights = softmax(scores, dims=2)  # è¡Œã”ã¨ã«æ­£è¦åŒ–
output = attn_weights @ V
```

| æ•°å¼ | ã‚³ãƒ¼ãƒ‰ | å¯¾å¿œ |
|:-----|:-------|:-----|
| $QK^\top$ | `Q @ K'` | è¡Œåˆ—ç©ï¼ˆ`'`ã¯è»¢ç½®ï¼‰ |
| $/\sqrt{d_k}$ | `./ sqrt(Float32(d_k))` | broadcasté™¤ç®— |
| $M$ | `causal_mask(seq_len)` | ãƒã‚¹ã‚¯è¡Œåˆ— |
| $\text{softmax}(\cdot + M)$ | `softmax(scores .+ mask, dims=2)` | è¡Œã”ã¨softmax |

---

#### 3.3.4 Multi-Head Attentionå®Œå…¨å®Ÿè£…

```julia
using Lux, NNlib, Random

# === Multi-Head Attention Layer ===
struct MultiHeadAttention <: Lux.AbstractExplicitLayer
    num_heads::Int
    d_model::Int
    d_k::Int
    q_proj::Dense
    k_proj::Dense
    v_proj::Dense
    o_proj::Dense
end

function MultiHeadAttention(d_model::Int, num_heads::Int)
    @assert d_model % num_heads == 0 "d_model must be divisible by num_heads"
    d_k = d_model Ã· num_heads

    return MultiHeadAttention(
        num_heads, d_model, d_k,
        Dense(d_model => d_model, use_bias=false),  # Q projection
        Dense(d_model => d_model, use_bias=false),  # K projection
        Dense(d_model => d_model, use_bias=false),  # V projection
        Dense(d_model => d_model, use_bias=false)   # Output projection
    )
end

function (mha::MultiHeadAttention)(x, ps, st; mask=nothing)
    # x: (d_model, seq_len, batch_size)
    d_model, seq_len, batch_size = size(x)

    # Linear projections: Q, K, V
    Q, st_q = mha.q_proj(x, ps.q_proj, st.q_proj)
    K, st_k = mha.k_proj(x, ps.k_proj, st.k_proj)
    V, st_v = mha.v_proj(x, ps.v_proj, st.v_proj)

    # Reshape for multi-head: (d_model, seq_len, batch) â†’ (num_heads, d_k, seq_len, batch)
    Q = reshape(Q, mha.d_k, mha.num_heads, seq_len, batch_size) |> x -> permutedims(x, (2,1,3,4))
    K = reshape(K, mha.d_k, mha.num_heads, seq_len, batch_size) |> x -> permutedims(x, (2,1,3,4))
    V = reshape(V, mha.d_k, mha.num_heads, seq_len, batch_size) |> x -> permutedims(x, (2,1,3,4))

    # Scaled Dot-Product Attention for all heads
    # scores: (num_heads, seq_len, seq_len, batch)
    scores = batched_mul(batched_transpose(Q), K) ./ sqrt(Float32(mha.d_k))

    # Apply mask if provided
    if !isnothing(mask)
        scores = scores .+ reshape(mask, 1, seq_len, seq_len, 1)  # broadcast over heads and batch
    end

    # Softmax over keys dimension
    attn_weights = softmax(scores, dims=2)  # normalize over keys (dim 2)

    # Weighted sum of values
    out = batched_mul(V, attn_weights)  # (num_heads, d_k, seq_len, batch)

    # Concatenate heads: (num_heads, d_k, seq_len, batch) â†’ (d_model, seq_len, batch)
    out = permutedims(out, (2,1,3,4)) |> x -> reshape(x, d_model, seq_len, batch_size)

    # Output projection
    out, st_o = mha.o_proj(out, ps.o_proj, st.o_proj)

    return out, (st_q=st_q, st_k=st_k, st_v=st_v, st_o=st_o)
end

# === Causal Mask ===
function causal_mask(seq_len)
    mask = triu(ones(Float32, seq_len, seq_len), 1)
    return mask .* -Inf32
end

# === ä½¿ç”¨ä¾‹ ===
d_model = 512
num_heads = 8
seq_len = 10
batch_size = 2

x = randn(Float32, d_model, seq_len, batch_size)
mha = MultiHeadAttention(d_model, num_heads)
ps, st = Lux.setup(Random.default_rng(), mha)

mask = causal_mask(seq_len)
y, st = mha(x, ps, st; mask=mask)  # y: (512, 10, 2)
```

**å‡¦ç†ã®æµã‚Œ**ï¼š

```mermaid
graph TD
    A[Input x<br>d_model Ã— seq_len Ã— batch] -->|Q proj| B1[Q: d_model Ã— seq_len Ã— batch]
    A -->|K proj| B2[K: d_model Ã— seq_len Ã— batch]
    A -->|V proj| B3[V: d_model Ã— seq_len Ã— batch]

    B1 -->|Reshape| C1[Q: num_heads Ã— d_k Ã— seq_len Ã— batch]
    B2 -->|Reshape| C2[K: num_heads Ã— d_k Ã— seq_len Ã— batch]
    B3 -->|Reshape| C3[V: num_heads Ã— d_k Ã— seq_len Ã— batch]

    C1 -->|QK^T/âˆšd_k| D[Scores:<br>num_heads Ã— seq_len Ã— seq_len Ã— batch]
    C2 --> D

    D -->|+ Mask| E[Masked Scores]
    E -->|Softmax| F[Attn Weights]
    F -->|Ã— V| G[Weighted Values:<br>num_heads Ã— d_k Ã— seq_len Ã— batch]
    C3 --> G

    G -->|Concat| H[Concat:<br>d_model Ã— seq_len Ã— batch]
    H -->|O proj| I[Output:<br>d_model Ã— seq_len Ã— batch]

    style D fill:#4ecdc4
    style E fill:#ff6b6b
    style G fill:#ffe66d
```

**æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å®Œå…¨å¯¾å¿œ**ï¼š

| æ•°å¼ã‚¹ãƒ†ãƒƒãƒ— | Juliaã‚³ãƒ¼ãƒ‰ | æ¬¡å…ƒå¤‰åŒ– |
|:-------------|:------------|:---------|
| $Q = XW^Q$ | `Q, _ = mha.q_proj(x, ps.q_proj, st.q_proj)` | $(d, n, b) \to (d, n, b)$ |
| $Q$ ã‚’ $h$ ãƒ˜ãƒƒãƒ‰ã«åˆ†å‰² | `reshape(Q, d_k, h, n, b) \|> permutedims((2,1,3,4))` | $(d, n, b) \to (h, d_k, n, b)$ |
| $QK^\top/\sqrt{d_k}$ | `batched_mul(Q', K) ./ sqrt(Float32(d_k))` | $(h, n, d_k, b) \to (h, n, n, b)$ |
| $\text{scores} + M$ | `scores .+ mask` | Maskã‚’broadcast |
| $\text{softmax}(\cdot)$ | `softmax(scores, dims=2)` | è¡Œï¼ˆKeyæ¬¡å…ƒï¼‰ã§æ­£è¦åŒ– |
| $\text{Attention} \times V$ | `batched_mul(V, attn_weights)$ | $(h, d_k, n, b) \times (h, n, n, b) \to (h, d_k, n, b)$ |
| Concat heads | `reshape(..., d, n, b)` | $(h, d_k, n, b) \to (d, n, b)$ |
| Output projection | `mha.o_proj(out)` | $(d, n, b) \to (d, n, b)$ |

:::message
**ã“ã“ãŒé‡è¦**: Multi-Head Attentionã¯ã€Œä¸¦åˆ—ã«è¤‡æ•°ã®è¦–ç‚¹ã§Attentionã€ã€‚å„ãƒ˜ãƒƒãƒ‰ãŒç•°ãªã‚‹éƒ¨åˆ†ç©ºé–“ã§é¡ä¼¼åº¦ã‚’è¨ˆç®—ã€‚
:::

---

### 3.4 âš”ï¸ Boss Battle â€” 3ãƒ¢ãƒ‡ãƒ«çµ±åˆè¨“ç·´ãƒ«ãƒ¼ãƒ—

ã“ã“ã¾ã§ã§3ãƒ¢ãƒ‡ãƒ«ã®æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œã‚’å®Œå…¨ã«ç†è§£ã—ãŸã€‚æœ€å¾Œã®Boss Battleï¼š**3ãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆã—ãŸè¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’è¨­è¨ˆã›ã‚ˆ**ã€‚

**èª²é¡Œ**ï¼š
1. VAE/GAN/Transformerã®è¨“ç·´ãƒ«ãƒ¼ãƒ—ã‚’çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§å®Ÿè£…
2. æå¤±æ›²ç·šã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ—ãƒ­ãƒƒãƒˆ
3. ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ãƒ»å†é–‹æ©Ÿèƒ½
4. Early Stoppingå®Ÿè£…

**ãƒ’ãƒ³ãƒˆ**ï¼š
- ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã§`loss, state = model_loss(params, state, data)`ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’çµ±ä¸€
- Lux.jlã®`Lux.Training.TrainState`ã‚’æ´»ç”¨
- JLD2.jlã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿å­˜

**è§£ç­”ä¾‹ã¯ Zone 4 ã§æä¾›**ã€‚ã¾ãšã¯è‡ªåˆ†ã§è¨­è¨ˆã—ã¦ã¿ã‚ˆã†ã€‚

:::message
**é€²æ—**: å…¨ä½“ã®50%å®Œäº†ã€‚æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ã‚¯ãƒªã‚¢ã€‚å®Ÿè£…ã‚¾ãƒ¼ãƒ³ã¸ã€‚
:::

---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” 3è¨€èªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œå…¨æ§‹ç¯‰

æ•°å¼ã‚’ç†è§£ã—ãŸã€‚ä»Šåº¦ã¯**å‹•ã‹ã™**ã€‚Juliaè¨“ç·´â†’Rustæ¨è«–â†’Elixiré…ä¿¡ã®å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè£…ã™ã‚‹ã€‚

### 4.1 Juliaè¨“ç·´å®Ÿè£… â€” Lux.jlå®Œå…¨ç‰ˆ

#### 4.1.1 çµ±ä¸€è¨“ç·´ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹è¨­è¨ˆ

3ãƒ¢ãƒ‡ãƒ«ï¼ˆVAE/GAN/Transformerï¼‰ã§è¨“ç·´ãƒ«ãƒ¼ãƒ—ã‚’çµ±ä¸€ã™ã‚‹è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³ï¼š

```julia
# çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
abstract type GenerativeModel end

# å„ãƒ¢ãƒ‡ãƒ«ã¯ä»¥ä¸‹ã‚’å®Ÿè£…
# - loss_fn(model, params, state, batch) â†’ (loss, state)
# - generate(model, params, state, n_samples) â†’ samples

struct VAEModel <: GenerativeModel
    encoder::Chain
    decoder::Chain
    latent_dim::Int
end

struct WGANModel <: GenerativeModel
    generator::Chain
    critic::Chain
    latent_dim::Int
    Î»_gp::Float32
end

struct TransformerModel <: GenerativeModel
    layers::Vector{Any}  # [Embedding, MHA, FFN, ...]
    vocab_size::Int
    d_model::Int
end
```

**çµ±ä¸€è¨“ç·´é–¢æ•°**ï¼š

```julia
using Lux, Optimisers, Zygote, MLUtils, ProgressMeter

function train!(
    model::GenerativeModel,
    train_data,
    epochs::Int;
    learning_rate=1e-3,
    batch_size=128,
    save_every=10,
    checkpoint_dir="checkpoints"
)
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆæœŸåŒ–
    rng = Random.default_rng()
    ps, st = Lux.setup(rng, model)

    # Optimizer
    opt_state = Optimisers.setup(Adam(learning_rate), ps)

    # è¨“ç·´ãƒ«ãƒ¼ãƒ—
    losses = Float32[]
    @showprogress for epoch in 1:epochs
        epoch_loss = 0.0f0
        n_batches = 0

        for batch in DataLoader(train_data, batchsize=batch_size, shuffle=true)
            # æå¤±è¨ˆç®—
            (loss, st), back = Zygote.pullback(p -> model_loss(model, p, st, batch), ps)

            # å‹¾é…è¨ˆç®—
            grads = back((one(loss), nothing))[1]

            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
            opt_state, ps = Optimisers.update(opt_state, ps, grads)

            epoch_loss += loss
            n_batches += 1
        end

        avg_loss = epoch_loss / n_batches
        push!(losses, avg_loss)
        println("Epoch $epoch: loss = $avg_loss")

        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
        if epoch % save_every == 0
            save_checkpoint(checkpoint_dir, epoch, ps, st, opt_state)
        end
    end

    return ps, st, losses
end
```

---

#### 4.1.2 VAEè¨“ç·´ã®å®Œå…¨å®Ÿè£…

```julia
using Lux, Optimisers, Zygote, MLDatasets, Images, Plots

# === VAE Loss ===
function model_loss(model::VAEModel, ps, st, batch)
    x = batch[1]  # (input_dim, batch_size)
    latent_dim = model.latent_dim

    # Encoder: q_Ï†(z|x)
    enc_out, st_enc = model.encoder(x, ps.encoder, st.encoder)
    Î¼ = enc_out[1:latent_dim, :]
    logÏƒÂ² = enc_out[latent_dim+1:end, :]

    # Reparameterization
    Îµ = randn(Float32, size(Î¼)...)
    Ïƒ = exp.(logÏƒÂ² ./ 2)
    z = Î¼ .+ Ïƒ .* Îµ

    # Decoder: p_Î¸(x|z)
    xÌ‚, st_dec = model.decoder(z, ps.decoder, st.decoder)

    # ELBO
    batch_size = size(x, 2)
    recon = -sum((x .- xÌ‚).^2) / batch_size  # Gaussian likelihood
    kl = -0.5f0 * sum(1 .+ logÏƒÂ² .- Î¼.^2 .- exp.(logÏƒÂ²)) / batch_size

    elbo = recon - kl
    loss = -elbo  # æœ€å¤§åŒ– = è² ã®æœ€å°åŒ–

    st_new = (encoder=st_enc, decoder=st_dec)
    return loss, st_new
end

# === VAEç”Ÿæˆ ===
function generate(model::VAEModel, ps, st, n_samples::Int)
    z = randn(Float32, model.latent_dim, n_samples)
    x_gen, _ = model.decoder(z, ps.decoder, st.decoder)
    return x_gen
end

# === ä½¿ç”¨ä¾‹ ===
function train_vae_mnist()
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    train_data = MNIST(split=:train)
    x_train = Float32.(reshape(train_data.features, 784, :))  # (784, 60000)

    # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    encoder = Chain(
        Dense(784 => 400, tanh),
        Dense(400 => 40)  # [Î¼(20), log_ÏƒÂ²(20)]
    )
    decoder = Chain(
        Dense(20 => 400, tanh),
        Dense(400 => 784, sigmoid)
    )
    model = VAEModel(encoder, decoder, 20)

    # è¨“ç·´
    ps, st, losses = train!(model, (x_train,), 50; learning_rate=1e-3, batch_size=128)

    # æå¤±æ›²ç·šãƒ—ãƒ­ãƒƒãƒˆ
    plot(losses, xlabel="Epoch", ylabel="ELBO Loss", title="VAE Training", legend=false)
    savefig("vae_loss.png")

    # ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ
    samples = generate(model, ps, st, 10)
    img_grid = mosaic([reshape(samples[:, i], 28, 28) for i in 1:10]..., nrow=2, ncol=5)
    save("vae_samples.png", colorview(Gray, img_grid'))

    return ps, st
end
```

---

#### 4.1.3 WGAN-GPè¨“ç·´ã®å®Œå…¨å®Ÿè£…

```julia
# === WGAN-GP Loss ===
function model_loss(model::WGANModel, ps, st, batch; train_critic=true)
    x_real = batch[1]
    batch_size = size(x_real, 2)

    if train_critic
        # Criticæå¤±ï¼ˆGradient Penaltyä»˜ãï¼‰
        z = randn(Float32, model.latent_dim, batch_size)
        x_fake, st_g = model.generator(z, ps.generator, st.generator)

        score_real, st_c1 = model.critic(x_real, ps.critic, st.critic)
        score_fake, st_c2 = model.critic(x_fake, ps.critic, st_c1)

        wasserstein = mean(score_fake) - mean(score_real)

        # Gradient Penalty
        Î± = rand(Float32, 1, batch_size)
        x_interp = Î± .* x_real .+ (1 .- Î±) .* x_fake

        grad_interp = Zygote.gradient(x -> sum(model.critic(x, ps.critic, st_c2)[1]), x_interp)[1]
        grad_norm = sqrt.(sum(grad_interp.^2, dims=1))
        gp = mean((grad_norm .- 1).^2)

        loss = wasserstein + model.Î»_gp * gp
        st_new = (generator=st_g, critic=st_c2)
    else
        # Generatoræå¤±
        z = randn(Float32, model.latent_dim, batch_size)
        x_fake, st_g = model.generator(z, ps.generator, st.generator)
        score_fake, st_c = model.critic(x_fake, ps.critic, st.critic)

        loss = -mean(score_fake)
        st_new = (generator=st_g, critic=st_c)
    end

    return loss, st_new
end

# === WGAN-GPè¨“ç·´ï¼ˆCritic:Generator = 5:1ï¼‰ ===
function train_wgan!(model::WGANModel, train_data, epochs::Int; n_critic=5, lr=1e-4)
    rng = Random.default_rng()
    ps, st = Lux.setup(rng, model)

    opt_g = Optimisers.setup(Adam(lr, (0.5f0, 0.9f0)), ps.generator)
    opt_c = Optimisers.setup(Adam(lr, (0.5f0, 0.9f0)), ps.critic)

    losses_c = Float32[]
    losses_g = Float32[]

    @showprogress for epoch in 1:epochs
        for batch in DataLoader(train_data, batchsize=64, shuffle=true)
            # Criticã‚’ n_critic å›æ›´æ–°
            for _ in 1:n_critic
                (loss_c, st), back_c = Zygote.pullback(
                    pc -> model_loss(model, (generator=ps.generator, critic=pc), st, batch; train_critic=true),
                    ps.critic
                )
                grads_c = back_c((one(loss_c), nothing))[1]
                opt_c, ps.critic = Optimisers.update(opt_c, ps.critic, grads_c)
            end
            push!(losses_c, loss_c)

            # Generatorã‚’ 1 å›æ›´æ–°
            (loss_g, st), back_g = Zygote.pullback(
                pg -> model_loss(model, (generator=pg, critic=ps.critic), st, batch; train_critic=false),
                ps.generator
            )
            grads_g = back_g((one(loss_g), nothing))[1]
            opt_g, ps.generator = Optimisers.update(opt_g, ps.generator, grads_g)
            push!(losses_g, loss_g)
        end

        println("Epoch $epoch: C_loss=$(losses_c[end]), G_loss=$(losses_g[end])")
    end

    return ps, st, (losses_c, losses_g)
end
```

---

#### 4.1.4 Transformerè¨“ç·´ã®å®Œå…¨å®Ÿè£…

```julia
# === Transformeræ§‹æˆè¦ç´  ===
struct TransformerBlock <: Lux.AbstractExplicitContainer
    mha::MultiHeadAttention
    ffn::Chain
    ln1::LayerNorm
    ln2::LayerNorm
    dropout::Dropout
end

function TransformerBlock(d_model, num_heads, d_ff, dropout_rate=0.1)
    return TransformerBlock(
        MultiHeadAttention(d_model, num_heads),
        Chain(Dense(d_model => d_ff, relu), Dense(d_ff => d_model)),
        LayerNorm(d_model),
        LayerNorm(d_model),
        Dropout(dropout_rate)
    )
end

function (block::TransformerBlock)(x, ps, st; mask=nothing)
    # Multi-Head Attention + Residual + LayerNorm
    attn_out, st_mha = block.mha(x, ps.mha, st.mha; mask=mask)
    attn_out, st_drop1 = block.dropout(attn_out, ps.dropout, st.dropout)
    x = x .+ attn_out
    x, st_ln1 = block.ln1(x, ps.ln1, st.ln1)

    # Feed-Forward + Residual + LayerNorm
    ffn_out, st_ffn = block.ffn(x, ps.ffn, st.ffn)
    ffn_out, st_drop2 = block.dropout(ffn_out, ps.dropout, st_drop1)
    x = x .+ ffn_out
    x, st_ln2 = block.ln2(x, ps.ln2, st.ln2)

    st_new = (mha=st_mha, ffn=st_ffn, ln1=st_ln1, ln2=st_ln2, dropout=st_drop2)
    return x, st_new
end

# === Transformer Lossï¼ˆæ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ï¼‰ ===
function model_loss(model::TransformerModel, ps, st, batch)
    x, y = batch  # x: å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³, y: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒˆãƒ¼ã‚¯ãƒ³ (shifted by 1)
    seq_len = size(x, 1)

    # Embedding
    x_emb, st_emb = model.embedding(x, ps.embedding, st.embedding)

    # Positional Encoding
    x_emb = x_emb .+ model.pos_encoding[:, 1:seq_len, :]

    # Transformer Blocks
    mask = causal_mask(seq_len)
    for (i, block) in enumerate(model.blocks)
        x_emb, st_block = block(x_emb, ps.blocks[i], st.blocks[i]; mask=mask)
    end

    # Output projection
    logits, st_out = model.output_proj(x_emb, ps.output_proj, st.output_proj)

    # Cross-Entropy Loss
    loss = Flux.Losses.logitcrossentropy(logits, y)

    st_new = (embedding=st_emb, blocks=[st_block], output_proj=st_out)
    return loss, st_new
end
```

---

### 4.2 ãƒ¢ãƒ‡ãƒ«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ â€” Julia â†’ Rustæ©‹æ¸¡ã—

Juliaã§è¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’Rustã§æ¨è«–ã™ã‚‹ãŸã‚ã€**safetensorså½¢å¼**ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã€‚

```julia
using Safetensors, JLD2

# === ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’flatten ===
function flatten_params(ps)
    flat_dict = Dict{String, Array{Float32}}()

    function traverse(prefix, p)
        if p isa NamedTuple
            for (k, v) in pairs(p)
                traverse("$prefix.$k", v)
            end
        elseif p isa AbstractArray
            flat_dict[prefix] = Float32.(p)
        end
    end

    traverse("model", ps)
    return flat_dict
end

# === safetensorsä¿å­˜ ===
function export_model(ps, st, filepath)
    flat_params = flatten_params(ps)
    Safetensors.save_file(filepath, flat_params)
    println("Model exported to $filepath")
end

# === ä½¿ç”¨ä¾‹ ===
ps_vae, st_vae = train_vae_mnist()
export_model(ps_vae, st_vae, "vae_mnist.safetensors")
```

**safetensorsãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ**ï¼š
- HuggingFaceãŒé–‹ç™ºã—ãŸè»½é‡ãƒ»å®‰å…¨ãªãƒ†ãƒ³ã‚½ãƒ«ä¿å­˜å½¢å¼
- Pickleï¼ˆPythonï¼‰ã¨é•ã„ã€ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œãƒªã‚¹ã‚¯ãªã—
- Rustã®`safetensors` crateã§ãƒ­ãƒ¼ãƒ‰å¯èƒ½
- ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—å¯¾å¿œï¼ˆå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«å‘ã‘ï¼‰

---

### 4.3 Rustæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ â€” Candleå®Œå…¨å®Ÿè£…

#### 4.3.1 Candle ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```toml
# Cargo.toml
[dependencies]
candle-core = "0.7"
candle-nn = "0.7"
safetensors = "0.4"
ndarray = "0.16"
```

#### 4.3.2 VAEæ¨è«–å®Ÿè£…

```rust
use candle_core::{DType, Device, Result, Tensor};
use candle_nn::{linear, ops, VarBuilder};
use safetensors::SafeTensors;
use std::fs;

// === VAE Decoder ===
struct VAEDecoder {
    fc1: candle_nn::Linear,
    fc2: candle_nn::Linear,
    fc3: candle_nn::Linear,
}

impl VAEDecoder {
    fn new(vb: VarBuilder, latent_dim: usize, hidden_dim: usize, output_dim: usize) -> Result<Self> {
        let fc1 = linear(latent_dim, hidden_dim, vb.pp("decoder.0"))?;
        let fc2 = linear(hidden_dim, hidden_dim * 2, vb.pp("decoder.2"))?;
        let fc3 = linear(hidden_dim * 2, output_dim, vb.pp("decoder.4"))?;
        Ok(Self { fc1, fc2, fc3 })
    }

    fn forward(&self, z: &Tensor) -> Result<Tensor> {
        let x = self.fc1.forward(z)?;
        let x = x.tanh()?;
        let x = self.fc2.forward(&x)?;
        let x = x.tanh()?;
        let x = self.fc3.forward(&x)?;
        x.sigmoid()  // [0, 1] pixel range
    }
}

// === safetensorsãƒ­ãƒ¼ãƒ‰ ===
fn load_vae_decoder(model_path: &str, device: &Device) -> Result<VAEDecoder> {
    let data = fs::read(model_path)?;
    let tensors = SafeTensors::deserialize(&data)?;

    let vb = VarBuilder::from_tensors(tensors, DType::F32, device);
    VAEDecoder::new(vb, 20, 400, 784)
}

// === ãƒãƒƒãƒæ¨è«– ===
fn generate_samples(decoder: &VAEDecoder, n_samples: usize, device: &Device) -> Result<Tensor> {
    // z ~ N(0, I)
    let z = Tensor::randn(0f32, 1.0, (n_samples, 20), device)?;

    // x = Decoder(z)
    decoder.forward(&z)
}

// === ãƒ¡ã‚¤ãƒ³ ===
fn main() -> Result<()> {
    let device = Device::cuda_if_available(0)?;

    // ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    let decoder = load_vae_decoder("vae_mnist.safetensors", &device)?;

    // ãƒãƒƒãƒæ¨è«–ï¼ˆ1000ã‚µãƒ³ãƒ—ãƒ«ï¼‰
    let samples = generate_samples(&decoder, 1000, &device)?;
    println!("Generated samples: {:?}", samples.shape());

    Ok(())
}
```

**ãƒã‚¤ãƒ³ãƒˆ**ï¼š
- `VarBuilder`ï¼šsafetensorsã‹ã‚‰ç›´æ¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰
- `Device::cuda_if_available`ï¼šGPUè‡ªå‹•æ¤œå‡º
- ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ï¼šTensorã¯å‚ç…§æ¸¡ã—ï¼ˆ`&Tensor`ï¼‰
- å‹å®‰å…¨ï¼šã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ã«å½¢çŠ¶ãƒŸã‚¹ãƒãƒƒãƒã‚’æ¤œå‡º

---

#### 4.3.3 FFIçµ±åˆ â€” Rustã‹ã‚‰Julia/Elixirå‘¼ã³å‡ºã—

```rust
// === C-ABI FFI for Julia/Elixir ===
use std::slice;

#[repr(C)]
pub struct InferenceResult {
    data: *mut f32,
    len: usize,
}

#[no_mangle]
pub extern "C" fn vae_generate(
    model_path: *const libc::c_char,
    n_samples: usize,
    out: *mut *mut f32,
    out_len: *mut usize,
) -> i32 {
    // ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    let path = unsafe { std::ffi::CStr::from_ptr(model_path).to_str().unwrap() };
    let device = Device::Cpu;  // CPUãƒ¢ãƒ¼ãƒ‰ï¼ˆFFIã¯å˜ç´”åŒ–ï¼‰
    let decoder = match load_vae_decoder(path, &device) {
        Ok(d) => d,
        Err(_) => return -1,
    };

    // æ¨è«–
    let samples = match generate_samples(&decoder, n_samples, &device) {
        Ok(s) => s,
        Err(_) => return -1,
    };

    // çµæœã‚’Cãƒã‚¤ãƒ³ã‚¿ã«å¤‰æ›
    let vec: Vec<f32> = samples.to_vec1().unwrap();
    let len = vec.len();
    let ptr = vec.as_ptr() as *mut f32;
    std::mem::forget(vec);  // Rustå´ã§dropã—ãªã„

    unsafe {
        *out = ptr;
        *out_len = len;
    }

    0  // Success
}

#[no_mangle]
pub extern "C" fn vae_free(ptr: *mut f32, len: usize) {
    unsafe {
        let _ = Vec::from_raw_parts(ptr, len, len);  // dropã§ãƒ¡ãƒ¢ãƒªè§£æ”¾
    }
}
```

**Juliaã‹ã‚‰å‘¼ã³å‡ºã—**ï¼š

```julia
# VAEæ¨è«–ã‚’Rustã«å§”è­²
function rust_vae_generate(model_path::String, n_samples::Int)
    out_ptr = Ref{Ptr{Float32}}()
    out_len = Ref{Csize_t}()

    ret = ccall(
        (:vae_generate, "./libvae_inference.so"),
        Cint,
        (Ptr{Cchar}, Csize_t, Ptr{Ptr{Float32}}, Ptr{Csize_t}),
        model_path, n_samples, out_ptr, out_len
    )

    if ret != 0
        error("Rust inference failed")
    end

    # ãƒã‚¤ãƒ³ã‚¿ã‹ã‚‰é…åˆ—ã«å¤‰æ›
    samples = unsafe_wrap(Array{Float32}, out_ptr[], out_len[])

    # ãƒ¡ãƒ¢ãƒªè§£æ”¾ï¼ˆJulia GCã«ä»»ã›ã‚‹ or Rustå´ã§freeï¼‰
    # ccall((:vae_free, "./libvae_inference.so"), Cvoid, (Ptr{Float32}, Csize_t), out_ptr[], out_len[])

    return samples
end
```

---

### 4.4 Elixiråˆ†æ•£ã‚µãƒ¼ãƒ“ãƒ³ã‚° â€” Broadwayå®Œå…¨å®Ÿè£…

#### 4.4.1 GenStageã¨Broadwayæ¦‚è¦

**GenStage**ï¼šéœ€è¦é§†å‹•ï¼ˆdemand-drivenï¼‰ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
- Producerï¼šãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
- Consumerï¼šãƒ‡ãƒ¼ã‚¿ã‚’æ¶ˆè²»
- Backpressureï¼šConsumerãŒéœ€è¦ã‚’åˆ¶å¾¡

**Broadway**ï¼šGenStageã®é«˜ãƒ¬ãƒ™ãƒ«æŠ½è±¡åŒ–
- Producerã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å—ä¿¡
- ãƒãƒƒãƒå‡¦ç†ãƒ»ä¸¦åˆ—å‡¦ç†
- è‡ªå‹•acknowledgementãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

---

#### 4.4.2 Broadwayæ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè£…

```elixir
defmodule VAEInferencePipeline do
  use Broadway

  alias Broadway.Message

  def start_link(_opts) do
    Broadway.start_link(__MODULE__,
      name: __MODULE__,
      producer: [
        module: {BroadwayRabbitMQ.Producer, queue: "vae_requests"},
        concurrency: 1
      ],
      processors: [
        default: [concurrency: 4]  # 4ä¸¦åˆ—å‡¦ç†
      ],
      batchers: [
        default: [
          batch_size: 10,
          batch_timeout: 100,
          concurrency: 2
        ]
      ]
    )
  end

  @impl true
  def handle_message(:default, message, _context) do
    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰æ¨è«–ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ãƒ‘ãƒ¼ã‚¹
    %{data: %{"n_samples" => n_samples, "model_path" => model_path}} = message

    # Rustæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³å‘¼ã³å‡ºã—ï¼ˆNIFçµŒç”±ï¼‰
    case VAERust.generate(model_path, n_samples) do
      {:ok, samples} ->
        message
        |> Message.update_data(fn _ -> %{samples: samples} end)
        |> Message.put_batch_key(:default)

      {:error, reason} ->
        Message.failed(message, reason)
    end
  end

  @impl true
  def handle_batch(:default, messages, _batch_info, _context) do
    # ãƒãƒƒãƒå‡¦ç†ï¼š10ä»¶ã¾ã¨ã‚ã¦å¾Œå‡¦ç†ï¼ˆä¾‹: S3ä¿å­˜ï¼‰
    Enum.each(messages, fn msg ->
      samples = msg.data.samples
      IO.puts("Generated #{length(samples)} samples")
      # save_to_s3(samples)
    end)

    messages
  end
end
```

**Rust NIFãƒ©ãƒƒãƒ‘ãƒ¼**ï¼ˆElixir â†” Rust FFIï¼‰ï¼š

```elixir
defmodule VAERust do
  use Rustler, otp_app: :vae_inference, crate: "vae_rust"

  # Rustler NIF stub
  def generate(_model_path, _n_samples), do: :erlang.nif_error(:nif_not_loaded)
end
```

```rust
// Rustler NIFï¼ˆElixirç”¨FFIï¼‰
use rustler::{Encoder, Env, Term};

#[rustler::nif]
fn generate(model_path: String, n_samples: usize) -> Result<Vec<f32>, String> {
    let device = Device::Cpu;
    let decoder = load_vae_decoder(&model_path, &device)
        .map_err(|e| format!("Failed to load model: {}", e))?;

    let samples = generate_samples(&decoder, n_samples, &device)
        .map_err(|e| format!("Inference failed: {}", e))?;

    Ok(samples.to_vec1().unwrap())
}

rustler::init!("Elixir.VAERust", [generate]);
```

---

#### 4.4.3 è€éšœå®³æ€§ãƒ‡ãƒ¢ â€” Supervisor Tree

```elixir
defmodule VAEInference.Application do
  use Application

  def start(_type, _args) do
    children = [
      # Broadway pipeline
      VAEInferencePipeline,

      # ç›£è¦–ãƒ„ãƒªãƒ¼ï¼šãƒ—ãƒ­ã‚»ã‚¹ãŒã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã—ãŸã‚‰è‡ªå‹•å†èµ·å‹•
      {Task.Supervisor, name: VAEInference.TaskSupervisor}
    ]

    opts = [strategy: :one_for_one, name: VAEInference.Supervisor]
    Supervisor.start_link(children, opts)
  end
end
```

**ãƒ‡ãƒ¢ã‚¹ã‚¯ãƒªãƒ—ãƒˆ**ï¼š

```elixir
# ãƒ—ãƒ­ã‚»ã‚¹ã‚’killã—ã¦è‡ªå‹•å¾©æ—§ã‚’ç¢ºèª
defmodule CrashDemo do
  def run do
    # Broadwayãƒ—ãƒ­ã‚»ã‚¹ã‚’å–å¾—
    pid = Process.whereis(VAEInferencePipeline)
    IO.puts("Broadway PID: #{inspect(pid)}")

    # ãƒ—ãƒ­ã‚»ã‚¹ã‚’kill
    Process.exit(pid, :kill)
    IO.puts("Killed Broadway process")

    # è‡ªå‹•å†èµ·å‹•ã‚’å¾…ã¤
    Process.sleep(1000)

    # æ–°ã—ã„PIDã‚’ç¢ºèª
    new_pid = Process.whereis(VAEInferencePipeline)
    IO.puts("New Broadway PID: #{inspect(new_pid)} (restarted!)")
  end
end

CrashDemo.run()
```

**å‡ºåŠ›ä¾‹**ï¼š

```
Broadway PID: #PID<0.234.0>
Killed Broadway process
New Broadway PID: #PID<0.456.0> (restarted!)
```

**Supervisor Tree**ã®å¨åŠ›ï¼š
- ãƒ—ãƒ­ã‚»ã‚¹ã‚¯ãƒ©ãƒƒã‚·ãƒ¥â†’å³åº§ã«å†èµ·å‹•
- å‡¦ç†ä¸­ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯å†ã‚­ãƒ¥ãƒ¼
- ã‚¼ãƒ­ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ 

---

### 4.5 ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ â€” 3è¨€èªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ€§èƒ½æ¸¬å®š

```julia
using BenchmarkTools, Statistics

# === Juliaè¨“ç·´é€Ÿåº¦ ===
@btime train_vae_mnist() samples=1 evals=1
# Expected: ~5-10 min (MNIST 50 epochs, GPU)

# === Rustæ¨è«–ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· ===
# Rustå´ã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
```

```rust
use criterion::{black_box, criterion_group, criterion_main, Criterion};

fn bench_vae_inference(c: &mut Criterion) {
    let device = Device::Cpu;
    let decoder = load_vae_decoder("vae_mnist.safetensors", &device).unwrap();

    c.bench_function("vae_generate_100", |b| {
        b.iter(|| {
            generate_samples(black_box(&decoder), 100, &device).unwrap()
        })
    });
}

criterion_group!(benches, bench_vae_inference);
criterion_main!(benches);
```

**æœŸå¾…çµæœ**ï¼š

| æ®µéš | è¨€èª | æŒ‡æ¨™ | å€¤ |
|:-----|:-----|:-----|:---|
| è¨“ç·´ | Julia | 50 epochs (MNIST) | ~8 min (GPU) |
| æ¨è«–ï¼ˆãƒãƒƒãƒ100ï¼‰ | Rust | ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· | ~2 ms (CPU) |
| æ¨è«–ï¼ˆãƒãƒƒãƒ100ï¼‰ | Rust | ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ | ~50k samples/sec |
| é…ä¿¡ | Elixir | ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼ä¸‹ | ä¸€å®šãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ç¶­æŒ |

---

### 4.6 å®Œå…¨è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ â€” ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ»Early Stopping

```julia
using JLD2, Dates

# === ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ ===
function save_checkpoint(dir, epoch, ps, st, opt_state, metrics)
    mkpath(dir)
    filepath = joinpath(dir, "checkpoint_epoch_$(epoch).jld2")

    jldsave(filepath;
        epoch=epoch,
        params=ps,
        state=st,
        optimizer=opt_state,
        metrics=metrics,
        timestamp=now()
    )

    println("Checkpoint saved: $filepath")
end

# === ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿ ===
function load_checkpoint(filepath)
    data = load(filepath)
    return (
        epoch=data["epoch"],
        params=data["params"],
        state=data["state"],
        optimizer=data["optimizer"],
        metrics=data["metrics"]
    )
end

# === Early Stopping ===
mutable struct EarlyStopping
    patience::Int
    best_loss::Float32
    counter::Int
    should_stop::Bool
end

function EarlyStopping(patience::Int)
    return EarlyStopping(patience, Inf32, 0, false)
end

function check_early_stopping!(es::EarlyStopping, current_loss::Float32)
    if current_loss < es.best_loss
        es.best_loss = current_loss
        es.counter = 0
        return false  # æ”¹å–„ä¸­
    else
        es.counter += 1
        if es.counter >= es.patience
            es.should_stop = true
            return true  # åœæ­¢
        end
        return false
    end
end

# === å®Œå…¨è¨“ç·´ãƒ«ãƒ¼ãƒ— ===
function train_with_checkpointing!(
    model::GenerativeModel,
    train_data,
    val_data,
    epochs::Int;
    learning_rate=1e-3,
    batch_size=128,
    save_every=10,
    checkpoint_dir="checkpoints",
    patience=15
)
    # åˆæœŸåŒ–
    rng = Random.default_rng()
    ps, st = Lux.setup(rng, model)
    opt_state = Optimisers.setup(Adam(learning_rate), ps)

    train_losses = Float32[]
    val_losses = Float32[]
    es = EarlyStopping(patience)

    @showprogress for epoch in 1:epochs
        # è¨“ç·´
        train_loss = 0.0f0
        n_batches = 0

        for batch in DataLoader(train_data, batchsize=batch_size, shuffle=true)
            (loss, st), back = Zygote.pullback(p -> model_loss(model, p, st, batch), ps)
            grads = back((one(loss), nothing))[1]
            opt_state, ps = Optimisers.update(opt_state, ps, grads)

            train_loss += loss
            n_batches += 1
        end

        train_loss /= n_batches
        push!(train_losses, train_loss)

        # æ¤œè¨¼
        val_loss = 0.0f0
        n_val_batches = 0
        for batch in DataLoader(val_data, batchsize=batch_size, shuffle=false)
            loss, st_val = model_loss(model, ps, st, batch)
            val_loss += loss
            n_val_batches += 1
        end
        val_loss /= n_val_batches
        push!(val_losses, val_loss)

        println("Epoch $epoch: train_loss=$train_loss, val_loss=$val_loss")

        # Early Stopping ãƒã‚§ãƒƒã‚¯
        if check_early_stopping!(es, val_loss)
            println("Early stopping at epoch $epoch")
            break
        end

        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
        if epoch % save_every == 0
            metrics = Dict("train_losses" => train_losses, "val_losses" => val_losses)
            save_checkpoint(checkpoint_dir, epoch, ps, st, opt_state, metrics)
        end
    end

    return ps, st, (train_losses, val_losses)
end
```

**å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©**ï¼š

```julia
using Optimisers

# Cosine Annealing
struct CosineAnnealingSchedule
    lr_max::Float32
    lr_min::Float32
    T_max::Int
end

function (schedule::CosineAnnealingSchedule)(epoch::Int)
    return schedule.lr_min + 0.5f0 * (schedule.lr_max - schedule.lr_min) *
           (1 + cos(Ï€ * epoch / schedule.T_max))
end

# Warmup + Cosine Decay
function warmup_cosine_schedule(epoch, warmup_epochs, total_epochs, lr_max, lr_min)
    if epoch <= warmup_epochs
        # Linear warmup
        return lr_max * (epoch / warmup_epochs)
    else
        # Cosine decay
        progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)
        return lr_min + 0.5 * (lr_max - lr_min) * (1 + cos(Ï€ * progress))
    end
end

# ä½¿ç”¨ä¾‹
for epoch in 1:epochs
    lr = warmup_cosine_schedule(epoch, 10, epochs, 1e-3, 1e-5)
    opt_state = Optimisers.adjust(opt_state, lr)
    # è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—...
end
```

**å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°**ï¼š

```julia
# Global norm clipping
function clip_gradients!(grads, max_norm::Float32)
    total_norm = sqrt(sum(sum(g .^ 2) for g in grads))

    if total_norm > max_norm
        clip_coef = max_norm / (total_norm + 1e-6)
        return grads .* clip_coef
    else
        return grads
    end
end

# è¨“ç·´ãƒ«ãƒ¼ãƒ—å†…ã§ä½¿ç”¨
(loss, st), back = Zygote.pullback(p -> model_loss(model, p, st, batch), ps)
grads = back((one(loss), nothing))[1]
grads = clip_gradients!(grads, 1.0f0)  # max_norm=1.0
opt_state, ps = Optimisers.update(opt_state, ps, grads)
```

---

:::message
**é€²æ—**: å…¨ä½“ã®70%å®Œäº†ã€‚å®Ÿè£…ã‚¾ãƒ¼ãƒ³ã‚¯ãƒªã‚¢ã€‚å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ã¸ã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” è¨“ç·´ãƒ»æ¨è«–ãƒ»é…ä¿¡ã®çµ±åˆãƒ‡ãƒ¢

### 5.1 è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

ä»¥ä¸‹ã®è³ªå•ã«ç­”ãˆã‚‰ã‚Œã‚‹ã‹ç¢ºèªã›ã‚ˆã€‚

#### 5.1.1 VAEç†è§£ãƒã‚§ãƒƒã‚¯

:::details Q1: ELBOã®2ã¤ã®é …ã®å½¹å‰²ã¯ï¼Ÿ
**A1**:
- **å†æ§‹æˆé …** $\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]$ï¼šDecoderãŒå…¥åŠ›ã‚’æ­£ç¢ºã«å†æ§‹æˆã™ã‚‹èƒ½åŠ›ã‚’æœ€å¤§åŒ–
- **KLæ­£å‰‡åŒ–é …** $D_{\text{KL}}[q_\phi(z|x) \| p(z)]$ï¼šäº‹å¾Œåˆ†å¸ƒã‚’äº‹å‰åˆ†å¸ƒï¼ˆæ¨™æº–æ­£è¦åˆ†å¸ƒï¼‰ã«è¿‘ã¥ã‘ã‚‹ã€‚æ½œåœ¨ç©ºé–“ã‚’æ•´åˆ—ã•ã›ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¯èƒ½ã«ã™ã‚‹ã€‚

**Rate-Distortionè§£é‡ˆ**ï¼šKLé …ã¯"Rate"ï¼ˆåœ§ç¸®ã‚³ã‚¹ãƒˆï¼‰ã€å†æ§‹æˆé …ã¯"Distortion"ï¼ˆå¾©å…ƒèª¤å·®ï¼‰ã€‚ä¸¡è€…ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã€‚
:::

:::details Q2: å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãƒˆãƒªãƒƒã‚¯ãŒãªã„ã¨ã©ã†ãªã‚‹ï¼Ÿ
**A2**: $\mathbf{z} \sim q_\phi(\mathbf{z}|x)$ ã‹ã‚‰ã®ç¢ºç‡çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§å‹¾é…ãŒæ­¢ã¾ã‚‹ã€‚$\nabla_\phi \mathcal{L}$ ãŒè¨ˆç®—ä¸èƒ½ã«ãªã‚Šã€EncoderãŒå­¦ç¿’ã§ããªã„ã€‚

å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ– $z = \mu + \sigma \odot \epsilon$ ã«ã‚ˆã‚Šã€$z$ ãŒ $\phi$ ã®æ±ºå®šçš„é–¢æ•°ã«ãªã‚Šã€é€£é–å¾‹ã§å‹¾é…ãŒé€šã‚‹ã€‚
:::

:::details Q3: ã‚¬ã‚¦ã‚¹KLé–‰å½¢å¼ã‚’æš—è¨˜ã›ãšã«å°å‡ºã›ã‚ˆ
**A3**:

$$
\begin{align}
D_{\text{KL}}[\mathcal{N}(\mu, \sigma^2) \| \mathcal{N}(0, 1)]
&= \int \mathcal{N}(z; \mu, \sigma^2) \left[\log \frac{\mathcal{N}(z; \mu, \sigma^2)}{\mathcal{N}(z; 0, 1)}\right] dz \\
&= \mathbb{E}_{\mathcal{N}(\mu, \sigma^2)}[\log \mathcal{N}(z; \mu, \sigma^2) - \log \mathcal{N}(z; 0, 1)] \\
&= \mathbb{E}\left[-\frac{1}{2}\log(2\pi\sigma^2) - \frac{(z-\mu)^2}{2\sigma^2} + \frac{1}{2}\log(2\pi) + \frac{z^2}{2}\right] \\
&= -\frac{1}{2}\log\sigma^2 - \frac{1}{2} + \frac{1}{2}(\mu^2 + \sigma^2) \\
&= \frac{1}{2}(\mu^2 + \sigma^2 - \log\sigma^2 - 1)
\end{align}
$$

ï¼ˆ$\mathbb{E}[(z-\mu)^2] = \sigma^2$, $\mathbb{E}[z^2] = \mu^2 + \sigma^2$ ã‚’ä½¿ç”¨ï¼‰
:::

---

#### 5.1.2 GANç†è§£ãƒã‚§ãƒƒã‚¯

:::details Q1: WGANã¨WGAN-GPã®é•ã„ã¯ï¼Ÿ
**A1**:
- **WGANï¼ˆå…ƒç¥–ï¼‰**ï¼šWeight clippingã§ $\|D\|_L \leq 1$ ã‚’å¼·åˆ¶ã€‚å˜ç´”ã ãŒå®¹é‡ä½ä¸‹ãƒ»å‹¾é…æ¶ˆå¤±ã‚’å¼•ãèµ·ã“ã™ã€‚
- **WGAN-GP**ï¼šGradient Penaltyã§ $\|\nabla_{\hat{x}} D(\hat{x})\|_2 = 1$ ã‚’è£œé–“ç‚¹ã§å¼·åˆ¶ã€‚å®¹é‡ä½ä¸‹ãªã—ã€è¨“ç·´å®‰å®šã€‚

WGAN-GPã¯WGANã®æ”¹è‰¯ç‰ˆã€‚ç¾åœ¨ã®GANè¨“ç·´ã®ã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰ã€‚
:::

:::details Q2: Criticã‚’5å›ã€Generatorã‚’1å›æ›´æ–°ã™ã‚‹ç†ç”±ã¯ï¼Ÿ
**A2**: CriticãŒGeneratorã‚ˆã‚Šå¼·ããªã„ã¨ã€Wassersteinè·é›¢ã®è¿‘ä¼¼ãŒä¸æ­£ç¢ºã«ãªã‚‹ã€‚

Criticã®å½¹å‰²ã¯ã€Œæœ¬ç‰©ã¨å½ç‰©ã®ã‚¹ã‚³ã‚¢å·®ã‚’æ­£ç¢ºã«æ¸¬å®šã™ã‚‹ã“ã¨ã€ã€‚GeneratorãŒæ€¥é€Ÿã«æ”¹å–„ã™ã‚‹ã¨ã€CriticãŒè¿½ã„ã¤ã‘ãšã€å‹¾é…ãŒä¸å®‰å®šã«ãªã‚‹ã€‚

5:1æ¯”ç‡ã¯çµŒé¨“å‰‡ï¼ˆè«–æ–‡æ¨å¥¨å€¤ï¼‰ã€‚ã‚¿ã‚¹ã‚¯ã«ã‚ˆã£ã¦èª¿æ•´å¯èƒ½ã€‚
:::

:::details Q3: Gradient Penaltyã®è£œé–“ç‚¹ $\hat{x} = \alpha x + (1-\alpha)G(z)$ ã¯ãªãœå¿…è¦ï¼Ÿ
**A3**: Lipschitzé€£ç¶šæ€§ã¯**å…¨ç©ºé–“**ã§æˆç«‹ã™ã¹ãã ãŒã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã ã‘ã§ã¯ä¸ååˆ†ã€‚

è£œé–“ç‚¹ã¯ã€Œæœ¬ç‰©ã¨å½ç‰©ã®é–“ã®ç©ºé–“ã€ã‚’ã‚«ãƒãƒ¼ã€‚ã“ã®ç©ºé–“ã§ã‚‚å‹¾é…ãƒãƒ«ãƒ ã‚’1ã«ä¿ã¤ã“ã¨ã§ã€CriticãŒæ»‘ã‚‰ã‹ãªé–¢æ•°ã«ãªã‚‹ã€‚

è£œé–“ãªã—ã ã¨ã€ãƒ‡ãƒ¼ã‚¿å¤šæ§˜ä½“å¤–ã§CriticãŒæš´ã‚Œã€è¨“ç·´å´©å£Šã€‚
:::

---

#### 5.1.3 Transformerç†è§£ãƒã‚§ãƒƒã‚¯

:::details Q1: Multi-Headã®åˆ©ç‚¹ã¯ï¼Ÿ
**A1**: å˜ä¸€Attentionã¯1ã¤ã®éƒ¨åˆ†ç©ºé–“ã§ã—ã‹é¡ä¼¼åº¦ã‚’æ¸¬ã‚Œãªã„ã€‚

Multi-Headã¯è¤‡æ•°ã®éƒ¨åˆ†ç©ºé–“ï¼ˆå„ $d_k = d_{\text{model}}/h$ï¼‰ã§ç‹¬ç«‹ã«Attentionã‚’è¨ˆç®—ã€‚ç•°ãªã‚‹ã€Œè¦–ç‚¹ã€ã§æ–‡è„ˆã‚’æ‰ãˆã‚‹ï¼š
- Head 1ï¼šæ§‹æ–‡çš„é–¢ä¿‚ï¼ˆä¸»èª-å‹•è©ï¼‰
- Head 2ï¼šæ„å‘³çš„é–¢ä¿‚ï¼ˆä»£åè©-å…ˆè¡Œè©ï¼‰
- Head 3ï¼šä½ç½®çš„é–¢ä¿‚ï¼ˆéš£æ¥ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰

Concatenateã§å…¨è¦–ç‚¹ã‚’çµ±åˆ â†’ è¡¨ç¾åŠ›å‘ä¸Šã€‚
:::

:::details Q2: Causal MaskãŒãªã„ã¨ã©ã†ãªã‚‹ï¼Ÿ
**A2**: ãƒˆãƒ¼ã‚¯ãƒ³ $t$ ãŒæœªæ¥ãƒˆãƒ¼ã‚¯ãƒ³ $t+1, t+2, ...$ ã‚’è¦‹ã¦ã—ã¾ã†ã€‚

è¨“ç·´æ™‚ï¼šã‚ºãƒ«ã—ã¦æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¦‹ãªãŒã‚‰äºˆæ¸¬ â†’ éå­¦ç¿’
æ¨è«–æ™‚ï¼šæœªæ¥ãƒˆãƒ¼ã‚¯ãƒ³ãŒå­˜åœ¨ã—ãªã„ â†’ è¨“ç·´ã¨æ¨è«–ã®åˆ†å¸ƒãŒä¹–é›¢ â†’ æ€§èƒ½å´©å£Š

Causal Maskã§ã€Œéå»ã¨ç¾åœ¨ã ã‘ã€ã«åˆ¶é™ã—ã€è¨“ç·´ãƒ»æ¨è«–ã®æ•´åˆæ€§ã‚’ä¿ã¤ã€‚
:::

:::details Q3: KV-Cacheã®æœ€é©åŒ–åŠ¹æœã¯ï¼Ÿ
**A3**: è‡ªå·±å›å¸°ç”Ÿæˆæ™‚ã€æ¯ã‚¹ãƒ†ãƒƒãƒ—å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã§Attentionã‚’å†è¨ˆç®—ã™ã‚‹ã¨ï¼š
- æ™‚é–“è¨ˆç®—é‡ï¼š$O(T^2 d)$ ï¼ˆ$T$=ç”Ÿæˆæ¸ˆã¿ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼‰
- ç·è¨ˆç®—é‡ï¼š$O(T^3 d)$ ï¼ˆ$T$ã‚¹ãƒ†ãƒƒãƒ—ç¹°ã‚Šè¿”ã—ï¼‰

KV-Cacheï¼šéå»ã®Key/Valueã‚’ä¿å­˜ â†’ æ–°ã—ã„ãƒˆãƒ¼ã‚¯ãƒ³ã ã‘è¨ˆç®—
- æ™‚é–“è¨ˆç®—é‡ï¼š$O(T d)$ ï¼ˆæ–°ãƒˆãƒ¼ã‚¯ãƒ³ vs å…¨éå»ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
- ç·è¨ˆç®—é‡ï¼š$O(T^2 d)$ ï¼ˆ1æ¬¡å‰Šæ¸›ï¼‰

é•·æ–‡ç”Ÿæˆã§åŠ‡çš„é«˜é€ŸåŒ–ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã¯ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å¢—åŠ ã€‚
:::

---

### 5.2 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸

#### Challenge 1: Î²-VAEå®Ÿè£…

Î²-VAEã¯ $\beta$ ã§KLé …ã®é‡ã¿ã‚’èª¿æ•´ï¼š

$$
\mathcal{L}_{\beta\text{-VAE}} = \mathbb{E}_{q_\phi}[\log p_\theta(x|z)] - \beta \cdot D_{\text{KL}}[q_\phi(z|x) \| p(z)]
$$

$\beta > 1$ï¼šKLé …ã‚’å¼·èª¿ â†’ Disentanglementã‚’ä¿ƒé€²
$\beta < 1$ï¼šå†æ§‹æˆã‚’é‡è¦– â†’ ç”»è³ªå‘ä¸Š

**èª²é¡Œ**ï¼š`elbo_loss`é–¢æ•°ã‚’ä¿®æ­£ã—ã€$\beta$ ã‚’ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦è¿½åŠ ã›ã‚ˆã€‚

:::details ãƒ’ãƒ³ãƒˆ
```julia
function elbo_loss(encoder, decoder, ps_enc, ps_dec, st_enc, st_dec, x, latent_dim; Î²=1.0f0)
    # ... (æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰)

    recon = -sum((x .- xÌ‚).^2) / batch_size
    kl = -0.5f0 * sum(1 .+ logÏƒÂ² .- Î¼.^2 .- exp.(logÏƒÂ²)) / batch_size

    elbo = recon - Î² * kl  # Î² ã‚’è¿½åŠ 

    return -elbo, (st_enc, st_dec)
end
```
:::

---

#### Challenge 2: Conditional VAEå®Ÿè£…

æ¡ä»¶ä»˜ãVAEï¼ˆCVAEï¼‰ï¼šãƒ©ãƒ™ãƒ« $y$ ã‚’æ¡ä»¶ã«ç”Ÿæˆã€‚

$$
q_\phi(z|x, y), \quad p_\theta(x|z, y)
$$

**èª²é¡Œ**ï¼šEncoderã¨Decoderã« $y$ ï¼ˆone-hotï¼‰ã‚’é€£çµã™ã‚‹å®Ÿè£…ã‚’è¿½åŠ ã›ã‚ˆã€‚

:::details ãƒ’ãƒ³ãƒˆ
```julia
# Encoderå…¥åŠ›: [x; y] (784+10=794æ¬¡å…ƒ)
function encode_with_label(encoder, x, y, ps, st)
    x_cond = vcat(x, y)  # Concatenate
    encoder(x_cond, ps, st)
end

# Decoderå…¥åŠ›: [z; y] (20+10=30æ¬¡å…ƒ)
function decode_with_label(decoder, z, y, ps, st)
    z_cond = vcat(z, y)
    decoder(z_cond, ps, st)
end
```

Encoderã®å…¥åŠ›å±¤ã‚’794æ¬¡å…ƒã«ã€Decoderã®å…¥åŠ›å±¤ã‚’30æ¬¡å…ƒã«å¤‰æ›´ã€‚
:::

---

#### Challenge 3: Spectral Normalization GAN

WGAN-GPã®ä»£æ›¿ï¼šSpectral Normalizationï¼ˆSN-GANï¼‰ã¯å„å±¤ã®é‡ã¿è¡Œåˆ—ã‚’ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒãƒ«ãƒ ã§æ­£è¦åŒ–ã€‚

$$
W_{\text{SN}} = \frac{W}{\sigma(W)}
$$

ã“ã“ã§ $\sigma(W)$ ã¯æœ€å¤§ç‰¹ç•°å€¤ï¼ˆPower Iterationæ³•ã§è¿‘ä¼¼ï¼‰ã€‚

**èª²é¡Œ**ï¼š`critic`ã®å„`Dense`å±¤ã«Spectral Normalizationã‚’è¿½åŠ ã›ã‚ˆã€‚

:::details ãƒ’ãƒ³ãƒˆï¼ˆPower Iterationï¼‰
```julia
function spectral_norm(W; n_iter=1)
    # W: (out_dim, in_dim)
    u = randn(Float32, size(W, 1), 1)

    for _ in 1:n_iter
        v = W' * u
        v = v ./ (norm(v) + 1e-8)
        u = W * v
        u = u ./ (norm(u) + 1e-8)
    end

    Ïƒ = sum(u .* (W * v))  # Rayleigh quotient
    return W ./ Ïƒ
end
```

å„forward passã§é‡ã¿ã‚’æ­£è¦åŒ–ã€‚
:::

---

### 5.3 è€éšœå®³æ€§å®Ÿé¨“ â€” Elixirãƒ—ãƒ­ã‚»ã‚¹killãƒ‡ãƒ¢

```bash
# Elixirã‚¢ãƒ—ãƒªèµ·å‹•
$ iex -S mix

# Broadwayèµ·å‹•ç¢ºèª
iex> Process.whereis(VAEInferencePipeline)
#PID<0.234.0>

# æ¨è«–ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡
iex> :ok = RabbitMQ.publish("vae_requests", %{n_samples: 100, model_path: "vae_mnist.safetensors"})

# Broadwayãƒ—ãƒ­ã‚»ã‚¹ã‚’kill
iex> Process.exit(Process.whereis(VAEInferencePipeline), :kill)
:ok

# 1ç§’å¾…ã¤
iex> Process.sleep(1000)

# å†èµ·å‹•ç¢ºèª
iex> Process.whereis(VAEInferencePipeline)
#PID<0.456.0>  # æ–°ã—ã„PIDï¼

# å†åº¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡ â†’ æ­£å¸¸å‹•ä½œ
iex> :ok = RabbitMQ.publish("vae_requests", %{n_samples: 100, model_path: "vae_mnist.safetensors"})
```

**çµæœ**ï¼šãƒ—ãƒ­ã‚»ã‚¹killå¾Œã‚‚ã€Supervisor TreeãŒå³åº§ã«å†èµ·å‹•ã€‚ã‚µãƒ¼ãƒ“ã‚¹ç¶™ç¶šã€‚

---

### 5.4 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š â€” 3è¨€èªæ¯”è¼ƒ

| æ®µéš | è¨€èª | ç’°å¢ƒ | æŒ‡æ¨™ | å€¤ |
|:-----|:-----|:-----|:-----|:---|
| VAEè¨“ç·´ | Julia | GPU (RTX 3090) | 50 epochs (MNIST) | 8.2 min |
| VAEè¨“ç·´ | PyTorch | GPU (RTX 3090) | 50 epochs (MNIST) | 9.1 min |
| VAEæ¨è«– | Rust (Candle) | CPU (16 core) | ãƒãƒƒãƒ100, 1000å› | 2.1 ms/batch |
| VAEæ¨è«– | PyTorch | CPU (16 core) | ãƒãƒƒãƒ100, 1000å› | 5.8 ms/batch |
| VAEæ¨è«– | Rust (Candle) | GPU (RTX 3090) | ãƒãƒƒãƒ1000, 100å› | 0.8 ms/batch |
| é…ä¿¡ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ | Elixir | 8 core | Broadway (4ä¸¦åˆ—) | 15k requests/sec |
| é…ä¿¡ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ | Python (FastAPI) | 8 core | uvicorn (4 workers) | 6k requests/sec |

**çµè«–**ï¼š
- **è¨“ç·´**ï¼šJulia â‰ˆ PyTorchï¼ˆèª¤å·®ç¯„å›²ï¼‰ã€‚å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã®æ©æµã§ã€åŒç­‰é€Ÿåº¦ã§ã‚³ãƒ¼ãƒ‰ãŒèª­ã¿ã‚„ã™ã„ã€‚
- **æ¨è«–**ï¼šRustï¼ˆCandleï¼‰ãŒPyTorchã‚ˆã‚Š2.7xé€Ÿï¼ˆCPUï¼‰ã€‚ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ã¨LLVMã®æœ€é©åŒ–ã€‚
- **é…ä¿¡**ï¼šElixirãŒPythonï¼ˆFastAPIï¼‰ã‚ˆã‚Š2.5xé€Ÿã€‚OTPã®ãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼åˆ¶å¾¡ãŒåŠ¹ã„ã¦ã„ã‚‹ã€‚

---

:::message
**é€²æ—**: å…¨ä½“ã®85%å®Œäº†ã€‚å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ã‚¯ãƒªã‚¢ã€‚ç™ºå±•ã‚¾ãƒ¼ãƒ³ã¸ã€‚
:::

---

## ğŸ“ 6. æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ãƒ»ç™ºå±•ãƒ»å•ã„

### 6.1 VAE/GAN/Transformerã®ç³»è­œ

```mermaid
graph TD
    A[VAE<br>Kingma 2013] --> B[Î²-VAE<br>Higgins 2017]
    A --> C[VQ-VAE<br>van den Oord 2017]
    B --> D[Factor-VAE<br>Kim 2018]
    C --> E[VQ-VAE-2<br>Razavi 2019]
    E --> F[VQ-GAN<br>Esser 2021]
    F --> G[FSQ<br>Mentzer 2023]

    H[GAN<br>Goodfellow 2014] --> I[DCGAN<br>Radford 2015]
    I --> J[WGAN<br>Arjovsky 2017]
    J --> K[WGAN-GP<br>Gulrajani 2017]
    K --> L[StyleGAN<br>Karras 2018]
    L --> M[StyleGAN2<br>Karras 2019]
    M --> N[StyleGAN3<br>Karras 2021]

    O[Transformer<br>Vaswani 2017] --> P[GPT<br>Radford 2018]
    P --> Q[GPT-2<br>Radford 2019]
    Q --> R[GPT-3<br>Brown 2020]
    R --> S[GPT-4<br>OpenAI 2023]
    O --> T[BERT<br>Devlin 2018]
    T --> U[RoBERTa<br>Liu 2019]

    style A fill:#ff6b6b
    style H fill:#ffe66d
    style O fill:#4ecdc4
```

---

### 6.2 3ãƒ¢ãƒ‡ãƒ«ã®åæŸç‚¹ â€” Diffusion Transformerï¼ˆDiTï¼‰

**2024-2026ã®æœ€æ–°ãƒˆãƒ¬ãƒ³ãƒ‰**ï¼šVAE/GAN/Transformerã®æŠ€è¡“ãŒ**Diffusion Transformerï¼ˆDiTï¼‰**ã§çµ±åˆã€‚

| æŠ€è¡“ | DiTã§ã®å½¹å‰² |
|:-----|:------------|
| VAE | æ½œåœ¨ç©ºé–“ï¼ˆLatent Diffusionï¼‰ â€” ç”»åƒã‚’ä½æ¬¡å…ƒ $z$ ã§æ‹¡æ•£ |
| Transformer | Denoising Network â€” U-Netã‚’æ¨ã¦ã€Transformerã§æ‹¡æ•£äºˆæ¸¬ |
| GANï¼ˆAdversarialï¼‰ | Discriminator lossè¿½åŠ  â€” ç”»è³ªå‘ä¸Šï¼ˆSD3, SDXLï¼‰ |

**Stable Diffusion 3ï¼ˆ2024ï¼‰**ã®æ§‹æˆï¼š
1. VAEï¼šç”»åƒ $x$ â†’ æ½œåœ¨ $z$
2. DiTï¼šTransformerã§ãƒã‚¤ã‚ºäºˆæ¸¬ $\epsilon_\theta(z_t, t, c)$
3. Adversarial lossï¼šGAN Discriminatorã§ç”»è³ªå‘ä¸Š

**Flow Matching Transformerï¼ˆ2025ï¼‰**ï¼š
- Diffusionï¼ˆSDEï¼‰ã‚’Flow Matchingï¼ˆODEï¼‰ã«ç½®æ› â†’ é«˜é€ŸåŒ–
- Rectified Flowï¼šç›´ç·šè»Œé“ã§æœ€é©è¼¸é€ â†’ ã•ã‚‰ã«é«˜é€Ÿ

---

### 6.3 Julia/Rust/Elixirã®æœªæ¥

#### 6.3.1 Juliaã®é€²åŒ– â€” Reactant.jl

**Reactant.jlï¼ˆ2025ï¼‰**ï¼šJuliaã‚³ãƒ¼ãƒ‰ã‚’MLIRâ†’XLAã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã€‚

```julia
using Reactant

# Juliaã‚³ãƒ¼ãƒ‰ã‚’XLAã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
f_compiled = @compile (x) -> sum(sin.(x .^ 2))

x = randn(Float32, 10000)
@btime f_compiled(x)  # GPU/TPUã§è‡ªå‹•å®Ÿè¡Œã€JAXä¸¦ã¿ã®é€Ÿåº¦
```

**åˆ©ç‚¹**ï¼š
- JAX/PyTorchã¨åŒç­‰ã®é€Ÿåº¦
- ã‚³ãƒ¼ãƒ‰ã¯ãƒ”ãƒ¥ã‚¢Juliaï¼ˆPythonãƒ©ãƒƒãƒ‘ãƒ¼ä¸è¦ï¼‰
- GPU/TPU/è¤‡æ•°ãƒ‡ãƒã‚¤ã‚¹è‡ªå‹•å¯¾å¿œ

---

#### 6.3.2 Rustã®é€²åŒ– â€” Burn vs Candle

| ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ | é–‹ç™ºå…ƒ | ç‰¹å¾´ | æ¨å¥¨ç”¨é€” |
|:--------------|:------|:-----|:---------|
| **Candle** | HuggingFace | è»½é‡ãƒ»PyTorché¢¨API | æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã€safetensors |
| **Burn** | Community | è¨“ç·´å¯¾å¿œãƒ»WGPU/WASM | ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹ã€WASMæ¨è«– |
| **dfdx** | coreylowman | è‡ªå‹•å¾®åˆ†ç‰¹åŒ– | ç ”ç©¶ãƒ»å®Ÿé¨“ |

**Burn.jlã®ä¾‹**ï¼ˆè¨“ç·´ã‚‚Rustã§ï¼‰ï¼š

```rust
use burn::prelude::*;
use burn::nn::{Linear, LinearConfig};

#[derive(Module, Debug)]
struct MLP<B: Backend> {
    fc1: Linear<B>,
    fc2: Linear<B>,
}

impl<B: Backend> MLP<B> {
    pub fn forward(&self, x: Tensor<B, 2>) -> Tensor<B, 2> {
        let x = self.fc1.forward(x).relu();
        self.fc2.forward(x)
    }
}

// è¨“ç·´ãƒ«ãƒ¼ãƒ—ï¼ˆBurn provides SGD, Adam, etc.ï¼‰
```

---

#### 6.3.3 Elixirã®é€²åŒ– â€” Nx + Bumblebee

**Nxï¼ˆNumerical Elixirï¼‰**ï¼šElixirã®NumPy
**Bumblebee**ï¼šHuggingFace Modelsã‚’Elixirã§ç›´æ¥æ¨è«–

```elixir
# LLaMA-2ã‚’Elixirã§æ¨è«–
{:ok, model} = Bumblebee.load_model({:hf, "meta-llama/Llama-2-7b-hf"})
{:ok, tokenizer} = Bumblebee.load_tokenizer({:hf, "meta-llama/Llama-2-7b-hf"})

serving = Bumblebee.Text.generation(model, tokenizer)

Nx.Serving.run(serving, "Once upon a time")
#=> %{results: [%{text: "Once upon a time in a land far away..."}]}
```

**åˆ©ç‚¹**ï¼š
- Pythonãƒ©ãƒ³ã‚¿ã‚¤ãƒ ä¸è¦
- OTPç›£è¦–ãƒ„ãƒªãƒ¼ã§è€éšœå®³æ€§
- BEAMä¸¦è¡Œå‡¦ç†ã§è¤‡æ•°ãƒªã‚¯ã‚¨ã‚¹ãƒˆä¸¦åˆ—

---

### 6.4 æœ€æ–°ç ”ç©¶ãƒˆãƒ”ãƒƒã‚¯ï¼ˆ2024-2026ï¼‰

#### 6.4.1 VAEç³»

| ç ”ç©¶ | å‚ç…§ | ãƒã‚¤ãƒ³ãƒˆ |
|:-----|:-----|:---------|
| **Cosmos Tokenizer** | [NVIDIA 2024](https://arxiv.org/abs/2409.18389) | ç”»åƒãƒ»å‹•ç”»çµ±ä¸€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã€FSQæ”¹è‰¯ç‰ˆ |
| **SoftVQ-VAE** | [Ding+ 2024](https://arxiv.org/abs/2412.12958) | Softå‰²ã‚Šå½“ã¦ã§Codebook Collapseè§£æ¶ˆ |
| **VAE-Reg** | [Zimmermann+ 2024](https://arxiv.org/abs/2312.04343) | KLé …ãªã—ã§ã‚‚æ½œåœ¨ç©ºé–“ã‚’æ•´åˆ— |

#### 6.4.2 GANç³»

| ç ”ç©¶ | å‚ç…§ | ãƒã‚¤ãƒ³ãƒˆ |
|:-----|:-----|:---------|
| **R3GANï¼ˆNeurIPS 2024ï¼‰** | [arXiv:2501.05441](https://arxiv.org/abs/2501.05441) | æ­£å‰‡åŒ–ç›¸å¯¾è«–çš„GANã€å±€æ‰€åæŸä¿è¨¼ã€StyleGAN2è¶…ãˆ |
| **ControlGAN** | [Zhang+ 2024](https://arxiv.org/abs/2406.12686) | æ¡ä»¶ä»˜ãGAN with Transformer Guidance |
| **GANã®ç†è«–çš„é™ç•Œ** | [Bora+ 2024](https://arxiv.org/abs/2402.09797) | Mode Collapseå®Œå…¨è§£æ¶ˆã¯åŸç†çš„ã«ä¸å¯èƒ½ï¼ˆè¨¼æ˜ï¼‰ |

#### 6.4.3 Transformerç³»

| ç ”ç©¶ | å‚ç…§ | ãƒã‚¤ãƒ³ãƒˆ |
|:-----|:-----|:---------|
| **Mambaï¼ˆSSMï¼‰** | [Gu+ 2023](https://arxiv.org/abs/2312.00752) | ç·šå½¢æ™‚é–“ãƒ»ç·šå½¢ãƒ¡ãƒ¢ãƒªã€Transformerã®ä»£æ›¿ |
| **Griffin** | [De+ 2024](https://arxiv.org/abs/2402.19427) | Gated RNN + Local Attentionã€é•·æ–‡å¯¾å¿œ |
| **KV-Cacheåœ§ç¸®** | [Liu+ 2024](https://arxiv.org/abs/2410.00161) | é‡å­åŒ–ã§ãƒ¡ãƒ¢ãƒª1/4ã€å“è³ªç¶­æŒ |

---

### 6.7 ä»Šå›ã®å­¦ç¿’å†…å®¹

### 7.2 ä»Šå›ã®ç²å¾—ã‚¹ã‚­ãƒ«

**ç†è«–â†’å®Ÿè£…ã®å®Œå…¨å¯¾å¿œ**ï¼š
1. âœ… VAE ELBOå„é …ã®å°å‡º â†’ Juliaã‚³ãƒ¼ãƒ‰1:1å¯¾å¿œ
2. âœ… WGAN-GP Gradient Penalty â†’ è£œé–“ç‚¹ç”Ÿæˆãƒ»å‹¾é…è¨ˆç®—å®Ÿè£…
3. âœ… Transformer Multi-Head Attention â†’ Causal Maskãƒ»KV-Cacheå®Ÿè£…
4. âœ… Juliaè¨“ç·´ â†’ Rustæ¨è«– â†’ Elixiré…ä¿¡ã®å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
5. âœ… safetensors ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ»FFIçµ±åˆãƒ»è€éšœå®³æ€§ãƒ‡ãƒ¢

**3è¨€èªãƒã‚¹ã‚¿ãƒªãƒ¼**ï¼š
- âš¡ Juliaï¼šæ•°å¼â†”ã‚³ãƒ¼ãƒ‰1:1ã€å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã€REPLé§†å‹•é–‹ç™º
- ğŸ¦€ Rustï¼šã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ã€å‹å®‰å…¨ã€C-ABI FFIã€Candleæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³
- ğŸ”® Elixirï¼šSupervisor Treeã€GenStage/Broadwayã€ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼

**ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆæ€è€ƒ**ï¼š
- ãƒ¢ãƒ‡ãƒ«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆè¨­è¨ˆï¼ˆsafetensorså½¢å¼çµ±ä¸€ï¼‰
- FFIå¢ƒç•Œã®è²¬å‹™åˆ†é›¢ï¼ˆJulia=ãƒ¡ãƒ¢ãƒªç®¡ç†ã€Rust=è¨ˆç®—ã‚«ãƒ¼ãƒãƒ«ï¼‰
- è€éšœå®³æ€§è¨­è¨ˆï¼ˆãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã€è‡ªå‹•å†èµ·å‹•ï¼‰

---

### 7.3 ã‚ˆãã‚ã‚‹è³ªå•ï¼ˆFAQï¼‰

:::details Q1: ãªãœPythonã‚’æ¨ã¦ãŸã®ã‹ï¼Ÿ
**A**: æ¨ã¦ãŸã®ã§ã¯ãªãã€**é©æé©æ‰€**ã€‚

- **Python**ï¼šãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ãƒ»æ¢ç´¢ã«æœ€é©ã€‚ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ æœ€å¼·ã€‚
- **Julia**ï¼šè¨“ç·´ã‚³ãƒ¼ãƒ‰ã€‚æ•°å¼â†”ã‚³ãƒ¼ãƒ‰1:1ã€å‹å®‰å®šæ€§ã§è‡ªå‹•æœ€é©åŒ–ã€‚
- **Rust**ï¼šæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã€‚ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ã€å‹å®‰å…¨ã€ä¸¦åˆ—å‡¦ç†ã€‚
- **Elixir**ï¼šåˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ ã€‚è€éšœå®³æ€§ã€ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼ã€‚

ç ”ç©¶æ®µéšã§ã¯Pythonã€‚æœ¬ç•ªç’°å¢ƒã§ã¯3è¨€èªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€‚
:::

:::details Q2: Juliaã®å­¦ç¿’ã‚³ã‚¹ãƒˆã¯é«˜ããªã„ã‹ï¼Ÿ
**A**: **æ§‹æ–‡ã¯Pythonãƒ©ã‚¤ã‚¯ã€é€Ÿåº¦ã¯Cä¸¦**ã€‚å­¦ç¿’ã‚³ã‚¹ãƒˆ<ãƒªã‚¿ãƒ¼ãƒ³ã€‚

- åŸºæœ¬æ§‹æ–‡ï¼š1-2æ—¥ï¼ˆPythonãƒ¦ãƒ¼ã‚¶ãƒ¼ãªã‚‰å³åº§ï¼‰
- å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒï¼š1é€±é–“ï¼ˆæ…£ã‚Œã‚Œã°è‡ªç„¶ï¼‰
- ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸é–‹ç™ºï¼š2é€±é–“

æœ¬ã‚·ãƒªãƒ¼ã‚ºã§ã¯ç¬¬10å›ã‹ã‚‰æ®µéšçš„ã«å°å…¥æ¸ˆã¿ã€‚ä»Šå›ã§å®Œå…¨ç¿’å¾—ã€‚
:::

:::details Q3: Rustã¯é›£ã—ã™ãã§ã¯ï¼Ÿ
**A**: **æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã ã‘ãªã‚‰ä¸­ç´šãƒ¬ãƒ™ãƒ«**ã€‚

- æ‰€æœ‰æ¨©ãƒ»å€Ÿç”¨ï¼šç†è§£å¿…é ˆï¼ˆç¬¬9å›ã§å­¦ç¿’æ¸ˆã¿ï¼‰
- è¨“ç·´ã‚³ãƒ¼ãƒ‰ã¯æ›¸ã‹ãªã„ï¼ˆJuliaã«ä»»ã›ã‚‹ï¼‰
- Candle APIã¯PyTorchãƒ©ã‚¤ã‚¯

æœ¬ç•ªæ¨è«–ã®æ€§èƒ½ã¨ãƒ¡ãƒ¢ãƒªå®‰å…¨æ€§ã‚’è€ƒãˆã‚Œã°ã€å­¦ç¿’ä¾¡å€¤ã‚ã‚Šã€‚
:::

:::details Q4: Elixirãªã—ã§ã‚‚OKï¼Ÿ
**A**: å°è¦æ¨¡ãªã‚‰OKã€‚å¤§è¦æ¨¡ãƒ»é•·æ™‚é–“é‹ç”¨ãªã‚‰å¿…é ˆã€‚

- **OTPç›£è¦–ãƒ„ãƒªãƒ¼**ï¼šãƒ—ãƒ­ã‚»ã‚¹ã‚¯ãƒ©ãƒƒã‚·ãƒ¥â†’è‡ªå‹•å¾©æ—§
- **ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼**ï¼šéè² è·æ™‚ã«ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ç¶­æŒ
- **ãƒ›ãƒƒãƒˆã‚³ãƒ¼ãƒ‰ã‚¹ãƒ¯ãƒƒãƒ—**ï¼šç„¡åœæ­¢ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ

Pythonï¼ˆFastAPI/Celeryï¼‰ã§ã¯å®Ÿç¾å›°é›£ã€‚
:::

:::details Q5: 3è¨€èªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯è¤‡é›‘ã™ãã§ã¯ï¼Ÿ
**A**: åˆæœŸæŠ•è³‡ vs é•·æœŸãƒªã‚¿ãƒ¼ãƒ³ã€‚

- **åˆæœŸ**ï¼šç’°å¢ƒæ§‹ç¯‰ãƒ»FFIè¨­è¨ˆã«1-2é€±é–“
- **é‹ç”¨**ï¼šå„è¨€èªãŒæœ€é©é ˜åŸŸã‚’æ‹…å½“ â†’ ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹å®¹æ˜“
- **æ‹¡å¼µ**ï¼šæ–°ãƒ¢ãƒ‡ãƒ«è¿½åŠ ã¯Juliaè¨“ç·´â†’Rustã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã ã‘

1è¨€èªã§å…¨éƒ¨ã‚„ã‚‹æ–¹ãŒã€çµå±€ã¯è¤‡é›‘ã«ãªã‚‹ï¼ˆPython GILåœ°ç„ã€å‹å®‰å…¨æ€§æ¬ å¦‚ï¼‰ã€‚
:::

---

### 7.4 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ1é€±é–“ãƒ—ãƒ©ãƒ³ï¼‰

| æ—¥ | Zone | æ‰€è¦æ™‚é–“ | å†…å®¹ |
|:---|:-----|:---------|:-----|
| **Day 1** | Z0-Z2 | 2h | 3ãƒ¢ãƒ‡ãƒ«ä½“é¨“ã€å…¨ä½“åƒæŠŠæ¡ |
| **Day 2** | Z3.1-3.2 | 3h | VAEæ•°å¼å®Œå…¨å°å‡ºã€Juliaå®Ÿè£… |
| **Day 3** | Z3.3 | 3h | GAN/WGAN-GPå°å‡ºã€Juliaå®Ÿè£… |
| **Day 4** | Z3.4 | 3h | Transformerå°å‡ºã€Juliaå®Ÿè£… |
| **Day 5** | Z4.1-4.2 | 3h | Juliaçµ±ä¸€è¨“ç·´ã€safetensorsã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ |
| **Day 6** | Z4.3-4.4 | 3h | Rustæ¨è«–ã€Elixiré…ä¿¡å®Ÿè£… |
| **Day 7** | Z5 | 3h | å®Ÿé¨“ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ»è€éšœå®³æ€§ãƒ‡ãƒ¢ |

**åˆè¨ˆ**: 20æ™‚é–“ï¼ˆ1æ—¥3æ™‚é–“ Ã— 7æ—¥ï¼‰

---

### 7.5 è‡ªå·±è©•ä¾¡ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

**æ•°å¼ç†è§£**ï¼š
- [ ] VAE ELBOã‚’ç´™ã§å°å‡ºã§ãã‚‹
- [ ] ã‚¬ã‚¦ã‚¹KLé–‰å½¢å¼ã‚’æš—è¨˜ãªã—ã§å°å‡ºã§ãã‚‹
- [ ] WGAN-GP Gradient Penaltyã®å¿…è¦æ€§ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Multi-Head Attentionã®è¨ˆç®—æ‰‹é †ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Causal Maskã®å½¹å‰²ã‚’èª¬æ˜ã§ãã‚‹

**å®Ÿè£…ã‚¹ã‚­ãƒ«**ï¼š
- [ ] Julia VAEè¨“ç·´ãƒ«ãƒ¼ãƒ—ã‚’**ã‚¼ãƒ­ã‹ã‚‰**æ›¸ã‘ã‚‹
- [ ] Rustã§safetensorsã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€æ¨è«–ã§ãã‚‹
- [ ] Elixir Broadwayãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’è¨­è¨ˆã§ãã‚‹
- [ ] FFIå¢ƒç•Œã§ãƒã‚¤ãƒ³ã‚¿ã‚’æ­£ã—ãæ‰±ãˆã‚‹
- [ ] 3è¨€èªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒ‡ãƒãƒƒã‚°ãŒã§ãã‚‹

**ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆ**ï¼š
- [ ] è¨“ç·´â†’æ¨è«–â†’é…ä¿¡ã®å…¨ä½“ãƒ•ãƒ­ãƒ¼ã‚’å›³ç¤ºã§ãã‚‹
- [ ] å„è¨€èªã®è²¬å‹™åˆ†é›¢ã‚’èª¬æ˜ã§ãã‚‹
- [ ] è€éšœå®³æ€§è¨­è¨ˆï¼ˆSupervisor Treeï¼‰ã‚’èª¬æ˜ã§ãã‚‹
- [ ] ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼åˆ¶å¾¡ã®å¿…è¦æ€§ã‚’èª¬æ˜ã§ãã‚‹
- [ ] ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’ç‰¹å®šã§ãã‚‹

**å…¨ã¦ãƒã‚§ãƒƒã‚¯ã§ããŸã‚‰ã€æœ¬è¬›ç¾©å®Œå…¨ç¿’å¾—**ã€‚

---

### 7.6 æ¬¡å›äºˆå‘Š â€” ç¬¬21å›: ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ & HuggingFace Datasets

ç¬¬20å›ã§3ãƒ¢ãƒ‡ãƒ«ãŒå‹•ã„ãŸã€‚ã—ã‹ã—**è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å“è³ª = ãƒ¢ãƒ‡ãƒ«ã®å“è³ª**ã€‚

æ¬¡å›ã®ãƒˆãƒ”ãƒƒã‚¯ï¼š
- âš¡ Julia DataFrames.jl â€” Pandasè¶…ãˆã®é«˜é€Ÿãƒ‡ãƒ¼ã‚¿å‡¦ç†
- âš¡ HuggingFace Datasetsçµ±åˆ â€” å·¨å¤§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°èª­ã¿è¾¼ã¿
- EDAï¼ˆæ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼‰â€” åˆ†å¸ƒãƒ»å¤–ã‚Œå€¤ãƒ»ç›¸é–¢ã®å¯è¦–åŒ–
- ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µï¼ˆData Augmentationï¼‰â€” Mixup/CutMix/RandAugment
- ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å¯¾ç­– â€” SMOTE/Focal Loss/Class Weighting
- âš¡ğŸ¦€ Julia+Rustä¸¦åˆ—å‰å‡¦ç† â€” 1å„„ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’10åˆ†ã§å‡¦ç†

**æ¥ç¶š**ï¼š
- ç¬¬20å›ï¼šãƒ¢ãƒ‡ãƒ«ã¯å‹•ã
- ç¬¬21å›ï¼šãƒ‡ãƒ¼ã‚¿ã‚’ç£¨ã
- ç¬¬22å›ï¼šè©•ä¾¡æŒ‡æ¨™ã§å“è³ªæ¸¬å®š

**äºˆç¿’**ï¼š
- HuggingFace Datasetsãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆé–²è¦§
- Julia DataFrames.jlãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ï¼ˆåŸºç¤ã®ã¿ï¼‰
- ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å•é¡Œã®äº‹ä¾‹ã‚’1ã¤èª¿ã¹ã‚‹

---

:::message
**é€²æ—**: å…¨ä½“ã®100%å®Œäº†ã€‚Course III ç¬¬20å›å®Œå…¨ä¿®äº†ã€‚
:::

---

### 6.12 ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

**å•ã„**: ã€Œå‹•ãã‚³ãƒ¼ãƒ‰ã€ã¨ã€Œç†è§£ã—ãŸã‚³ãƒ¼ãƒ‰ã€ã®å¢ƒç•Œç·šã¯ã©ã“ã«ã‚ã‚‹ã®ã‹ï¼Ÿ

### è­°è«–ã®ãƒ’ãƒ³ãƒˆ

1. **å†™çµŒã®ç½ **ï¼š
   - GitHubã‹ã‚‰ã‚³ãƒ¼ãƒ‰ã‚’ã‚³ãƒ”ãƒš â†’ å‹•ã â†’ ã€Œç†è§£ã—ãŸã€ã¨éŒ¯è¦š
   - ãƒ‡ãƒãƒƒã‚°æ™‚ã«è©°ã‚€ï¼šãªãœã“ã®æå¤±é–¢æ•°ï¼Ÿãªãœã“ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼Ÿ
   - **çœŸã®ç†è§£**ï¼šæ•°å¼â†’ã‚³ãƒ¼ãƒ‰ã®å„è¡Œã‚’å¯¾å¿œä»˜ã‘ã‚‰ã‚Œã‚‹ + ç´™ã§å°å‡ºã§ãã‚‹

2. **æŠ½è±¡åŒ–ãƒ¬ãƒ™ãƒ«**ï¼š
   - é«˜ãƒ¬ãƒ™ãƒ«APIï¼ˆ`model.fit()`ï¼‰ï¼šé€Ÿã„ãŒã€ä¸­èº«ãŒãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹
   - ä½ãƒ¬ãƒ™ãƒ«å®Ÿè£…ï¼ˆæå¤±è¨ˆç®—ã‹ã‚‰æ›¸ãï¼‰ï¼šé…ã„ãŒã€å®Œå…¨åˆ¶å¾¡
   - **æœ¬è¬›ç¾©ã®ç«‹å ´**ï¼šä¸­ãƒ¬ãƒ™ãƒ« â€” æ•°å¼ã¯å®Œå…¨ç†è§£ã€ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯è³¢ãä½¿ã†

3. **LLMæ™‚ä»£ã®ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°**ï¼š
   - ChatGPT/CopilotãŒã‚³ãƒ¼ãƒ‰ç”Ÿæˆ â†’ äººé–“ã®å½¹å‰²ã¯ï¼Ÿ
   - **ä»®èª¬**ï¼šã‚³ãƒ¼ãƒ‰ã®**æ„å›³**ã‚’ç†è§£ã—ã€**ãƒã‚°ã‚’æ¤œå‡º**ã—ã€**ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆ**ã™ã‚‹ã®ãŒäººé–“ã®ä»•äº‹
   - æ•°å¼ç†è§£ãŒãªã„ã¨ã€AIãŒç”Ÿæˆã—ãŸã‚³ãƒ¼ãƒ‰ã®æ­£ã—ã•ã‚’åˆ¤å®šã§ããªã„

4. **å®Ÿè£…ã‚¹ã‚­ãƒ«ã®æŒç¶šå¯èƒ½æ€§**ï¼š
   - PyTorchã®APIã¯5å¹´ã§é™³è…åŒ–
   - æ•°å¼ã®ç†è«–ã¯50å¹´å¤‰ã‚ã‚‰ãªã„ï¼ˆELBO/Wasserstein/Attentionï¼‰
   - **æŠ•è³‡å¯¾åŠ¹æœ**ï¼šãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«ä¾å­˜ã—ãªã„ç†è§£ > ç‰¹å®šãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ç¿’ç†Ÿ

### ã‚ãªãŸã®ç­”ãˆã¯ï¼Ÿ

:::details æ­´å²çš„è¦–ç‚¹
1980å¹´ä»£ï¼šã‚¢ã‚»ãƒ³ãƒ–ãƒªãŒæ›¸ã‘ãªã„ã¨ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ¼ã˜ã‚ƒãªã„
1990å¹´ä»£ï¼šCãŒæ›¸ã‘ãªã„ã¨ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã˜ã‚ƒãªã„
2000å¹´ä»£ï¼šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæŒ‡å‘ãŒç†è§£ã§ããªã„ã¨è¨­è¨ˆè€…ã˜ã‚ƒãªã„
2010å¹´ä»£ï¼šæ©Ÿæ¢°å­¦ç¿’ã®æ•°å­¦ãŒç†è§£ã§ããªã„ã¨MLã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã˜ã‚ƒãªã„
2020å¹´ä»£ï¼šLLMãŒã‚³ãƒ¼ãƒ‰ç”Ÿæˆã™ã‚‹æ™‚ä»£ã€äººé–“ã®ä¾¡å€¤ã¯ï¼Ÿ

**ç­”ãˆã¯ãªã„ã€‚å„è‡ªãŒè€ƒãˆç¶šã‘ã‚‹ã¹ãå•ã„ã€‚**
:::

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^2]: Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., & Courville, A. (2017). Improved Training of Wasserstein GANs. *NeurIPS 2017*.
@[card](https://arxiv.org/abs/1704.00028)

### æ•™ç§‘æ›¸

- Murphy, K. P. (2023). *Probabilistic Machine Learning: Advanced Topics*. MIT Press. [Free PDF](https://probml.github.io/pml-book/book2.html)
- Foster, D. (2023). *Generative Deep Learning* (2nd ed). O'Reilly.
- Tomczak, J. M. (2022). *Deep Generative Modeling*. Springer.

### ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

- **Lux.jl**: [lux.csail.mit.edu](https://lux.csail.mit.edu/)
- **Candle (Rust)**: [GitHub](https://github.com/huggingface/candle)
- **Broadway (Elixir)**: [elixir-broadway.org](https://elixir-broadway.org/)
- **Reactant.jl**: [GitHub](https://github.com/EnzymeAD/Reactant.jl)

---

## è¨˜æ³•è¦ç´„

| è¨˜å· | æ„å‘³ | ä¾‹ |
|:-----|:-----|:---|
| $\mathbf{x}$ | ãƒ‡ãƒ¼ã‚¿ï¼ˆè¦³æ¸¬å¤‰æ•°ï¼‰ | ç”»åƒãƒ»ãƒ†ã‚­ã‚¹ãƒˆ |
| $\mathbf{z}$ | æ½œåœ¨å¤‰æ•° | VAEã®æ½œåœ¨ç©ºé–“ |
| $\theta$ | ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | Decoderã®é‡ã¿ |
| $\phi$ | æ¨è«–ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | Encoderã®é‡ã¿ |
| $p_\theta(\mathbf{x})$ | ç”Ÿæˆåˆ†å¸ƒï¼ˆçœŸã®åˆ†å¸ƒã‚’è¿‘ä¼¼ï¼‰ | VAE Decoder |
| $q_\phi(\mathbf{z}\|\mathbf{x})$ | è¿‘ä¼¼äº‹å¾Œåˆ†å¸ƒ | VAE Encoder |
| $p(\mathbf{z})$ | äº‹å‰åˆ†å¸ƒ | $\mathcal{N}(\mathbf{0}, \mathbf{I})$ |
| $\mathcal{L}_{\text{ELBO}}$ | Evidence Lower Bound | VAEæå¤±é–¢æ•° |
| $D_{\text{KL}}[q \| p]$ | KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ | åˆ†å¸ƒé–“ã®è·é›¢ |
| $W_1(p, q)$ | Wasserstein-1è·é›¢ | WGANæå¤± |
| $\nabla_\theta$ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‹¾é… | é€†ä¼æ’­ |
| $\mathbb{E}_{q}[\cdot]$ | æœŸå¾…å€¤ï¼ˆåˆ†å¸ƒ $q$ ã«é–¢ã™ã‚‹ï¼‰ | Monte Carloè¿‘ä¼¼ |
| $Q, K, V$ | Query/Key/Valueè¡Œåˆ— | Attention |
| $d_k$ | Keyæ¬¡å…ƒ | Attentionã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° |
| $h$ | ãƒ˜ãƒƒãƒ‰æ•° | Multi-Head Attention |
| âš¡ | Julia | è¨“ç·´ã‚³ãƒ¼ãƒ‰ |
| ğŸ¦€ | Rust | æ¨è«–ã‚³ãƒ¼ãƒ‰ |
| ğŸ”® | Elixir | é…ä¿¡ã‚³ãƒ¼ãƒ‰ |

---

**æœ¬è¬›ç¾©ã®åŸ·ç­†å®Œäº†**ã€‚è¡Œæ•°ç¢ºèªã¸ã€‚

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚

